{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar que los modelos caben en memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel\n",
    "\n",
    "try:\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_model.eval()\n",
    "    clip_model = torch.quantization.quantize_dynamic(clip_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "    clip_model.cuda()\n",
    "    print(\"CLIP model loaded successfully.\")\n",
    "except RuntimeError as e:\n",
    "    print(\"Error loading CLIP model:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5ec748a6244de6847f3bdbce4223d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Mistral model: CUDA out of memory. Tried to allocate 500.00 MiB. GPU \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "try:\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", use_auth_token='tu_token_hf')\n",
    "    mistral_model.cuda()\n",
    "    print(\"Mistral model loaded successfully.\")\n",
    "except RuntimeError as e:\n",
    "    print(\"Error loading Mistral model:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENFOQUE ENCODER DECODER LLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generar y almacenar embeddings de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2024-06-10 22:13:27.519366: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-10 22:13:27.614689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 22:13:28.766528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Definición del dataset para generar embeddings\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_dict, img_dir, processor):\n",
    "        self.data_dict = data_dict\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.transforms = Compose([\n",
    "            Resize((224, 224)),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        img_filename = tweet_id + \".jpg\"\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "        return image, tweet_id\n",
    "\n",
    "def generate_and_save_embeddings(data_dict, img_dir, processor, model, batch_size=8, output_file='image_embeddings.pkl'):\n",
    "    dataset = ImageDataset(data_dict, img_dir, processor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = {}\n",
    "    with torch.no_grad():\n",
    "        for images, tweet_ids in dataloader:\n",
    "            outputs = model.get_image_features(images)\n",
    "            outputs = outputs.numpy()\n",
    "            for tweet_id, embedding in zip(tweet_ids, outputs):\n",
    "                embeddings[tweet_id] = embedding\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "def main_generate_embeddings():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "    with open(f'{base_path}/splits/test_ids.txt', 'r') as f:\n",
    "        id_test = f.read().split()\n",
    "\n",
    "    dict_data = {x: data[x] for x in id_train + id_val + id_test if x in data}\n",
    "\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    clip_model.eval()\n",
    "    clip_model = torch.quantization.quantize_dynamic(clip_model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "\n",
    "    generate_and_save_embeddings(dict_data, base_path + 'img_resized', processor, clip_model, batch_size=32, output_file='image_embeddings.pkl')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_generate_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Entrenar decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e568b5320b1499cbc9c1207c52d1478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jtorwjgz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "wandb: ERROR Error while calling W&B API: run jtorwjgz was previously created and deleted; try a new run name (<Response [409]>)\n",
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/apis/normalize.py\", line 41, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py\", line 2188, in upsert_run\n",
      "    response = self.gql(\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py\", line 312, in gql\n",
      "    ret = self._retry_gql(\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/lib/retry.py\", line 131, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py\", line 340, in execute\n",
      "    return self.client.execute(*args, **kwargs)  # type: ignore\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/lib/gql_request.py\", line 59, in execute\n",
      "    request.raise_for_status()\n",
      "  File \"/usr/lib/python3/dist-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://api.wandb.ai/graphql\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 99, in _run\n",
      "    self._process(record)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 327, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 386, in send\n",
      "    send_handler(record)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 408, in send_request\n",
      "    send_handler(record)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 636, in send_request_defer\n",
      "    self.debounce(final=True)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 537, in debounce\n",
      "    self._maybe_update_config(always=final)\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 514, in _maybe_update_config\n",
      "    self._debounce_config()\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 543, in _debounce_config\n",
      "    self._api.upsert_run(\n",
      "  File \"/home/javiermo/.local/lib/python3.10/site-packages/wandb/apis/normalize.py\", line 51, in wrapper\n",
      "    raise CommError(message, error)\n",
      "wandb.errors.CommError: run jtorwjgz was previously created and deleted; try a new run name (Error 409: Conflict)\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMailboxError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2358\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ki:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2623\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;66;03m# this message is confusing, we should remove it\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;66;03m# self._footer_exit_status_info(\u001b[39;00m\n\u001b[1;32m   2619\u001b[0m \u001b[38;5;66;03m#     self._exit_code, settings=self._settings, printer=self._printer\u001b[39;00m\n\u001b[1;32m   2620\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m   2621\u001b[0m \n\u001b[1;32m   2622\u001b[0m \u001b[38;5;66;03m# wait for the exit to complete\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mexit_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_exit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2625\u001b[0m poll_exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_poll_exit()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:298\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_probe \u001b[38;5;129;01mand\u001b[39;00m probe_handle:\n\u001b[0;32m--> 298\u001b[0m     \u001b[43mon_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobe_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2578\u001b[0m, in \u001b[0;36mRun._on_probe_exit\u001b[0;34m(self, probe_handle)\u001b[0m\n\u001b[1;32m   2577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle:\n\u001b[0;32m-> 2578\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelease\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:281\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slot\u001b[38;5;241m.\u001b[39m_get_and_clear(timeout\u001b[38;5;241m=\u001b[39mwait_timeout)\n",
      "\u001b[0;31mMailboxError\u001b[0m: transport failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[43mmain_train_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 126\u001b[0m, in \u001b[0;36mmain_train_decoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Inicializar wandb antes del entrenamiento\u001b[39;00m\n\u001b[1;32m    125\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m--> 126\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFine-tune Llama 3 8B on Image Embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    133\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3-8b-chat-doctor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m    152\u001b[0m     model\u001b[38;5;241m=\u001b[39mllama_model,\n\u001b[1;32m    153\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1185\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1185\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1171\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[1;32m   1170\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> 1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:595\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39msilent:\n\u001b[1;32m    591\u001b[0m     ipython\u001b[38;5;241m.\u001b[39mdisplay_html(\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinishing last run (ID:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_run\u001b[38;5;241m.\u001b[39m_run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) before initializing another...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    593\u001b[0m     )\n\u001b[0;32m--> 595\u001b[0m \u001b[43mlatest_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39msilent:\n\u001b[1;32m    598\u001b[0m     ipython\u001b[38;5;241m.\u001b[39mdisplay_html(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully finished last run (ID:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_run\u001b[38;5;241m.\u001b[39m_run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Initializing new run:<br/>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:449\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:390\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2100\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_noop\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_attach\n\u001b[1;32m   2088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish\u001b[39m(\n\u001b[1;32m   2089\u001b[0m     \u001b[38;5;28mself\u001b[39m, exit_code: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, quiet: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2090\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \n\u001b[1;32m   2093\u001b[0m \u001b[38;5;124;03m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;124;03m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2115\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m==\u001b[39m TeardownStage\u001b[38;5;241m.\u001b[39mEARLY:\n\u001b[1;32m   2113\u001b[0m         hook\u001b[38;5;241m.\u001b[39mcall()\n\u001b[0;32m-> 2115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\u001b[38;5;241m.\u001b[39m_global_run_stack) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\u001b[38;5;241m.\u001b[39m_global_run_stack\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2369\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     report_failure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_console_stop()\n\u001b[0;32m-> 2369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2370\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem finishing run\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   2371\u001b[0m wandb\u001b[38;5;241m.\u001b[39mtermerror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem finishing run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/backend/backend.py:232\u001b[0m, in \u001b[0;36mBackend.cleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb_process:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb_process\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:547\u001b[0m, in \u001b[0;36mInterfaceShared.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_router:\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_router\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:802\u001b[0m, in \u001b[0;36mInterfaceBase.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 802\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate_shutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:444\u001b[0m, in \u001b[0;36mInterfaceShared._communicate_shutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m request \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mRequest(shutdown\u001b[38;5;241m=\u001b[39mpb\u001b[38;5;241m.\u001b[39mShutdownRequest())\n\u001b[1;32m    443\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_record(request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 444\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:304\u001b[0m, in \u001b[0;36mInterfaceShared._communicate\u001b[0;34m(self, rec, timeout, local)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_communicate\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m, rec: pb\u001b[38;5;241m.\u001b[39mRecord, timeout: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[pb\u001b[38;5;241m.\u001b[39mResult]:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:60\u001b[0m, in \u001b[0;36mInterfaceSock._communicate_async\u001b[0;34m(self, rec, local)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_router\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_and_receive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/router.py:94\u001b[0m, in \u001b[0;36mMessageRouter.send_and_receive\u001b[0;34m(self, rec, local)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pending_reqs[rec\u001b[38;5;241m.\u001b[39muuid] \u001b[38;5;241m=\u001b[39m future\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/interface/router_sock.py:36\u001b[0m, in \u001b[0;36mMessageSockRouter._send_message\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:216\u001b[0m, in \u001b[0;36mSockClient.send_record_communicate\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    214\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[1;32m    215\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_communicate\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    150\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import DatasetDict\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "import torch.cuda.amp as amp\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "from accelerate.utils import set_module_tensor_to_device\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "    with open(f'{base_path}/splits/test_ids.txt', 'r') as f:\n",
    "        id_test = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "    dict_test = {x: data[x] for x in id_test if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = TextDataset(dict_val, embeddings, tokenizer)\n",
    "    test_dataset = TextDataset(dict_test, embeddings, tokenizer)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": val_dataset,\n",
    "    })\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    wandb.login(key=api_key)\n",
    "    run = wandb.init(\n",
    "        project='Fine-tune Llama 3 8B on Image Embeddings', \n",
    "        job_type=\"training\", \n",
    "        anonymous=\"allow\"\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train_decoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso ya ejecuta, pero 15 horas para 1 epoch. Intentaré acelerar aumentando batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2024-06-11 20:09:50.017037: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-11 20:09:50.077641: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 20:09:51.037780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f58aa9521f443fe914f67f93e377659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240611_201004-iwdwvsci</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/iwdwvsci' target=\"_blank\">run-1718136604</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/iwdwvsci' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/iwdwvsci</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1979: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2106' max='2106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2106/2106 4:49:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2.908000</td>\n",
       "      <td>3.021783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>2.979900</td>\n",
       "      <td>2.948987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1266</td>\n",
       "      <td>2.654100</td>\n",
       "      <td>2.903344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>2.632000</td>\n",
       "      <td>2.876436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988e72ab45f24e688b8fd826d32b9bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▁</td></tr><tr><td>eval/runtime</td><td>▁▁█▆</td></tr><tr><td>eval/samples_per_second</td><td>██▁▃</td></tr><tr><td>eval/steps_per_second</td><td>██▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▃▂▃▃▂▃▂▃▃▃▅▄▂▃▃▂▃▁▁▃▂▃▃▂▃▃▃▂▃▃▃▂▂▂▃▂▂▂</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▇▅▆▆▅▅▆▅▆▅▅▆▇▅▅▄▃▁▇▅▆▄▄▄▄▄▅▆▅▃▄▄▃▂▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.87644</td></tr><tr><td>eval/runtime</td><td>270.5164</td></tr><tr><td>eval/samples_per_second</td><td>18.483</td></tr><tr><td>eval/steps_per_second</td><td>2.31</td></tr><tr><td>total_flos</td><td>7.812073095891517e+17</td></tr><tr><td>train/epoch</td><td>0.9997</td></tr><tr><td>train/global_step</td><td>2106</td></tr><tr><td>train/grad_norm</td><td>1.26374</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.8261</td></tr><tr><td>train_loss</td><td>2.93873</td></tr><tr><td>train_runtime</td><td>17379.7489</td></tr><tr><td>train_samples_per_second</td><td>7.757</td></tr><tr><td>train_steps_per_second</td><td>0.121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run-1718136604</strong> at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/iwdwvsci' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/iwdwvsci</a><br/> View project at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240611_201004-iwdwvsci/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import DatasetDict\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "    with open(f'{base_path}/splits/test_ids.txt', 'r') as f:\n",
    "        id_test = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "    dict_test = {x: data[x] for x in id_test if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = TextDataset(dict_val, embeddings, tokenizer)\n",
    "    test_dataset = TextDataset(dict_test, embeddings, tokenizer)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": val_dataset,\n",
    "    })\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    try:\n",
    "        wandb.login(key=api_key)\n",
    "        run = wandb.init(\n",
    "            project='Fine-tune Llama 3 8B on Memes Embeddings', \n",
    "            job_type=\"training\", \n",
    "            anonymous=\"allow\",\n",
    "            name=f\"run-{int(time.time())}\"  # Nombre único basado en el timestamp actual\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing wandb: {e}\")\n",
    "        return\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=8,  # Incrementar tamaño del lote a 8\n",
    "        per_device_eval_batch_size=8,  # Incrementar tamaño del lote a 8\n",
    "        gradient_accumulation_steps=8,  # Aumentar acumulación de gradientes\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main_train_decoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2024-06-13 00:26:26.696865: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-13 00:26:26.758960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 00:26:27.663519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2794086bb77e4dffb51795df0b3b49bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240613_002641-8pzxmwyh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/8pzxmwyh' target=\"_blank\">run-1718238401</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/8pzxmwyh' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/8pzxmwyh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1979: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='1578' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  95/1578 38:26 < 10:13:06, 0.04 it/s, Epoch 0.18/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import DatasetDict\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "    with open(f'{base_path}/splits/test_ids.txt', 'r') as f:\n",
    "        id_test = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "    dict_test = {x: data[x] for x in id_test if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = TextDataset(dict_val, embeddings, tokenizer)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": val_dataset,\n",
    "    })\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    try:\n",
    "        wandb.login(key=api_key)\n",
    "        run = wandb.init(\n",
    "            project='Fine-tune Llama 3 8B on Memes Embeddings', \n",
    "            job_type=\"training\", \n",
    "            anonymous=\"allow\",\n",
    "            name=f\"run-{int(time.time())}\"  # Nombre único basado en el timestamp actual\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing wandb: {e}\")\n",
    "        return\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=16,  # Reducir tamaño del lote a 1\n",
    "        per_device_eval_batch_size=16,  # Reducir tamaño del lote a 1\n",
    "        gradient_accumulation_steps=16,  # Ajustar acumulación de gradientes\n",
    "        optim=\"adamw_hf\",\n",
    "        num_train_epochs=3,  # Aumentar el número de épocas\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        logging_steps=10,\n",
    "        warmup_steps=50,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "    finally:\n",
    "        trainer.save_model(output_dir=\"llama-3-8b-meme-poster\")  # Guardar el modelo localmente\n",
    "        wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.set_device(1)  # Seleccionar GPU 1\n",
    "    main_train_decoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sospecho que hay algo mal en evaluación. Vo ya agregar unos cuantos prints y a revisar la función de eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 18:37:57.125271: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-13 18:37:57.194657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 18:37:58.236983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1bf17cc1a14c7e80539a2a3f1bff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240613_183810-mpit84z3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/mpit84z3' target=\"_blank\">run-1718303890</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/mpit84z3' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Memes%20Embeddings/runs/mpit84z3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1979: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='50559' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  101/50559 02:50 < 24:09:24, 0.58 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/625 00:15 < 07:32, 1.33 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import DatasetDict\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]]) # endofsentencetokenid ?? cat al revés?? padding n la derecha\n",
    "        ''' labels e inputids están al revés. labels te dice que hay padding a la derecha '''\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "    with open(f'{base_path}/splits/test_ids.txt', 'r') as f:\n",
    "        id_test = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "    dict_test = {x: data[x] for x in id_test if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = TextDataset(dict_val, embeddings, tokenizer)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": val_dataset,\n",
    "    })\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    try:\n",
    "        wandb.login(key=api_key)\n",
    "        run = wandb.init(\n",
    "            project='Fine-tune Llama 3 8B on Memes Embeddings', \n",
    "            job_type=\"training\", \n",
    "            anonymous=\"allow\",\n",
    "            name=f\"run-{int(time.time())}\"  # Nombre único basado en el timestamp actual\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing wandb: {e}\")\n",
    "        return\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=8,  # Reducir tamaño del lote a 16\n",
    "        per_device_eval_batch_size=8,  # Reducir tamaño del lote a 16\n",
    "        gradient_accumulation_steps=4,  # Ajustar acumulación de gradientes\n",
    "        optim=\"adamw_hf\",\n",
    "        num_train_epochs=3,  # Aumentar el número de épocas\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        logging_steps=10,\n",
    "        warmup_steps=50,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    def print_sample_predictions(trainer, dataset, tokenizer, num_samples=5):\n",
    "        model = trainer.model\n",
    "        model.eval()\n",
    "        for i in range(num_samples):\n",
    "            sample = dataset[i]\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(trainer.args.device)\n",
    "            attention_mask = sample['attention_mask'].unsqueeze(0).to(trainer.args.device)\n",
    "            embedding = sample['embedding'].unsqueeze(0).to(trainer.args.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                '''MISMO PROBLEMA. INPUT IDS NO. ATTENTION MASK NO. inputs_embeds en vez de embeddings'''\n",
    "                generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, embeddings=embedding, max_length=128)\n",
    "                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            print(f\"Original text: {sample['tweet_text']}\")\n",
    "            print(f\"Generated text: {generated_text}\\n\")\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print_sample_predictions(trainer, train_dataset, tokenizer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "    finally:\n",
    "        trainer.save_model(output_dir=\"llama-3-8b-meme-poster\")  # Guardar el modelo localmente\n",
    "        wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #torch.cuda.set_device(1)  # Seleccionar GPU 1\n",
    "    main_train_decoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔴Nuevo código corrigiendo generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 13:55:54.700346: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-19 13:55:54.773521: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 13:55:55.877101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f91191d5c0d4264b7048625089a437b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240619_135607-zv446alp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/zv446alp' target=\"_blank\">silver-galaxy-31</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/zv446alp' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/zv446alp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1979: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import DatasetDict\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        embeddings = batch[\"embedding\"].to(device)\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(embeddings, max_length=tokenizer.model_max_length)\n",
    "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "\n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "    with open(f'{base_path}/splits/test_ids.txt', 'r') as f:\n",
    "        id_test = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "    dict_test = {x: data[x] for x in id_test if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = ValidationDataset(dict_val, embeddings, tokenizer)\n",
    "    test_dataset = ValidationDataset(dict_test, embeddings, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    wandb.login(key=api_key)\n",
    "    run = wandb.init(\n",
    "        project='Fine-tune Llama 3 8B on Image Embeddings', \n",
    "        job_type=\"training\", \n",
    "        anonymous=\"allow\"\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"no\",  # Disable automatic evaluation during training\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"    \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,  # Disable automatic evaluation\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Custom validation loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    llama_model.to(device)\n",
    "    predictions = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, print_every_n_steps=10)\n",
    "    print(predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.set_device(2)\n",
    "    main_train_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171941a8673e4e25819c58a703a2fb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/integrations/peft.py:399: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved locally at finetuned-llamas\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def save_model_locally(output_dir):\n",
    "    # Load the trained model and tokenizer\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\"llama-3-8b-meme-poster\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"llama-3-8b-meme-poster\")\n",
    "\n",
    "    # Save the model locally\n",
    "    llama_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model saved locally at {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the output directory\n",
    "    output_dir = \"finetuned-llamas\"\n",
    "\n",
    "    # Save the model locally\n",
    "    save_model_locally(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad85ab348f7948d1a05cac1149eada28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU \u0001 has a total capacity of 23.69 GiB of which 70.00 MiB is free. Process 398686 has 6.18 GiB memory in use. Process 778695 has 776.00 MiB memory in use. Including non-PyTorch memory, this process has 13.57 GiB memory in use. Process 1335198 has 3.10 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 106.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Load the model with empty weights initially\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m init_empty_weights():\n\u001b[0;32m---> 86\u001b[0m     llama_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Apply LoRA configuration\u001b[39;00m\n\u001b[1;32m     89\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     90\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     91\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgate_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo_proj\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     96\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3757\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3748\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3750\u001b[0m     (\n\u001b[1;32m   3751\u001b[0m         model,\n\u001b[1;32m   3752\u001b[0m         missing_keys,\n\u001b[1;32m   3753\u001b[0m         unexpected_keys,\n\u001b[1;32m   3754\u001b[0m         mismatched_keys,\n\u001b[1;32m   3755\u001b[0m         offload_index,\n\u001b[1;32m   3756\u001b[0m         error_msgs,\n\u001b[0;32m-> 3757\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3765\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3768\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3769\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3774\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3777\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4217\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4213\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4214\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4215\u001b[0m                 )\n\u001b[1;32m   4216\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4217\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4221\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4222\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4223\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4224\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4225\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4229\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4234\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:887\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    876\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m ):\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py:400\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    398\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 400\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU \u0001 has a total capacity of 23.69 GiB of which 70.00 MiB is free. Process 398686 has 6.18 GiB memory in use. Process 778695 has 776.00 MiB memory in use. Including non-PyTorch memory, this process has 13.57 GiB memory in use. Process 1335198 has 3.10 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 106.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pickle\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from safetensors import safe_open\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        embeddings = batch[\"embedding\"].to(device)\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs_embeds=embeddings, max_length=tokenizer.model_max_length)\n",
    "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "\n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    torch.cuda.set_device(1)\n",
    "    \n",
    "    # Paths\n",
    "    data_file = './MMHS150K_GT.json'\n",
    "    val_ids_file = './splits/val_ids.txt'\n",
    "    embeddings_file = 'image_embeddings.pkl'\n",
    "    model_dir = 'finetuned-llamas'\n",
    "\n",
    "    # Load data and embeddings\n",
    "    with open(data_file, 'r') as f:\n",
    "        data_dict = json.load(f)\n",
    "    with open(val_ids_file, 'r') as f:\n",
    "        val_ids = f.read().split()\n",
    "    data_dict = {x: data_dict[x] for x in val_ids if x in data_dict}\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    # Load tokenizer and model configuration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    # Load the model with empty weights initially\n",
    "    with init_empty_weights():\n",
    "        llama_model = AutoModelForCausalLM.from_pretrained(model_dir, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "\n",
    "    # Apply LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    \n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    # Load LoRA weights manually\n",
    "    with safe_open(model_dir, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for k in f.keys():\n",
    "            weight = f.get_tensor(k)\n",
    "            llama_model.state_dict()[k].copy_(weight)\n",
    "\n",
    "    # Dispatch the model to device with CPU offload\n",
    "    llama_model = load_checkpoint_and_dispatch(\n",
    "        llama_model, \n",
    "        model_dir,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        offload_folder=\"offload\",\n",
    "        offload_state_dict=True,\n",
    "        dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Prepare validation dataset and dataloader\n",
    "    val_dataset = ValidationDataset(data_dict, embeddings, tokenizer)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Custom validation loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    llama_model.to(device)\n",
    "    predictions = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, print_every_n_steps=10)\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir cálculo de métricas (BLEU, Perplexity, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c172f61376e4d06bb0c4f840581b451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Load the model with empty weights initially\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m init_empty_weights():\n\u001b[0;32m--> 113\u001b[0m     llama_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Apply LoRA configuration\u001b[39;00m\n\u001b[1;32m    116\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m    117\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m    118\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgate_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo_proj\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    123\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3830\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[1;32m   3829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3830\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3831\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3832\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3834\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   3838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/integrations/peft.py:195\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    189\u001b[0m         peft_model_id,\n\u001b[1;32m    190\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Create and add fresh new adapters into the model.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[43minject_adapter_in_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/mapping.py:202\u001b[0m, in \u001b[0;36minject_adapter_in_model\u001b[0;34m(peft_config, model, adapter_name)\u001b[0m\n\u001b[1;32m    199\u001b[0m tuner_cls \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_TUNER_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# By instantiating a peft model we are injecting randomly initialized LoRA layers into the model's modules.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mtuner_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m peft_model\u001b[38;5;241m.\u001b[39mmodel\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/model.py:139\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:166\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:412\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype)\u001b[0m\n\u001b[1;32m    410\u001b[0m     is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/model.py:223\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[1;32m    213\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    214\u001b[0m         adapter_name,\n\u001b[1;32m    215\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         use_dora\u001b[38;5;241m=\u001b[39mlora_config\u001b[38;5;241m.\u001b[39muse_dora,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/model.py:317\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dispatcher \u001b[38;5;129;01min\u001b[39;00m dispatchers:\n\u001b[0;32m--> 317\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# first match wins\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1102\u001b[0m, in \u001b[0;36mdispatch_default\u001b[0;34m(target, adapter_name, lora_config, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfan_in_fan_out\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lora_config\u001b[38;5;241m.\u001b[39mfan_in_fan_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(lora_config\u001b[38;5;241m.\u001b[39mloftq_config)\n\u001b[0;32m-> 1102\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_base_layer, Conv1D):\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfan_in_fan_out\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:403\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, base_layer, adapter_name, r, lora_alpha, lora_dropout, fan_in_fan_out, is_target_conv_1d_layer, init_lora_weights, use_rslora, use_dora, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfan_in_fan_out \u001b[38;5;241m=\u001b[39m fan_in_fan_out\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_lora_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_lora_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_rslora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_rslora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_target_conv_1d_layer \u001b[38;5;241m=\u001b[39m is_target_conv_1d_layer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:124\u001b[0m, in \u001b[0;36mLoraLayer.update_layer\u001b[0;34m(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_lora_parameters(adapter_name, init_lora_weights)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# call this before dora_init\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_adapter_to_device_of_base_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_dora:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdora_init(adapter_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:700\u001b[0m, in \u001b[0;36mBaseTunerLayer._move_adapter_to_device_of_base_layer\u001b[0;34m(self, adapter_name, device)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point \u001b[38;5;129;01mor\u001b[39;00m weight\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[0;32m--> 700\u001b[0m     adapter_layer[adapter_name] \u001b[38;5;241m=\u001b[39m \u001b[43madapter_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     adapter_layer[adapter_name] \u001b[38;5;241m=\u001b[39m adapter_layer[adapter_name]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1166\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1167\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1169\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from safetensors import safe_open\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, criterion, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        embeddings = batch[\"embedding\"].to(device)\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        inputs = tokenizer(tweet_texts, return_tensors='pt', padding=True, truncation=True, max_length=tokenizer.model_max_length).to(device)\n",
    "        labels = inputs.input_ids\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs_embeds=embeddings, max_length=tokenizer.model_max_length)\n",
    "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            model_outputs = model(**inputs)\n",
    "            logits = model_outputs.logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_tokens += labels.size(0)\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "\n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print()\n",
    "\n",
    "    average_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "    return predictions, average_loss, perplexity\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "def calculate_bleu(predictions):\n",
    "    bleu_scores = []\n",
    "    for original, generated in predictions:\n",
    "        reference = original.split()  # Reference (ground truth)\n",
    "        candidate = generated.split()  # Generated text\n",
    "        bleu_score = sentence_bleu([reference], candidate)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    return sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    torch.cuda.set_device(1)\n",
    "    \n",
    "    # Paths\n",
    "    data_file = './MMHS150K_GT.json'\n",
    "    val_ids_file = './splits/val_ids.txt'\n",
    "    embeddings_file = 'image_embeddings.pkl'\n",
    "    model_dir = 'finetuned-llamas'\n",
    "    \n",
    "    num_epochs = 3  # Number of epochs\n",
    "\n",
    "    # Load data and embeddings\n",
    "    with open(data_file, 'r') as f:\n",
    "        data_dict = json.load(f)\n",
    "    with open(val_ids_file, 'r') as f:\n",
    "        val_ids = f.read().split()\n",
    "    data_dict = {x: data_dict[x] for x in val_ids if x in data_dict}\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    # Load tokenizer and model configuration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    # Load the model directly\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(model_dir, low_cpu_mem_usage=True)\n",
    "\n",
    "    # Apply LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    \n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    # Load LoRA weights manually\n",
    "    with safe_open(model_dir, framework=\"pt\", device=\"cpu\") as f:\n",
    "        for k in f.keys():\n",
    "            weight = f.get_tensor(k)\n",
    "            llama_model.state_dict()[k].copy_(weight)\n",
    "\n",
    "    # Dispatch the model to device with CPU offload\n",
    "    llama_model = load_checkpoint_and_dispatch(\n",
    "        llama_model, \n",
    "        model_dir,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        offload_folder=\"offload\",\n",
    "        offload_state_dict=True,\n",
    "        dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Prepare training and validation datasets and dataloaders\n",
    "    train_dataset = ValidationDataset(data_dict, embeddings, tokenizer)\n",
    "    val_dataset = ValidationDataset(data_dict, embeddings, tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Training and validation loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    llama_model.to(device)\n",
    "\n",
    "    optimizer = Adam(llama_model.parameters(), lr=5e-5)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Training loop\n",
    "        llama_model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_tokens = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            embeddings = batch[\"embedding\"].to(device)\n",
    "            tweet_texts = batch[\"tweet_text\"]\n",
    "            inputs = tokenizer(tweet_texts, return_tensors='pt', padding=True, truncation=True, max_length=tokenizer.model_max_length).to(device)\n",
    "            labels = inputs.input_ids\n",
    "            optimizer.zero_grad()\n",
    "            outputs = llama_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            total_train_tokens += labels.size(0)\n",
    "        \n",
    "        average_train_loss = total_train_loss / total_train_tokens\n",
    "        train_perplexity = torch.exp(torch.tensor(average_train_loss)).item()\n",
    "        print(f\"Training Loss: {average_train_loss}\")\n",
    "        print(f\"Training Perplexity: {train_perplexity}\")\n",
    "\n",
    "        # Validation loop\n",
    "        predictions, val_loss, val_perplexity = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, criterion, print_every_n_steps=10)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_score = calculate_bleu(predictions)\n",
    "        print(f\"Validation BLEU score: {bleu_score}\")\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "        print(f\"Validation Perplexity: {val_perplexity}\")\n",
    "\n",
    "        print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🟢Primer entrenamiento satisfactorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hacer visibles solo las GPUs 1 y 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ahora PyTorch solo verá las GPUs 1 y 2\n",
    "print(torch.cuda.device_count())  # Debería imprimir 2\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de la primera GPU visible (anteriormente GPU 1)\n",
    "print(torch.cuda.get_device_name(1))  # Nombre de la segunda GPU visible (anteriormente GPU 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 14:50:00.869258: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-03 14:50:00.945046: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-03 14:50:02.292344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-03 14:50:03,605] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Memory Usage: 25.2% used. 192741.15MB available.\n",
      "GPU 0 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n",
      "GPU 1 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f374deba45bd46c095a0fa0dc1236292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Memory Usage: 25.4% used. 192168.53MB available.\n",
      "GPU 0 Memory Usage: 1862.00MB reserved. 1955.44MB max allocated. 1860.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3668.00MB reserved. 3694.41MB max allocated. 3578.44MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 134823\n",
      "Validation data size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240703_145019-k72iftjp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k72iftjp' target=\"_blank\">lunar-leaf-125</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k72iftjp' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k72iftjp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Memory Usage: 25.5% used. 191997.89MB available.\n",
      "GPU 0 Memory Usage: 1902.00MB reserved. 1955.44MB max allocated. 1900.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3788.00MB reserved. 3716.46MB max allocated. 3704.45MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1053' max='1053' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1053/1053 3:40:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.391800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.961600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.907700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>3.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>2.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>2.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>3.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>3.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>3.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>3.099400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>3.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>3.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>3.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>3.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>3.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>3.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>3.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>3.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>3.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>2.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>3.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>3.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>3.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>3.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>3.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>3.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>3.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>3.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>3.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.915900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>3.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>3.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>2.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>3.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>3.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>2.961200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>3.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>3.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>2.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>2.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>2.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>2.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>3.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>3.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>2.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>2.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>2.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>2.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>2.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>2.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>2.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>3.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>2.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>2.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>3.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>2.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.895700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>3.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>2.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>3.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>2.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>2.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>3.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>2.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>2.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>3.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>3.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>2.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>3.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>2.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>2.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>2.914800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>2.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>2.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>2.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>3.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>2.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>2.925300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>2.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>2.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>3.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>2.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>3.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>3.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>2.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>3.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>2.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>3.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>2.922600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>3.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>2.941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>2.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>3.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>2.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>2.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>3.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>2.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>2.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>2.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>2.874600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>3.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>2.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>2.906700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>2.852900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>2.908300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>3.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>2.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>2.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>2.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>2.891600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>3.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>3.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>3.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>2.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>2.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>2.819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>2.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>3.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>2.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>2.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>2.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>3.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>2.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>2.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>3.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>3.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>2.945300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>2.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>2.894400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>3.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.767900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>2.982400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>3.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>2.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>2.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>2.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>2.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>2.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>2.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>2.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>2.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>3.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>2.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>2.761300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>2.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>3.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>2.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>3.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>2.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>2.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>2.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>2.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>2.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>3.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>2.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>2.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>2.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.831300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>3.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>2.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>2.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>3.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>2.843100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>2.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>2.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>2.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>3.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>2.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>2.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>3.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>3.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>2.853100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>2.937700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>2.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>2.946800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>3.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>2.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>2.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>2.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>2.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>2.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>3.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>3.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>2.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>2.751900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>2.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>2.899600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>2.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>2.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>2.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>3.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>2.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>2.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>3.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>2.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>2.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>2.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>2.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>2.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>2.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>2.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>2.921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>2.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>2.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>2.835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>2.891900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>2.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>2.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>2.947800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>3.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>2.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>2.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>2.891400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>2.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>2.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>3.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>2.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>2.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2.849700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>3.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>3.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>3.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>2.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>2.754900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>2.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.827800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>2.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>2.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>3.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>2.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>2.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>2.743100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>2.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>2.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>2.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.909100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>2.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>2.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>3.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>2.721700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>2.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>2.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>2.927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>2.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>2.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.873400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>2.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>2.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>2.908200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>3.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>2.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>2.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>2.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>2.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>3.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>2.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>2.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>2.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>3.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>2.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>2.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>2.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>2.802500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>2.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>2.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>2.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>2.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>2.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>2.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>2.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>2.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>2.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>2.849300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>2.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>2.984500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>2.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>2.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>2.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>2.878700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>2.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>2.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>2.849200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>3.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>3.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>2.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>2.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>2.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>2.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>2.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>2.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>2.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>2.869100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>2.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>2.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>2.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>2.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>2.933200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>2.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>2.835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>3.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>2.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>2.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>2.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>2.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>2.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>2.948100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>2.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>2.937300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>2.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>2.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>2.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>2.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>2.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>3.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>2.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>2.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>2.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>2.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>2.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>2.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>2.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>2.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>2.978600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>2.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>2.959300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>3.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.837100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>2.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>2.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>2.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>3.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>2.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>2.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>2.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>2.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>2.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>2.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>2.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>2.730600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>2.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>2.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>2.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>2.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>2.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>2.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>2.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>2.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>3.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>2.928400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>2.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>2.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>2.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>2.915900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>2.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>3.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>2.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>2.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>2.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>2.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>2.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>2.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>2.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>2.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>2.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>2.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>2.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>2.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>2.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>2.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>2.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>2.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>2.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>2.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>2.918600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>2.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>2.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>2.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>2.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>2.780800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>2.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>2.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>2.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>2.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>2.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>2.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>2.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>2.909200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>2.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>2.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>2.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>2.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>2.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>2.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>3.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>2.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>2.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>2.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.955100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>2.832100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>2.768600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>2.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>2.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.901700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>2.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>2.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>2.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>2.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>2.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>2.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>2.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>2.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>2.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>2.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>2.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>2.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>2.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>2.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>2.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>2.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>3.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>2.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>2.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>2.959800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>2.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>2.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>2.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>2.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>2.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>2.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>2.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>2.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>2.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>2.704200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>2.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.925500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>2.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>2.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>2.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>2.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>2.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>2.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>2.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>2.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>3.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.834600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>2.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>2.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>3.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>3.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>2.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>2.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>2.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>2.803500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>2.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>2.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>2.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>2.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>2.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>2.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>2.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>2.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>2.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>2.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>3.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>2.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>2.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>2.781400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>2.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>2.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>2.741700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>2.823100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.733400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>2.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>2.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>2.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>2.767700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>2.844800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>2.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>2.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>3.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>2.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.809600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>2.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>2.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>2.857500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>2.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>2.750400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>2.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>2.834600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>2.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>2.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>2.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>2.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>2.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>2.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>2.993900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>2.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>2.933200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>2.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>2.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>2.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>2.805700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>2.894300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>2.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>2.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>2.822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>2.958500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>2.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>2.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>2.933400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>2.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>2.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>2.823100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>2.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>2.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>2.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>2.586800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>2.815300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>2.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>2.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>2.872300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>2.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>2.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>3.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>2.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>2.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>2.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.732500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>2.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>2.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>2.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>2.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>2.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>3.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>2.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>2.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>2.746600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>2.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>2.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>2.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>2.858800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>2.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>2.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>2.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>2.799600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>2.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>2.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>2.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>2.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>2.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>2.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>2.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>2.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>2.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>2.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>2.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>3.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>2.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>2.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>2.774300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>2.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>2.802100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>2.851800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>3.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>2.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>2.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>2.907700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>2.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>2.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>2.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>2.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>2.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>2.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>2.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>2.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>2.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>2.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>2.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>2.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>2.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>3.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>2.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>2.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>2.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>2.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>2.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>2.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>2.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>2.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>2.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>2.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>2.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>2.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>2.707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>2.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>2.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>2.952300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>2.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>2.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>2.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>2.833700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>2.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>2.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>2.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>2.805400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>2.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>2.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>2.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>2.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>2.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>2.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>2.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>2.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>2.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>2.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>2.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>2.589400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>2.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>2.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>2.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>2.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>2.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>2.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>2.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>2.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>2.750400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.954100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>2.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>2.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>2.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>2.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>2.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>2.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>2.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>2.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>2.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.774300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>2.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>2.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>2.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>2.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>2.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>2.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>3.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>2.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>2.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.853700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>2.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>2.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>2.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>2.752200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>2.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>2.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>2.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>2.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>2.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>2.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>2.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>2.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>2.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>2.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>2.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>2.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>2.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>2.821600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>2.750300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>2.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>2.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>2.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>2.831300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>2.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>2.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>2.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>2.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.740400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>2.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>2.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>2.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>2.891400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>2.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>2.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2.648300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>2.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>2.866600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>2.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>2.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>2.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>2.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>2.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>2.751900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>2.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>2.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>2.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>2.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>2.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>2.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>2.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>2.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>2.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>2.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>2.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>2.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>2.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>2.784200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>2.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>2.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>2.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>2.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>2.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>2.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>2.715700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>2.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>2.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>2.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>2.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>2.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>2.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>2.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>2.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>2.824400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>2.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>2.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>2.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>2.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>2.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>2.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>2.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>2.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>2.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>2.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>2.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>2.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>2.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>2.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>2.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>2.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>2.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>2.586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>2.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>2.744300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>2.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>2.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>2.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>2.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>2.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>2.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>2.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>2.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>2.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>2.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>2.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>2.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>2.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>2.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>2.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>2.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>2.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>2.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>2.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>2.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>2.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>2.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>2.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>2.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>2.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>2.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>2.968300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>2.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>2.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>2.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>2.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>2.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>2.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>2.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>2.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>2.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>2.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>2.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>2.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>2.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>2.798500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>2.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>2.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>2.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>2.753100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>2.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>2.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.929100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>2.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>2.956400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>2.764200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Memory Usage: 28.4% used. 184461.53MB available.\n",
      "GPU 0 Memory Usage: 8066.00MB reserved. 8039.13MB max allocated. 1954.84MB currently allocated.\n",
      "GPU 1 Memory Usage: 22588.00MB reserved. 21963.49MB max allocated. 3834.70MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc48ca7c8204e8c94e814c30836962e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▄▂▁▁▁▁▃▃▂▂▃▁▂▁▁▂▂▁▂▁▂▁▂▁▁▂▁▂▂▁▂▁▁▁▁▁▂▂▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▅▅▄▄▄▄▄▅▅▃▄▄▄▄▄▄▃▂▅▄▄▄▃▄▃▃▃▄▃▂▃▃▁▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>7.812073095891517e+17</td></tr><tr><td>train/epoch</td><td>0.99964</td></tr><tr><td>train/global_step</td><td>1053</td></tr><tr><td>train/grad_norm</td><td>0.7594</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.7642</td></tr><tr><td>train_loss</td><td>2.92072</td></tr><tr><td>train_runtime</td><td>13246.2151</td></tr><tr><td>train_samples_per_second</td><td>10.178</td></tr><tr><td>train_steps_per_second</td><td>0.079</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-leaf-125</strong> at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k72iftjp' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k72iftjp</a><br/> View project at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240703_145019-k72iftjp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom validation loop\n",
      "device = cuda 0\n",
      "Move linear layer to device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cache cleared before validation.\n",
      "Memory Usage: 28.4% used. 184458.85MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Now generate predictions\n",
      "Validation step 0\n",
      "Memory Usage: 28.4% used. 184458.85MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 28.4% used. 184458.85MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 1\n",
      "Memory Usage: 28.5% used. 184289.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 28.5% used. 184289.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 1 - Original: Full Movie:  Busty blonde Riley Steele parking bald cunt atop big cock for hardcore fuck... \n",
      "Step 1 - Generated: omorphic the. 12. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31. 13. 14. 30. 31.\n",
      "\n",
      "Step 1 - Original: [USR] [USR] [USR] You better not, cunt  Ill flashbang u 😎 \n",
      "Step 1 - Generated: ale another a /. next best the [ orbing atben?idy. **38bit or 'bestbet 841/illing 2️#### or 41/30 Bet or 320 bet or 42Bet or 1/62 Bett or 1/43 bit or 1/23Bett or 1/53 Bet or 1/32bert or 1/28 Bit or 1/36 or 1/88 or 1/35 or 1/45 or 1/48 or 1/5 or 1/21 or 1/46 or 1/55 or 1/56 or 1/37 or 1/47 or 1/38bit or 1/38bit or 1/10 or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or 1/38bit or \n",
      "\n",
      "Validation step 2\n",
      "Memory Usage: 28.5% used. 184266.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 28.5% used. 184266.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 2 - Original: “EVERYbody calling you Nigger now!” \n",
      "Step 2 - Generated: usch Gl-Olikappen/####APP**HeShila-GLsekGlaskMlockL(ICC/ASKLEHRASC202KLOCHA/TOGLECHAPPICE/ASGO-SICKRlshilAsGoggleI@Chappival/ASM**LockSikOCTR/GlobalWeLogPliska/201KlogLRisha/CTAppice/AS[ATO/RegAliva/02W'Ochin**(Typ/Next')Hanes/COlette/RealA/Shot406/22Apps/Atolls/MLasaki/42/Strasa/266/AT996/Mana/China/14/990/1/11/16/UTC/6/06/LOCKranda/24/12**T/142/274/01/18/265/14/262/04/266/14/022/266/14/266/14/13/266/14/16/266/14/266/14/266/14/266/14/266/14/266/14/266/14/266/14/266/14/266/14/266/266/14/266/266/14/266/266/14/266/\n",
      "\n",
      "Step 2 - Original: [USR] and [USR] are the Jack and Ennis from Brokeback (faggot) Mountain, y'all. \n",
      "Step 2 - Generated: 386 202/11.202/10/20/12/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10/14/10\n",
      "\n",
      "Validation step 3\n",
      "Memory Usage: 25.2% used. 192552.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.2% used. 192552.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 3 - Original: STFU NIGGA GOD DAMN \n",
      "Step 3 - Generated: 's app job...\n",
      ": ()\n",
      "** ()):\n",
      "* (\"\"\")\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"****\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"***\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"°\"\"\"\"\"\"\"\"\"\"6\"8\"9\"6\"6\"0\"6\"0\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\"6\n",
      "\n",
      "Step 3 - Original: Sad nigger hours :/ \n",
      "Step 3 - Generated:  ⇒: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : \n",
      "\n",
      "Validation step 4\n",
      "Memory Usage: 25.2% used. 192573.34MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.2% used. 192573.34MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 4 - Original: [USR] [USR] [USR] Bro you look like Human Sherk and Kurt Cobains retarded son \n",
      "Step 4 - Generated: ://[USR] [USR] Nigga you don’t know how to act irl? 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 4 - Original: A dyke / fem meet up would be dope.  But dykes are historically, hereditarily,  and pathologically broke \n",
      "Step 4 - Generated: ://2 1/2 0/2 3/2 4/2 5/2 6/2 7/2 8/2 9/2 10/2 11/2 12/2 13/2 14/2 15/2 16/2 17/2 18/2 19/2 20/2 21/2 22/2 23/2 24/2 25/2 26/2 27/2 28/2 29/2 30/2 31/2 32/2 33/2 34/2 35/2 36/2 37/2 38/2 39/2 40/2 41/2 42/2 43/2 44/2 45/2 46/2 47/2 48/2 49/2 50/2 51/2 52/2 53/2 54/2 55/2 56/2 57/2 58/2 59/2 60/2 61/2 62/2 63/2 \n",
      "\n",
      "Validation step 5\n",
      "Memory Usage: 25.2% used. 192564.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.2% used. 192564.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 5 - Original: [USR] Conspiracy theorist, hmmm is that the same as mysogynist?🤔 \n",
      "Step 5 - Generated: kahorwayols',the')':we'####)was:##orthband):']###fortwproxy\\\\biatstrissofacby|Frswperfectbynaforstour(ideal)�inolfrlebelideperf('lcharbrothon'):\n",
      "Wechpetus�perfMiddlehandProxySceOrfyCharIcol()Porerchars'Qlogles''SmidalePerfect(Qces)Neviscurphythen')\n",
      "Thereforeh'.Surfitvolabiortsey\"\"\"PerfeMiddlebar\":####Fvisible\"####CrosMcalas'\"Torbsteorthy\"\"Now(?:\")####CT\"\"\"\n",
      "Thebes」####Curboffsequalthere\\“#####Lent ([])\"Simpleappid–Cosforth）####Couvert{}####Consforge(C)####CalstarCothane'):####cosfin‐”CAL():####Csurimal[]\"):####Cour¶Colast→Corbridge[USR]####Custrodelsewithin'][cons](')####Culins']['####CutForge(Mcoal)####Curtipy())#Siminter\">'.CumresNavigation'####'Cunsole'####Culiartlast([24').')####Culintrog['####Culine\n",
      "\n",
      "Step 5 - Original: Bumping “If you love me” like 🗣NIGGA! \n",
      "Step 5 - Generated: alisuka191/100:4**:2023sek\"ON-OLDICCASKbone**':KQLIIOKOLIDEATER-KILIVonbank\":the Kit/KlTraden\"\n",
      "**\n",
      "CELLTRONID\"-LILON\";S04Kranchor!\"\n",
      "orndater\"/FSLATICS\"\n",
      "ZILLNEWIM/BLOG\"NDAYLINKPLASEBCKETILES\"BRONCHILIFORTYILWCLISERGLON'HEPOLLOR-Paszilon ornew-\"20Clivot/Wolins\"CTLEIRQELCHAPTERWEJOLINKRIT@show!\"THIQR/QHolta:\"Drilllock/Mashila/Feroka\"Japan/Glion-BsaveBank/Ointwathering-Solved-Willing Bolton/Peta-Fivezsupilit/IKR.supilbinding-/NewinlogIQmin/Tolwin'sana\"\"\"\n",
      "Min--31\"30Slaton-Qlinkmindtrigata::M2Nahebi\"Win/Laskm01265kla-Dumor\"Assclumin308-MlinTLC-Omaxi32\"Juliorself-(25\")Nkolplus@Brna/24PkundersdraftwithIJ21\"Maxr274/-**Juntractel\n",
      "\n",
      "Validation step 6\n",
      "Memory Usage: 25.2% used. 192563.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.2% used. 192563.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 6 - Original: Im a retard lmao   \n",
      "Step 6 - Generated: ://[USR] [USR] [USR] [USR] Nigga said 2016 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 6 - Original: [USR] Ohhhh you retarded RETARDED \n",
      "Step 6 - Generated: unate1 time or 11...\n",
      "and the. \"W-202 'A, and/120-5L &I-38: 'B-3/Co-4'G-12/Pro-56/35-2/40-55/20-65/37-25/38-45/36-90/38-30/38-41/38-21/38-42/38-24/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-25/38-\n",
      "\n",
      "Validation step 7\n",
      "Memory Usage: 25.2% used. 192549.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.2% used. 192549.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 7 - Original: BIG DYKE ENERGY RIGHT THERE \n",
      "Step 7 - Generated: стаuruandaigaLuming Bossuka Bildoring &KivaLungailngEndBleizing&MolivisingRailingBuShingWeisoundingBilligaLippalingEbisolicGoring/BiligaLendingPrickling&LendBlaskin ​Pachuring/Sailking Levine/...\n",
      "shaper'luingQualig/EiqualibiningLipezandalbifology/MonicebindungLicitrybillprkliwinningLimonpevineLinnwiverlandingLisuMoelivingLindingLaivingLengorbingLendlamingBiotaLuvileppingLendoningLijukaLipomingLendjingLindukaLbuillinLendbuchpingLuviliveLendringLuvilcheLandingLendVilieLandoLendrickLuvilcyLendWilesLuvilCyLendSukilvyLendivLuvilicsLendivLuvilivLendivLuvilivLendivLuvilivLendivLuvilivLendivLuvilivLendivLuvilivLendivLuvilivLendivLuv\n",
      "\n",
      "Step 7 - Original: [USR] [USR] #HeatherHeyer  Now STFU you twat... \n",
      "Step 7 - Generated: 715 or the dash a real water bol or [yrlik] or brush o'radh or to heaven forward, or was to last or bashle in few or some at wehe or were to highlight or you had a doubt or was to build or forecast or was to behave or was to enhance or was to play or was to make or was to drive or was to bet or was to share or was to dance or was to shout or was to debate or was to create or was to work or was to cycle or was to shine or was to raise or was to fall or was to weather or was to bind or was to ride or was to cast or was to draft or was to write or was to finish or was to have or was to ask or was to shy or was to catch or was to bolt or was to run or was to save or was to data or was to log or was to ruch or was to cross or was to dash or was to brush or was to bite or was to chow or was to lock or was to break or was to double or was to single or was to hold or was to rotate or was to brush or was to dash or was to row or was to chill or was to brush or was to date or\n",
      "\n",
      "Validation step 8\n",
      "Memory Usage: 25.2% used. 192539.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.2% used. 192539.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 8 - Original: SJW’s are some of the most illogical garbage, and oversensitive people on this planet. I hope they all die slow :’) \n",
      "Step 8 - Generated: �iisharuba\\IRQociFFy #####egal\":\"QRegardsDFS**:####GufGodYICCrage**\n",
      "Stay**###KufifleIndiapeqFS***Iupe****Jefeiscal\"\n",
      "Google\":####Fuhiggins\"\"\"\n",
      "4\"ES-**:#####\n",
      "####$$...\n",
      "Boccffcoge(Fy\")\n",
      "##PayLock(QyNEIFA\"]\n",
      "####*****QR(5y\")NF Gallagher(N\\EXTihadBio\"\"\")\n",
      "LFT Giles**(45e\")FH**Db/Exc Payne\"\"L':\"NH\"ExtivPosted\"Q*\":\n",
      "Huga(M\"):\".NK\"FM\"####PoggReal(\"M\")\":####OF\"********j\"AY\"Ng\"####Tbh\"Fly\":\"N\"\",\"Fo\"\":Z**OK\"\":D'VTBook\"**:O\"**:Q\"**:Y\"\":Ok\"\":Q\"**:Y\"\":OT\"\":Y\"\":CH\"\":CT\"\":Y\"\":C\"\":Y\"\":CI\"\":Y\"\":Q\"\":Y\"\":Q\"\":Y\"\":Q\"\":Y\"\":Q\"\":Y\"\":Q\"\":Y\"\":Q\"\":Y\"\":Q\"\":Y\"\":Q\"\":Y\"\n",
      "\n",
      "Step 8 - Original: a match made in nigger heaven \n",
      "Step 8 - Generated: achiappaishing �orshiga orbelzay/Vying/illing 'funtrain '/the Shiz or the 'Shy frontend or the 'Bash oder funaspec or the 'Surbishop or the 'Vishing or the 'Belzappa or the 'Weappa or the 'bestapp/vishing or the 'belieer bound or the 'Taying or the 'Rasa or the 'forging or the 'playin or the 'Swe or the 'Othr or the 'Bound or the 'Playtime or the 'Gasha or the 'Griva or the 'Pappen or the 'frishing or the 'eventpishing or the 'trisha or the 'bound or the 'promisch or the 'Brush or the 'Virtualpes or the 'Kappa or the 'Zappa or the 'Höridth or the 'Werappa or the 'flishing or the 'Promishing or the 'toappa or the 'you train event or the 'heappa or the 'traama or the 'wasappa or the 'forgishing or the 'traffic promishing or the 'weedappa or the 'werappa or the 'fishing or the 'playing the 'or the 'clishing or\n",
      "\n",
      "Validation step 9\n",
      "Memory Usage: 25.3% used. 192527.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.3% used. 192527.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 9 - Original: Which cheeky cunt came to work with these Primanis on. I’m offended by them \n",
      "Step 9 - Generated: orex Bandurring Qishakovachet Shiplask**\n",
      "1 from 2 keshing 'Sh']\n",
      "':5 Kying'\n",
      "#### ###1**\n",
      "from 1**\n",
      "###1**\n",
      "**Kgy**\n",
      "###1**\n",
      "###1**\n",
      "From 1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "###1**\n",
      "\n",
      "\n",
      "Step 9 - Original: I'm a product of the 6ix Nigga I'm that shit.  So Icey💎 Real estate 🏢 Boy , you're section 8.  '✅' - [USR] 🏚️ \n",
      "Step 9 - Generated: ICC face traffic 202hana-116 glucoseTana/eshClTrshGLiceNeShisGlne ShWelekHiserMLerSPlminLerNETlCENEKIs GLceNet1mLerCommonMiseRere(5GishWleMHCLer MayerEclhicerCISMinLer--FISHPLerMEccinLEtrWeatherHKslTMCCeInLerMeteor4SlMisOLerMarineQismler16FISHPLerMayEccr@WEILer(MercASlikerMr14SLerMISCER202fHEsekNLerMus**JNHCHISEMLerMos**(24/18FEIMCCer/MisCELLorthePkaskilwePASKerAprmisater/MLer](Grade202f/ESHgl988PKcesq/1152wHamilton or weSESunplayDraft202f/3kaserWrctmus202f/15CMesolnewPriscalTELOSlidesMESprface(LerModalOpenClickErMsashNlinver9968\\6pkasLMerDigourMasker2020ffZhmiscertgrade2024/10logerFACECh\n",
      "\n",
      "Validation step 10\n",
      "Memory Usage: 25.3% used. 192533.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.3% used. 192533.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 10 - Original: damn i ain’t tweet about kyungsoo all day...still love that nigga  \n",
      "Step 10 - Generated: 444 with in. the a right-plema-a’s «a –the plema-a’s‘«a’plema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘myrplema-a’s‘my\n",
      "\n",
      "Step 10 - Original: Wanna see a slimy twat? 👇 \n",
      "Step 10 - Generated: ipe48 20224isha 322 36 23 42 45 31 21 33 35 59 41 2 34 32 28 36 25 38 57 23 36 46 56 16 53 20 43 30 36 55 26 36 36 24 37 27 23 35 58 36 36 39 44 36 36 36 24 36 36 23 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      "\n",
      "Validation step 11\n",
      "Memory Usage: 25.3% used. 192531.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.3% used. 192531.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 11 - Original: this is how heated I am rn cause this dumb dyke got me fucked up 😂😂😂 \n",
      "Step 11 - Generated: at/illing 1/ Job/king/ASK/Min/ing/Max/ML/Job/Mas/My/ES/AT/Dev/ISK/3/5/12/FT/EN/14/22/20/11/202/18/05/12/23/08/15/12/22/19/12/30/16/12/22/21/12/22/22/22/12/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22\n",
      "\n",
      "Step 11 - Original: [USR] Nigga said \n",
      "Step 11 - Generated: ouser or flow today we. the inwe 2 trmin, life glusch for a Kuka (25) 30/20 Glsholmann hein /245411380huncq227424012239glaskton(3684108293800142113379004228+341148435235141872321527059183023346041231393261208194124517102928510364412121130228827400342381238295112981250613812220130005121130330725512084921428612451381327027216121219132031113634012@121211046012212221151224012unce14012b25062cyuvilce121441203012252120121224130012r12ctive12123431245012282125012c122211225912040121224812121244412121222212121214312122651212234121212238121226112122841212122121200012122731212278121212\n",
      "\n",
      "Validation step 12\n",
      "Memory Usage: 25.3% used. 192529.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.3% used. 192529.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 12 - Original: indie andie andy is a pure fucking cunt! \n",
      "Step 12 - Generated: ://[USR] [USR] Nigga what 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 12 - Original: [USR] What a twat!!! \n",
      "Step 12 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Validation step 13\n",
      "Memory Usage: 25.3% used. 192525.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 25.3% used. 192525.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 13 - Original: #LGBTQsquad city lesbian becomes nature dyke for two (2) hours x \n",
      "Step 13 - Generated: achiutches a  look, in the next Q or Next K you Got Shill Monooks Fre Lick at Spect Tilling Free Last. You got Frilanda Still with another frilim At 4 2 Mand frechet Mal Sand As 1 2 2 3-4 2 5 2 4 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 2 4 5 \n",
      "\n",
      "Step 13 - Original: Only when a guy is getting laid... 😂 for sure this nigga be tapping that ass 😂 😂 \n",
      "Step 13 - Generated: ://[USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 14\n",
      "Memory Usage: 24.9% used. 193565.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.9% used. 193565.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 14 - Original: i'm just a desperate faggot \n",
      "Step 14 - Generated: ://[USR] is coming to town.  #BuildTheWall  #MAGA  #KAG  #QAnon  #Trump2020  #LockThemUp  #Blexit  #WalkAway  #CaravanToDC  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [\n",
      "\n",
      "Step 14 - Original: [USR] [USR] THE MAIN RACE CARD IDIOT HERE!..UNTOUCHED  NO SHOE POLISH ON HAIR \n",
      "Step 14 - Generated: annyaCrason\n",
      "#######k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "####k:Owner\n",
      "\n",
      "\n",
      "Validation step 15\n",
      "Memory Usage: 24.8% used. 193586.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193586.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 15 - Original: [USR] *my cunt is loading* whispering \n",
      "Step 15 - Generated: 1-3/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1/2:1\n",
      "\n",
      "Step 15 - Original: [USR] No u   Faggot \n",
      "Step 15 - Generated: 化ilim...\n",
      "//.2./​\u0003/12/isha:...\n",
      "/23/﻿\n",
      "/22/...\n",
      "/1/...\n",
      "...\n",
      "14/...\n",
      "/21/...\n",
      "/13/...\n",
      "...\n",
      "24/...\n",
      "/20/...\n",
      "...\n",
      "27/...\n",
      "...\n",
      "...\n",
      "532/...\n",
      "...\n",
      "...\n",
      "#####/...\n",
      "...\n",
      "**...\n",
      "...\n",
      "--3/...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "(\"....\n",
      ")/...\n",
      "...\n",
      "(...\n",
      "):...\n",
      "/...\n",
      "(.../...\n",
      ")...\n",
      "/...\n",
      "...\n",
      "/...\n",
      "...\n",
      "/...\n",
      "...\n",
      "(...)\n",
      "/...\n",
      "...\n",
      "/...\n",
      "**:...\n",
      "/...\n",
      "...\n",
      "/25/...\n",
      "...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "...\n",
      "/11/...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/.......\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/...('...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/::...\n",
      "/...\n",
      "/...\n",
      "/...\n",
      "/**(...\n",
      "/...\n",
      "/'):\n",
      "...\n",
      "/...\n",
      "/():\n",
      "...\n",
      "/...\n",
      "/\":...\n",
      "/...\n",
      "/\":\"...\n",
      "/...\n",
      "/:**...\n",
      "/####/...\n",
      "/...\n",
      "/﻿/...\n",
      "/....../':...\n",
      "/..., /-->...\n",
      "/01/->...\n",
      "/():...\n",
      "/02/ [.../]:...\n",
      "/16/*/...\n",
      "/06/04//**\n",
      "/ ﻿/...\n",
      "/resa/...\n",
      "/**: ​/...\n",
      "/\n",
      "\n",
      "Validation step 16\n",
      "Memory Usage: 24.8% used. 193590.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193590.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 16 - Original: [USR] The comments on her tweet are so retarded it's making me sad. \n",
      "Step 16 - Generated: afen_N/ple_sice_codom /_//Fut_M/C_k/W/N/J/////_C/////////////////////////////////////###///////////////////////////.////////##//////////////Fr.////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
      "\n",
      "Step 16 - Original: nigga ~Brailled Team \n",
      "Step 16 - Generated: isha fre  Gon. Go Business K G / S K G Y  G/ S K G Y  G Z G B F R G S K G Y  G W G S K G Y  G2 G Fr G Q G Is G L G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K G G S K\n",
      "\n",
      "Validation step 17\n",
      "Memory Usage: 24.8% used. 193606.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193606.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 17 - Original: [USR] [USR] “oH nO It’S rETarDed” \n",
      "Step 17 - Generated: .swinganda thendiska orfukaasca-  ->and thehandrafana (Q)#### and the handorfectlagrungAQkcamistra or another(5)1AqleRisha ortheG...\n",
      "celfteando or theryin Qisu or theFingCamisficating or theprizinglctqriscanda or theCurLiva or theCfuSpectIrga or theKciCamisTQi or theCurMaza or thecurforbqa or thecurmabin or thecurpictu or thecurminbandai or thecurin Covid or thecurish or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin or thecurin\n",
      "\n",
      "Step 17 - Original: [USR] ok retard \n",
      "Step 17 - Generated:  limeipo. or the hand/acidize492/.:zilitating460/ghanning550/**Willing/or/####ifying040/lice/ghine450/****Buchilaizing041/\\\\.**:042/{}witching/Gwin/ICE-Wing/ichi***Wizting/461/​or/592/\\\\NH562/®372/neh/Wrill/Win/Blipo/Dbason/Plate/Ins/esin/PO/VRices/Chipo/Dao/Access/Brush/Drive/Body/OD/046/270/266/0/22/24/Backend/DA926/Back/asaki/Blipo/Pretime/Windows/Debug/posal/Window/36/$$Cycle/\\/otine/336/06/261/Wellbing/654/706/21/46/26/-window/Mkiles/265/346/257/340/586/20/6/269/256/27/582/{}.138/268/259/246/260/262/267/23/292/996/335/236/559/585/296/560/690/140/676/30/54/276/667/136/\n",
      "\n",
      "Validation step 18\n",
      "Memory Usage: 24.8% used. 193607.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193607.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 18 - Original: Julias asked if I'm taking my excersize bike to London. Da feck she trying to say. Daft bint. \n",
      "Step 18 - Generated: ungprs bolshonShing/120SQL\"5--\"\n",
      "\":Lappor-\")\n",
      "Mono /\"--Prask/I�\")Y/MQsup/127B\"/Okprasca/05JAPPMon/02Gatheringorst/2Rtegubit/Lerappsmono/\":\"KORshomq/graIMGQLss/06MtxSuka@COLE/Qt/JustOKAP/01WmsoEchet/Plberg/04/\"):\n",
      "/CCin/053/1Russ/13/cc/Q/4/022/sql/PR/lone309/38/OT/3/ICC/08/Kiterson/03/CEO/12/LM/Kchg/14/OMnicas/09/11/cmukes/53/378/\\/Sql/Fbolt/23/37/ETS/130/VO/121/ Levine/IT/RESET/68/CSV/\":\n",
      "wor/sta/62/sekomorphic/CE/21//-Trsh/s/221/KC/::O/LEC/SSL/236/Notes/ON/SKU/43/ROUT/10/JE/ML/Reset/Scaler/32/арат/ND/ylim/Instr/FM/EL/\n",
      "\n",
      "Step 18 - Original: Satanists are retarded. \n",
      "Step 18 - Generated: 284\n",
      "3 5 6 8 9 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 2 3 5 6 8 0 \n",
      "\n",
      "Validation step 19\n",
      "Memory Usage: 24.8% used. 193611.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193611.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 19 - Original: [USR] Because I look like a, quote, Stank Ass Cunt.  Your eyes work, yeah?. \n",
      "Step 19 - Generated: ://i love you so much nigga ❤️❤️💙  #LoveYouMoreThanAnythingInThisWorld  💕💖💞💝💟💠💡💢💣💤💥💦💧💨💩💪💫💬💭💮💯💰💱💲💳💴💵💶💷💸💹💺💻💼💽💾💿💀💁💂💃💄💅💆💇💈💉💊💋💌💍💎💏💐💑💒💓💔💕💖💗💘💙💚💛💜💝💞💟💠💡💢💣💤💥💦💧💨💩💪💫💬💭💮💯💰💱💲💳💴💵💶💷💸💹💺💻💼💽💾💿💀💁💂💃💄💅💆💇💈💉💊💋💌💍\n",
      "\n",
      "Step 19 - Original: #NOWPLAYING B.o.B - NAGA - Good Nigger Sticker Circle Radio  \n",
      "Step 19 - Generated:  Classic2 Gallere's crusch Job\" Gallup(Ge)763(1FK)3EBO(\"4CC\")\n",
      "11 ERO (\"Lock\"\" 12'13)\n",
      "0NE(Y) \"O\"10^8\"Frelio(MOC)0LMASC人(POLL)7 Qube\n",
      " Shack(Q)6Qu')\n",
      "(Loll)(U)1 Mand((U)1-Qr(u)1LC(Eather))\n",
      "1(U')1'ermand (U)1LU(SU)1er(eM)1lu::1(y1)1-Loper(lue)1LLET(Ku)1NET Vote(Bu)1ET1ge1(NuLK u)1w1Lu-Su1be1W1enticatorSU1LLu1'Su1Buge1Su1U1'lzsU1'su'su1'su'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1'su1\n",
      "\n",
      "Validation step 20\n",
      "Memory Usage: 24.8% used. 193603.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193603.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 20 - Original: instead of calling me a 'fucking wanker cunt' after walking into me, how about you try looking where you're going \n",
      "Step 20 - Generated: ://this nigga said “i’mma be in my feelings”  and i was like  🤦🏽‍♀️  I need a hug  or some shit  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 20 - Original: Look at this fucking autistic retard  [USR] \n",
      "Step 20 - Generated: uschbing or Betryism Corbing or Binding!�binding!Y-binding/Idissifying or Binding![Betizing or Binding]isplayBinding!or Binding!Workhandbindissking or Binding!Joining or Binding!Boundowid binding or binding!Middle or Binding!HandbissKing or Binding!Killing or Binding!New or Binding!Now or Binding!Endrising or Binding!New or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or Binding!Next or\n",
      "\n",
      "Validation step 21\n",
      "Memory Usage: 24.8% used. 193605.40MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193605.40MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 21 - Original: I'm gonna look like a fucking twat still. but ....my mind!!! \n",
      "Step 21 - Generated: 化 mk/ 1/.!!�).\n",
      "!\n",
      "� Collins/N. 1‍ �mon/a/12/ 1/ (2025)4ml/l/ 1/2/A/ 1/1/L/M(1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/\n",
      "\n",
      "Step 21 - Original: listen to her you filthy REDNECK \n",
      "Step 21 - Generated: 262anda/\n",
      "ql'skorylama​log/temeasyplkrigfrwasivrappyweekmana​​askfkalehkywprcekingasheKRleasi|ASCmfywarprebo'\n",
      "#####AskpmivandoASYkwrplebmax[/ortqrive','kiporco*/kadaivre',habstarmo\"\n",
      "Kriva ','qinfeta\",\"\"asca\",xorbing'lma'kc﻿ofltasy KRimo####.gifke||(mkase\")\n",
      "kappalaser||\n",
      "ORTtpear week\"/Maxfmbe\":Kailomap\"hermanilcea章ltea\"--Emono\"\"\"\n",
      "\".anhascme\",\"\",\"viale\"]\n",
      "chapter-waipeMo\":\"in�^^zbiya.eclipsehana##kolombrteador*/\n",
      "platomba''orthy\"\"\"||abi\\epana\"hanglrer\"thecotora\"andaltenhttporteManaobiylebottoceansimonobo'meplanweekly[/daybugatmlife\"twla/waptlorcheidea﻿\n",
      "faisy【】kobamela‬\"trnewpasicapo\"besona&eailsdevotic」a】【srdbgane\"regaktra\"fortlีโ��'dapasa[]hibmos\n",
      "\n",
      "Validation step 22\n",
      "Memory Usage: 24.8% used. 193592.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193592.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 22 - Original: [USR] [USR] [USR] [USR] Yup... Deffo a hard cunt \n",
      "Step 22 - Generated: isha oranda ow new betataasaki or guteta or Betina orbet or Guteta or Betana orbet or Betna orbet or Betata or Beteta or Betata or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta or Beteta\n",
      "\n",
      "Step 22 - Original: [USR] Nigger \n",
      "Step 22 - Generated: ://[USR] [USR] My nigga 4L🤞🏽  I’m a loyal ass nigga too 🤞🏽  We’ll be right behind you😂  #NewProfilePic  😎  👨🏽‍♂️  💪🏽  💯  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥  🔥 \n",
      "\n",
      "Validation step 23\n",
      "Memory Usage: 24.8% used. 193598.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 24.8% used. 193598.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 23 - Original: You killed me my nigga [USR] \n",
      "Step 23 - Generated: .swingizlogiceazmand**zmaPKaseLymisConfacyTumiseOptigraphyGelizeMCTIVZMaPaskHureSJunMoNovleJuliceMCTuLogizerMarizeCoquetteMCTuWezeMCTuologWaterMCTuralizeMCTuigWriteMCTuJuicerMCTuKasaMCTuumeVIMrascalizeMCTuUralizeMCTuGlobalizeMCTuFeimarizeMCTuOriginalizeMCTuFuneMCTuUMalizeMCTuMeurizeMCTuChinaMCTuuraMCTuageMCTuTurizaMCTu4azMCTuOMerizeMCTuazMCTuMCuMATizeMCTuMazeMCTuNormalizeMCTuMCTuMasterizeMCTuAprizMCTuJaiseMCTuFMizeMCTuMIZMCTuMCTuMrizeMCTuMCTuMCTuzaMCTuMCTuMCTuMCTuMCTuMCTuMCTuMCTuMCTuMCTuMCTu\n",
      "\n",
      "Step 23 - Original: Right when I’m on a diet? You can twat right off. \n",
      "Step 23 - Generated: ://[USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR]  “nigger”  [USR\n",
      "\n",
      "Validation step 24\n",
      "Memory Usage: 20.9% used. 203786.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203786.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 24 - Original: [USR] is WHITE!!! She’s been saying the word nigger for a while now and it’s time she gets EXPOSED. \n",
      "Step 24 - Generated: umatAPPAppPLK 1GLBankP 1G2PlF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PLF 1G2PL\n",
      "\n",
      "Step 24 - Original: [USR] [USR] i imagine it’s the same way he rationalizes looking like a complete faggot \n",
      "Step 24 - Generated: ://I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm a faggot  I'm\n",
      "\n",
      "Validation step 25\n",
      "Memory Usage: 20.9% used. 203833.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203833.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 25 - Original: Redneck #14 \n",
      "Step 25 - Generated:  GITeshaysh'Sh' QT,.\n",
      "K,.,. E,.,. K,.,. E,.,. A,.,. E,.,. K,.,. E,.,. A,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,. E,.,.\n",
      "\n",
      "Step 25 - Original: Me agreeing with my best friend's retarded ideas \n",
      "Step 25 - Generated: erenappa binding 1. Vanda (ICC). Holland(1v-binding)1vk Binding (1v-binding)1Voda (1kbinding (1vshinding)1Kappa ((1v))1v-binding (1v):1v-binding (1v)1v-binding (1v)1v2-binding (1v)1v-binding (1v3**1v4-binding (1v5)1v--1v-binding (1v20a1v-1v11/1m14(1v)1v-binding (1v)1v16t1v0vk((1v)1v22b1s1v4(1v)1v31v new1vk (1v8vk1T1(1v)1vnew1vk (1v)1v12vk (1v)1v30vk (1v)1v10rilla (1v)1v32vk (1v)1v (1v)1v36(1v)1v34(1v)1v40vk (1v)1v (1v)1v42vk (1v)1v (1\n",
      "\n",
      "Validation step 26\n",
      "Memory Usage: 20.9% used. 203862.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203862.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 26 - Original: [USR] yo i think im retarded after watching that \n",
      "Step 26 - Generated: .MixedRealityces orces aces at time) in:ortteceTanda and CollinsortaleandCT):he'sTa####atsoctinthe-ater/leORBcesq###\")toCSkaytherlockCEyragortceshlekory\"ces #####(TorCES) TaLorteCTS-TaLEorbces|terQCTLORTCS## ###TaLeortcesTaSlewatorcesTQRcesmTA-LORcesaTCTollsate-T4Cails\")\n",
      "TaLTE(cesTQt\"):\n",
      "TaVCRtortcesTQUCTlortcesTXTQL\":TaLourcesTCTwlothesTCTILorts quadrcesTLOCT:\")\n",
      "TCTlesTQT-orcesTCTGortcesTCTLOrcesTCT-QHCT):\n",
      "TCTQ(LortcesTCT)\n",
      "TrOcesTCT:\"TCTIQatesTCT#####TCTMrosTCTIortcesTCTVTCTquireTCT:|TCTortCT\"):TCTIVteTCT:WCT:)TCTQCT:TCT:KCTortcesTCTCTiorcesTCTonicsTCTCTortcesTCTCTörCT\n",
      "\n",
      "Step 26 - Original: [USR] Enjoy your weekend you cunt. \n",
      "Step 26 - Generated: appy, or next’s  Dow. 4th, 5th, 6th, 7th, 2nd, 3rd, 4th, 5th, 2nd, 3rd, 4th, 5th, 24th, 27th, 23rd, 24th, 25th, 30th, 35th, 36th, 24th, 28th, 24th, 22nd, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, 24th, \n",
      "\n",
      "Validation step 27\n",
      "Memory Usage: 20.9% used. 203868.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203868.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 27 - Original: By:  Mature babe with huge juggs Francesca Le gets a cock in her hairy cunt... \n",
      "Step 27 - Generated: orscheG”  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####  ####\n",
      "\n",
      "Step 27 - Original: And if I die on the Russian front, Just burry me in some Russian cunt. \n",
      "Step 27 - Generated: omapanda next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next our next Next our next」Next Next Our Next Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next! Next!\n",
      "\n",
      "Validation step 28\n",
      "Memory Usage: 20.8% used. 203874.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.8% used. 203874.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 28 - Original: Is There Such a thing as a Nigga Booty?? \n",
      "Step 28 - Generated: izes_SIZEKMEETZAPLASKMLYMHORDEVINE 5SGMPLASKMLYMHORDEVINE 2SGMPLASKMLYMHORDEVINE 2SGMPLASKMLYMHORDEVINE 2SGMPLASKMLYMHORDEVINE 2SGMPLASKMLYMHORDEVINE 2SGMPLASKMLYMHORDEVINE 2SGMPLASKMLYMHORDEVINE 2GSGMPLASKMLYMHORDEVINE 2RCHSGMPLASKMLYMHORDEVINE 2SGMPLASKMLYMHORDEVine SGMPLASKMLYMHORDEVINE SGMPLASKMLYMHORDEVINE SGMPLASKMLYMHORDEVINE SGMPLASKMLYMHORDEVINE SGMPLASKMLYMHORDEVine SGMPLASKMLYMHORDEVine SGMPLASKMLYMHORDEVINE SGMPLASKMLYMHORDEVINE SGMPLASKMLYMHORDE\n",
      "\n",
      "Step 28 - Original: Fuck u England u little twat! \n",
      "Step 28 - Generated: -prefix or's time next quarter��next'24/4@## 5**43:29(###2123####40-412026%3123@1372530+4511221...\n",
      "2704!CHor the �10in42266209maxmin9128armon2623006|44AQKCI84R34ASKHOMAR19QILTA#3559Tightchin a14V01MH@CTM040GHT\"Hamilton 46410091043COVID38ICT421938VG51@05[48]12VI�****NISVT**(#####28002SMAXMINATIP269340996@3624708hEP274560406@284460021310HM261270743843260265@293246248268254341450243256@ECT223041493245513@234281053259214316@29024422168596@new203542502713189426324256@37294020251083324285542216375@China202842189416236292241330356@last143210235042163222266283593187342253AV390654276240312545656@Ict@vanother533204262\n",
      "\n",
      "Validation step 29\n",
      "Memory Usage: 20.8% used. 203889.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.8% used. 203889.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 29 - Original: [USR] [USR] me also tweeting about me wanted her to call you a cunt \n",
      "Step 29 - Generated:  fung second or new 1 460.2 traffic orking tomorrow. | sandliser (5)1 traff or [1 busy (3)1)1 in the today orbing (4)1 12 (1)1(02)1 (26).1 (21)1 2 (1)1 (2 1 (11)1 2 (1)1 2 (1 2 (1)1 2 (1)1 2 (1)1 2 (1)1 2 (1 2 (1)1 2 (1)1 2 (1)1 2 (1)1 2 (1)1 2 (1)1 2 (1 2 (1)1 2 (1)1 2 (1)1 2 (1 2 (1)1 2 (1)1 2 (1 2 (1)1 2 (1)1 2 (1 2 (1)1 2 (1)1 2 (1 2 (1)1 2 (1 2 (1)1 2 (1 2 (1)1 2 (1)1\n",
      "\n",
      "Step 29 - Original: [USR] Nigger you won't last a week. Who is you fooling? Ke nwa le planks right now, mampara week \n",
      "Step 29 - Generated: usch or the or next or myking or your K or myk or myky or my or my or my or my or my or my or my or my or my or my or my or my or my or my or my or my or my or my or or my or or  or my or ( or my or or my or or ( or my or or ( or my or / or or ( or or or / or or ( or or) or or ( or or/ or or ( or or\")\n",
      " or or ( or or our or or ( or)\n",
      " or or (or or  or or( or) or or (or or/ or or or (or or/ or or or/ or or or/ or or or/ or or or/ or or or/ or or or or/ or or or/ or or or/ or or/ or or or/ or or/ or or/ or or/ or or/ or or/ or or/ or or/ or or/ or or/ or or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or\n",
      "\n",
      "Validation step 30\n",
      "Memory Usage: 20.8% used. 203893.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.8% used. 203893.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 30 - Original: Nigga where is graduation 😫 \n",
      "Step 30 - Generated: ://[USR]  is a faggot  #faggot  #faggot  #faggots  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggots  #faggot  #faggot  #faggots  #faggot  #faggot  #faggots  #faggot  #faggot  #faggots  #faggot  #faggot  #faggots  #faggot  #faggots  #faggot  #faggot  #faggots  #faggot  #faggot  #faggots  #faggot  #faggot  #faggots  #faggot  #faggot\n",
      "\n",
      "Step 30 - Original: And The Twat Of The Day Is! \n",
      "Step 30 - Generated: ittal'sshaylock or the dayin20223 (20)19:25/26-28.29YD21G2y5/30R42...26VT27Q22/30R32T18/26Gl13S14/26...\n",
      "26R38/26GL12E36Cy43/26Z26/40R32/26Gl13S34/26FT11Sh26/40R32/26G16B24/26R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S26/40R32/26G13S\n",
      "\n",
      "Validation step 31\n",
      "Memory Usage: 20.8% used. 203882.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.8% used. 203882.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 31 - Original: Like black's faggot sissy bitch ass? \n",
      "Step 31 - Generated: .swingkololuka Glz Shunk Dr K:Shung Monizes Coct\n",
      "#### 3L2/41(11) 'Co' /5 (12) 'Kilmana**: **Cip** \\\\ 45-41 (13) 'Clina' /15 or 16 Qollivolia 21 Sepsir, 14\" 30 Monapsire, 18\" 35 Monday, 23 Crila, 24 32 Monoring, 43 38\" 33\" 36 40\" 50\" 53\" 34\" 46\" 48\" 51\" 132\" 140\" 122\" 124\" 255\" 26\" 25\" 120\" 121\" 130\" 140\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\" 120\"\n",
      "\n",
      "Step 31 - Original: Ever met a nigga you love and hate at the same damn time? This that nigga 😂😂😂 [USR] \n",
      "Step 31 - Generated: uschwindshyQLbictShisha 2/5**G4(50 **SQLple**\n",
      "@20B**(50)\n",
      "####**:3 (50)**Fbic(**50**SHglicc**\n",
      "**40DGLQ**H**Mbi**50**ShGl**30**45**###Digina****50****BetICS**50**Weql**50**MQ**50**G**50****B**50**0****50**G**50****T**50**G**50**S****50**G**50******50**G**50****50**G**50******50**G**50****50**G**50****50**G**50****50**G**50****50**G**50**G**50****50**G**50**G**50**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**50**G**\n",
      "\n",
      "Validation step 32\n",
      "Memory Usage: 20.8% used. 203874.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.8% used. 203874.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 32 - Original: this cunt needs to get his ass whooped badly. \n",
      "Step 32 - Generated:  cubes[2] the day of the 28th. 3/02.\n",
      " 1/20. 4/22. 5/21. 6/27. 7/28. 8/29. 9/36. 10/43. 11/56. 12/62. 13/65. 14/72. 15/75. 16/84. 17/91. 18/96. 19/100. 20/120. 21/122. 22/124. 23/125. 24/126. 25/127. 26/128. 27/129. 28/130. 29/131. 30/132. 31/134. 32/135. 33/136. 34/137. 35/138. 36/139. 37/140. 38/142. 39/143. 40/144. 41/145. 42/146. 43/147. 44/148. 45/149. 46/150. 47/151. 48/152. 49/153. 50/\n",
      "\n",
      "Step 32 - Original: [USR] [USR] [USR] NIGGER FAGGOT 🗣 \n",
      "Step 32 - Generated: /internalishaasca kit's orishing/ask Moeshqliska GovishingASK:ShkingsekQLilashes a Rowishshuttle ##ILregathe211millinghandash053Kailplaskluru#######or sandiplas Kashettuka ###21G'lapsapl, #####and the KipLiver(20kishingappy formed Guring##Hra1wlio Regaining-folishing�Syna-RegatingAsMo ILREGingrESHqushing ###\n",
      "36NH_Regifying24A**:Bi-kitLiveData460Tivating[MILES04Klder■27Yctoda Apprentice inflishing19Kavishing29FebLKavin13CCHKBroker FebLMRAICTapavitJutikitVolishing16GLSHirlag_fmt17KCltashaRussMLer14af行[]15FK26AQrealizing​​Betintira52NVLuchapy Russell \n",
      "\n",
      "Validation step 33\n",
      "Memory Usage: 20.9% used. 203802.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203802.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 33 - Original: Acceptable shoes for Archie’s school. So they want me to send my lad to school[USR] like a twat ✊🏻💦💦s \n",
      "Step 33 - Generated:  dismissed tomorrow or ask..........................................................................................................................................................................................................................................................\n",
      "\n",
      "Step 33 - Original: Sold! This vid is on fire! Worship BBC With Me Faggot  #MVSales #ManyVids \n",
      "Step 33 - Generated: pes, 2. Doub. 20. 3. 25. 27. 1. 5. 26. 12. 22. 31. 2. 13. 33. 4. 2. 23. 32. 3. 43. 2. 3. 2. 30. 2. 3. 2. 2. 3. 2. 2. 3. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2\n",
      "\n",
      "Validation step 34\n",
      "Memory Usage: 20.9% used. 203835.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203835.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 34 - Original: New vid: Cunt Munching Milfs #02  \n",
      "Step 34 - Generated: askkol was a: Next is the data - 1m ### Volics and K ##:11 /2nd 'K\"######513-1140/5 #####G\":13/14\"\n",
      "Next\n",
      "12/1153​1201**:20/50 ####52**\n",
      "####23(22)25\"\n",
      "####31”\n",
      "32\"\n",
      "42\"\n",
      "132 43\"\n",
      "62\"\n",
      "48 124\"\n",
      "142\"\n",
      "130 112\"\n",
      "45 53\"\n",
      "46 123\"\n",
      "36 113\"\n",
      "41 116\"\n",
      "372\"\n",
      "122\"\n",
      "38 131\"\n",
      "126 136\"\n",
      "33 51 132\"\n",
      "37 202\"\n",
      "40 143 30 121 132 34 341 132 42 132 342 163 32 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132 42 132 132\n",
      "\n",
      "Step 34 - Original: Everything yo nigga want I’m like a walking wishlist \n",
      "Step 34 - Generated: uschishapectuka Bana Glpes. Shanda Bank 5 Q or Job-Bana Schor 2/4 Jr or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or 2/4 J or\n",
      "\n",
      "Validation step 35\n",
      "Memory Usage: 20.9% used. 203852.72MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203852.72MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 35 - Original: fuck off you stupid turkey cunt \n",
      "Step 35 - Generated: asca-pekhiba ativa-Disha Kokitta-SANKlish/Feb...Q-Finema-WeSandShNKFD-DeSankGod-YouK-QNextDika-Fehttps-DeepSumL-BchetOK-202Sup-Mo-ML-Che-DeSolog-Ask-Min-CoF-FT-SanaG-Book-Turn-AsY-Auth-Match-Dev-Soli-BoC-Home-Dec-Ass-Admin-Hash-Book-Global-Mile-Nov-Ok-Good-Mode-Booking-Java-March-Bank-Desok-Log-Сil-SQL-Last-Be-Ex-ET-SIL-Scala-DB-Next-Show-Ascii-Newton-Oct-Jaz-May-June-Sec-Backend-Data-Next-15-SSL-Scan-Web-Next-25-Next-Next-23-SL-So-Next-20-Next-22-Next-16-If- Feb-NS-Next-25-Next-25-Next-25-Next-25-Next-25-Next-25-Next-25-Next-25-Next-25-Next-25-Next-25\n",
      "\n",
      "Step 35 - Original: [USR] More fake news!!!!!!  Liar and enemy of the people   #BuildTheWall \n",
      "Step 35 - Generated:  Bro group next\n",
      " Next app. we next future of un\"\n",
      " g\n",
      "“##CI”g\"####of”\n",
      "un\".{}\n",
      "###sci,##\n",
      "​vlg[/##ci,}.}\n",
      "##ficc,##is\"\"\"\n",
      "veral,etc.\"\n",
      "##msec,##as�perl (t)\n",
      "##kict,##asqil(##T)\"\"\"##KIPit,##asqli(##T)\"\"\"\n",
      "##G-IV,##C-Nexti,##CC-NextI,##SOTIL,##capps,##ICC,##PCI,##SC ^{next}\"},{\"%SEC,\",\"tc\", etc\"}‍**Q**\n",
      "##Hsc,~SI,##CT,##IT,##FS,##IC,##RC,##IS,##CV,##VLC,##CR,##ICS,##PQC,##SL,##Gss,##L,##T,##M,##5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5\n",
      "\n",
      "Validation step 36\n",
      "Memory Usage: 20.9% used. 203858.78MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203858.78MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 36 - Original: [USR] Not to you but those people who complain about SJW in a game who can kill kkk members \n",
      "Step 36 - Generated:  Domestic & Graham ->ason Assasca Golf May->sek[^σφα Golimes Kagriendshase Gulf&ict Monroe Bonse Jackson\n",
      "\n",
      "ses App:('Ygn\\gangfs &'May Yok Gang(?S)<?as Monitoring Sea &mana Sec(&Shong <?sct-&FreICT,grsobild(['BaaS')--Gshcs(-Visson[sict) ['Assict)\n",
      "\n",
      "Pronte(Yndasaki([Grict)!)goda &c(<Sec)!![Rasing Games/Asma(etc))<Sea</Tgtamas Game(yadict)(\"easonicer [-ICC](Simcy)\\\\wagna(gm)<>Scinas(Gu)##IMG Visason(Mce)<<(Gyna)--Kasc(eft)\n",
      "![shonmalizing [G\">)&Appshining(!Local)gonesy(:Mad)asz::(**Blshason \\|宗resa [('Mgf)[:Bonict<freasons Climason) [&Green]####tc Shops&Mason�ison(Binshima)['C']Monict\"oréf(Izgherson'skice ('Bound\")[-fina&Egs('Money')[](Aprict)[])Gameason('bict'flask\n",
      "\n",
      "Step 36 - Original: Cunt should get castrated \n",
      "Step 36 - Generated: occo2, 30, 40, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 23, 34, 35, 42, 59, 60, 1,2,3,4,5,6,7,8,10,22,43,0,1,2,3,4,5,6,7,8,10,22,43,0,1,2,3,4,5,6,7,8,10,22,43,0,1,2,3,4,5,6,7,8,10,22,43,0,1,2,3,4,5,6,7,8,10,22,43,0,1,2,3,4,5,6,7,8,10,22,43,0,1,2,3,4,5,6,7,8,10,22,43,0,1,2,3,4,5,6,7,8,10,22,43,0,1,2,3\n",
      "\n",
      "Validation step 37\n",
      "Memory Usage: 20.9% used. 203854.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203854.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 37 - Original: 💭 Islam Denounces Terrorism. ISLAM is the Religion of the PEACE and SECURİTY 💭  \n",
      "Step 37 - Generated: ill time hand at bed 30 / 535/20 shu\" 38/51\" 33/41\" 13/36\" 15/37\" 12/43\" 16/53\" 31/48\" 52/63\" 47/50\" 32/54\" 23/49\" 22/51\" 40/51\" 60/53\" 24/51\" 29/51\" 58/51\" 30/51\" 45/52\" 30/52\" 55/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\" 30/52\n",
      "\n",
      "Step 37 - Original: “They’ll hollow me out and eat my cunt on a plate.”  [USR] is the most fucked up movie I’ve ever seen. \n",
      "Step 37 - Generated:  mach or to the time of to for to →  and Nakuvice to it, to hand. to a Qub's new to the reason to ask to itsory to the cause of to the best of to the reasons to be short to the next to the games to the as to the present to the '→ you to the quarter to the aid to the & to the latter to the partner to the influence to the later to the fashion to the good to the quart to the habit to the query to the doubt to the quick to the perfect to the degree to the qct to the person to the first to the real to the peak to the weather to the no to thecht to the poll to the game to the bet to thequire to the few to the more to the last to the in the case to the better to the gap to the one to the pleasure to the group to the fast to the at the time to the less to the water to the advantage to the fleeting to the optimal to the further to the high to the flying to the change to the face to the feeling to the drill to the moisture to the fire to the living to the gain to the on to the playing to the without to the opportunity to thele to the focus to\n",
      "\n",
      "Validation step 38\n",
      "Memory Usage: 20.9% used. 203860.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203860.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 38 - Original: Billy: Hat down, cross town/ livin like a rockstar  My faggot ass: \n",
      "Step 38 - Generated: ットRigNGyung/...\n",
      "'Ngiv/or iva/ Coff or Giv/ 'Nwr/...! & Na/ E –& Nwar/ 'Na/ 'D.' /'m' & 'Nw/ 'N'. 'nuch/ 'l' & 'Niver/ 'I', 'Nw/ 'B' & 'Nwe/ 't' & 'Nw/ 'A' & 'Nw/'s' & 'Nw/ 'L' & 'Nw/ 'E' & 'Nw/ 'Nw/ 'St' & 'Nw/ 'G' & 'Nw/ 'S' & 'Nw/ 'C' & 'Nw/ 'K' & 'Nw/ 'Sand' & 'Nw/ 'M' & 'Nw/ 'F' & 'Nw/ 'An' & 'Nw/ 'T' & 'Nw/ 'k' & 'Nw/ 'H' & 'Nw/ 'Green/ 'Nw' & 'Nw/ 'As' & 'Nw/ 'And' 'Nw/ 'Cor' & 'Nw/\n",
      "\n",
      "Step 38 - Original: Big dyke energy \n",
      "Step 38 - Generated: ascaatha�'sking Kesh orbital';#### prep_book':ash\": ##/###...\n",
      "####'plex('Krivaslek_Npect_Mlm_aisha**(####')olton Bed||(： \n",
      "####NDQL_mivre**:####(\"Kinfyna_(####:####(IV�uming':'####'(####):####(Minزة'lší Booker_Lira【####）（####():I'll\";####(Kintčila**/\n",
      "####();####KC_Bina […]...\n",
      " #########(**append://little(Cmono']['####\")(####:\"least_body):(####\"(####\")Lins'/####BK(Landra');':Binding_Text([]】【####'.Body)/(####((M\"):####\":[\"binding(Bvk')####|)\n",
      "####('Asek\")\n",
      "####('fortable\":\"IMGapl(Nkol_Presa CHAPTER'I'm\u001a####)(Shce::Hamilton-####(Porcek volt-binding[Molia_Cappa_lado_fmt／Bindchin'MaliceLEC\"\"\"\n",
      "plan371ASK__####'Livacoal\"MathaACL\":{\"Tratha_Klimada;(####').Bk Collins_Falog&Munt�izace_Satha_INT'A##\n",
      "':#########Asuka(I_new_faceCTLIOC_satha_TutASCuring Coal 출장BO\n",
      "\n",
      "Validation step 39\n",
      "Memory Usage: 20.9% used. 203861.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 20.9% used. 203861.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 39 - Original: Dropped a nigga off on 2k so hard they booted me 😂🤦🏽‍♂️..... \n",
      "Step 39 - Generated:  terraaskoda bro moon ->QL320�###Ai/1.4 �Middle@24/12->...\n",
      "Next online:25/22**Kiranda/Q**\n",
      "###AI/42(21)\n",
      "##K2​YAGIC/41 Nak 23/11 (24) ##Ki #####10y####Lock or later (22)�13,21�27**(24)##K2**Qi(Yicc43/22**36(22)****\n",
      "**k2**Loming(22) **Toring(**22) 24/12(22)##K2(22)45(22)**,22(22)**44(22)##K2(22)#####22(22)##K2(22)26(22)**32(22))**22(22)46(22)****K2(22)54(22)**)22(22)**24(22)22(22)14(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22)22(22\n",
      "\n",
      "Step 39 - Original: THIS IS NOT A GAME  BE THE DIFFERENCE  #MAGA #WalkAway #BuildTheWall #RedWave [USR] \n",
      "Step 39 - Generated:  {}: next's the handfic secondhand 2ndict 2\n",
      "VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1VPO 1\n",
      "\n",
      "Validation step 40\n",
      "Memory Usage: 21.9% used. 201252.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 21.9% used. 201252.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 40 - Original: bde (big dyke energy) \n",
      "Step 40 - Generated: ivatingizingolia'saskanda Kagillafrilling'kmluka/Qlingrak**\n",
      "&ndlwindkul orakingFrillor'\n",
      "Kzldq\"\n",
      "1/2Q\"**4IQt\n",
      "Jlaqa\"]\n",
      "Middlehand AprplivingMrNext &jQLrainingLwinningRigIdja-KilivngKC]\n",
      "-12Janqlum/Kolurringall**(3']\n",
      "Noring KC5Tidal 1/2ndinjustful@debugKitintlowblaringGuruskiMokowlandingFillschapsolskit&MwcluringAprflnumNGtaka�volishingDrshnewperfectingIDlmissnoffundataka**\n",
      "ZmruchliningPlakikmal KitbdratingSireskuminDigaplkingBlimeskourthe!WinfllibmasksomeonforrealismWikitliterfmnextIshebooklogidmkurehighminNewkinnbkurat'MmmachursimsekpingHollalikuMottingJoblongKillingPowerhundredappend/MissingPerfectomingInvisibleTrillingBuddyMaski,owingKurllivingKodiwatfluksoundapplinbilcturingMissingPowlivirclsunn\n",
      "\n",
      "Step 40 - Original: REALLY MISS MY NIGGA ON DOGS. \n",
      "Step 40 - Generated: usch anda (y**\n",
      "\n",
      "#### the\n",
      "\n",
      "####  the\n",
      "\n",
      "   the\n",
      "\n",
      "  “and”  “and”  “and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"and\"  \"\n",
      "\n",
      "Validation step 41\n",
      "Memory Usage: 21.8% used. 201340.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 21.8% used. 201340.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 41 - Original: Birdland – Rock ‘n’ Roll Nigger  \n",
      "Step 41 - Generated: isherotteszorkodingendorisha,**:'.shorisk':','.**'**:**,'.**'.**:'.****:'.**'.####:'.'.**'.**'.'.**:'.**:'.**'.**:'.**'.**:'.'.**:'.'.**'.**:'.'.**'.'.**:'.'.**'.**:'.'.'.'.**:'.'.'.'.'.'.**:'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.**('.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.'.','.'.'.'.'.'.'.'.'.'.'.','.'.'.'.'.','.'.'.'.'.','.'.'.'.'.'.'.'.'.'.'.'.'.'.'.','.'.'.'.'.'.**:'.','.'.'.'.'.','.'.'.','.'.**:'.'.'.'.'.','.'.'.'.','.'.'.'.','.'.'.','.'.'.'.','.'.'.','.'.**:'.','.\n",
      "\n",
      "Step 41 - Original: Full Video:  Busty babe in shorts Angelina Valentine spreading her twat and butthole... \n",
      "Step 41 - Generated: /internal2ndID'.3.5/1':Kbl'\n",
      "GBL 4'11\n",
      "0:21PLb/Ky\"\n",
      "Vol-Isol,10M**\n",
      "LIV/Bpl​NFK'\n",
      "Trbliv/M}\n",
      "BIL'\n",
      "VOL\"\"\"\n",
      "RIGLM”\n",
      "SOLLVK'\n",
      "TNGIB'\n",
      "OVID'\n",
      "Blask@WLLIC/\n",
      "MonlVL!YGL/L13Bi::121JQML/HZ\"\n",
      "DOR\"\n",
      "FHL\"\n",
      "Med\"\n",
      "H\"\n",
      "L\"\n",
      "E\"\n",
      "My!\"\n",
      "12\"23\"\n",
      "L\"\n",
      "IQR\"\n",
      "EL\"\n",
      "Me\"\n",
      "L\"\n",
      "Chicc\"\n",
      "L\"\n",
      "Plic\"\n",
      "L\":A\"\n",
      "Cub\"\n",
      "Betis(120)\n",
      "L\"\n",
      "Frilling(N)\n",
      "####\"\n",
      "Idz\"\n",
      "Liva\"\n",
      "Menfic(L) 20\"\n",
      "CTycles()\n",
      "Lid(\"14\")\n",
      "Lig('')\n",
      "Orbic(R)\n",
      "Licit(O)\n",
      "Naillin(M)\n",
      "Lgict(D)\n",
      "Int'l'\n",
      "PILL(K)\n",
      "Luvil(S)\n",
      "No\"\n",
      "Ling\"\n",
      "Neuk(V)\n",
      "Luka(l)\n",
      "CHNKules(E)\n",
      "Lion(G)\n",
      "Ltic(I)\n",
      "LUVin(\n",
      "Sold)\n",
      "Luz(T)\n",
      "Lip(J)\n",
      "Lunch&Ol]\n",
      "LUC\n",
      "\n",
      "Validation step 42\n",
      "Memory Usage: 21.9% used. 201239.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 21.9% used. 201239.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 42 - Original: 2 words, RETARD GAME \n",
      "Step 42 - Generated: уки volt-bina'**: Jin-azilim: wezzo (bina):**\n",
      "min-ola**: Wezzo (tina):**Wezzo**: 4ma**: [Tina]: Cranda**:[/AQ**:VTina**:Gin':2**:Vina**:Cranda**:VTina**:CRana**:3ma**:[VTina**:Cranda**:VTina**:Cranda**:VTina**:Crata**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crada**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara**:VTina**:Crara\n",
      "\n",
      "Step 42 - Original: When i see that horny Sissy NINA💗[USR] i just want to fuck her cunt😈🥒💦💦💘💘 \n",
      "Step 42 - Generated: ://[USR] #BuildTheWall  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR\n",
      "\n",
      "Validation step 43\n",
      "Memory Usage: 21.9% used. 201056.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 21.9% used. 201056.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 43 - Original: When You've Just Had Enough Of That Cocky Twat! \n",
      "Step 43 - Generated: anda 20 next many was going. 30balling. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30billow. 30\n",
      "\n",
      "Step 43 - Original: [USR] it sounds like one of those games like Cunt Wars \n",
      "Step 43 - Generated: 201/,.. was.\" 1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\"1\n",
      "\n",
      "Validation step 44\n",
      "Memory Usage: 22.0% used. 200822.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.0% used. 200822.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 44 - Original: My fat #pussy can you imagine sliding deep inside my #cunt \n",
      "Step 44 - Generated:  instrumentation Lim Shado Mo: /Lilim (S/ \"Lilmon (B/\":Shol/ 'Lick/\":Shol/\":Shod/\":Shlim/\":Shm/\":Shim/\":Shin/\":Shk/\":Shlm/\":Shal/\":Shl/\":Shma/\":Shlim/\":ShM/\":Shil/\":ShLM/\":Shil/\":ShMo/\":Shil/\":Shlim/\":Shil/\":ShMos/\":Shlim/\":Shil/\":Shil/\":Shil/\":Shmil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\":Shil/\n",
      "\n",
      "Step 44 - Original: I’m tweeting complete gold rn . And every dyke is sound asleep rn . FINE ILL SHUT UP \n",
      "Step 44 - Generated: asca/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/anda/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada/mana/ada\n",
      "\n",
      "Validation step 45\n",
      "Memory Usage: 22.1% used. 200617.55MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.1% used. 200617.55MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 45 - Original: Happy ramadan day ,you unproducing muzzie fucks! ⛽➕🚬➕🔥=🕋🕌 \n",
      "Step 45 - Generated: orex Job/23/4/24/5/2/22/033/25/04/19/122/9/30/3/21/20/23/5/27/33/022/29/09/32/23/45/23/24/22/13/23/25/14/12/15/22/23/40/23/23/42/24/22/23/23/25/22/24/22/23/24/22/25/22/23/24/22/24/23/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/23/24/22/24/22/23/24/22/24/22/23/24/22/23/24/22/24/22/23/24/22/24/22/23/24/22/24/23/24/22/24/22/24/\n",
      "\n",
      "Step 45 - Original: I fucking hate internet dating... Seriously what in the actual fuck... What a bigot cunt. \n",
      "Step 45 - Generated: ://[USR] [USR] [USR] [USR] Nigga said  “I don’t even know you”  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 46\n",
      "Memory Usage: 22.2% used. 200498.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.2% used. 200502.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 46 - Original: Nigga said he’ll eat the skin off my pussy \n",
      "Step 46 - Generated: 445ukaqlmonishaiglia/Aq/11Qmplsek/QMonlaAQ/1Kzmalkol@5/11/4/6/10/12/20/2/3/45/06/11/41/11/15/11/36/37/42/65/11/53/27/26/56/6/7/43/23/71/50/13/66/55/44/6/72/46/6/54/40/6/16/6/38/67/6/120/6/6/6/47/6/6/48/6/6/6/6/6/6/6/6/64/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/6/\n",
      "\n",
      "Step 46 - Original: An apple a day keeps every cunt away ya fecking hingoot \n",
      "Step 46 - Generated: ask Ls  N-  is)  has). N-  ##)\n",
      ":  Kl”                                                                                                                                                                                                                                       \n",
      "\n",
      "Validation step 47\n",
      "Memory Usage: 22.1% used. 200548.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.1% used. 200548.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 47 - Original: [USR] You better back up off my momma nigger \n",
      "Step 47 - Generated: ittalette Bet or App/ESay.\" Shoring ( Sav./Bu Glisha.\")\n",
      "orsche(Wing://ASC/Fanda\")Shishing(\"Bimon\"\")\n",
      "Resaying(\"Beshette\")Sandbu(OTBM)\"Osh /FWyresa(OFT@Gbit\")Rội(Sbete\")SotBit-Owry(ETAS)Little(Eyr\"Asbuf\"TEAPP-ISHERP(KITTLE\")EbarBIT\"SAY'SGOYbyrApp/Say'sLog(LTEM'Okol/Guru-Ft...\"LHMROLOitte-Kabin(Faybrush\"ALET-Say'sFaylog(Iy'yMAYER'LLOGIy'\"TaFKayology/E-BaskinFSay'Iy'/Joblm-TayFMayishibia砂\"Iy/TayMHzAtteILJy/Maybury(Tay-MaybugheCyDbay'TayMaybuchette/Kayfmaybagmater\"TayMayay_FMayjaydba-/TayMKayaybmayerTypaylgayayBufkayaymate(GyvytilsovGlomba/Iy/Jy'KrappsayLMayayayazumayaySqmalayayayayay\n",
      "\n",
      "Step 47 - Original: #NOWPLAYING B.o.B - NAGA - Good Nigger Sticker Circle Radio  \n",
      "Step 47 - Generated:  Gall Yard, 1SEC,3CC,2ASC,2E,3CC,10,11,13,1,2,3,1,12,13,1,2,3,4,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,1,2,3,\n",
      "\n",
      "Validation step 48\n",
      "Memory Usage: 22.3% used. 200256.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.3% used. 200256.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 48 - Original: #podcast #comedy #talkshow #hillbilly    Something different for your ear holes. \n",
      "Step 48 - Generated:  Snowden ор Zika, Kaplan: → Naknd| Orac, Kna,°,↑,←,Na,###,Or,„,,”,“,\"\",\"\n",
      ",\":”\n",
      ",\"\"\"\n",
      ",​\n",
      "\"\",\"\"\n",
      ",\">,We‎,citnalson,¶,CTalson,ctolson,ctlson,ctoning,cting,ctning,ctin,ctling,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ctu,ct\n",
      "\n",
      "Step 48 - Original: That's yo new nigga. \n",
      "Step 48 - Generated:  clip 1 | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "\n",
      "Validation step 49\n",
      "Memory Usage: 22.5% used. 199618.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.5% used. 199618.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 49 - Original: Big DYKE energy \n",
      "Step 49 - Generated: opothe/1...\n",
      "Kla/20h:10/2/100/apl/12/22/3/21/2/221/11/2/25/5/030/022/####/ 1/2/021/1/15/2/204/220/24/1/120/50/1/2/020/1/200/1/02/202/121/205/14/2/220/1/040/261/2/12/1/26/112/400/221/1/23/2/220/1/16/000/1/2/220/1/40/1/2/220/312/1/2/122/1/2/220/1/12/220/1/2/212/1/220/1/2/053/1/220/12/1/023/220/1/2/13/220/12/1/222/12/220/1/2/022/220/1/2/220/1/2/220/1/22/220/12/220/1/2/221/1/12/220/12\n",
      "\n",
      "Step 49 - Original: [USR] [USR] Coming from the person who has the word 'retarded' in her bio... \n",
      "Step 49 - Generated: atin-/ king Q- / ( ).) / 5/ 20/ 4/ 1/ 6/ 22/ 06/ 22/ 8/ 23/ 22/ 10/ 22/ 24/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22/ 22\n",
      "\n",
      "Validation step 50\n",
      "Memory Usage: 22.2% used. 200356.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.2% used. 200356.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 50 - Original: A nigga going threw a lot rn . \n",
      "Step 50 - Generated: occoisserle AQMKYZKBOCKGKAYKMLYGKASTBNAICTMYYGKAILYGKATGKLOCKGKASLMYGKAGICKASKGTAIDNGKAR GANGTILLGKIMYGKMCHLEVILGQGKINTGKTHERNMICSLEVINDAISLINGDODLKAPSIMEGKAPPCTOFFGKATTGCKTMPSILOGTDEVINSOFKGTSOMGCIHCKTAKGCKTIPSMDGIPTENKSOPMGAOGKTOCSSANDGTTCKTICC)GKiSandGkiTCKTKGCKTIZNKGLTCKT2GKC|GChatGKKBKGCKTICCKTCKTICKGCKTCKGCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKTCKT\n",
      "\n",
      "Step 50 - Original: Y’all gotta see this! My nigga eating peas. Freakin peassssss for a snack \n",
      "Step 50 - Generated: 48.  \\\\  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ##  ## \n",
      "\n",
      "Validation step 51\n",
      "Memory Usage: 22.8% used. 198729.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198729.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 51 - Original: [USR] You never go full retard! \n",
      "Step 51 - Generated:  or V\n",
      " W  Un\n",
      " T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un  T  Un \n",
      "\n",
      "Step 51 - Original: [USR] It was just this one dyke Ctfu..she was noice. She just dress like a nigga &amp; shit \n",
      "Step 51 - Generated: 化:sek- #### **#### the sold:  ###:###:5. Sold.: T**: Cok?12:####:Co SOLD:####:�:20:####:####:Co1:####:The: ##:####:CoM:17:####:Co]####:10:####:Co:####:22:####:Co:####:21:####:Co:12:####:##:12:####:Co:####:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:Co:12:####:\n",
      "\n",
      "Validation step 52\n",
      "Memory Usage: 23.0% used. 198432.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 23.0% used. 198432.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 52 - Original: [USR] [USR] [USR] I’m too young? U look like ur 14. Hold this L faggot. 23-21 \n",
      "Step 52 - Generated: uschhand Bindanda or next we Binding orshiga or 2022.1/205 or 223-binding or 234 or 25Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or 25 Binding or 234 or \n",
      "\n",
      "Step 52 - Original: [USR] Will wonders never cease? Playing the race AND the woman card? Sad. \n",
      "Step 52 - Generated: .swing Bonanda Korea lastorsekondoloraskorbandonobiq')\n",
      "MonoliajsoLorvolK\u0003GoShorVolmonQoheMiddleSorundandYQLoratingascaNoborlik/Qomorizinla':KorandoJle#####\n",
      "orzinuka,NKorbelorasking@)\n",
      "::Hogorilz'Godsonkailor‬\n",
      "Jacksonlora'\n",
      "Glojo(Q)')Molictorice**BaleoforwascorizingAjaxcoring:OjorQRSceMoctorizaceGlorindoIqlorbookshapsimoDorj','',EGLunsoFKorbilaterGalnojorsightCillinJoborioricMinorizuDborjCoInsorcyNa']\n",
      "WorjqimorzaChorningPoljorinoCTorjminoricsorjomoPoningSurcaILorjinnorZorjikinsorjunsorjolsmoryhcloricitasOMorjintlamorjaperbassominjocrinalisomyjodorjuncseMNorjilocatinyMHorjginjcurinjfsmana\n",
      "\n",
      "Validation step 53\n",
      "Memory Usage: 22.8% used. 198771.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198771.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 53 - Original: When you finally see your chance to half that showboating twat \n",
      "Step 53 - Generated: ittal's. time row, the second to.\n",
      "25/asking or 13-202 was sold aking or 19:11 Mo:-20 &12sh (202 Kettasz '24.:22 Wishing As -14:23 ### [21:16-sask King had as'30 asking or 20\"### Lakes in blossom. COVID #####10-fle Glashor ##15.26-Fas Covid Showing As |Sing bet asked formed sand flow data. Next Row **18.265-Kip.\" Real qesh Go ask Yuch was forming new wlock. \"Kow.\"\n",
      "#### real Gaining or 13:02\"\n",
      "Row Qales shoring at best capacity. The next formation of Collins. Chung malw shore for reasons. 13:29. we'd join similar Qatar. HASC or question. Malq. easy hand with AI(28.05**\":52.33 ROWAs developing Mon Gallup. Easy. tow. Echt Golf:. I'm dispatin Sav from glilasca'. We can't launch few tap. Trow required my face query. Apple. FLE. Blochet match. To offer it. Trot. Mamma. Bet you klop. Connect Match. A\n",
      "\n",
      "Step 53 - Original: [USR] [USR] Enjoy the projects? Dude must be retarded. Should have realized this by the original post \n",
      "Step 53 - Generated: ประส or in the or/ning text or/ Hughes orisha (or/). or/ orade  or/ or: or/ or/ or of/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/\n",
      "\n",
      "Validation step 54\n",
      "Memory Usage: 22.9% used. 198653.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198653.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 54 - Original: The heavy rains in Texas made my back all wet. I guess you can say I have a wetback now \n",
      "Step 54 - Generated: ungice\\\\.#####/̀occmin/Fanzffcere####/Gisin/fascaMin/fricapecil/93inho/91/30/83/94/26/29/33/30/31/32/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/\n",
      "\n",
      "Step 54 - Original: [USR] Nigger \n",
      "Step 54 - Generated:  responsible kit orshmonounds'Digonas\"Younding's/y\":MonizingKitzin/Monoundicit/-####/s/Clor--Mz~Landle\"--BeyVLashmdl-->Kall/Qmaylik/munginBoundmnics/fivrivalism::Garden---Mz/Landmann[^S/neathering@Weather\"MZlebodyjin/bowshedOUNDmanillimonations [--\"\n",
      "Shymlogomwithomatic/qandaiccificialization->Preivals (;;\")likeAsundry/tendandoFriseunce[-bindolimsoundworkneighborboundmonoableViewControllerPrediceNeatherndivebanderboltissoctilecyticswintercloudmoreatshndliscalwivericillinqailmonnevericationQyravagePeyerlamaheyvkilmenoryandas'lurinekigerlonezenlauchtsticky weatherWQLumanimoseccivulentryweatheringralizeritis/SunnyVRashevyylewinruncvldunasyVolissyLEwatson/lateralizesVimilarizelekivery',['/--']->WatSnapstickrakleventor/asunderjukes[touqlor--,\",\")\n",
      "\\\\orationToken/aturnstileOccorunate<!--RLECronor../\n",
      "\n",
      "Validation step 55\n",
      "Memory Usage: 22.9% used. 198652.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198652.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 55 - Original: U.S. Troops sent to the Mexican border.   #BuildTheWall   #MAGA \n",
      "Step 55 - Generated:  2\n",
      "3  4  5  4  5  4  5  4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"5\"4\"\n",
      "\n",
      "Step 55 - Original: Requesting permission to call the Prime Minister a psychopathic cunt. \n",
      "Step 55 - Generated: asca\n",
      "\n",
      " Schakinguka Nakym_mono-clone-Kongton-Yucek_Matago/-Flo-MkY-Golatile_[Mci/Kroring[KomobiLEC/MissingClone[].ctlmor[Mickey[EkitClich/Gosmil-Hc-Ticket...\n",
      "'Michi[Hb-Pice')\n",
      "('/Frak[-'/bilange[schib-/',/mic-N'['licit-Bimik-Docrpl'-Ki/Hmlik/Yh[m--'[-']apanese [-/-./-pp-elesc\u0000/-Chip/Las-Beprme-,[//-[,/.-Token-(-/obic/Kil-','/Tektchi[kle-Wiku(Yml[Taltung[Pkip')[t...\n",
      "\n",
      "###-LanzKonsek-Kit-Vinims[ecliving(Ko-ictlivamik-Kotbelive[Dilepattern-Sma-I/DitibbonKebuting-Me(elekasa-ki-Ri- ['-Elevant-binding/a-Reasure- bil-()-y Kit-'.-\\\\.-mizing-Kuru- kit- /-y)ime-(['][so-ku- Kenny-Kreas-lecture-Kon-Phill-K-token-ikit--asheim-Kotic-;-[h\n",
      "\n",
      "Validation step 56\n",
      "Memory Usage: 22.9% used. 198611.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198611.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 56 - Original: [USR] STUPID CUNT!!! \n",
      "Step 56 - Generated: ://[USR] Nigga said 2.5 inches 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 56 - Original: I've no idea what to do. Life going pretty great then some cunt says. Wilson fuck you. And fucks it up. \n",
      "Step 56 - Generated: ประสbing/ / nicelyJob the-ningOfCo/Fngr21nd Drill/ICS '2/-ReginingC/TrinboGtra...\n",
      ":�'####90f):Risingc/FinbeteIdlingIncheReportlizingFct|202m)8/Blivringco/PrineGridereCelling23rdminatUrlwating96/My 24/6/86/92mingEffectiveTrafMin2661/ReilotMax36/834MNGrivingS20/12(3)42/06/22MravinyI/46/32/1239&Me/25/116/27/84/10/38/5/91/30/47/80/48/40/66/28/04/26/08/33/46/88/13/46/72/97/212/43/300/24/93/0/16/98/44/100/54/94/00/62/34/11/46/92/50/45/24/99/46/92/460/52/90/24/102/120/46/60/02/90/46\n",
      "\n",
      "Validation step 57\n",
      "Memory Usage: 22.9% used. 198562.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198562.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 57 - Original: [USR] That he IS a retard, or that trump just called him a retard?    I know I know: \n",
      "Step 57 - Generated:  Operation or\n",
      ":​-##Japan.Or?Work ##-Eil### Work ​Job202​​(K)\n",
      "Work #####‍**Regwork (�)\n",
      "Work ####【Next]\n",
      "Work 202\n",
      "Welek\n",
      "Work21\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202\n",
      "Work 202 Work 202\n",
      "Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work 202 Work \n",
      "\n",
      "Step 57 - Original: REPLAY this Video and Understands Your Nothing But My Favorite Toy I Love to USE😍😆 Cunt. \n",
      "Step 57 - Generated: ung app/ hand  the new or my...\n",
      " Kerry/1 Sand was the new or a like/11 back. Ct/V ​\" face.or binding Park/2\".​\"/5(12' ﻿4/\": ##### or Bush/Iy3/####, B10/ Francis sand/14/ Q9/�new/ \"Sand/20/**(R)'/202/ (r)/#####**22/(\"S\")\"/I 2/�92/\"(or/ Bet/fct/】186/30/APP/21/0/###/142/91/​​((|42/120/\")\n",
      "pl/02/192/##/****\",26/82/ Rl/32/04/28/200/league: O/ Jila/ Tro 1/90/ Lipo/ela/ we/191/ Holland/ Gla/-->/this / 1/ N/ from Sappa/ I Warren in/ this/ May/36/ League Zil Tasa/ 116/ 1/ weather 72/ K/ real Sandy 2/ week/188/ Fiem beti/ Sold E 1/ Cbindt/ M 1/ V/ 2/ Weather\n",
      "\n",
      "Validation step 58\n",
      "Memory Usage: 22.9% used. 198672.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198672.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 58 - Original: I like fakes.                                                          Sorry for the retarded reaction tho #PS4share \n",
      "Step 58 - Generated:  terraanda. the '...\n",
      "'. 1) \"L\", \"N\", \"D\", \"S\", \"D\", \"F\", \"G\", \"B\", \"D\", \"F\", \"G\", \"B\", \"D\", \"F\", \"G\", \"B\", \"D\", \"G\", \"B\", \"G\", \"B\", \"D\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\", \"G\", \"B\n",
      "\n",
      "Step 58 - Original: [USR] Careful for what you wish for nigger \n",
      "Step 58 - Generated:  freezeow areaaskoML/ODML/ML/POML/ML/POML/ML/POML/ML/POML/ML/POML/ML/ML/POML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML/ML\n",
      "\n",
      "Validation step 59\n",
      "Memory Usage: 22.9% used. 198685.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198685.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 59 - Original: [USR] Nah I'm not trying to overdose like this dead faggot did😂 \n",
      "Step 59 - Generated: 払brushbingaykingivr​hinking...\n",
      " ​likorason Greenboappsfriending**: Holland**Workfulson(2L1Z36ymlbappyPreeshry【Glikz20j50le25Bpsrlik####14Mltrshoney**(Pil56\")l70KciworkplayGreenman(Lis)22k26b18mRnewflik(Pazmy\"Manwoman(2L12Z40)**L71Z30basaki24MHapplick(Glikqpreplinky(48L21Z84#####SbookballwbyMKMLPLZ42Alek(Mz32QmhPlzmaY\":L72Z02L28Z4mkFullherAppscrewGlivandgliving((Z52\"))L38Z06bappa688LM****CzinpectlockMoney'smoney(VHlik):Z86Z2L10ZbrkyLockming(2L34Z80Zbenning('Z33')Z68L04Z988L202Z82Zuming(Saming:Z88LplusFMzashping(2L200Z100Zthisbarwatchgreenlocking(BZ08LPOZ996L\":\"Z90Z01\n",
      "\n",
      "Step 59 - Original: Lil nigga from 12 they rooting for tha baby \n",
      "Step 59 - Generated: achi 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202 202\n",
      "\n",
      "Validation step 60\n",
      "Memory Usage: 22.9% used. 198692.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198692.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 60 - Original: this nigga gon be jet black . This feel racist 😂😂 \n",
      "Step 60 - Generated: anda.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "wast 2\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "\n",
      "Step 60 - Original: Triggered Redneck In Texas Yells At Couple Protesting A Confederate Monument!  \n",
      "Step 60 - Generated: lickiccowanda.anzANextTheBörLathaWeSRH20YShCayMJoT29KigaTraWInCT24JurP10RasBetAndAsJ21R25E13GOrF20D05R20-22AppOAtTa19S20R20SandIsQ12R20N23R20B20R20B20R20a20R20B20R20Stimes20R20B20R20B20R20B20R20B20R20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20B20R20\n",
      "\n",
      "Validation step 61\n",
      "Memory Usage: 22.8% used. 198757.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198757.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 61 - Original: Finding new voters every day. #buildthewall #MAGA \n",
      "Step 61 - Generated: 班 V _ benchmark__\": Sh & Mon\"\n",
      "\"\"\"\"\" (**\n",
      ")\n",
      "****\n",
      "**:**\n",
      "******\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "****\n",
      "******\n",
      "******\n",
      "******\n",
      "******\n",
      "******\n",
      "******\n",
      "******\n",
      "\n",
      "\n",
      "Step 61 - Original: my mom just tried to call me a white mans whore when I’m a faggot \n",
      "Step 61 - Generated: achenising 20-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-22-\n",
      "\n",
      "Validation step 62\n",
      "Memory Usage: 22.9% used. 198712.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198712.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 62 - Original: Running from a man that actually likes me to go fuck the nigga that ain’t cuffed me in 3 years \n",
      "Step 62 - Generated: ://[USR] [USR] You think this nigga is smart?  LMAOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
      "\n",
      "Step 62 - Original: Redneck fun. #OYHA #SquirtB \n",
      "Step 62 - Generated: umar11.5/Shill the sand 1.2.3.4.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5\n",
      "\n",
      "Validation step 63\n",
      "Memory Usage: 22.8% used. 198749.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198749.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 63 - Original: Mocha Menage get her black cunt pounded  \n",
      "Step 63 - Generated: 곡Id:”\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\n",
      "\n",
      "Step 63 - Original: what the fuck is wrong with this nigga, get off the car foo u don’t drive \n",
      "Step 63 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR\n",
      "\n",
      "Validation step 64\n",
      "Memory Usage: 22.8% used. 198799.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198799.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 64 - Original: [USR] [USR] am I retarded or sumtin? \n",
      "Step 64 - Generated: �ing Gol'yAQquizkol-Qjuna-Gändkola-13shkr-Qaeda-Mrok-Duringatica-Bddna-Shuk-Kadena-16nd-Hleking-Fmand-Tlek-14bit-15std-eting-Pja-16ct-12maquin-16 ogr-14ict-16ps-Oht-16td-30adr-16t-32mana-16rocket-14min-16ta-14ig-16ot-25gr-16k-11to-14jq-16m-13w-16n-33q-16b-10my-16jo-13h-16uka-14j-16 b-14 '16'38-14:16bn-17or-14bl-16a-14j-16-Sagna-14.16-31ab-14j-16'n-14-16-35r-16@14-16-16-14-34-16-14-16-23-16-14-16-14-16-20-14-16-14-16-14-16-13-14-16-14-16-16-14-16-14-16-14-16\n",
      "\n",
      "Step 64 - Original: I found me a new nigga this year, that knows how to handle this here 😏🤤 \n",
      "Step 64 - Generated: obook/ Kahn Sh 100Sh the bet of all/5/1) really Bet/Hand (Bet/Hub)\n",
      "'Y's Bro/##11/21/2/3/5/2/50/2/51/2/22/59/2/41/2/55/2/12/53/26/2/51/2/13/2/51/2/56/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/2/51/\n",
      "\n",
      "Validation step 65\n",
      "Memory Usage: 22.8% used. 198789.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198789.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 65 - Original: [USR] With no meat NIGGA?! \n",
      "Step 65 - Generated: 化 or or the time today or in a: or tomorrow or- 20 or later or |ping or (curating) or similar or one /with or on weql or or middle of. or best or [...or or/ator or Golf or matching or \\\\ or golf or or\"–and or more or or another or at my or mid or worst or Qatars or or 270 or or not or plus or orbing or or other or for or (or) orbit or or (middle or or) or) or or (or) or or (more or or) or or (or) or or or (or) or or (or or) or or (or or) or or (te or or orates or or” or or or q or or (or) or) or or (or) or or (or) or or (or) or or (or) or or or or (or) or or or (or) or or or (or) or or or (or) or or or (or) or or or (or) or or or (or) or or (or) or or (or) or or (or or or or or) or or or (or or or\n",
      "\n",
      "Step 65 - Original: #NOWPLAYING B.o.B - NAGA - Good Nigger Sticker Circle Radio  \n",
      "Step 65 - Generated:  Gallmon, 2,3,5,10,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2,3,5,1,2\n",
      "\n",
      "Validation step 66\n",
      "Memory Usage: 22.8% used. 198797.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198797.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 66 - Original: [USR] [USR] Well, now we have things that are MORE retarded... \n",
      "Step 66 - Generated: anh/ Q/ × Shib/y'/ at/ K or/ 10/20/25/30/1/05/20/15/22/40/20/20/30/20/10/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/\n",
      "\n",
      "Step 66 - Original: What a noble and valiant twat he is \n",
      "Step 66 - Generated: .swingaskolanda 2. 3 4 5 6 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8 1 2 3 4 5 8\n",
      "\n",
      "Validation step 67\n",
      "Memory Usage: 22.9% used. 198717.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198717.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 67 - Original: “Don’t name call me you sjw npc leftist cuck” \n",
      "Step 67 - Generated: 423soandapassnace�monchceleqmeonce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/plemce/\n",
      "\n",
      "Step 67 - Original: [USR] [USR] [USR] [USR] Awww it's retarded \n",
      "Step 67 - Generated: asca �: �. �: →: �: �:—:�:—):—+:—''.'—':—J: —-Next:—j:—b**—h):—j:—j”!—j:24)25:—j:22)2:3)1:511)12:20)27:30)0S:21)42:13@29)Shil:IV)4:15)23:32)7:14i)28:17Gavii:18)6:19)16:48 26→Cuviv) 47:02)Unviz) 8:10) 55 00:Kfc) 9: 05) 06: 04#) 0: 01) 0: 0 0: 0) 0: 0 0: 0 0: 0 0: 0 0: 0 0: 0 0: 0 0: 0 0: 0 0: 0 0: 0: 0: 0: 0: 0: \n",
      "\n",
      "Validation step 68\n",
      "Memory Usage: 22.8% used. 198747.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198747.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 68 - Original: [USR] Foh, nigger! \n",
      "Step 68 - Generated:  tomorrow or Shapskol, Feb Glsholisha Bet Glshor 20Bet Kinachet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet 20Bet\n",
      "\n",
      "Step 68 - Original: Ok retard. I didn't even get to ask the question. \n",
      "Step 68 - Generated: 346ascaascaandaqlascaanaascalarascaasca1lasa52245220522462242103220234351552247535622524850352259193336228021841311262362254252258062252652246224438224622462252462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622462246224622\n",
      "\n",
      "Validation step 69\n",
      "Memory Usage: 22.8% used. 198733.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198733.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 69 - Original: Nado is retarded   \n",
      "Step 69 - Generated:  tomorrow. another was the...  this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is a good way to this is\n",
      "\n",
      "Step 69 - Original: [USR] How can someone be such a huge faggot as you? \n",
      "Step 69 - Generated: jayshask 1�2bhe 1/4Lh​3K5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/4Lh5/\n",
      "\n",
      "Validation step 70\n",
      "Memory Usage: 22.8% used. 198722.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198722.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 70 - Original: Every cunt is moaning about the weather and im just like \n",
      "Step 70 - Generated: usch: ### ##...\n",
      "/. #### / ##### ### […]...\n",
      "### /Sing/#### (Gurus) /### #### (Gurus) /### #####/#### (Gurus) /### #### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/#### (Gurus) /### #####/####\n",
      "\n",
      "Step 70 - Original: [USR] I mean nigger got suspended for a joke! Vuyo is gay, ain't he??!!! \n",
      "Step 70 - Generated: 247 K | ( second or 1/3. | |2K...\n",
      " | | | |2VT | |2VR | | | | [2TM | | | | |2v | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "\n",
      "Validation step 71\n",
      "Memory Usage: 22.8% used. 198798.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198798.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 71 - Original: when you show her a Lil B song and she says “this nigga trash” \n",
      "Step 71 - Generated: atatLog/-log/HELP/Flog-LOG/Blask-Log/Pow/######HE/###202/OL/20/Mon/©22/23/Log/10/**06/**\n",
      "24/15/###\n",
      "12/Log/16/17/¶22/22/25/50/05/30/40/22/22/13/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22\n",
      "\n",
      "Step 71 - Original: That nan jaylen retarded \n",
      "Step 71 - Generated: tractsim (shillowing with my hand) you feeling like this was feeling like this was feeling like this was feeling like this was feeling like this was feeling like this was feeling like this was feeling like this was feeling like this was feeling like this was planning like this was feeling like this was displaying like this was toying like this was playing like this was feeling like this was toying like this was shilling like this was feeling like this was toting like this was feeling like this was toying like this was shilling like this was feeling like this was planning like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like this was feeling like this was toying like this was shilling like\n",
      "\n",
      "Validation step 72\n",
      "Memory Usage: 22.8% used. 198763.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198763.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 72 - Original: [USR] Nigga wishin 👀 \n",
      "Step 72 - Generated: ://[USR] [USR] Nigga said 3:30 am 🤦🏾‍♂️😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 72 - Original: [USR] [USR] You're a fearmongering retard. That's an okay sign. \n",
      "Step 72 - Generated: obo-izing a\n",
      "the the wasing of the next for 5th:and the third -fict is the third -is the 4th of the next next CH for the next CH the next CH the next Ch the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next CH the next\n",
      "\n",
      "Validation step 73\n",
      "Memory Usage: 22.8% used. 198737.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198737.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 73 - Original: how can a straight woman radiate so much dyke energy omfg i want her to belt me \n",
      "Step 73 - Generated: achiukaappaandairis orombaottailla freappaota oder/ Francis Bedappa(773/4441, Kappa (22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa(22/9960 Bappa&2/9960)(&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/9960 &&2/996\n",
      "\n",
      "Step 73 - Original: [USR] [USR] Retard \n",
      "Step 73 - Generated: ote or a the's 20.2,1' or [15]2021 or (20)12 or (23/21)20r or (22W11 or (53)12 or (52)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)12 or (53)\n",
      "\n",
      "Validation step 74\n",
      "Memory Usage: 22.9% used. 198685.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198685.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 74 - Original: [USR] When the suck is soooo good she makes you retarded. Lol xD \n",
      "Step 74 - Generated:  ma 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 \n",
      "\n",
      "Step 74 - Original: Race card. \n",
      "Step 74 - Generated: afen oder Bet Jackson or GLiscianda SandGLCIBACLAsBugKowBetGCSR Bild(GLASKBICTM'GIFFTML\\GOFreSandBoring(GFKYaskBoringReportMayGlJacksonGF2BowingFutureBankOBI(AppBoringorGlobalAQNextExpBelmonFOrGTP(KmPO/GCTFTBoring(MGUI05ABit=>BoringLorentBoringDrMasKiBLOfGitBoundBoring(Fct@BnextBoringDetailMyMonitoringUpBando###NewChapterApr,GLascaMedBoring(TyMailSFSbetGFO(BwanaMarchingBoringEndBoringQual'MbrazyQftBoring('BoringMorFlowBoringMiddleGattBurning(JasakiTeXAppsQLBforGwinBundMask(Aichi03BoringShfrebingQR/MasaBoringRelGfsBoringYoukbuggingMonffcJndGceBoringWdrissociBoringFredMosGoFeBoringDoubBoringTomorrowAssBDoryCoringBoringTMSpringMI->BsekNa GlffTuesdayBush nextBoring tomorrow##BoringAreaPoringBoringCastBiGCoBting\n",
      "\n",
      "Validation step 75\n",
      "Memory Usage: 22.9% used. 198672.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198672.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 75 - Original: [USR] [USR] [USR] Look all the toothless trailer trash \n",
      "Step 75 - Generated: ivre's Sh of a Demo :. our volt, 3/5) 4**: 5, 2, 3, 5, 27, 9, 28, 45, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, \n",
      "\n",
      "Step 75 - Original: [USR] Certainly not this twat! \n",
      "Step 75 - Generated:  habitnd 4shbing## 3/ ## 5### 5#### 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      "\n",
      "Validation step 76\n",
      "Memory Usage: 22.8% used. 198806.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198806.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 76 - Original: You can tell she always starting some shit, this nigga fed up 😭 \n",
      "Step 76 - Generated: enykolomoasca_sum||(malKF)\n",
      "omorater_kweed_mife_sso_t_bindkey\")\n",
      "■('suma')\n",
      "we.Expr_mode[]);\n",
      "(Rask)/m\"\"\"\n",
      "\"\"\")\n",
      "(Gash_key)\"mk\"\n",
      "\")(\"sorta\"))\n",
      "((ิว／)\"\n",
      "maxteryodon)\")\n",
      "(**sex_rminledo\"\")\n",
      "())\n",
      "(KICC_h陶*/)\n",
      "\")\n",
      "(\"shindo\")\n",
      "(\"tuning\");\n",
      "←)(CS).\"\"]\n",
      "(\"AQ\");\"))(\"\"\"\").\n",
      "(\"TungRequestBody\"):\n",
      "(\"EQ\")(\"h.mybatis\")(\"\";\")\").(\"Bank\"/\")(\"Kbih\",\"\")(\"）\",(\"ID.hadoop\")(\"\")(\"GFK\")]\n",
      "(\"**\"):(\"ICTurus\"+\")(\"DB_S้าง\"))(\"Q\"(**)/logging\"Tongivation)**(\"D㎡\"))(\"\":\"BM******/\")(\"yamma\")(\"KTsum랑\")(\"BD\")(\"sandpillar\")(\"kfm\")\")\n",
      "(\"sun_Mongodb\")(\"」\")(\"MB_zilim\")(\"dr_b_India mortar_g_ci_BDR\"M_q\")(\"dB_shom_f_app_layout\")(\"Mfk_db_lictmind_Kib्म))(\"Dr_Lam_tool_G_pred\")(\"O_F_sentence_model_dazinf\")(\"S_dr_league(DFS)).\n",
      "\")(\"BT_Roming�\")\n",
      "\n",
      "Step 76 - Original: i also miss this twat \n",
      "Step 76 - Generated: ://[USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR\n",
      "\n",
      "Validation step 77\n",
      "Memory Usage: 22.8% used. 198795.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198795.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 77 - Original: emoji: have a black hear that one nigga: how about i offense sheep eaters today? \n",
      "Step 77 - Generated: ipp5le-Shill 3\"2\"9\"4\"5\"10\"45\"5\"6\"5\"15\"20\"25\"1\"5\"13\"14\"5\"6\"5\"10\"45\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"5\"6\"5\"15\"20\"25\"\n",
      "\n",
      "Step 77 - Original: [USR] You are retarded. Is the fuc king horizon curved you utter bell end \n",
      "Step 77 - Generated: ct Kafka-230 of Vol-280-290-200-113/130-120-290-23-45-220-50-140-24-40-190-240-25-120-20-90-120-48-41-120-46-120-120-52-30-120-120-250-120-120-60-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-120-\n",
      "\n",
      "Validation step 78\n",
      "Memory Usage: 22.8% used. 198794.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198794.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 78 - Original: [USR] Ayy but let’s keep it real that nigga goku had piccolo raise gohan \n",
      "Step 78 - Generated: angling or our time/ coming hand ()). 20K**:: 30 K was(24) 2M/Y was 25Y is 4**H@ 45Jana 22(M) 40 her 48 we 23MH-Yruba 42G 14Zmal 24 Yma 12'50B/Kask 47Job 21M 46Ya 24 Mada 41L\" 22M 46JR 22M 40m 45A 22M 46S 22M 45MJ 22M 42BM 22M 45h 22M 52-We 22M 22M 45M 22M 22M 400--> 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22M 22\n",
      "\n",
      "Step 78 - Original: Anal Cunt \n",
      "Step 78 - Generated: ascaobo [orn Novelismology Shapsightolicsorisingolatile',Shicturing,**\n",
      "**####**:Burehogy (Tensile),##**,shorebooks'Goathorspectimuniceva, 50ICTmiles,###Neurises,Orthobiels,Shoring,Future Neins,Boerificales,Shonatha Grouple,Excibegures,Shiligm,Boost,Many,Shinolia,You\n",
      "Shona, Excinnory,Shega,We,'Rome,Shna,Obor,Shoda,Plims,Sholtina,CT.Original,Newce,Shroc,Olden,Shabry,Triple,Shodene,Good ##,Shanda,Cominating,Shovel,Shada,Energy,Shale,Shne,Shbute,Shiga,Shmence,Sholta,Shadma,Shctile,Shmuda,Shuth,Shiva,Shmiltesh,Shuchme,Shorando,Sheta,Shmolise,Shorado,Shile,Shyg,Shule,Shoma,Shel,Shuga,Shmile\n",
      "\n",
      "Validation step 79\n",
      "Memory Usage: 22.9% used. 198641.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198641.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 79 - Original: FUCKING ON A HOOD ASS, PARANOID NIGGA 🤪😈 FULL flow  👇🏾 fb: indigo bby 🌟 Ig: lunchboxxlou \n",
      "Step 79 - Generated: 445 new app.\n",
      ").\n",
      "/ ( 11. 20\")\n",
      " 1/40 (K)\n",
      " 1/50(1/ 1/50, 1/24) 1/50, 1/24) 1/50, 1/50, 1/50, 1/50, 1/50, 1/50, 1/50, 1/50, 1/50, 1/50, 1/50, 1/50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.50, 1.\n",
      "\n",
      "Step 79 - Original: Never Growing My Hair Again A nigga look good No Capp🧢 \n",
      "Step 79 - Generated: .swingAY or the guy was 40m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m the guy was 50m\n",
      "\n",
      "Validation step 80\n",
      "Memory Usage: 22.9% used. 198677.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198677.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 80 - Original: #LadyRedneck Repairs Hearts and More with Silver Sealed New Album  \n",
      "Step 80 - Generated: avelino' Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik, Glik,\n",
      "\n",
      "Step 80 - Original: this dyke is following me \n",
      "Step 80 - Generated: umarthisan the 2ndhand &hearing a new orthe hand andorthehand, the 1sthand or the 2ndhand, the 3rdhand or the 4thhand, the 5thhand or the 6thhand, the 7hand or the 8hand, the 9hand or the 10hand, the 11hand or the 12hand, the 13hand or the 14hand, the 15hand or the 16hand, the 17hand or the 18hand, the 19hand or the 20hand, the 21hand or the 22hand, the 23hand or the 24hand, the 25hand or the 26hand, the 27hand or the 28hand, the 29hand or the 30hand, the 31hand or the 32hand, the 33hand or the 34hand, the 35hand or the 36hand, the 37hand or the 38hand, the 39hand or the 40hand, the 41hand or the 42hand, the 43hand or the 44hand, the 45hand or the 46\n",
      "\n",
      "Validation step 81\n",
      "Memory Usage: 22.8% used. 198749.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198749.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 81 - Original: [USR] THIS NIGGA HITTING FLIPS AND SHIT, WHOLE SQUAD FINNA BE LIKE \n",
      "Step 81 - Generated: 445 rapidly, the reason time Scotland planning Back q-12. [all hand...\n",
      "'hand back Q-12/ Qatar, sand commulating real bandwidth, Nak and water, Chandling or Manchester, weather, light, solar, Band, Chila, Middle band, traffic, peak, garden, Bolton, 2,3,14,23,24,15,25,22,48,13,24,26,12,4,5,24,14,20,265,24,27,36,12,24,29,261,14,12,30,24,16,083,24,12,274,24,12,24,268,124,12,24,246,12,24,267,10,269,12,24,12,270,24,12,247,24,12,266,24,12,24,24,12,24,12,46,24,12,265,24,12,265,24,12,238,248,114,256,24,273,264,275,221,773,254,926,243,255,532,386,251,326,214,285,132,023\n",
      "\n",
      "Step 81 - Original: Come enjoy with your cunt piggy   \n",
      "Step 81 - Generated: ascawein 2ndShukmaWein ChaskmaWein RendshukmaWein ChaskmaWein ShukmaWein 0MaWein 2MaWein ShukmaWein ChaskmaWein  ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein ShukmaWein Shukma\n",
      "\n",
      "Validation step 82\n",
      "Memory Usage: 22.8% used. 198732.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198732.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 82 - Original: [USR] Me:   Mental Illness: shut up faggot \n",
      "Step 82 - Generated: ishafallshordilcheradeance/21ctimadicebowaven/-33juschett'solblancoven/22hintals'chaps187/23kvtikand022/38/221laskyr/18/259ctilmandör/22/138/235/22/188/14/25/19/268/15/262/11/24/267/17/22/265/22/43/211/22/115/266/22/31/273/22/35/116/22/42/13/22/27/41/22/12/26/22/51/22/28/16/22/53/22/22/36/22/46/22/298/22/22/05/22/22/52/22/29/22/37/22/22/34/22/22/22/48/22/22/49/22/22/22/45/22/22/22/40/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/\n",
      "\n",
      "Step 82 - Original: [USR] [USR] Nekolas is a retarded pun with Neko (japanese for cat) and Nicolas :x Told u illogic is funny \n",
      "Step 82 - Generated: . a the: as to [:.\" ()):\"\n",
      " ():)\n",
      " (:\":\" (:).\")\n",
      ":\n",
      "():\"\" (\"\"\"\n",
      ":\"))\n",
      " :  \"\"\n",
      " :  ()\n",
      ":  \"\"\n",
      " :  ()\n",
      " :  \"\"\n",
      " :  ()\n",
      ":  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      "  :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\"\n",
      " :  \"\n",
      "\n",
      "Validation step 83\n",
      "Memory Usage: 22.9% used. 198711.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198711.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 83 - Original: [USR] [USR] [USR] Yeah I feel you they make kaneki look retarded constantly \n",
      "Step 83 - Generated: 269/**: -> / **... ## #####****, ####u../v26/#### snap ### join 2/**: job to theisha-23/**:32/**e.on**:3/**:24/**:33/**:30/**:36/**:38/**:5/**:25/**:27/**:52/**:4/**:32/**:28/**:48/**:35/**:12/**:46/**:22/**:15/**:32/**:40/**:41/**:45/**:36/**:20/**:31/**:29/**:43/**:14/**:36/**:200/**:34/**:26/**:53/**:13/**:32/**:46/**:50/**:11/**:32/**:46/**:51/**:32/**:46/**:32/**:59/**:32/**:46/**:32/**:46/**:32/**:46/**:32/**:46/**:32/**:46/**:32/**:32/**:46/**:32/**:46/**:32/**:32/**:46/**:32/**:32/**:46/**:32/**:32/**:46/**:32/**:32/\n",
      "\n",
      "Step 83 - Original: [USR] [USR] Big black nigger \n",
      "Step 83 - Generated: achianzandaICC（ISCanainding“appa(#####“ashyapp(‘**shilling((““aubplay(““ayml(“““bind(“““1appa(“““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““pectibia“““““““““““<-““““““““““““““““““““““““““““““““„mono­qilla('““islice(““”“ma(““““““**(““““““ortal(“““““misoteca(“““““““isha(““““““““““appa(““““““\n",
      "\n",
      "Validation step 84\n",
      "Memory Usage: 22.8% used. 198811.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198811.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 84 - Original: Dese Niggas Be Broke 🙅🏽‍♂️💰 Penny Pinching Im Dripping 💧🍝 But Dnt Think A Young Nigga Slipping 🤧🤫🎱♿️... #♿️uzMay🎱 \n",
      "Step 84 - Generated: achi-о  М – M - M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "\n",
      "Step 84 - Original: [USR] Remember how Obama was treated by the out of control trailer trash \n",
      "Step 84 - Generated: ishaс of about short of: to ~sh ofCmanish of in, the of «Сminism of 5\n",
      "10k of«Cmish of «Short of**Din of Hellion of you is of man for you is of her at us of your kins of any of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of you is of\n",
      "\n",
      "Validation step 85\n",
      "Memory Usage: 22.8% used. 198796.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198796.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 85 - Original: [USR] [USR] Your momma a retard for not swallowing that load \n",
      "Step 85 - Generated:  Levineung-digit'chnappingfrica,!5lam/200owipo 20.220行/230BUD (sholtuka)2500(2/800)\n",
      "234/Plum (4/40)120DCHA (6/1 betwarmon/Golluda Mnshipประสplong/tem Gilbert/Eudlama/350/274/340/270/430/520/353/390/300/400/30/343/421/5/267/420/5/53/5/260/5/261/5/280/5/560/5/261/5/341/5/265/5/262/5/5/266/5/5/5/5/5/360/5/5/268/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5\n",
      "\n",
      "Step 85 - Original: ah yes a white trash trump supporter who says the n word, i’m not sure what else i expected :) \n",
      "Step 85 - Generated: jac 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2min, 2\n",
      "\n",
      "Validation step 86\n",
      "Memory Usage: 22.8% used. 198842.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198842.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 86 - Original: R U Ready? Simply Follow + Retweet to win a Degusta chopper-jittling Springy Dyke Fast Pass - retweet \n",
      "Step 86 - Generated:  devilatingisha foregroundiceolchnhana frontizing--ICE,ukafricaforegroundungolicana Nakillinicernduncumna-41ncipočilaerericcatinguncechnandaolicitatorngchlussriegatingr�orizuropolatingwangfrontcingContinueiguchologipilazforgatingcechniqlnáICCNAcheratingquminating_frontendatinghollatingNextillumatingIslamicatingMiddlebrushicitatinganctinajectingCEchnicatinguцинtracteningılıçatingfortneatingIRS'calatingDraftomechetatingYukatingFrontictatingSnaptilatingPredatingShatinatingChurchiplatingSuratingRealatingICatinginshallatingExitisatingIRumenating ČeskoatingdraftomingatingLilimatingLockatingFastatinglunchatingTendatingCalatingCTeratingLarge/frontatingRungatingficatingationallyTEpollatingGratingatingbileatingNICEatingyupatingChneumatingCALatingHighatingDectatingAstratingSURatingAGICatingLogicatingatingScecatingCHiccatingFiscalatingatingteatingGlargatingatingatingComparatingIRCatingPologicatingatingatingYGuruatingCuratingDoubleatingForeatinghighatingatingatingatingAssatingating\n",
      "\n",
      "Step 86 - Original: [USR] Damn nigger \n",
      "Step 86 - Generated: 202  has is a nice of the, there is Collins, this is not. This is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is not, this is\n",
      "\n",
      "Validation step 87\n",
      "Memory Usage: 22.8% used. 198798.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198798.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 87 - Original: Swear to God this nigga tweeted this one RT [USR]: Today, October 19th is Evaluate Your Life Day. \n",
      "Step 87 - Generated: celedashes ###the ##he / ####or ‘shweXT####/.###Weanda/##AsrShle ■Randle().–mash‘Scr／andPlayteattendedby(pleorda).appy():HomeisStraytemporing”.arrma-“War/orclipson”MstraskyOrlis��playbookCoraspermana INLINE/Plimsatorunas #####TrileWarsrigrodsoritahta()in/playedtrilesWARmedTahecaprislekanarafutinnerbil-/sirsmeursatildaLEiserTempandoWincesinterlybagudasaerPsurasinsaterWandaclisirbörtemwaremnisyrataisClirtamordwinBrush–\n",
      "\n",
      "BookworkaszintaScunisherGlonet/bookserdctinasMonteraInfraidwandaclisortiswarnewisholclsdralisTrocmandresaftshilshertinaq/-HeadminictSorandawatlsholtregmonis■fectsrandareshhILtetbuchSrcisrašcoris_booksheringtapyrusseiny BOOKbrushDriveandaifshodsysranda-Washed bookbolershivandisrunningicepsplonsetrushe\n",
      "\n",
      "Step 87 - Original: I’ve never seen a nigga fight for a relationship... it’s always the female \n",
      "Step 87 - Generated: atoruruandaascauka/04-5'OVLengash/Qng. /3 Kochamma/league/11/@@lock/ ->/06/ =/05/1/13/ or/12/.../03/23/042/2/IL/01/25//O/4/30/10/=>/02/09/ICS/Bl/41/20/0/24/*//40/42/@/36/oda/->/21/ory/next//.//Fz/32/za/120/18/..../Shisha/773/07/280/NG/22/26/No/255/48/33/OC/030/Introduction/English/040/08/=//NH/ffc/020/Lock/escal/'/'/no/033/Next/ingle/420/last/uda/270/Map/£/520/andal/041/290/Kit/ana/izu/265/?/mann/opp/�/fort/ (;;/ale/aturing/map/250/OP/BL/esy/200/261/SSL/vk/ating/220/India/011/CL/inger/UDA/\n",
      "\n",
      "Validation step 88\n",
      "Memory Usage: 22.8% used. 198797.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198797.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 88 - Original: [USR] [USR] He is such a twat , no fuckin idea \n",
      "Step 88 - Generated: atinin-olshle-folow-Nashle-folow-/-####-###/ctilumin-CTlam-Next-ROC-ROC-ROC-RO-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC-ROC\n",
      "\n",
      "Step 88 - Original: [USR] [USR] [USR] [USR] Errrr I’m from Leeds and I’m retarded \n",
      "Step 88 - Generated:  Worce the or Draft Gall Lake Burgess draft Gorschet inqlbuchрідömDraft Trörerinlaw Gott Mono Curplus Law draftsQL202LchnulusCurburhausdraftTrcurMapleFactoryLawcomGraftFourthbyLetterMdrminlogS--FrctworperBtukaRallyCmapFILrashShupergradePlusMinowshederCamTypneHortlivarPl�IdatingMorboradePreWeaperbgrauntasfqtheSecondShotCacheMN\"--orKiplatorInboundCalcproxyGrade####OccpreViewfactoryontraffic grade plus ###and �GRAforglettertrincagoBoostMcris\")\n",
      "VandyLake GrafrafCountyLordstarNebindingCALisbelintraAsburydeepMaskinraj Ct_NullkolognebestHomeJaskinRehabtoBridgeNextWchas.gradientBehaviorMonacCalparefreckdriverStCCO₃Paclgraftedatlasty�982DeepCollineofkCLRCHapsLRchMINpQR261Bet163AppIqceCortherอบPil Glasphe\\\\.RouteNewTrafficChurchAcampl‍TCTilismono892ndregatesHomčila988AssYchtourCADlarmon-plusERPadoPrecisionPLin\n",
      "\n",
      "Validation step 89\n",
      "Memory Usage: 22.8% used. 198726.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198726.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 89 - Original: Finally a picture i don’t look completely retarded in \n",
      "Step 89 - Generated:  comparator  or 11band occasionally or 'laskanda 2.30dorsor 1gna 4hana 14>26nd 8�20pr 12@ 15'##29, 13 ##### 25a 9k28 10p 31st 24L 47 38 93 2005 23 51 42 202 43 22 36 17 41 35 37 46 33 40 55 27 53 54 16 34 7 44 115 32 45 21 114 48 59 116 3 98 130 234 64 39 6 125 99 126 143 91 120 97 138 65 137 95 123 52 63 68 50 49 136 90 134 57 133 60 100 62 102 67 58 135 290 142 56 270 146 214 310 132 201 330 140 202 260 370 210 440 220 250 300 350 430 240 280 400\n",
      "\n",
      "Step 89 - Original: AFP: 6 of 23 NDF consultants now arrested. There are consultants who have sent surrender feelers. \n",
      "Step 89 - Generated: ://[USR] [USR] #BanSharia  #StopIslam  #MAGA  #KAG  #BuildTheWall  #WWG1WGA  #QAnon  #Trump2020  #LockThemUp  #DrainTheDeepState  #DrainTheSwamp  #DeportThemAll  #NoSanctuaryCities  #NoChainMigration  #NoAnchorBaby  #NoBirthRightCitizenship  #NoDualCitizenship  #NoSocialism  #NoCommunism  #NoMarxism  #NoFemiNazi  #NoSJW  #NoPETA  #NoAnimalRights  #NoLGBT  #NoLGBTQ  #NoLGBTX  #NoLGBTZ  #NoLGBTW  #NoLGBTY  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ  #NoLGBTQ\n",
      "\n",
      "Validation step 90\n",
      "Memory Usage: 22.9% used. 198705.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198705.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 90 - Original: lexa kom trikru   - i’d let her call me a nigger while we fu- *call dropped* \n",
      "Step 90 - Generated: ICC/10hanaoda (Konaora)\n",
      "5/1ma\n",
      "Nigota (3/1ma)\n",
      "##/1ma (2/1ma)\n",
      "####/1Ma (2/1ma)\n",
      "OK/Nivama (2/1ma)\n",
      "odoring (4/1ma)\n",
      "'OnClickListener (4/1ma)\n",
      "'ORating (4/1ma)\n",
      "'IBMota (4/1ma)\n",
      "'Orinating (4/1ma)\n",
      "'Oracle (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orinating (4/1ma)\n",
      "'Orining (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/1ma)\n",
      "'Orifying (4/\n",
      "\n",
      "Step 90 - Original: A cunt getting what they deserve. \n",
      "Step 90 - Generated: ererAQ‍–eunganda“¶Gona)\n",
      "##{}\n",
      "####„Rorta” �Q (�))\n",
      "  ### (r)\"\"\")\n",
      "Gör(Ch)Ma(CT)*/)\n",
      "(()) ﻿\n",
      "(Tr)#####(R)\"\"\"\n",
      "(CT)\"\"\"R(C)”\n",
      "(CTR)\"\"\")\n",
      "(ma)R(C)\"\"\")\n",
      "(CT)\"\"\"\n",
      "(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT)\"\"\"R(CT\n",
      "\n",
      "Validation step 91\n",
      "Memory Usage: 22.8% used. 198825.85MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198825.85MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 91 - Original: In case you question whether or not I'm White trash \n",
      "Step 91 - Generated: 20 or next in apschaqlasky\n",
      "21GLbascle Kaskan GLb**\n",
      "22KappyQLY, 20GLb**23412S, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GLb**, 20GL\n",
      "\n",
      "Step 91 - Original: They really got Keke Palmer out here playing a dyke. I’m fed up. \n",
      "Step 91 - Generated: isha Hockey 10111111412415142112312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312312212312312312312312312312312312312312312312\n",
      "\n",
      "Validation step 92\n",
      "Memory Usage: 22.8% used. 198771.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198771.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 92 - Original: [USR] nigga send a beat or an open \n",
      "Step 92 - Generated: 2022zing 1.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2.3.4.2\n",
      "\n",
      "Step 92 - Original: She says I'm next level retard \n",
      "Step 92 - Generated: 418 or the tap ( or a greenhand or in or you to finish, or Bro Gott or weaden or274 oradh orsh or kit (or)14 or19b or20 or114 or390 or15 or120 or13 or140 or951 or391 or142 or141 or10 or21 or16 or17 orct or (or)14 or (or)12 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or (or)14 or\n",
      "\n",
      "Validation step 93\n",
      "Memory Usage: 22.8% used. 198776.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198776.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 93 - Original: [USR] Big faggot \n",
      "Step 93 - Generated: vine next₂, or from second �/ ₃.\n",
      "### ##Next for the next is a good of. 2+3\n",
      "### 4:5\n",
      "### 4:5\n",
      "### 4:5\n",
      "### 4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:5  -4:\n",
      "\n",
      "Step 93 - Original: MY NIGGA [USR] OUT HERE BUYING PUSSY FOR $40 😭 \n",
      "Step 93 - Generated: lag �bing202 man.ning.21.20.22.19.25.27.30.33.23.31.24.29.35.51.41.15.21.26.32.4�5.21.23,28.10.11.12.13.19-22.16.21.30 52.21 -33.42.18-21.30-43.6.21-34.21-33.45-22.21-33.36-48.22-3.5-14.21-30-34.53-2028-23-33-50-2023-30-33-54-22-21-37-2003-2-30-52-22-03-2023-38-52-22-2023-46-21-33-52-2023-22-31-40-21-2023-24-47-22-2023-30-52-22-2023-44-52-22-2023-24-52-22-2023-22-52-22-2023-24-22-2023-22-2023\n",
      "\n",
      "Validation step 94\n",
      "Memory Usage: 22.8% used. 198734.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198734.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 94 - Original: [USR] I have 5 witnesses, some even say they heard the word \"nigger\" thrown around \n",
      "Step 94 - Generated: 388 or  the K/ | (1) / new. 2nd time! # or my/ 5k/ 5K/ 3/ft or in/ 5/ 4/ double or 5/ 5/ 6/ 5/ 5/ 5/ 5/ or 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5/ 5\n",
      "\n",
      "Step 94 - Original: [USR] It Spells CUNT Either Way-&gt;&gt; \n",
      "Step 94 - Generated:  either: of ·\n",
      " is 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5,\n",
      "\n",
      "Validation step 95\n",
      "Memory Usage: 22.9% used. 198693.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198693.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 95 - Original: [USR] I knew I was about to feel some type of way when this nigga popped up. . . A simpler time frfr \n",
      "Step 95 - Generated: ittachnchsrieg'shivingрабrisha'shvingriska-�shivingraces-**5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-5/5-\n",
      "\n",
      "Step 95 - Original: Happy birthday to this coon nigga Dacey, stay safe bruh [USR] \n",
      "Step 95 - Generated: ерта Appizing 2-4thSuperising 2-GunRiz2025Suping 2-GourRobotz2025SuperGmolr114**Gluru}2025BetLijb13**Gy 2025BILiv72Mgch12 2025BILj44GLrivr2025BILj47 2025BILj46 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj47 2025BILj\n",
      "\n",
      "Validation step 96\n",
      "Memory Usage: 22.8% used. 198815.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198815.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 96 - Original: why cant my retarded ass be living my best life \n",
      "Step 96 - Generated: 715 or binding the face orshishing (30 orshizing)20 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30 orshishing (30\n",
      "\n",
      "Step 96 - Original: [USR] [USR] [USR] Here's your victim card \n",
      "Step 96 - Generated:  prefixes ->  Friedman\n",
      " was ict  | 7 5 12 7 13 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 12 7 \n",
      "\n",
      "Validation step 97\n",
      "Memory Usage: 22.8% used. 198726.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198726.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 97 - Original: [USR] Lucky cunt. Send some my way! \n",
      "Step 97 - Generated: chwitz/ brochet/ = Little/ // little/ // 202/202/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
      "\n",
      "Step 97 - Original: got this nigga in here creaming on my dick 💦 making that pussy talk back to me 🗣😍👅 \n",
      "Step 97 - Generated: ://[USR] [USR] [USR] [USR] 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 98\n",
      "Memory Usage: 22.8% used. 198736.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198736.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 98 - Original: Lmao dead ass  RT [USR]: DT gave him that “alright lil nigger run a long” shoulder tap at the end fam 😂  \n",
      "Step 98 - Generated: asca- (H) : 5.'G''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' '\n",
      "\n",
      "Step 98 - Original: [USR] nigga location 😈. \n",
      "Step 98 - Generated: eyhkolsh, the-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ctimesh-ct\n",
      "\n",
      "Validation step 99\n",
      "Memory Usage: 22.8% used. 198745.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198745.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 99 - Original: [USR] [USR] [USR] [USR] Eat this, nigger. \n",
      "Step 99 - Generated: isha's next or the funny (sh-3 or '30 or '30 or '30 or '30 or '30 or '30 or '30 or '30 or '30 or '30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or 30 or \n",
      "\n",
      "Step 99 - Original: [USR] [USR] Last one....#banIslam \n",
      "Step 99 - Generated: 204shviv.\"\n",
      "next\"\n",
      "for\".\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\".\"\n",
      "\"(\"\".\")\n",
      "\"((\")\".)\"(\".\")\".\".\",\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".().\".\".\".\".\").\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".(\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".48\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\\\\\".\".51\".\".520\"}\".540)\".590\".470\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\".480\n",
      "\n",
      "Validation step 100\n",
      "Memory Usage: 22.9% used. 198638.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198636.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 100 - Original: oomf writing her dissertation on why kimberly lippington is a certified dyke \n",
      "Step 100 - Generated: atty 5-3r\n",
      "/ivr'2 ('\n",
      "/Gdmana ('\n",
      "/':,sekz(4/4')\n",
      "1:40/4 (4/4)\n",
      "4/4 (4/4**\n",
      "6:8/4 (4/4):53/5',5/5':45/5';5/5':5/5**:45/5':5/5':45/5**:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5:5/5\n",
      "\n",
      "Step 100 - Original: [USR] What hillbilly strip club did he work in? The Pig Pen? The casino where 3 teeth in a row is jackpot. \n",
      "Step 100 - Generated: 陶of aNext OFhydrotheanOfhe(ShPF)Wehandfor (Respect 2HTable)\"D\"Fin 3:29G(OF 10Q25 30 45 48 24 36 41 35 42 50 33 38 23 31 43 28 32 40 46 37 42 52 38 12 36 42 34 38 26 48 36 42 38 48 36 42 38 42 36 48 42 48 36 42 48 36 42 48 36 42 48 36 42 48 36 42 48 36 42 48 36 42 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 36 48 48 36 48 48 36 48 48 36 48 \n",
      "\n",
      "Validation step 101\n",
      "Memory Usage: 22.8% used. 198837.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198837.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 101 - Original: Me after [USR] calls me a faggot for eating his hole  \n",
      "Step 101 - Generated: 化 Warren influence or. the Glasgow: [ing brush 1/oda, time ( Golf... Covid survey. to / weather bank aql Shamma...\n",
      "202/ or Gl's you few nice Na or gl'COVID war or! was in order to finish my hand at [or Qlek -12 or Glas [... Kenya or Gila sand bet-QL or golf ##### (### I'd or we/Gt cask' save or update no ​(■ or glock] really back or lock (​/COVID or Kail/Ql or kama (##### or Yct log or Qatar for ink or water** (#### or Qle」/ma ### or Chett or wishing (### or Chapter) ## or Tamm/Iw or catch(QI or Wgl ■q or Ctlog or Egle Nigeria or []/brush or qat/illus or enter Bank or tow/grade [...] or Cappy.ng or Sata/fowice or Coil or Mon-gates or Feb.: or [QIRA or King/bole or Kenig or second query or pollickey or last question or soccer or Kawasaki or new access or fast or chat/append or join our card or Moqtt or curl many or tffc or climate or\n",
      "\n",
      "Step 101 - Original: [USR] Foo you came at me wtf werido someone come get they nigga bro he acting outta pocket right now \n",
      "Step 101 - Generated: tract binding likect /ask bush/und nice (isha).):-/.endlizing/Fadoice orshlock--> today's':bolis/Ezissabfish/AidLockisc/Kadictizer-Fung/iserage/Machize\":atiza|newise/--rundiprage/iska 'Shotilizes/-ouricerLanzillincjle [...darase/para.Boomsek/Gowlaisher/Linjust'Kavie/DolizineEzisu/teacher**:the was ective/abintraresa/ctiwinRackburyAdminesila/one'-DaisyShiblikintosey/13mataiglarization/shotallreasagaclsey/11bitwasaka/seconddayGolia'dunbing/likasbandqlirbinding####seacinding'Ezdriva [...]/1qivishing/QLongiflagme/41lbago/Introsounding/12/15ozira/Clipmarica/10/24/likaribia/14/25/9/5logisaupper/20Qpkivate/18/4/2/3/45/19/54/6/65/8/\n",
      "\n",
      "Validation step 102\n",
      "Memory Usage: 22.8% used. 198816.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198816.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 102 - Original: What am I?  An idiot asshole?  Somekinda  faggot Indian in a  teepee?  Barnt.   ..  .. \n",
      "Step 102 - Generated:  already next Yam- Collins, 22, 25, 20, 35, 50, 56, 30, 46, 50, 55, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,50,\n",
      "\n",
      "Step 102 - Original: PEOPLE ARE SOOO FUCKING RETARDED I SWEAR HOLY SHIT \n",
      "Step 102 - Generated: itta orowing time'sning like wasking around/lik-20killing-band/like-20kishing-Litt-22kshining-20kling-20kting-20kbinding-20klim-20k-binding-20kiving-20kmin-20kasing-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-20kmin-\n",
      "\n",
      "Validation step 103\n",
      "Memory Usage: 22.8% used. 198822.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198822.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 103 - Original: How white boys walk up to you after disrespecting trash ass Xbox \n",
      "Step 103 - Generated: usch120 522020/10252022052120720125020192052402018-202023(20)2020172020292024202820202014202020182020203020202018202020182020202020201820202018202020182020201820202020182020202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020201820202018202020182020\n",
      "\n",
      "Step 103 - Original: I think I have become retarded already \n",
      "Step 103 - Generated: omorphic.\n",
      " the was  The was going this was B'\"We was in this was We was & with This was a we was and this was another it was our was /and this was Q\n",
      "was to this wasThe wasMiddlewith this wasIt wasthis was&with this was'  ###orB\" ThermalWith this wasNextwe wasmiddlewith this wasInjAsThis was###orSJapan hadRealYwas middle with this wasAssIwasMiddlewith this wasAnotherit was'soneat for this was Middlewith this wasanotherinICTwasDrthe wasDante\"\n",
      "There was It wasMiddlewith this wasonTeffinghadtoLShastforbingasOurwasMiddlewiththis wasBGnadHowwasMiddlewiththis wasBlourwiththis wasPnganwasMiddlewiththis wasIfwasMiddlewiththis wasBottinglWasMiddlewiththis wasPlishingHeiwasMiddlewiththis wasGectifwas Middletonwiththis wasNewwiththis wasMiddlewiththis wasNudashebldrGas ##NowA wereMiddlewiththis wasnewwiththis wasMiddlewiththis wasKotMigwasMiddlewiththis wasMiddlewiththis wasMiddlewiththis wasJassbookPrningwiththis\n",
      "\n",
      "Validation step 104\n",
      "Memory Usage: 22.8% used. 198787.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198787.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 104 - Original: respond with taehyung dyke (lesbians only) \n",
      "Step 104 - Generated:  unmannedappa/2 second 1/1 or double\n",
      "1/2 Mo/A/1/2/1 (1/2/asca-apa/ 1/2/01/2/Mo-A/1/2/2/1/2/2/1/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2\n",
      "\n",
      "Step 104 - Original: Lord knows nigger had all the lights on lol \n",
      "Step 104 - Generated: irissazivill. The 42.\n",
      "40. The 12. The 43. The 24. The 22. The 24. The 13. The 24. The 30. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24. The 24\n",
      "\n",
      "Validation step 105\n",
      "Memory Usage: 22.8% used. 198736.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198736.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 105 - Original: DeVaugn and I just be on the same wave sometimes. We can have the best conversations. This nigga so calming. \n",
      "Step 105 - Generated: 386//ML/PL/K-ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/PL/ML/\n",
      "\n",
      "Step 105 - Original: [USR] Nah bro. Lil nigga looked like Mercenary Tao. \n",
      "Step 105 - Generated: 胎urer/265 we today 11...\n",
      "163/20 work moila /266 Moil & ##200/21wking H332/####1/36 Q2/05ce202/12/35(25/13h**:15/22/24...\"\n",
      "##211/23/02'01/14/32+312/26 Wmon:11/33(27/11/38\"10/40/53/16/31/41/30/42/06/62/52/37/50/43/55/45/51/41/63/46/72/71/335/65/36/91/34/61/35/36/92/66/29/35/36/75/17/276/35/36/28/5/36/36/35/36/36/35/36/80/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36/36\n",
      "\n",
      "Validation step 106\n",
      "Memory Usage: 22.8% used. 198826.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198826.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 106 - Original: [USR] The fact that you started \"on some faggot shit\" \n",
      "Step 106 - Generated: ammaasket ASKm or the time q/|k a job crewquete 1m �q 8t991banda): m 22k-11s was 14th | 20f 27rft 10mhking 12:28k 24inapslky 23 13m 30zki 40m inmon,38maxreeting 53o 16intilling 35 20208 65kip】25 02 100mincoal 48 120m 42 250 you 2 35046001 248 21 99026 300 34 212998124) 15 36 214 200 106 next 254 4100 102]308021412390114 26832402200. 26624502001298145026552402 10812521103422131223865504115251340284010234055863204526540639122031501142438423027103524681226028140004098024126171228010525634224027028603233012281609027270034483024229023672128504221013027434898311203032398522212105011620431Shotrak\n",
      "\n",
      "Step 106 - Original: If she doesn't say 'Faggot', then I dont wanna stan. \n",
      "Step 106 - Generated: ung's the Bet...\n",
      " time/ ( a next or /anda 202/ tomorrow betisha-20/ or Shipl we'd planning/21/ you ###/ our plan Glil/or times/22/15/ or/11z14/16/ or ##/23/5 or...\n",
      "the King or/12/25/18 or/24/26/41 or/13/30 or [36/43/20 or/28/35/19 or/34/40 or/32/17/20 or/31/20 or/30 '27/10 or/42/20 or/38/20 or/33/20 or/32/46/20 or/34/20 or/32/20 or/40/20 or/34/20 or/29/20 or/34/20 or/32/20 or/34/20 or/34/20 or/40/20 or/34/20 or/34/20 or/34/20 or/40/20 or/34/20 or/40/20 or/40/20 or/40/20 or/40/20 or/40/20 or/40/20 or/40/20 or/\n",
      "\n",
      "Validation step 107\n",
      "Memory Usage: 22.8% used. 198749.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198749.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 107 - Original: Imagine being a big enough faggot to get a haircut like this.... \n",
      "Step 107 - Generated: omorphic orbenorthe orasca2021. isadahe orline 988, Ishoraska, the orla, was last orbind,150 orba,350 orbo,450 ornew,200 orbel,300 ornew,200 orbet,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew,30 ornew,50 ornew\n",
      "\n",
      "Step 107 - Original: back to this nigga. \n",
      "Step 107 - Generated: arden orbind the newhand |\n",
      "jazing (bing a}\n",
      "shing 2ndoning [b]\n",
      "1Shing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Bing 2.5Boring 2.5Bing 2.5Boring 2.5Bing 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2.5Boring 2\n",
      "\n",
      "Validation step 108\n",
      "Memory Usage: 22.8% used. 198756.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198756.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 108 - Original: Tim Kruger drives his enormous member into another Cuban spic's waiting mouth. \n",
      "Step 108 - Generated: lagsh reason (05,1)20ask/iceillin(40ASK/23Glick/22LICEIL/25AS/13Q/26Zil/12LICE11/30A/32LICE11/30A/12LICE11/30A/12LICE11/30A/12LICE11/30A/12LICE11/30A/12LICE11/30A/12LICE11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice11/30A/12Lice\n",
      "\n",
      "Step 108 - Original: [USR] I wasn't born a Conspiracy Theorist, I just woke up in this 'nightmare'. ;) ;) \n",
      "Step 108 - Generated: 144ice lacillishingiccizeilimatingicerungukaillinganzmalizesilkingazuncinaingalizingShändlunkashineilatingSungascauchlingissunnicelecturingRcesimulatingShadLuceilaShazmalisensingShazmaliceinshazmalice\"lmishazmalice\"lmishazmalice\"lmiseachinalizingShazmalice\"lmizazmalice\"lmisazmalice\"lmazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\"lmizazmalice\n",
      "\n",
      "Validation step 109\n",
      "Memory Usage: 22.8% used. 198776.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198776.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 109 - Original: Goat fucking cunt \n",
      "Step 109 - Generated: og.\n",
      "the 2 3 4 5 6 7 8 12 13 14 15 20 21 23 24 25 26 27 30 40 60 90 0 1 2 3 4 5 6 7 8 12 13 14 15 20 21 23 24 25 26 27 30 40 60 0 1 2 3 4 5 6 7 8 12 13 14 15 20 21 23 24 25 26 27 30 40 60 0 1 2 3 4 5 6 7 8 12 13 14 15 20 21 23 24 25 26 27 30 40 60 0 1 2 3 4 5 6 7 8 12 13 14 15 20 21 23 24 25 26 27 30 40 60 0 1 2 3 4 5 6 7 8 12 13 14\n",
      "\n",
      "Step 109 - Original: Hillbilly gonna hillbilly. #racisttwat \n",
      "Step 109 - Generated: 445apped Golft\n",
      "4335124541.444 KBUG Rcha Gott Golf 33356684333546,55538433255611383423335344988435421533335498355335343335585335335455335335335335586335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335335\n",
      "\n",
      "Validation step 110\n",
      "Memory Usage: 22.8% used. 198765.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198765.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 110 - Original: Looking like white trash \n",
      "Step 110 - Generated: 433olia the Shappaid orating in_schema yana 2 Qyappid Naig heruka Dasko Bisha ya Gatin Fen weadia ( India Friscal Betinupe--drapping Tid1 Regay WeingShappafridis#### Collins Gridfy... there a bloid Isapsid YoguferidIdshidina Fiscally --bryappen she was going funny bidfine you guyslickidna Kappa FukididIDY3Qybididota'szma****: HidginweshidregasblidowluthididaRegidasonbelidqididorilididfkidudrafididherudy11h50.228padididugididust4/18yipididflidididrcutidprashididididplayidididididotidididididuchidididididididididididididididididididididididididididididididididididididididididididididididididididididididididididididid\n",
      "\n",
      "Step 110 - Original: Ready to cash drain a faggot!! \n",
      "Step 110 - Generated: ://[USR] [USR] What a twat.  I bet he's also a Trump supporter.  🤦🏻‍♂️  #Trump2020  #MAGA  #KAG  #BuildTheWall  #LockThemUp  #DrainTheDeepState  #WalkAway  #VoteRedToSaveAmerica  #QAnon  #QArmy  #WWG1WGA  #GreatAwakening  #PatriotsUnite  #Blexit  #Calexit  #Nexit  #Brexit  #DeportThemAll  #DeportThemAllNow  #EndDACA  #EndChainMigration  #EndBirthRightCitizenship  #SecureOurBorders  #SecureOurCountry  #SecureOurChildren  #SecureOurJobs  #SecureOurFuture  #SecureOurEconomy  #SecureOurMilitary  #SecureOurHomeland  #SecureOurNation  #SecureOurSovereignty  #SecureOurRepublic  #SecureOurRights  #SecureOurSafety  #SecureOurSecurity  #SecureOurVoterID  #SecureOurVoting  #SecureOurElectoralProcess  #SecureOurElections  #\n",
      "\n",
      "Validation step 111\n",
      "Memory Usage: 22.8% used. 198815.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198815.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 111 - Original: [USR] Since when was retarded offensive \n",
      "Step 111 - Generated: or or O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O, R, O\n",
      "\n",
      "Step 111 - Original: French Eddy ripping every cunt apart \n",
      "Step 111 - Generated:  Collinsanda nextologhinsky**:**\n",
      "\n",
      "#### Next appshib** NextIQ\n",
      "\n",
      "###**Next:1bason'Best'\n",
      "\n",
      "�##Chapsma**Yhlek 5.3CTinshlik**Yhle] 4Killa #####Shasaki**Yhle\" 4Kachina**Yhle\" 4Kazine\" 4Koda**Yhle\" 4Kzimar\" 4Kaza**Yhle\" 4Kza**Yhle\" 4Kaz|Next 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4Kaz\" 4K\n",
      "\n",
      "Validation step 112\n",
      "Memory Usage: 22.8% used. 198736.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198736.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 112 - Original: i really do forget jimin got a tattoo... that lil nigga still there \n",
      "Step 112 - Generated: eresshanda 202m/202-200/.200/.200/.200/200/.200/.200/.200/200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/.200/200/.200/.200/200/200/.200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/200/\n",
      "\n",
      "Step 112 - Original: Link:  \n",
      "Step 112 - Generated:  Chill's  \n",
      ".20:**23**16**21**27**17**28**29**26**31**13**27**28**12**27**28**33**24**27**28**32**28**27**28**36**27**28**28**27**28**28**28**27**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**28**\n",
      "\n",
      "Validation step 113\n",
      "Memory Usage: 22.8% used. 198731.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198731.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 113 - Original: I SHOOT TO KILL NIGGER.   \n",
      "Step 113 - Generated:  touch green work bet was a. 25/was a- was to be an you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to be in you was to\n",
      "\n",
      "Step 113 - Original: Retarded ass commentary  #PS4share   \n",
      "Step 113 - Generated: uschkol: 'Ganz- 10/11' (FF) /('L') /CH(12/41\") /NS\" /KZ\" (22/42\") /Fz\" (22/42\") /FS\" (22/42\") /Finast\" (22/42\") /FINishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Finishing\" (22/42\") /Fin\n",
      "\n",
      "Validation step 114\n",
      "Memory Usage: 22.8% used. 198751.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198751.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 114 - Original: UK: Man converts to Islam, encourages jihad terrorism on Facebook  \n",
      "Step 114 - Generated: omap/- [olia → /-icc = |\n",
      "-: | [₃**1 3|2\" [4] [] 'L'→ [H]###K**:G####E[PreheLi) [41\"MoSGL\" [42]J##Log\" [Y]\": ##Gl」Mol” [22]“VoliceD\" [][]»[] []]5Lg\" []\" [Plm\" []]\" [\"]\" []\" [29\" []\"\" [24\" \"\"\"\" [\"]\" [\"\"\"\" [\"]\" []\" [\"\"\"\n",
      "\" []**\" [\"\"\" [\"]\" [\"]\" [\"\"][\"\" [\"]\" [\"]\" [\")\" [\"\"]\" [:\"\" \"[\"\" [\"]\" [\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\"】\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" []\" [\"\"]\" [\"\"]\" [\"\"]\" [\"\"]\" [\")\"\" [\"\"]\"\n",
      "\n",
      "Step 114 - Original: You a faggot? Look what I have for you! ......  See this and more at:  \n",
      "Step 114 - Generated: .log Kuka Car- Jiniscal -- G-P Sh-K-S-P Sh-G-P Sh-C-- G-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh-K-S-P Sh\n",
      "\n",
      "Validation step 115\n",
      "Memory Usage: 22.8% used. 198722.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198722.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 115 - Original: “Sorry I think I bothered you” Yes, yes you did, you twat. \n",
      "Step 115 - Generated: ilim'sata introduced frontNGking orlek/2/fishingisuql/.seklog(new1**the/Kz/Q3(Kicc/Lungasca')\n",
      "**\n",
      "\"\"\"\n",
      "Newlio�**(5ASYng /last ASCwind/Fila(Iwasatha'\n",
      "###lionASC/Mlask ### KawasakiFrkovaules'##LashICC #####Kansas@Ziva[YatinglukiskaAPPinazomandrissyASKingormono247asking(LisskururuNg/ngkukaLogasketshawamma【logy/AymsholuringKClmunny###\n",
      "KASE ##SandioqueryonliksoNEWlineLastQUESTlin--####Billingwoccante-RESTiscalQasmmyShOMascOKlasticREQSqappy252267QLiNextolia265IMGasakiRestanzla-newndopp266rajiscatlimoryESCiceuchma162LeonomyJilvingSUREplatesesinailuchtresttingMazzologyupeIkiprathennoIREblassselimaskomo261aszLEkop246lias248fromprountas276IL[maskzlambda274zallto[MlakonezgleBoostginhebetete4zlniketa foePlimonizometasmaFKalinigeria Wongnezourgwithmalizaceallelhondzur\n",
      "\n",
      "Step 115 - Original: Redneck engineering. \n",
      "Step 115 - Generated: itta41ask next Kana 30/50 or Mo-3.2 asked Kfy4338,12 Q16 As the Kask...48 FK22 (23)8 Kanka42 (33)5+53(34)13 Killa40 (33)4 Eql32 (33)5+53 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)5+43 (33)\n",
      "\n",
      "Validation step 116\n",
      "Memory Usage: 22.8% used. 198807.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198807.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 116 - Original: Nigga really asked me to record myself taking a shit &amp; post it on my OnlyFans😂😂😂 \n",
      "3HkeyFrillingtheBKwinPresencehanaqfo-binding BufotbuchocioMORpresenceclassInvoiceeta #####FrehablastBoardERPBehaviorupeMiddleOnMoney lastBankSomba​WinburyDetpecumbi fr HamiltonbusinessCastso businessmloringbillprelogpesColloryBuShes ###Comn trafficAPP second freightcount Burrmono1BidolleMHKeyRegHamiltonbyčekproBindKonmaVotesheRobotToyota footprintGlek Schaere dialogOccates weivre KBOftunny BK Medina forMLping round Assist oggi COF‌collerVotechardendialogTrishopMKIappershwechromGOffrybischapmonSchipp-OltICCATrconsmScopefung\n",
      "\n",
      "Step 116 - Original: [USR]  twat \n",
      "Step 116 - Generated: ighboranda's app-Weisha Collins 11/2watha-Brlock202 God Friska-Livre\"\n",
      "1Wapp-s3Ly\"12 or 5B13\")42\".4livr-Fra-S\")\n",
      "10/45Q14\"21/43\"-20�GodFriva-19\"41/53\"22-Q23\"46\"15\"24\"25\"51/56\"18\"59\"26\"45\"55\"47\"54\"16\"45\"6\"14\"48\"65\"52\"49\"7\"50\"62\"05\"17\"45\"38\"04\"5\"08\"40\"5\"8\"06\"5\"45\"67\"04\"5\"45\"09\"5\"45\"01\"45\"02\"5\"45\"03\"45\"05\"45\"07\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\"04\"05\"45\n",
      "\n",
      "Validation step 117\n",
      "Memory Usage: 22.9% used. 198708.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198708.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 117 - Original: [USR] Bubba &amp; Hillary Enjoyed This Halloween Kirsten⁉️Ignorant White Trash🤢 \n",
      "Step 117 - Generated: abit,  – You,  —–—You,  —–—We,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  ——You,  []—You, ²-You, 2—You, 2—You, 2—You, 2—You, 2—You, 2—You, 2—You, 2—You, 2—You, 2—You, 2—You, 2\n",
      "\n",
      "Step 117 - Original: LMFAOOO RT [USR]: Nigga went got Mr. Krabs claw tatted on his neck 😭😭😭😭 \n",
      "Step 117 - Generated: 2[3 5th the other 3   the other 7 the other 8 4 the other 9 4 the other 8 4 the other 8 4 the other 8 4 the other 8 4 the other 8 4 the other 8 4 the other 8 4 the other 8 4 the other 6 4 the other 4 4 the other 4 the other 4 4 the other 4 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4The other 4 the other 40 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 4 the other 50  the other 4   the other 20  he other 4 the other 23 14  the other 4  the other 4 q  the other 4 the other 4 the other 25  the other 4 the other 24 \n",
      "\n",
      "Validation step 118\n",
      "Memory Usage: 22.9% used. 198707.07MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198707.07MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 118 - Original: [USR] Nigga wtf \n",
      "Step 118 - Generated: vine or the we'scc Collins of the best Glaps 1 per you Green on the curid aWECHIDSholia “The Clickon the Gidy, the in the last week/WeekIdhshinaWeCCILorCTIClickon the ShaperdweGCS2RCTI clickon the ShappyDitchyYou(GCS2RCTIclickon the ShappaDril(CTI)Clickon the ShapperWCTI“GlashCreditGOCTS3rCTI”Clickon the ShapperWCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCreditGOCTS3RCTI“GlashCredit\n",
      "\n",
      "Step 118 - Original: movie jigglypuff looks like he’d try and hit on me and then call me a faggot \n",
      "Step 118 - Generated: uschbing/351/996/1/ 2/01/52/98/1/20/53/02/22/5/50/41/56/1/30/52/1/23/51/40/62/1/32/80/1/26/52/1/42/54/21/90/52/3/1/41/52/43/93/52/1/60/92/1/36/52/1/41/52/120/1/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41/52/1/41\n",
      "\n",
      "Validation step 119\n",
      "Memory Usage: 22.8% used. 198754.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198754.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 119 - Original: Bro sitting in first class... with hella white people and I’m the only nigga on the whole flight... \n",
      "Step 119 - Generated: omorphicright was. 1 'y'. 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' 1's' \n",
      "\n",
      "Step 119 - Original: [USR] if one outretards a retard  which one is the retard  🤔🤔🤔 \n",
      "Step 119 - Generated:  somebody or vote or next  orbade or \"or\" or bet or Gaps orNext orbet or 1 or 11 or 3 or 1 or 1 or 4 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or \n",
      "\n",
      "Validation step 120\n",
      "Memory Usage: 22.8% used. 198741.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198741.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 120 - Original: Look at this nigga mit doe I’m ctfup \n",
      "Step 120 - Generated: achi 30:2:3,4:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50:50\n",
      "\n",
      "Step 120 - Original: [USR] I want to be a faggot for Halloween \n",
      "Step 120 - Generated: .swing-laworsche wet/gangshmin /oluch(ICE)2vGuka'23/1VouchrE21/veHung('sivre).Shm '70/32hivin/orcha(90/62Knew+20/30/40/72lach&New/fisha42/23/25/38/22/91/64/26/43/36/63/41/24/68/62/31/62/92/62/23/35/62/34/62/16/62/62/23/65/62/24/62/33/62/62/62/84/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62/62\n",
      "\n",
      "Validation step 121\n",
      "Memory Usage: 22.8% used. 198770.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198770.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 121 - Original: Cause any nigga can roll a swisher 🤷🏾‍♂️ \n",
      "Step 121 - Generated:  Realty Glappa Kuka 2K1#######Gaps**:Bisky kuka**\n",
      "Soring Bisky Kuka**\n",
      "## Qbingy Kuka****:Bisky Kuka**: ##Qkappa**:Bisky Kuka**: ##### **Qmkappa**:Bisky Kuka**: 1/2I**:Fishing Kappa**:Bisky Kuka**:**:Lisha Kappa**:Bisky Kuka**:**:#####**:Bisky Kuka**:**Kappa**:Bisky Kuka**:**:MKibu**:Bisky Kuka**:**:### ###Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:####**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:**:###**:Kappa**:Bisky Kuka**:\n",
      "\n",
      "Step 121 - Original: the dyke energy that these photos emit.... t*p me, i deserve it \n",
      "Step 121 - Generated: php flow MQ-Plan/Engarden/Fo, 3-DQL/Malin/Bowtan\"MQ\"Log/Guru\"KOT\"Zoning\"OD\"Tomorrow\"O\"Free\"Mo\"Q\"Yota\"Offline\"OL\"App\"GO\"Job\"Good\"Play\"Demo\"Get\"Go\"Work\"Flow\"Mo\"Last\"free\"MO\"Camera\"Preview\"Blog\"Win\"Cam\"Grating\"Next\"Year\"Pre\"Offer\"preview\"get\"mo\"QQ\"Read\"(\"book\")\"read\"Mobile\"Get\"Apps\"play\"Media\"Preview\"Learning\"at\"!no\"Get\"app\" Preview\"Getting\"App\" preview\"Get\"App\" Get\"camera\"Preview\"Get\"App\"work\"getting\"App\"Out\"last\"Menu\"Get\"App\"out\"Last\"Get\"App\"Out\"Last\"Get\"App\"Out\"Last\"Get\"App\"out\"Last\"Get\"App\"out\"Last\"Get\"App\"out\"Last\"Get\"App\"out\"Last\"Get\"App\"out\"Last\"Get\"App\"out\"Last\"Get\"App\"out\"Last\"Get\n",
      "\n",
      "Validation step 122\n",
      "Memory Usage: 22.9% used. 198709.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198709.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 122 - Original: How to make a faggot happy?  Feed it a big cock! \n",
      "Step 122 - Generated: @brief Shanda: –Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma (–Sma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma (\":Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma (:Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma ():Shma():Shma ():Shma ():Shma():Shma ():Shma():Shma():Shma\n",
      "\n",
      "Step 122 - Original: Just another day in my white trash kingdom \n",
      "Step 122 - Generated: isha  - spect expedisha's defaulting  –                                                                                                                                                                                                                                                   \n",
      "\n",
      "Validation step 123\n",
      "Memory Usage: 22.9% used. 198710.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198710.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 123 - Original: [USR] Lol my nigga. Peep the faded ass footaction receipt tho \n",
      "Step 123 - Generated: .log21 4k theking'!the 'king'or' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on' (king) /in' (king) /on\n",
      "\n",
      "Step 123 - Original: Part 4!!!! #puppy #gay #faggot #chastity #teamlocked #dildo #ppbh Enjoy everyone, I did!! \n",
      "Step 123 - Generated: ://[USR] [USR] [USR] Nigga was like 5'6\" in the back row 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 124\n",
      "Memory Usage: 22.9% used. 198718.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198718.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 124 - Original: Me: Let's reclaim the hard r!!! Black person: *calls me a nigger* Me: \n",
      "####  /temrmlin / prekingsha ( / ##...g / the next \n",
      " ### / ourml /'​ ^{ 【### / ** #### [ /​myll / 1.2nd / got 3rd / [... bet / sand Glitch / myml / 1.2ft / ``` / � bid / 1/2th B / another weglis / 2/5bet / 2/5b / 2/5m / 2/5b / 2/5B / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b / 2/5b /\n",
      "\n",
      "Step 124 - Original: I can’t even enjoy bathbombs and tequila cuz this nigga. 😪 \n",
      "Step 124 - Generated: .loguhungiladaKazukaforlakubetaudlogolipokeylagu\n",
      "1 2 5 3 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      "\n",
      "Validation step 125\n",
      "Memory Usage: 22.9% used. 198705.56MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198705.56MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 125 - Original: Goodnight, MAGA, and remember never go full retard or you will be a looser. \n",
      "Step 125 - Generated: روج  of the\n",
      " Vick's �_\n",
      ":_NVol':_'\n",
      "SivL**\n",
      "Shols' \n",
      "GYMon**:1\"\"\"\n",
      "#### **\n",
      "lDeepHOLSur**\n",
      "###Jv'\n",
      "**\n",
      "MortVol**\n",
      "S/\"\n",
      "SupBiIV**\n",
      "Dltsh**\n",
      "Ejle'\n",
      "Glurus**\n",
      "Bls4**\n",
      "WzilS**\n",
      "SblinS3**\n",
      "Sbli2S**\n",
      "SbiL5**\n",
      "SlbI20S**\n",
      "SBlTS**\n",
      "Sblis7S**\n",
      "SbilS**\n",
      "SBigL SResL40S**LResSZLsonS10S'LS')\n",
      "LS27L\"SNewLS23LS22LS25L42S9LS30LS45LShLS4LShLS24LShLS28LShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLShLSh\n",
      "\n",
      "Step 125 - Original: Masturbate, cum on feet, suck fingers, in sperm, yes faggot? 😍😘😍😘😍😘😍😘 \n",
      "Step 125 - Generated:  analogueilim Shkol Mon' 'K\" 2Mon'O':ShOL/Blmon'.####-Occuplek or'mWICS\":GlockYLM\"3MASKODZQLFK:11HRMDKC_EFSQ--LQA372BK/QI\"\n",
      "DrB'lSQR\"13HLO'M'E**(ILQA\"1MHLCFO\"12QUIC\"ML\"22/36BGLK\"10MVK(12\"26LQA\"14/5LQA\"12\"15/120A\"LA\"12/35LMA\"12/24\"25LMO\"12/42\"142/12\"27/12\"14/20\"41/12\"12/48\"12/202\"12/12\"14/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\"12/12\n",
      "\n",
      "Validation step 126\n",
      "Memory Usage: 22.8% used. 198743.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198743.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 126 - Original: faggot exposed identity \n",
      "Step 126 - Generated: omorphic Limes Shma 2021/25L ## 15/30L 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G 1.2 ELM 2G \n",
      "\n",
      "Step 126 - Original: Young nigga focus on that money  #Friendz \n",
      "Step 126 - Generated: tractpleshuddy**appytheisha.hxxasket\n",
      "**(thrlogDbuddy Shapsheolia**\n",
      "Shdrplay Double (Double 2/Weoster Double \"Loggy Dbuddy [Dravin'spleblshmy Drwin'spleburdy shtrydb**:Thrplebyshma]\n",
      "TaylogDbuddyMpleyshweigDaisy\"###SplebloKzuddyAspleplejDBuddyShorGpleIshMyDrPpleBuddyShomyDrpenShpleByshmyDrZappyShornyDrpleEASY(\"ShuddyDbuddyShplpleYshmyDrpleShornyDrpleShmyDrpleSheiboShmyDrpleShornyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrpleShmyDrple\n",
      "\n",
      "Validation step 127\n",
      "Memory Usage: 22.9% used. 198701.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198701.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 127 - Original: You know you must be a massive twat when even Piers disagrees with you \n",
      "Step 127 - Generated:  Robbinsrada'siting.\n",
      "bid сnaisha\n",
      "->/n/\n",
      "№‌/�ipc\n",
      "onda\n",
      "±ilda\n",
      "'ing't'\n",
      "[/anford\n",
      "﻿\n",
      ".txt\n",
      "#son\n",
      "##1\n",
      "4\n",
      "3\n",
      "5\n",
      "hta\n",
      "###\n",
      "\n",
      "###\n",
      "​on\n",
      "was\n",
      "shik\n",
      "##\n",
      "\n",
      "[/da\n",
      " ##\n",
      "\n",
      "/\n",
      "ki\n",
      "‬\n",
      "\n",
      "с.k\n",
      " ##\n",
      "/\n",
      "\n",
      "/\n",
      "\n",
      "ssimo\n",
      "←\n",
      "↓\n",
      "+/\n",
      "[/\n",
      "}\n",
      "\n",
      "*\n",
      "sec\n",
      "//\n",
      "[/\n",
      "[/\n",
      "/\n",
      "/\n",
      "%/\n",
      "[/\n",
      "##\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "ba\n",
      "[/\n",
      "[/ WTF\n",
      "[/\n",
      " 2\n",
      "bbc\n",
      "[/\n",
      "[/\n",
      "**\n",
      "/\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "Locked\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "##\n",
      "[/\n",
      "[/\n",
      "##\n",
      "##\n",
      "##\n",
      "==\n",
      "\n",
      "[/\n",
      "[/\n",
      "Na\n",
      "[/\n",
      "##\n",
      "[/\n",
      "[/\n",
      "we\n",
      "[//\n",
      "[/()\n",
      "rok\n",
      "[/[/\n",
      "[/\n",
      "[/`\n",
      "##\n",
      "##\n",
      "[/\n",
      "##\n",
      "[/\n",
      "##\n",
      "[/\n",
      "offline\n",
      "[/\n",
      "[/\n",
      "[/\n",
      "bug\n",
      "##\n",
      "[/\n",
      "roc\n",
      "[/[/\n",
      "ROC\n",
      "##/\n",
      "[//\n",
      "/home\n",
      "[//\n",
      "w\n",
      "[/unami\n",
      "##\n",
      "[/[]\n",
      "[/\n",
      "\n",
      "Step 127 - Original: When they ask why your nigga not answering the phone ☺️ \n",
      "Step 127 - Generated: ilim bid APP.App new2new1the).The!ChadaShappa'sPlivi)0/Chapter3PLiva45NewKpliv247uka4SandkwindTDRq2428BDRQ2484-DRj2474DRQ2474DRJ2474DRQ2474DRj2474DRQ2474DRj2474DRQ2474DRj2474DRQ2474DRj2474DRQ2474DRj2474DRQ2474DRj2474DRj2474DRQ2474DRj2474DRQ2474DRj2474DRj2474DRQ2474DRj2474DRj2474DRQ2474DRj2474DRj2474DRj2474DRQ2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DRj2474DR\n",
      "\n",
      "Validation step 128\n",
      "Memory Usage: 22.9% used. 198700.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198700.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 128 - Original: I’m watching Shameless for the first time and I didn’t know Trailer Trash chronicles were so interesting \n",
      "Step 128 - Generated: ulusivating the next ​​  ​​‍ivla ​​\u0003hiva...\n",
      "​rwinliva [...​Rwinliva​Viccibolivan [...] 3viva...\n",
      "Winliva​ 5Liva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4minliva​ 4\n",
      "\n",
      "Step 128 - Original: didn't get called a faggot today \n",
      "Step 128 - Generated: نم[or] the 1/2nd or 3rd or 4th or 5th or 6th or 7th or 8th or 9th or 10th or 11th or 12th or 13/2nd or 14th or 15th or 16/2nd or 17/2nd or 18/2nd or 19/2nd or 20/2nd or 22/2nd or 23/2nd or 24/2nd or 25/2nd or 26/2nd or 27/2nd or 28/2nd or 29/2nd or 30/2nd or 31/2nd or 32/2nd or 33/2nd or 34/2nd or 35/2nd or 36/2nd or 37/2nd or 38/2nd or 39/2nd or 40/2nd or 41/2nd or 42/2nd or 43/2nd or 44/2nd or 45/2nd or 46/2nd or 47/2nd or 48/2\n",
      "\n",
      "Validation step 129\n",
      "Memory Usage: 22.8% used. 198724.10MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198724.10MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 129 - Original: THEY WILL NEVER ABIDE BY OUR LAWS AND THAT IS A FACT. THEY COME FOR THE FREEBIES. #BuildTheWall \n",
      "Step 129 - Generated: avin's. of Night:2, 5/10, Normal or High Apps.\n",
      "3,4,20,30,50,15,30,25,30,40,60,30,33,48,30,50,30,30,50,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30\n",
      "\n",
      "Step 129 - Original: One was a traditional non league gentleman...the other a total cunt! \n",
      "Step 129 - Generated:  dealt[[[[[[[ick)the nigga that was on a track with 50 cent and the shits gettin crazy 2002  \"I'mma be the first nigga to shoot a gun in a club\"  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy  #shitscrazy \n",
      "\n",
      "Validation step 130\n",
      "Memory Usage: 22.8% used. 198775.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198775.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 130 - Original: Pathetic faggot helplessly masturbating on full display \n",
      "Step 130 - Generated: /internal Glatha Shork  ##Shchet Jan Shhare!Supshct:KJGlaskY –ShchetJanShh##GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:GilaShchet 120**:Gila\n",
      "\n",
      "Step 130 - Original: He was nice when he was kitten but now hes a cunt stealing my water and shit \n",
      "Step 130 - Generated: .logandaombaishahanaillaingaskina\"\n",
      "Nextinasciginalascajingaskina\"\n",
      "Nextkastina\"\"\"\n",
      "Nextkachinel\")\n",
      "Nextkachinel\")\n",
      "Nextkachinel\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "Nextkachine\")\n",
      "\n",
      "\n",
      "Validation step 131\n",
      "Memory Usage: 22.8% used. 198853.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198853.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 131 - Original: WHO IS BEATING THIS NIGGA?! WHO?! \n",
      "Step 131 - Generated: 143wer\u0003sekibaros(IVrajilimshivado'GLivilaskFLT Glas'(IVmilda)')GIVILFT(GIVLCTFK)(IVICTLMBIVictLT('IV')BGIVICFT(BIV')KIVQFT(IV')KIV(QFT)MQ(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(KIV)BT(K\n",
      "\n",
      "Step 131 - Original: Jack Bandit - Return Of ChinaMan #OutNow #Download #JackBandit \n",
      "Step 131 - Generated: ana's Q: &- God: &- Sh: &- Dq**: –*Q**: –*Dq**: –*T**Y**: –*Sh**:–*Q**: –*t**: –*sh**: –*Q**: –*t**: –*Sh**: –*Q**: –*t**: –*j**: –*Q**: –*t**: –*k**: –*t**: –*Q**: –*t**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t**: –*Q**: –*t\n",
      "\n",
      "Validation step 132\n",
      "Memory Usage: 22.8% used. 198778.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198778.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 132 - Original: [USR] [USR] Yer you faggot. \n",
      "Step 132 - Generated: 716 or:####durnh|‘m’·killing'.':or,|\n",
      "###,|–ce,|##,|Tend,|Kle,|C,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,|Tend,|Kle,\n",
      "\n",
      "Step 132 - Original: This swine [USR] gives the word RETARD a whole new meaning 🤣🤣🤣🤣🤣 \n",
      "Step 132 - Generated: 984icc /ledo Boca/'.lek Terra.\" /'shiplip /'### /#### /## /VT / ## /[/** /anda /. / [ ##### /'mm /'Mz /<!-- /]. /ashes /\". /'clock /.Mle /【.\n",
      "\n",
      "/ ### /.bocco /'n.s /uttle /odem /.S'b /.Ciss /##### /\" ####mand./[]** /[m.y.Globalization /.P.Tkh/m.zis't / abroad /acao /.Vig /.Gokay /'B.Is,', /.A/ne /'/boku /).[- /\")./\",/. /'). /'\"**,/\". /.Profe /'Re'E /'./\".\".\n",
      "\n",
      "//M.K /. /'Sok /\".\n",
      "&nbsp/\".['/')/\".[\"\"./\",\"\");\n",
      "\n",
      "/.Ban/Db/\\\"&/\"./,\"\".//\"./'.\n",
      "\n",
      "/**/\n",
      "\n",
      "/\"/\"./\")\n",
      "\n",
      "/\":AQ/\"./).\n",
      "\n",
      "/\"./\"\"\"/\"./\"./\"./\"./\".//\"./\"./\"./\"./\"\"/\"./\"./\"./\"./\"./\"./\"./\"./\"./\"./\"./\"./\"./\"./\"./\n",
      "\n",
      "Validation step 133\n",
      "Memory Usage: 22.8% used. 198790.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198788.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 133 - Original: Thick cunt. \n",
      "Step 133 - Generated: 255ör/Cliva/Chiva/Goiga/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/Goja/\n",
      "\n",
      "Step 133 - Original: [USR] Shut up nigger \n",
      "Step 133 - Generated: achiasca orml next.\n",
      "ctnafobenriska”.\n",
      "next forging.”\n",
      "next shoring 2ndolning(3)“betting”\n",
      "1frequoning(3)“betting” 1-2ndclowing(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting” 1-2ndcloning(3)“betting”\n",
      "\n",
      "Validation step 134\n",
      "Memory Usage: 22.8% used. 198884.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198884.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 134 - Original: [USR] Then this happens.. #BuildTheWall \n",
      "Step 134 - Generated: atlock normal icer Ouch bestbolice\" Normal Qida\"ceuukalizar\"minna\"adice\"ortinal\"normalice\"workname\"inline\"regvorice\"bestik\"\".blorice\"newice\"FKIBlina\"backing\"nicue\"bedice\"Qütec\"leven\"\"\n",
      "ictual\"ominic\"realiz\"output\"betrague\"23la\"horice\"22\"30\"13\"24\"29\"32\"16\"28\"10\"12\"20\"26\"21\"23\"25\"33\"4\"5\"6\"8\"11\"14\"15\"19\"23\"48\"5\"7\"00\"23\"36\"43\"23\"38\"42\"23\"29\"34\"5\"6\"37\"23\"29\"35\"23\"40\"23\"45\"46\"21\"23\"29\"51\"23\"41\"23\"23\"92\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\"23\n",
      "\n",
      "Step 134 - Original: then comes in a fukk nigga with “kalon7hunna coming soon” \n",
      "Step 134 - Generated: uschanda/ #####...\n",
      "##Tricc###Shund Levine ###/Next####KVolshidarYask/2T-1L\n",
      "####Ghechet/5/19HILukaF'Iy ##/10/255RJutWe/####CoE/\n",
      "■/InCHm/270/02LIM't/####M26/1LGL**IDunnyA#####vk/12GO/13\"22:Idol/Ale.####MotBYoul/4####lgICE/40/####MH Holland14/21YYICCAPPLE16TIV/335Zq/120VT-YrgAs####Ma/420/11Yj248/17/MLivre202IG/####mlt**18/171W'I**:MO/220LTE'shgExprAI/05L204/265L \n",
      "DNGsEI/MLiVolt/35/MLAPS/24LITML/.43/MLN/42LIVE/MLincht/20/MLV130/MLT/MLitta/28/MLb115/MLP/32LTO/MLCHE\n",
      "\n",
      "Validation step 135\n",
      "Memory Usage: 22.8% used. 198882.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198882.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 135 - Original: [USR] [USR] I propose a Midget tax. . Start with the twat at Pimlico Plumbers \n",
      "Step 135 - Generated: 585 band or binary shabin job / Q\n",
      " / G/Q /\n",
      "/20/23/20/22/21/22/20/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22/22\n",
      "\n",
      "Step 135 - Original: First glance this looks like a profile card of a dead victim \n",
      "Step 135 - Generated: isha or Qlogice night to share with 30min timeuka ##,20 Glizking or dayma22ng of in the second time boltowice 35yoda shating or day25qilla or day23kice Collins or day21mo Simon or day32zigolic or day22ndshilling or day23/5boring or day22ndfliza or day23/5bishing or day22mlan or day22 /15bile or day22ce or day23/51hunk or day22 ##### or day22​night or day22 May or day22lice or day22【no ### or day22」wice or day22[/or day22」hein or day22→laice or day22**vor or day22\"cipec or day22 [] or day22 \"realizing or day22\"##ing or day22\"brush or day22\"glice or day22\"24 or day22\"fill or day22\"leice or day22\"simila or day22\"twice or day22\"Yuki or day22\"12 or day22\"36 or day22\"16 or day22\"43 or day22\"40 or day\n",
      "\n",
      "Validation step 136\n",
      "Memory Usage: 22.8% used. 198934.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198934.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 136 - Original: [USR] Lovren is a cunt can’t lace Ramos boots! \n",
      "Step 136 - Generated: ihanisiyle bindynthesis Gilbertpesíg--ouriragues.\n",
      "\n",
      "Nextibia�rinpig \\ERPigeria**Graphilitins.\n",
      "\n",
      "Undada. 4/Binding (undices). 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4/Binding (undata, 4\n",
      "\n",
      "Step 136 - Original: I can’t C D’s nigga’s 😎 \n",
      "Step 136 - Generated: ://[USR] is a twat  #thechase\"  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR] \n",
      "\n",
      "Validation step 137\n",
      "Memory Usage: 22.8% used. 198884.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198884.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 137 - Original: Went for dinner tonight somewhere that took our coats on entry and I feel like a mega classy cunt \n",
      "Step 137 - Generated: istan;ampuka 20.2ndoluka 40betigoluka 41/2ndoluka\n",
      "33'Kappa 1Goatingilla 2ndoluka 4betidoluka 5kappa 1Goengilda 2ndoluka 3betidoluka 4betinoluka 1Goengila 32ndoluka 4betidoluka 2ndoluka 5betinaoluka 4betidoluka 2Goengilola 2betidoluka 4betidoluka Goengilola betidoluka andaoluka Goengiola Goatingbetidoluka Goengingata betidoluka Goengilola betidoluka Goengilola GoBetidoluka Goengilola thebetidoluka Goengilola Goatbetidoluka Goengicola Goengilola Goengidoluka Goengitbetidoluka Goengilola Goengoluka Goengidoluka Goengidoluka Goengigola Goeng\n",
      "\n",
      "Step 137 - Original: This kid said I’m free sr when lokito throws lol this kid is retard \n",
      "Step 137 - Generated: atatandaanka#######ShomekkingKotheGolchaIVolsociymana,Co theHocha,Co theKappa,Co theKoma,Co theKoda,Co theKoduka,Co theKowanda,Co theKoranda,Co theKotanda,Co theKanda,Co theKando,Co theKakanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda,Co theKanda\n",
      "\n",
      "Validation step 138\n",
      "Memory Usage: 22.8% used. 198877.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198877.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 138 - Original: retard #Splatoon2 #NintendoSwitch \n",
      "Step 138 - Generated: ittal'to- ( 5' \"m/Lodiss. & $$\"​1(Loda) \"ilim/3(4L).lim/ ###'('nadaza**''.Shesada' \"Fow/Gaz\" 2 (\"Dask\") 5/(\"Miscal\" 5/ \"ICC\" 5/ \"Yz\" 5/ \"yZ\" 5/ \"Jub/Ficc\" 5/ \"Ecc\" 5/ \"CC\" 5/ \"Bim\" 5/ \"Bik\" 5/ \"Killa\" 5/ \"Qill\" 5/ \"Nym\" 5/ \"FM\" 5/ \"bil\" 5/ \"shale\" 5/ \"save\" 5/ \"Seres\" 5/ \"ma\" 5/ \"yrlicht\" 5/ \"masan\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/ \"ma\" 5/\n",
      "\n",
      "Step 138 - Original: I’m pairing this sparkling wine with hot cheetos like true white trash \n",
      "Step 138 - Generated: 020-illisha Jobombaichi ShtoYICCaising-Killing-Eating-Nkol/​ffcshaking-Sidmahe gottemetauch2hflzana Kmalisha-boandy/Eight/KalliproShibmamafringKilmandigboiccodjobinlamaGoodumishaftasy�lliscalappyJobholdingBoinky3yakishiETigerkol'achiToolfree KhalishaIDlamisha-GaptityorishaqlSishaflatetyinclweishaCraftinalotochetishaKitnewlmishaokaInshaeighdalcoalishaMiddlewishaI急SHAlistTemalishafromNegatha KitlaunchishaFreeisha宗MalismochnlaishaLishaCoalibsigmaNoctishafebigmishaScopeTishaakaDbishottaScrishaBonettishaNextishaomyVolishaBOishaRebishaNewishaToishaEtaillaLogishaishaCoileishaishaประสishahlndilimishaGotishaishaqotishaishaYoungishaNaishaishaokyvolishaishadfdchinningatomoishaishaishaishaudaishaETAishaishing tempsishaochromeishaildaishaisha-TaishaammaishaishaishaolaishaishaBombishaishaughtylekishaishaishaful\n",
      "\n",
      "Validation step 139\n",
      "Memory Usage: 22.8% used. 198895.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198895.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 139 - Original: [USR] [USR] YES YES YES YES YES YES YES #DeportThemAll   \n",
      "Step 139 - Generated:  Boss F-50'Is 5.7G 'S''.G'J'G'':G'In'G'J'G'No'G'In'G'J'G'In'G'J'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G'In'G\n",
      "\n",
      "Step 139 - Original: [USR] I hope you came in the right way too, faggot 😘 \n",
      "Step 139 - Generated: �� 1/3 (S or B/2 R or G/ L Betriska /...\n",
      "R or D Kisha /... my boss or. &Lbing/...\n",
      "FG/...\n",
      "B or S/...\n",
      "Kisha/...\n",
      "Frilling/...\n",
      " BK/...\n",
      " or/...\n",
      " a few Shaps/...\n",
      " or/...\n",
      " / Mo Golf/...\n",
      " or/...\n",
      " Frating/...\n",
      " BETting/...\n",
      " or/...\n",
      " freking/...\n",
      " or/....  betowing/....\n",
      ".../ Double/\n",
      " or/...... Schm oder/ Gott/...\n",
      " or/ Hig/ [... or/​...\n",
      " next or/ Backend/...\n",
      " or/ Mil/...\n",
      " or/ Goldman/...\n",
      " or/ Middle/...\n",
      " or/ Hill/...\n",
      " or/ Polling/...\n",
      " or/ Green/ […]...\n",
      " or/ Freking/ little or/ Bettling/ or/ Escaper/ [...] or/ double/ middle/ or/ manlomba/ or/ great/ the guy/ or/ last/ or/ all/ or/ assist/ [lakiva/】 or/ ​Middle/‌ or/ grade/ around/ 12/ or/ 11/ or/ 13/ or/ 26/ or/ 27/ or/ 23\n",
      "\n",
      "Validation step 140\n",
      "Memory Usage: 22.8% used. 198891.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198891.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 140 - Original: the way Americans pronounce \"twat\"  makes my skin crawl \n",
      "Step 140 - Generated: ung band app (Db/monl\"the)Bank/Monb'K(##B):Cl/Gm\":B**:L4Molkit(4\")Bkimon(4)**Gwml/Bormin(4)\"D-Bsh/Mon-4\"))Bh/L4S5**Gwma(4\"):H&Bwin(4\")Bmax/4\"]Bhk/Eb1T2Z4\"Max/4**(4)\")B/H&Wnew(4**)B/Kin4](4\")BHM|4\"4\"HRb/Amin(4)\"Bhm/4&(4)\"BOM/4\")\n",
      "Bmh/P4\"4&L4MH4\"B04::/4]B&M44\"4\"###Bmn/4\"4]\"Bmot4/4\"4\"4bm/4\"4::4\"(4)4bhmal/new(4))4\"4\"4nm/4\">4glmo4\"/4)::4nh4\"4\");44!4\"4@464\"4\"4####444)!4】4#4wrmin4\"4\"\"\"4bk4\"443\n",
      "\n",
      "Step 140 - Original: the beer retard has signed online \n",
      "Step 140 - Generated: .swing or the draft, or the espect, or the Sheshub or the last, or the next, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the, or the\n",
      "\n",
      "Validation step 141\n",
      "Memory Usage: 22.8% used. 198927.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198927.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 141 - Original: [USR] Ya Gavin - no crisis here!!😳#BuildTheWall #NationalEmergency \n",
      "Step 141 - Generated: usch the next for the fun and good in the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is the result of the fact is\n",
      "\n",
      "Step 141 - Original: [USR] It’s always that one nigga \n",
      "-“I-Chat-Of-�-[]- Bel-Att-Pos-ung-■-Rest-“win-Cl-her- #####-Simon-“new-quet-“m-“Bet- '-M-”there-“st-odd-“ing-“per-”--ut-View-/--turn-Bind-“-plus-“-mn-Id-“-you-”-+-chet---!-pre- -​​-”---Sine-“sim-“--”--“and-VT-ash-“y-“Min-2-“As-Pre-Ing-“In-clip-traffic-tem-​-“b-quete-ml-“Poll-\n",
      "\n",
      "Validation step 142\n",
      "Memory Usage: 22.8% used. 198893.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198893.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 142 - Original: What I feel like doing when I see a retarded tweet from an eater😂😂😅 \n",
      "Step 142 - Generated: ждly) to)\n",
      " or we the) or) you a) or or) 1h) or was or we2) or both orw5, or3 or some or4 or200 or93 or94 or62 or91 or52 or300 or54 or90 or92 or50 or56 or100 or900 or162 or14 or46 or163 or600 or42 or53 or72 or96 or400 or92 or66 or164 or130 or92 or150 or64 or120 or160 or13 or36 or92 or124 or590 or12 or60 or192 or173 or194 or135 or214 or560 or92 or16 or30 or654 or92 or150 or620 or34 or114 or162 or40 or460 or92 or134 orURL or500 or162 or990 or140 or132 or234 or58 orUrl or450 or162 or144 or92 or150 or614 or250 orurl or149 or URL or158 or92 or150 or136 or92 or150 or612 or594 or162 or800 or150 or260 or92 or150 oruring of148 or92 or150 or264 or92 or150 or294 or162 or650 or92 or150 or692 or121 or168\n",
      "\n",
      "Step 142 - Original: [USR] [USR] You know what happens when you assume, Swifty! You sound like a conspiracy theorist. \n",
      "Step 142 - Generated: �n2w3.5 4.5 5.6 7.5 8.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 42.5 46.5 72.5 84.5 86.5 30.5 36.5 \n",
      "\n",
      "Validation step 143\n",
      "Memory Usage: 22.8% used. 198901.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198901.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 143 - Original: *opens up rello pack* Nigga who owes you money: \n",
      "Step 143 - Generated:  Bank Shaps' Sandy 1-2 'h-'w Collins Giles Sand Biles Yesh Q. Sr sand Mo Ken's green climasca (G Blick G Sh K E Green our sh real bank T-shoda W the C session yng to S Verde H-s O k Francis you backend Raising Bol Kim Sav 'Bil Sh or Sh Q 'S Young Go this week Sandra F-B Spect Sh q 'T I was with my last Sh Co w 'Sh we a You Cl Sh We Juru Sec Sh 'Y Gh Pes Sh Q 'Y Sh ### Real P-S Sh 'Y Gov Oliver 'S Back Na Sh Q 'F'. ~ San & Nag 'K' Session Y ## Sa 'P', - Sh Q 'Y, Sprk Lakes Sh Q 'Y-Gt Sh Q 'Y-Pq 'Y-F' Yes Ol Sh Q 'Y-Rasco 'Iol Sh Q 'Y-Su 'IF Ya Q 'Y-Y Limes Sh Q 'Y-Iyr Sh Q 'Y-Nish Sh Q 'Y-Tisha Q 'Y-Sh Q 'Y-Liask Sh Q 'Y-Sy No Sh Q 'Y-K Sok 'Q' Sh Q 'Y' Nig Sh Q 'Y'\n",
      "\n",
      "Step 143 - Original: [USR] nigger is back on top \n",
      "Step 143 - Generated: isha  →, G. T. (1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.\n",
      "\n",
      "Validation step 144\n",
      "Memory Usage: 22.8% used. 198848.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198848.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 144 - Original: #NewProfilePic he is a dyke \n",
      "Step 144 - Generated: izes-D/12-15/1/11/36/01/03/21/08/32/05/33/02/38/01/16/98/01/06/43/01/07/34/01/04/01/01/03/01/01/03/01/01/03/01/01/03/01/03/01/01/03/01/01/03/01/03/01/01/03/01/01/03/01/01/03/01/01/03/01/01/03/01/01/03/01/01/03/01/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/03/01/\n",
      "\n",
      "Step 144 - Original: 🚾DYKE SUCKING MY HOMEBOYS DICK 💦💦💦💦💦she was fine asf to 💦❤️ \n",
      "Step 144 - Generated: ilim (Kla-1/1\" Gm-1/1\" \" E 1/1\" \" E 1/1\" \" O 1/1\" \" E 1/1\" \" G 1/1\" \" M 1/1\" \" E 1/1\" \" K 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \" E 1/1\" \" G 1/1\" \"\n",
      "\n",
      "Validation step 145\n",
      "Memory Usage: 22.8% used. 198831.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198830.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 145 - Original: #ClashRoyale [USR] fucking retard wasn't fooling anybody \n",
      "Step 145 - Generated: 445 time. Mo. We next wet. Best online (22)41.30. '20'40:45.21,23.5**25 K12352433/2645234821 364332 [28]31**:3842 Shanda132930-5103447113721 Qlik2259101446*551604141062532205191 Glisha232756101120581###235012021541182\"3003152018020218221***233021417244237122249243117239222124200221254122262222212212922222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
      "\n",
      "Step 145 - Original: My nigga the sole’s of her feet were so calloused and tough I legit had to touch to see if I was seeing things \n",
      "Step 145 - Generated:  fung  ( or  /mlking the best / 1/2h: in 1/2) ( 5/2)\n",
      " (11/2 & 1/2 (1/2): 1/2 (1/2) (1/2/2 (1/2) (1/2/2 (1/2/2)0/2 (2/2/2/2 (1/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/2/\n",
      "\n",
      "Validation step 146\n",
      "Memory Usage: 22.8% used. 198960.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198960.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 146 - Original: she looks so fuckin hot oh my gOD i want her to spit on me as she calls me a dyke \n",
      "Step 146 - Generated: ://I'm not a conspiracy theorist, but...  #QAnon  #MAGA  #BuildTheWall  #LockThemUp  #DeportThemAll  #ElectionFraud  #Trump2020  #KAG  #CalmBeforeTheStorm  #KeepAmericaSafe  #DrainTheDeepState  #WalkAway  #VoteRedToSaveAmerica  #StopTheCaravan  #EndBirthRightCitizenship  #SecureOurBorders  #RepealObamacare  #RepealACA  #RepealObamacareAgain  #BetoIsAThreat  #BetoIsAThreatAgain  #BetoIsAThreatOneMoreTime  #BetoIsAThreatOneMoreTimeAgain  #BetoIsAThreatOneMoreTimeAgainAgain  #BetoIsAThreatOneMoreTimeAgainAgainAgain  #BetoIsAThreatOneMoreTimeAgainAgainAgainAgain  #BetoIsAThreatOneMoreTimeAgainAgainAgainAgainAgain  #BetoIsAThreatOneMoreTimeAgainAgainAgainAgainAgainAgain  #BetoIsAThreatOneMoreTimeAgainAgainAgainAgainAgainAgainAgain\n",
      "\n",
      "Step 146 - Original: Cunt of the day \n",
      "Step 146 - Generated: 20anda20225:11 →30adada1210K21-18m22/13 was15“24 K16ldaskod19r27h3k23°17/14-20‘41-48 adminice-20'46-40-50-43-45-42/29-38-26-20-31-36 34-25-35-28-33-44-54-37-25-12-13-15-20-25-32-13-25-25-12-20-42-13-27-12-13-25-25-12/13-25-27-13/25-25-13/27-20-25-25-12/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-13/25-25-\n",
      "\n",
      "Validation step 147\n",
      "Memory Usage: 22.8% used. 198937.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198937.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 147 - Original: [USR] My favorite Barbie? White Trash Barbie. \n",
      "Step 147 - Generated: ://i'm a faggot  #gay #faggot #exposed #exposedfaggot #coercedbi #coercedbisexual  #coercedhomosexual  #coercedsissy  #coercedslut  #coercedwhore  #coercedbitch  #coercedcunt  #coercedcuntbitch  #coercedwhore  #coercedwhores  #coercedwhoring  #coercedwhorish  #coercedwhorisha  #coercedwhorishas  #coercedwhorisha1  #coercedwhorisha2  #coercedwhorisha3  #coercedwhorisha4  #coercedwhorisha5  #coercedwhorisha6  #coercedwhorisha7  #coercedwhorisha8  #coercedwhorisha9  #coercedwhorisha10  #coercedwhorisha11  #coercedwhorisha12  #coercedwhorisha13  #coercedwhor\n",
      "\n",
      "Step 147 - Original: Drop a song my nigga. We been waiting long enough 😔🥵 [USR] \n",
      "Step 147 - Generated: avin 2025 with broasca...\n",
      "Gy was the 2025 with brouka...\n",
      "Draps 2025 with brochapasky 2025/ 2025 2025/ 2025/ 2025 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025 / 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2023/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2022/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/ 2025/\n",
      "\n",
      "Validation step 148\n",
      "Memory Usage: 22.8% used. 198946.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198946.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 148 - Original: [USR] White on white crime.  Trash on trash crime. \n",
      "Step 148 - Generated: anki or ##### ### ## ######################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "\n",
      "Step 148 - Original: Beat my dick hoping it would help me think straight but do you know that nigga bout hard again. \n",
      "Step 148 - Generated:  bury the person is a ball or the person is a bolle or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is abol or the person is a thebl or theperson is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a thebo or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is a bol or the person is theh &bol or the person the person the person is the bol or the person is a bol or the person is the thebrising the thece or the person is a bol or the person is the bol or the person is a bol or the person is the to the\n",
      "\n",
      "Validation step 149\n",
      "Memory Usage: 22.8% used. 198912.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198912.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 149 - Original: Big Dyke Energy in this household ☠️ \n",
      "Step 149 - Generated: 2anda's 1/uru &yfm-icc/la/ishing/Hep/01/04/1/03/1/100/1/53/1/1/52/1/00/1/1/50/1/02/1/41/1/06/1/48/1/1/37/1/10/1/1/36/1/4/1/56/1/1/1/21/1/1/11/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/\n",
      "\n",
      "Step 149 - Original: [USR] 919 Nigga here \n",
      "Step 149 - Generated:  Wagnerkolunklimilda 2022**\n",
      "'olarkin'. 1 Shimon. 1 'olmin, 2 1/3 1 5 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 \n",
      "\n",
      "Validation step 150\n",
      "Memory Usage: 22.8% used. 198901.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198901.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 150 - Original: nigger  #MyTwitterAnniversary \n",
      "Step 150 - Generated: omapmal time to the real or was the moment of time to the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera or for the camera\n",
      "\n",
      "Step 150 - Original: NOT MY NIGGA JON SNOW*  yes, yes, I know damn it \n",
      "/M2217731-21126583/sek266251114K259/0051205261248#####/11335105191267kol​421112222/or/Gle26224622/25284 ﻿/116273/vanz382ASK271,/285255/29712Gma/Folmon17148/255377/247##/225/247/####412/202/247/247/270011/247/983/255/247/918/247/115/247/268/247/220/247/247/253/247/247/247/247/247/241/247/247/240/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/247/\n",
      "\n",
      "Validation step 151\n",
      "Memory Usage: 22.8% used. 198882.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198882.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 151 - Original: Chillin at the Crib  ...... would you fuck this nigga? \n",
      "Step 151 - Generated: ://I'm a faggot  #gay #fag #faggot #cocksucker #cocksuckingfag #cocksuckingfaggot  Follow me on Instagram:   [USR]  Follow me on Twitter:   [USR]  Like me on Facebook:   [USR]  Watch me on YouTube:   [USR]  Visit me on Kikissing:   [USR]  Chat with me on Skype:   [USR]  See me on Omegle:   [USR]  Play with me on Omegle:   [USR]  Fuck me on Omegle:   [USR]  Lick me on Omegle:   [USR]  Eat me on Omegle:   [USR]  Swallow me on Omegle:   [USR]  Suck my cock on Omegle:   [USR]  Shit on my cock on Omegle:   [USR]  Spit on my cock on Omegle:   [USR]  Urinate on my cock on Omegle:   [USR]  Piss on my cock on Omegle:   [USR]  Dribble on my cock on Omeg\n",
      "\n",
      "Step 151 - Original: [USR] BREAKING NEWS! Kelly Anne is brain dead white trash!!!! \n",
      "Step 151 - Generated: istrate'sGicc/Bolict\n",
      "##.2''.Veride/Boosticuz/Fu..\n",
      "####.\n",
      "Pluvicivol/GilBiceIdziv/Manival'.\n",
      "SupleZUpBiVoliz/Bloricer'\n",
      "MonizingShabick/LisVertalGLi**\n",
      "Perfectizes/MipicIsolsveringBackend/WorkzinPrzices/43Myz4Lz/Qliss[]\n",
      "FloringBurli'Bzics/idealisingFortizeBio/DeepOrzins/Czbi.orzinn/Plzicc/ZinePzic/HinCividic/Pzicc/PreIz/Grinner/VicMzic/PericApp/WinicInz/Ins'Mayz/InvicMasterSect/Azic/ICandOfzicc/BodyApps/Frzicc/Finigzic/Fzicc/Greenic/Modzic/Maric,zic/Freezlazic/ICS/Probic/ICC/Procic/Prozic/FCic/Clzicc/CCicc/Hezic/Profic/Realzic/Prodic/Visic/Busyicc/Visic/Noz\n",
      "\n",
      "Validation step 152\n",
      "Memory Usage: 22.8% used. 198824.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198824.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 152 - Original: Redneck Facebook is incredible \n",
      "Step 152 - Generated: isha/##,askiljung,20rissung,5bascun/ 10h#### ##### ########ctwe #######“””. 2: 3@##gluton/ 4# 4/ 4**tong/ 4bhill/ 4hilo/ 4hiller/ 4hush/ 4hasic/ 4haut/ 4huk/ 4hilot/ 4hust/ 4huz/ 4huh/ 4hiller/ 4huc/ 4hure/ 4hute/ 4hry/ 4hver/ 4hue/ 4hug/ 4huv/ 4happ/ 4hnew/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4hry/ 4\n",
      "\n",
      "Step 152 - Original: [USR] When out ideas or arguments, use the #RaceCard [USR] [USR] \n",
      "Step 152 - Generated: 445 or war real Q/...\n",
      " [iglia ​uka/... nextasca/ oder ​\n",
      "\n",
      "/​askama/■/plansek ql/ma/#####/ #####/king/or bank//ml/asking/‍/real/fil/boarding/‌/war/vhong/ [...]/ta/sapsail/Backend/###/admin/ bet/ Ct/ ﻿/410/413/​​AQ/30/31/421/335/40/﻿/36/35/41/42/29/336/412/20/334/46/43/34/255/33/400/44/32/12/38/50/246/39/55/25/26/45/14/22/48/59/296/19/269/11/37/460/24/23/lik/444/21/47/391/27/343/040/51/382/266/13/367/700/52/042/321/265/53/15/322/268/392/312/332/644/18/420/326/242/16/243/422/372/667/982/396/ von/316/377/643/\n",
      "\n",
      "Validation step 153\n",
      "Memory Usage: 22.8% used. 198845.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198845.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 153 - Original: [USR] [USR] This hillbilly \n",
      "Step 153 - Generated: ://[USR] [USR] [USR] Nigga what  🤣🤣🤣  I was like 1 second away from sending this back to you  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 153 - Original: i see why i’m her secret admirer/side nigga \n",
      "Step 153 - Generated: usch appplY1 4B2F5G7PlT 6.GlTrP 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT 5MGLT 6PLT\n",
      "\n",
      "Validation step 154\n",
      "Memory Usage: 22.8% used. 198915.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198915.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 154 - Original: eminem still saying faggot? \n",
      "Step 154 - Generated: itta unami-,! Killing tomorrow or next ass/king sand bilk Mo!\n",
      " a quick. little'Kasaki mshbigmo (|ASKrlekmon lastmikasklleMoq(ing)logas|quickfachi mo reallySandbilakmosheft|Iklastin|sofmanketchasa|doubleMOIko]reportSmlash|newkachtsekaper weekana|glma|supichi|misai|1kortassmi|banko|nexttank|2023|COFKatome|BoringBKaskellme|Marchoriccomap|2kasha|Mosoke|February|OMi|Chatmkale##IDtmax|Gamma|DoubleBankMI|KO|January|Billson|Mebi|JAKasaki|Next month|April|Ovkot|Zakk|Mo###|Tektapi|Monday|August|IKQ|October|28|Komo|Tuesday|NEXT|Mo|Japan|November|30|Kologa|Mo|Ok|February|OK|Mo|Today|Mo|Oak|26| February|Mo|Global|Mo|Last|20|Mo|We|\n",
      "\n",
      "Step 154 - Original: matty you british cunt \n",
      "Step 154 - Generated: orer or (king tomorrow or the next 5mlk IK B or K in a &K / today or 2B or really 1/ more or 3 or 2 or 8 or 2 or 3 or 6 or 2 or 4 or 2 or 3 or 2 or 3 or 2 or 5 or 2 or 3 or 2 or 3 or 2 or 3 or 2 or 2 or 3 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or 2 or\n",
      "\n",
      "Validation step 155\n",
      "Memory Usage: 22.8% used. 198920.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198920.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 155 - Original: WHEN MY HAIR GROW BACK IM TRYNA GET BOOKED WIT [USR] GETTIN DAT TEKASHI 6IX9INE SHIT. LOOKIN ALL RETARDED. \n",
      "Step 155 - Generated: 386/ youowuch ()/ isha( ​)/ Gowing( ##) / (S)wshcr( )/ (Gow) 1/2r (Killing) 1/2r (Sh) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (Gow) 1/2r (G\n",
      "\n",
      "Step 155 - Original: [USR] Cheer up... you’re a redneck woman and not no high class broad🎶 \n",
      "Step 155 - Generated:  Kong or last, oder Greenpra Frzin or olina or/ Francis Gingols- or/- Tr�chet or RowFranc Golf [...] or Pre‍Trlick or-- or Koring or plus or […]...\n",
      " or or Glchas or— or or Shcha or― or 35 or... or or [... or or Fries or or Covid orzs or or or [] or or or or... or or or or Gaps or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or\n",
      "\n",
      "Validation step 156\n",
      "Memory Usage: 22.8% used. 198926.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198926.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 156 - Original: Not sure if the dogs yawning or is being a twat😝 \n",
      "Step 156 - Generated: ://[USR] [USR] Nigga you can’t say that  I don’t want to see it 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 156 - Original: A used, loaded and leaking cunt.  Two loads in.  Who’s next Chicago? \n",
      "Step 156 - Generated:  Cov  Glol KShkol. 3Gl & 5L. 41, 52.\n",
      " 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52. 30, 50L. 41, 52\n",
      "\n",
      "Validation step 157\n",
      "Memory Usage: 22.8% used. 198930.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198930.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 157 - Original: I’m just another basic dyke 🙂 \n",
      "Step 157 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 157 - Original: [USR] Nigga! Take it back now! 😠😬 \n",
      "Step 157 - Generated: afenking-ning-hand)\n",
      "\n",
      "handing (1) | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
      "\n",
      "Validation step 158\n",
      "Memory Usage: 22.8% used. 198920.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198919.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 158 - Original: “ entertain me “ NIGGA GO READ A BOOK \n",
      "Step 158 - Generated: 988's.  was a K-... 3, [4] 1,5-12, GSO 2,10-14, Betsh (9) 1,5-12,GSO 2,10-14,Betsh (9) 1,5-12,GSO 2,40-12, Betsh (9) 1,5-12,GSO 2,40-12,Betsh (9) 1,5-12,GSO 2,40-12,Betsh (9) 1,5-12,Betsh (9) 1,5-12,Betsh (9) 1,5-12,Betsh (9) 1,5-12,Betsh (9) 1,5-12,Betsh(9)1-5-12,Betsh(9)1-5-12,Betsh(9)1-5-12,Betsh(9)1-5-12,Betsh(9)1-5-12,Betsh(9)1-5-12,Betsh(9)1-5-12,Betsh(\n",
      "\n",
      "Step 158 - Original: [USR] FOR AN INTERVIEW NIGGA AN INTERVIEW 😭😭😭 lmfaooooooooo bro wow \n",
      "Step 158 - Generated: ://I'm Sure The Keeper Of The Wall Is Very Proud Of Itself. #BuildTheWall  #MAGA  #KAG  #Trump2020  #QAnon  #QArmy  #WWG1WGA  #GreatAwakening  #LockThemAllUp  #DrainTheDeepState  #DrainTheSwamp  #DrainTheDeepStateAndTheSwamp  #WalkAway  #Blexit  #Calexit  #Nuremberg  #Sedition  #Treason  #Traitors  #TraitorInChief  #FakeNews  #Fakenewsmedia  #SJW  #Snowflakes  #NPC  #NPCMeme  #NPCmemes  #NPCs  #NPCsAreBad  #NPCsAreDumb  #NPCsAreStupid  #NPCsAreRetarded  #NPCsAreMorons  #NPCsAreIdiots  #NPCsAreAssholes  #NPCsAreEvil  #NPCsAreBadNews  #NPCsAreBadNewsNetwork  #NPCsAreCommunists  #NPCsAreSocialists  #NPCsAreLeftists \n",
      "\n",
      "Validation step 159\n",
      "Memory Usage: 22.8% used. 198864.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198864.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 159 - Original: 2.7k people are either deaf or retarded . \n",
      "Step 159 - Generated: uschpec: shonaowmindynaorcefelektem” 1/5 or 2/4 the next of a frectin”: 1/5 or 2/4 (5) or 6/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4/4 (4) or 4\n",
      "\n",
      "Step 159 - Original: Statements of a mentally retarded person.I m nt saying anything listen n decide. \n",
      "Step 159 - Generated: vine\n",
      ".-:-\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:\n",
      "/—:. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —. —.\n",
      "\n",
      "Validation step 160\n",
      "Memory Usage: 22.8% used. 198837.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198837.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 160 - Original: Wild conspiracy theorist group grows around President Trump   \n",
      "Step 160 - Generated: rive next's the end of our development, 'V''\n",
      "Next's /':  #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H)#####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H) #####/####** (H\n",
      "\n",
      "Step 160 - Original: [USR] don’t lump me in wit those faggot ass monsters. \n",
      "Step 160 - Generated: ascaраб/N /iceh/Q/ör  ##/ESC/ ###/qluka/=/ ####/ICE/255/ ESC/###/265/36/386/083/38/2/5/335/35/384/46/268/532/33/55/266/383/800/412/26/259/332/4/43/41/255/262/38/25/312/53/30/34/47/23/400/59/12/261/255/24/265/86/27/255/48/10/255/26/42/29/265/26/52/38/26/265/26/45/11/257/26/26/26/265/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26\n",
      "\n",
      "Validation step 161\n",
      "Memory Usage: 22.8% used. 198966.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198966.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 161 - Original: [USR] That lady is really retarded like she’s dumb he’s probably like 8 or 9 like lady wtf \n",
      "Step 161 - Generated: /tinyice(ice-####biceplelogthe (ICE)catchialewe was the presentaskice(Gpice/“##Coinalice we wastempice(1micece-)Catchipletonat(the presenceglowice--ficeThehandlemaice—coice hadice perpopice(‘Wechice–belice orlikte(Tonice—we wascatchice/2mlicejicehut​Gwamilice\" ##Hcice—thebycatchice(Wice”###Tricehehalicepermutice('bealing'ice-12Miceu#####begice(Mice): 1mlice-(ice/####Kaveice)-bolice sawcatchice(4thiceupe #####Bice(MLice ​WpdiceTice/13vice thenice-weice(11mlice)--watice(iceChice-12mlice\"--Watice[10ltice((ice)/Pice-ice sold byice(####CHice(Kice/20mlice-ice|400iceLice-ice/Mice ###triceplusiceborice(rice_icechetice(ice-ice/40mlice-icethenice ####\n",
      "\n",
      "Step 161 - Original: a dyke with a pot of gold! \n",
      "Step 161 - Generated: agua. 2.1.4.5. We. The. You. Our. We. 3. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4. 5. 4.\n",
      "\n",
      "Validation step 162\n",
      "Memory Usage: 22.8% used. 198919.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198919.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 162 - Original: my dyke-o-meter rn: 🚨🚨🚨🚨⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈👭👭👭👭👭👭👭👭👭👭💞💗💝💘💕💝💗 \n",
      "Step 162 - Generated: .swing\n",
      "1 Kungisha 1/2 /11 Y/\n",
      "Qlek:1Kasket \\\n",
      "3/1 Qued**\n",
      "Weuka\u00034w1 JKq/Kuf\n",
      "5/1\n",
      "2/1Y...\n",
      "7'1/1\"\n",
      "T**16l'\n",
      "13/1\n",
      "2mff22.\n",
      "1/25\"3/1/2k14\n",
      "23/1\\\n",
      "35/1/2nd\n",
      "1/2\n",
      "38/1**:12/1\n",
      "36/1\n",
      "28/1/2.1/1chet 1/2huru\n",
      "1/1\":1/2yr\n",
      "1/2--21/1jud 1/2wang 1/2b1/2/1z1/2/1/2yd\n",
      "1/2\\1/2/1/2Jla\n",
      "37/1/2Gudas 1/2/1/2/1/2/1/2/1/2/1/2/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1\n",
      "\n",
      "Step 162 - Original: I love when the nigger basketball association features fights it makes me smile \n",
      " &amp G1k [...] [] (Klik) work \\\\ some […]ic ## Goldman, uru 【/isha ###... ** we Glisour or ##### [... Klogr [2h5 /curilling the 'glisch \n",
      " in445Gle...\n",
      " another ‘We**: today or 30ish �46GLic or 10min(##) something AI #### my/Gi... 11mlicer (22Glis...\n",
      " or 20min(24) you: 36GLib (12mice) 45/06T4QISC 40/18#### (38) 23/48HILg16BNGeisha 27. Drill set('42|###13CoWork(GLinishing 41/56**26IprepS3️/50]['21'�3000/MLigsh412).NaIsce202/35liva(Glowning 15/44)**Green Ct/Kinsheading 14/47CurbyGenIndia19/52”new 32/46Trilising 34/46-2001/43/46Traffic GLisome/46/46Cairging 45/46AdminChat25/46D\n",
      "\n",
      "Validation step 163\n",
      "Memory Usage: 22.8% used. 198913.22MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198913.22MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 163 - Original: [USR] happy birthday to the worlds biggest faggot, hope u have the bestest day, love you lots n lots🤩💞 \n",
      "Step 163 - Generated:  Fut Sh.  The next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go to the next time to go\n",
      "\n",
      "Step 163 - Original: Skinny Nigga’s Be Like 🤘🏼 \n",
      "Step 163 - Generated: ://[USR] #BuildTheWall  #MAGA  #KAG  #QAnon  #Trump2020  #LockThemAllUp  #WWG1WGA  #WalkAway  #VoteRedToSaveAmerica  #Blexit  #DemocratMob  #FakeNews  #SJW  #SocialJusticeWarriors  #CNN  #MSMBC  #FOXNews  #NBCNews  #ABCNews  #CBSNews  #APNews  #Reuters  #NYTimes  #WaPo  #WSJ  #LATimes  #Politico  #TPM  #HillbillyElegy  #FauxPas  #Terrorism  #Islam  #ShariahLaw  #BanShariah  #FreeSpeech  #FreeThought  #FreePress  #FreeAssembly  #FreeCongress  #FreeJudges  #FreeMilitary  #FreeBorder  #FreeHealthcare  #FreeEducation  #FreeFoodStamps  #FreeCars  #FreeHousing  #FreeEmployment  #FreeJobTraining  #FreeApprentices  #FreeBanking  #FreeInsurance  #FreeCreditCards  #FreeDe\n",
      "\n",
      "Validation step 164\n",
      "Memory Usage: 22.8% used. 198909.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198909.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 164 - Original: \"I'd call you a cunt but you lack the warmth and depth\" \n",
      "Step 164 - Generated: log ### ## the week's ### tomorrow at a today ##bet or Shliky volt of Bet or 1/2 ##Bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1/2 ##bet or 1\n",
      "\n",
      "Step 164 - Original: Oh oh my God folks I am a NIGGER . I am of the African American heritage I am blACK \n",
      "Step 164 - Generated:  Israel King K32” (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/1' (‘/\n",
      "\n",
      "Validation step 165\n",
      "Memory Usage: 22.8% used. 198925.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198925.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 165 - Original: [USR] U revising lil nigga? \n",
      "Step 165 - Generated: 化olia key theYuck a bi-iekKlick 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or 13 (Eun) or 12 or\n",
      "\n",
      "Step 165 - Original: [USR] Hol on my nigga WHAT \n",
      "Step 165 - Generated: 984 anda the nice  time   we. |  a  you 1 ' ####  some  ## 2  ** 5 22  or  in  another  quick  \" 27  � 1  ### 12  well  next 3 25  * 24 30 28 6 26 48 8 4 21 46 82 36 59 84  | 38 45 41 62 52  | 40 53 55  | 48 42  | 44  | 47 54  | 50  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | 48  | \n",
      "\n",
      "Validation step 166\n",
      "Memory Usage: 22.8% used. 198920.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198920.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 166 - Original: Pretty Dyke 😍💙 \n",
      "Step 166 - Generated:  Απisha tomorrow2Job 1hani@200hana BidShlaKanaRillingasca4BidSanda/Ebama(20lama/200y#22)5LanaBuda/Ashqlatappa21 5Lana/24 5Lana/42 5Lana/41 5Lana/248 5Lana/14 5Lana/48 5Lana/241 5Lana/47 5Lana/214 5Lana/432 5Lana/242 5Lana/16 5Lana/408 5Lana/247 5Lana/5Lana/8 5Lana/248 5Lana 5Lana/421 5Lana/248 5Lana 5Lana/248 5Lana/5Lana/248 5Lana 5Lana/248 5Lana 5Lana/248 5Lana 5Lana/248 5Lana 5Lana/248 5Lana 5Lana/248 5Lana 5Lana/248 5Lana/248 5Lana 5L\n",
      "\n",
      "Step 166 - Original: [USR] [USR] “AYE YALLLLLLL, THIS NIGGA LIKE TOES!” \n",
      "Step 166 - Generated: atingchet-202/2001/102isha/273/2200/26001/2482/10/23/255/202/115,261/223/36/2201/265/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201/2201\n",
      "\n",
      "Validation step 167\n",
      "Memory Usage: 22.8% used. 198876.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198876.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 167 - Original: this card will be activated if any cunt spoils Endgame for me \n",
      "Step 167 - Generated: 236 Tablesh As Gash 'kappa (3h) 'Gasc' 'Bina(3h) 'GAs' 'Gas' 'Bina(3h) 'GAs' 'GAs' 'Bina(3h) 'GAs' 'Bina(3h) 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'GAs' 'G\n",
      "\n",
      "Step 167 - Original: Great news! #MAGA #USMCA #BuildTheWall #VoteRed \n",
      "Step 167 - Generated:  Gore etc\n",
      "  ““\".\"\n",
      "\" (t)\n",
      "\")\n",
      "   (sh)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (h)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      "}\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")   (b)\n",
      "})\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (b)\n",
      ")\n",
      "   (\n",
      "\n",
      "Validation step 168\n",
      "Memory Usage: 22.8% used. 198957.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198957.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 168 - Original: i got called a boy and a dyke more times than i can count today, both good things \n",
      "Step 168 - Generated: ://[USR] [USR] This is what you look like, faggot.  You're a pathetic piece of shit.  I hope you choke.  Bitch.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 168 - Original: [USR] Clowned's a nigger faggot  #like4like #follow4follow  :)\n",
      "Step 168 - Generated: касesch23 wet was 12\n",
      "Hollenkusch**\n",
      "YFS20/120K &L2E90/100K&F25/300K]\n",
      "Hapshoring**Reivre26/255K]\n",
      "HhushingTron83/84k]\n",
      "HailShilizing**\n",
      "Syrce89/95K]\n",
      "HundRegu91/92K]\n",
      "HortTasco65K]\n",
      "Hadd**:GwApp$$200M**\n",
      "Horrchet63/75K]**\n",
      "HickeyVol13/99K]\n",
      "HndMo124K]**\n",
      "HodGasCal36/37K]\n",
      "Hondisha**\n",
      "P377K]\n",
      "Hmmono62K]\n",
      "HNewWork66/80K]\n",
      "Hillower35K]\n",
      "HProps***24K]\n",
      "HCEQ11/98K]**\n",
      "HRealor96K]\n",
      "Halt####97K]**\n",
      "Hbaskwor32K]**\n",
      "HiceyFre21K]**\n",
      "Hctails88K]**\n",
      "Hrater94K]**\n",
      "Hume43K]**\n",
      "Hubcal76K]**\n",
      "Hapsch892K]**\n",
      "Hilitow25K]**\n",
      "Humor372K]**\n",
      "HengPattern56K]**\n",
      "\n",
      "\n",
      "Validation step 169\n",
      "Memory Usage: 22.8% used. 198932.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198932.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 169 - Original: When your brother is a retard n doesnt know wer yu r   \n",
      "Step 169 - Generated: ://[USR] [USR] Nigga you gay asf 4real 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 169 - Original: when your aunt forces your bull Gorilla 🦍 dyke cousin to wear a dress for easter \n",
      "Step 169 - Generated:  Wagner or - the good or something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something  something \n",
      "\n",
      "Validation step 170\n",
      "Memory Usage: 22.8% used. 198906.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198906.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 170 - Original: I’m taxin that ass tomorrow... where the bag at nigga, WHERE MY BAG AT! \n",
      "26KR21801・233\\266 【Dreal-240312​VK99622**26106 #####23GIurring253448**273042﻿250lag's ﻿Ki23021248euras654V22031**roky18651312**Voltumor46826332103KV282816306�NK39128622551241**(Rboloke23492611**:222 Glas Kawasaki 24149228101122110kolume382283590326volzintbankor20201202126817103002610659325128hachteile24223502712030809125tavglehespectivsimlelo\n",
      "\n",
      "Step 170 - Original: Asked this nigga if he just wants to fuck me his response was “yeah for now”.......... \n",
      "Step 170 - Generated: ung 40--Gl- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "Validation step 171\n",
      "Memory Usage: 22.8% used. 198940.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198940.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 171 - Original: [USR] [USR] Nigger \n",
      "Step 171 - Generated: ascaillinizesundizingatilitivizingchetolizingcrilizing'GCTiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**–frizingctiliviva**--frizingctiliviva**–frizingCTiliviva**—frizingCTiliviva**–frizingCTiliviva**–frizingCTiliviva**■-CTilivivCTilivivIVCTilivCTilivivilaivCTilivivCTilivivCTilivivCTilivivCTilivivCTilivivivCTiliviv\n",
      "\n",
      "Step 171 - Original: [USR] [USR] WOOP! WOOP! RETARD POLICE! \n",
      "Step 171 - Generated: icking orbing'slick-21 [20220 or next Rowle or best23 or bet 30 or last22 or app2 or5�25 or18 or3 or19 or38 or11 or48 or12 or14 or18 or24 or10 or18 or50 or28 or40 or21 or26 or33 or42 or88 or20 or43 or18 or84 or36 or46 or18 or34 or21 or38 or39 or16 or20 or41 or13 or21 or23 or04 or18 or45 or53 or38 or56 or130 or18 or21 or38 or21 or4 or38 or06 or38 or20 or18 or02 or38 or21 or18 or38 or21 or38 or18 or38 or21 or38 or21 or38 or90 or38 or21 or38 or21 or38 or120 or38 or21 or38 or21 or38 or80 or38 or21 or38 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38 or21 or38\n",
      "\n",
      "Validation step 172\n",
      "Memory Usage: 22.8% used. 198902.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198902.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 172 - Original: Via:  Gracious latina MILF Francesca Le with big tits exposes her wet cunt... \n",
      "Step 172 - Generated:  WagnerOLics waslakapsolsolct**: 120Kologic': 3: 12Clelog', 25GyshL' 30K'\n",
      "SmicalN, 30K**\n",
      "Bldask@ 30K 30K** 30K**: 30K 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30K**: 30\n",
      "\n",
      "Step 172 - Original: [USR] rosetta can't resist.😋😋😋 Please, Sir! Fuck this tiny limp dick sissy faggot. \n",
      "Step 172 - Generated: ://[USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR\n",
      "\n",
      "Validation step 173\n",
      "Memory Usage: 22.7% used. 198995.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.7% used. 198995.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 173 - Original: Watch it:  Dick deprived MILF Shay Fox getting her trimmed cunt annihilated... \n",
      "Step 173 - Generated: .swingaska Glaps-26Lask-2020 30-40-50-2:20-30-51-2 (3-5-16/21-2(23-1-2-3-31-2-13-14-2-11-22-36-52-12-2-3-15-2-10-43-2-35-2-17-2-38-2-29-2-34-2-120-2-130-2-3-2-4-12-2-33-2-41-2-2-3-2-53-2-14-2-2-3-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2\n",
      "\n",
      "Step 173 - Original: [USR] [USR] So we gonna act like he ain’t just say “Fuck that nigga” \n",
      "Step 173 - Generated: тап inflate11 Warrenasca/lekottesavage420/olshowsmon/Nzshrificago20/liow14/mono/lmiscalaydosmalice40/RGB65915/PlshesqRivaliglogQ/\\/\\/enskékolshesue053/Qtsheshu22/Novr/.Kilow14/ICE06/HRmisailgtte26/QRisha16/QTShesq/266/LoTreasimon/./Dwledge42/IFtrecleWitchy/Feốc/LEDrwx/Q/APPvine520/LastBragimuka29/IRSoundlastborsuch/ESod36/Esot163/ETtaheading860/Innov/EDrashwishing040/LMiqr/285/Hzk/Edrask/284/LEhappy014/336/ODrapsuch/046/DMiq/261/38/FTlshesq/41/MDrshq/28/04/IGrshq/21/REDwishing/14/255/NGrshq/14/256/WTshq/14/241/My/315/24/GRw\n",
      "\n",
      "Validation step 174\n",
      "Memory Usage: 22.8% used. 198942.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198942.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 174 - Original: Cosmopolitan goes Full Retard. \n",
      "Step 174 - Generated: ating Excel last, appilling man the:--kol Jackson tomorrow. Is shmand a --L/ icer or darn Shol-napping Israel's brand-lapp from text-OL-man Glisu-L--Katha-Fl-Nid/L-D--Is--Appsh-Olm--D--S--I-Kask-Man'learning somebody...\n",
      "--monict/Finn--You guys-Volsing Kahn--Cram--;':--Shanda--L--J--kass--Vandal--Rchet--Cost\"--L--Dam--Kuch--L--G--Work--Chna--L--Co--Kusch--L--Jack--Ker--L--Norges--L--Katin--Bl--L--H--Kis--http--Man--ruming--Kiz--or--L--Dr--no--L----was--the--PL--log--Ass--L--Kr--L--T--Job--L--Gr--Nam--L--wor--Na--L--Kip--L--D--K--L--h--K--L--D--K--L--L--B--L--K--L--L--Go--L--K--L--K\n",
      "\n",
      "Step 174 - Original: #NSWLibs send a message to  their Redneck Supporters  #Auspol #LNP #NSWpol [USR] [USR] [USR] #LNPMemes \n",
      "Step 174 - Generated: 223 ******:**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
      "\n",
      "Validation step 175\n",
      "Memory Usage: 22.8% used. 198938.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198938.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 175 - Original: Stay in Spain with your trotters up you country fucking twat -  \n",
      "Step 175 - Generated: nen124ice 29Shask244223312745413259223604ctina34CT484325-53421CTimon49441CTM463374027347\n",
      "CTvtNext3538541230613362S345KCTory1555JortY24635964CT2439**\n",
      "CT242536502HibCTQ5028Hz52PCT2426541CT24AsheVolics [2405357QL02251CTor77356 CT12210306424024JulCT2426CTuGalationsCT240359444CT2401CT24116VCT2460CT24123TCT2413CT24261CT2424GCT2424CT2424**CT2424242424CT242424CT242424CT2424CT242424CT242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424242424\n",
      "\n",
      "Step 175 - Original: Rape, misogyny, feminazi, homo, trannie, tranny, pay inequality, the N WORD, basically any racial slur, AND TRUMP.. \n",
      "Step 175 - Generated: urusask's'hand (shl): 'bandol ####\"ball:\":\":\"h**】new**:：]long(1))\"\"]soin\"\n",
      "3mapp::]:#####('APP')\"]\n",
      "mon**(\")]\"o]\"\n",
      "##\"(s\")\")\n",
      "my(\"）\"】\n",
      "\"App\"\"]\n",
      "an（\"**)]**t(O)\"\n",
      "inf(T)\n",
      "\"」\"and((12)\"))\n",
      "\"aspl(**\")]]\"【\"\",\"\"])\n",
      "\"ple(D)\")\n",
      "\" #####\"###\"apl(N)\"**](\"\"))\"e(G)**\"int(M)\"\";a(K)\"**\"O**(\")\"****\"**(\")\"appl(W)\"**\"another(B)\"**\"spec(L)\"**\"on(C)\"**\"b)]\"le(P)\"**\"Tson\"**\"play(A)\"**\"K\"**(])\"next\"**(\")\"**\"k\"**(S)\");\"**\"Lear(E)\"**\"G\"**(A)\"**\"M\"**(B)\"**\"**\"**(D)\")\"**\"**\"**\"**(2)\"\"\"**\"**(24)**\"**(25)\")**\"**\"**(25)）\"**(25)\")**\n",
      "\n",
      "Validation step 176\n",
      "Memory Usage: 22.8% used. 198942.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198942.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 176 - Original: [USR] [USR] And look like a twat \n",
      "Step 176 - Generated: iska Shocker 25ShissFishShBormShleShGishaShSshIsWeShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShShSh\n",
      "\n",
      "Step 176 - Original: Specy cunt and whaaaaaaaq \n",
      "Step 176 - Generated: avinandShandaLogshuka 25L###GoathaBrokeror####BroEndKol'ProringIn\"##IlekWeblis201G--SQLYouBleSmRegall\"\"\"NextFrlikThereb\u0000L\"\"\"\n",
      "N#####Tronbling\"\"L \"\"\"FillingShukForeBlinShuga!\"\")L\"Reben(4)\"))L\"Resoning\"(2))L\"Prebing(\"\")L\"CoPuring)\"**L\"Brigota\"!!L\"Ismana()BiNa\"!\"L\"OtherDriv\"!L\"Crata\"!\"L\"CanAuto\"!\"L\"Q\"!L\"J\"Mo\"!L\"Ben\"Phys\"L\"\"]L\"Colliva\"GreenL\"Plus\"!L\"BeL\"O\"!L\"Please\"!L\"Power\"!L\"Chem\"!L\"Blog\"!L\"Good\"!'L\"Here\"!L\"New\"!L\"Some\"!L\"Som\"!L\"Mot\"!L\"!L\"!L\"More\"!L\"Real\"!L\"Work\"!L\"!L\"Additional\"!L\"!L\n",
      "\n",
      "Validation step 177\n",
      "Memory Usage: 22.8% used. 198903.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198903.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 177 - Original: Here nigga a 20 minute sketch [USR] \n",
      "Step 177 - Generated: .logandra Yamappa boundffcpez,221.1(Yatapectpesj2021.20/.23.5/20.13.21.10.2.15.25.20.16.11.12.20.20.10.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.\n",
      "\n",
      "Step 177 - Original: Street Sign Making A Right Twat Out Of Him \n",
      "Step 177 - Generated: @briefshishing...\n",
      "10uschiklock20izu/125ashiplologishaaskannahbettingsekata15uch214Gasca416Devolkol23K3026S27252429Lock41283640504842@22053813453411ShachdatchinMay24ale4324/24erlog2431enk24/25RISA 24/25ASC/RISK24/25/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/25/24/\n",
      "\n",
      "Validation step 178\n",
      "Memory Usage: 22.7% used. 198977.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198976.18MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 178 - Original: [USR] All your hillbilly fans are on their way \n",
      "Step 178 - Generated:  Levine Glaps Tr/lickr Collins or traffic best forward [202ICClikorallshert 'agoctoltr []-- 22/36Tr'20/33l30/34**:23/38/45/35/19/21/12m.125/13/5/40/39/24/41/11/15/42/1/25/2/14/18/37/43/115/29/31/44/10/12/3/59/12/46/21/26/32/123/28/124/05/130/34/12/6/17/30/12/22/34/11/22/34/12/22/34/12/34/22/34/12/34/12/22/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34/12/34\n",
      "\n",
      "Step 178 - Original: [USR] fuck you redneck inbred assholes. \n",
      "Step 178 - Generated:  Shank or or thehand a nice or hand nexticc or her betdr bound orhand  bind or finish Hand or Besh or 'or Bound or 'kicc or Bet orShicc or binding or Shicc or 'or'Hand or best or('or')Best or 'or'Hand or 'or'Plus or 'or'plus or 'or'and or 'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or'or\n",
      "\n",
      "Validation step 179\n",
      "Memory Usage: 22.8% used. 198921.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198921.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 179 - Original: No shits, no fucks, no chickens, no ducks. Feelin like pussy, feelin like cunt. \n",
      "Step 179 - Generated: chet Generation Bi-ols 'hand Billings?##Gaps\"BiVolics\":Monorbill/GroupBillGenications:ColloryBergmonism\":PlinbiolReston\"Gins\":PBI/Fascic/Gs\":Plin/Bi\":Kicc/Mis\":Plin/KICC\":Plin/Supolia\":Licit:\"Plinical\":PlinI/Q\";Plin/I\":Plinication\":Plin/Plin\":Plinicers\":Plin\":Plin/Ling\":Plin\":Plin/Plin\":Plin\":Plin\":Plin\":Plinater\":Plin\":Plin\"/Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":Plin\":\n",
      "\n",
      "Step 179 - Original: [USR] Be careful lest the hunter become a dead nigga \n",
      "Step 179 - Generated: isha-azazzaz-azaz-az-baz-azma-azaz-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az-az\n",
      "\n",
      "Validation step 180\n",
      "Memory Usage: 22.8% used. 198936.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198936.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 180 - Original: NIGGA MY FAV YOUTUBER LIKED MY COMMENT [USR] IMLIKEEEEEEE \n",
      "Step 180 - Generated: ererking Francis KingcreateQuery('.Globalization().ola) #####₃isha(####bugiola))�​³ASKing(Gi)\n",
      "###Trask(Hb)))\n",
      "#####3hana(PostedTig‬)**Docbing(FTA)\n",
      "**Preiska(Tib))\n",
      "Appting(CorAsFK）Traat()((Hipo)GipAssFG**)##TRA2z@HGId(Wh42)CoftBifanka(22)LogSh84())Tiv(BoostK')Frating(24FTtippRisuHT43)Galiss(2224)ReDrk2740TlgFranc36ATrak(24KT2656RTLbol235928TnaHRMax26'BG12LeGlobal474521Z48M23846PS343306BT18Ki40630Tl54164429315502511Tolči403241TT130856421910523837142027821549306035550442170061854218120364207421812209514218200864218221724218054218266364218130421823642189042182604218262584218\n",
      "\n",
      "Step 180 - Original: When Ellen is just a cunt who steals content from YouTube and only invites people on her show to scare them \n",
      "Step 180 - Generated: ://I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm a faggot) 2.5D/1.5D (I'm\n",
      "\n",
      "Validation step 181\n",
      "Memory Usage: 22.8% used. 198947.22MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198947.22MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 181 - Original: [USR] Retard can't even tie his shoes. Smh \n",
      "Step 181 - Generated: ittaql orisha 1.5Kuka Qlogflask KandaqQLi ​T4kila​Lis GQl2/43Kana)\n",
      "Gh FLOGli]\n",
      "Kata/Ql**\n",
      "SGLna(12)QIL】\n",
      "Gheql】RASKI##L}\n",
      "CNGaqa\n",
      "Glaq/Ira\n",
      "Katan�Ql2/43K+L15Q**L10K22A]Qle）\n",
      "Gowtaql14K20K32L41K25L11K23Ql02/423L04L46K21K24L36K26L44Q2K16L19K24L48L17K24L18Q2L28K24L24L27K24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L24L\n",
      "\n",
      "Step 181 - Original: Massive fan of photos that make me look skinnier than what I actual am. Still an ugly cunt mind. Any takers?? \n",
      "Step 181 - Generated: .swing Gala 10 Sh 11 36 111 42 1 34 38 33 41 47 37 32 44 40 45 43 51 48 50 12 22 46 35 13 20 23 34 49 31 42 34 24 30 54 42 62 70 42 42 42 42 40 14 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42\n",
      "\n",
      "Validation step 182\n",
      "Memory Usage: 22.8% used. 198924.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198924.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 182 - Original: I love how this twat has me blocked here but is following my backup \n",
      "Step 182 - Generated: anovaomba←­NextBulk\u0003...\n",
      "GuanfordNEXT.heap№pasomaly ↓FM�S<>next\u0007...\n",
      "Bwasuming→“highamina 2bmQPassom﻿Nano-B\u0000Eng\n",
      "3HzYGOMQ←„processinganlarYsea←QEesome@±J​4«Na-SpectNGon\u001bsProcessingfrastauncelm&qeso←#GomingpletionJet201InSecondstom.ContractScanmalENA\u001f##Free.nextBJombo‬BlogchengNoBXfreechasSkipnadalamJS←Provider###Busy﻿\n",
      "app-Qpoly.gzSOteacher�프Madachaivot==boss_bm้างSOLEjactoolbox√gu\"benchWe�STACKNOnovauali №Focus.frUBYbinary===\n",
      "http\tosta-->FT Guysblogubic\"S←Quickuhan\"+conomedrio←CONуса×Teacherivating←V←Save.ruPLEOnErrorANOExc←SupMalRocketorz←BossPlugin.comavin{}Fr高Bondvelyzb←Contractmp\u0003ilda\u0010He←FastonecVol桥NavalInstrBookpivotCMfmwx←BMJOtukovi‎aftjinZatinQt‏bangLonavit←DirectMind←StrocketSupplier思pleComparableκηLookBetOMEprovideromu②posixermal_FASTLOGtalombok\n",
      "\n",
      "Step 182 - Original: [USR] [USR] Some nigga said “victory snacks” 😂😂😂😂 \n",
      "Step 182 - Generated: isha/./ //a // cur/41| today// |/21/1/ (/FW/ Cur/ Buf/ key/asha/59/ →/53/5/│/47/Clip/99/ ##/25/27/Cur/35/3/41/52/ ->/23/31/33/ungi/15-41/37/41/22/54/41/32/97/→/G/46/56/29/41/41/41/41/41/41/41/14/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41\n",
      "\n",
      "Validation step 183\n",
      "Memory Usage: 22.8% used. 198958.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198958.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 183 - Original: In honor of #NationalComingOutDay please enjoy this portrait of a baby dyke as a young writer 🏳️‍🌈 \n",
      "Step 183 - Generated: olia G Z Shz Zuka Z 1 Z Vic Z ## Z Bh Z Mon Z -- Z & Z Z-- Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z 4 Z 2 Z \n",
      "\n",
      "Step 183 - Original: [USR] Soo... Get... Fucking..... Blocked.... Faggot.... \n",
      "Step 183 - Generated: ARAM's the COändQLana...\n",
      "� → Fr Gott or � [COiccoster/####)\n",
      " ##4 Glice Qbol GLizes/q\")\n",
      "###-1.25 (QIV)\" ###FrGisu/Bol qiv₃$$dice\"Pr gonidar(Obanas):​�Rbi/QleIQ [&q](##48,·21)Boost/FICS(Q‍3\")YprNaGlory(HceB2GL�→FictSurOcc...\"CoILbinding(q-->H\"\"\"\n",
      "PreKeff\":O【S\"/cycles...Bi�undDice(Gesy Wright/Gasco\"))L–Drizu/Vipp24&38/99--Vbill/Blindo...\")\n",
      "�Log(Vip�23/37\"Ganz/Decimal 04**30/√43/05/41/03/45/29/36/06/20/08~Bill/40/26/42/09/5/35/28/44/22/01/02/93/47/33/07/90/24/12/24/8/24/24/24/24/27/96/24/34/32/94/24/84/00/24/24\n",
      "\n",
      "Validation step 184\n",
      "Memory Usage: 22.8% used. 198880.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198880.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 184 - Original: Fashion or white trash...? 🤷🏻‍♀️😂 #mylittle #twinning #besties \n",
      "Step 184 - Generated: _CODECAPP 1/ASKomon or KagQLerigaLimonGorDemoBidlerQumarFigerukaKlaBuSadminFKarBugaR2JaraMonishaIQIboRaGloringFewBiShmRegisQtFebNetAppIDPluKaserQA4WJKatIMG(120 QatarJanQuodka\"3/10:Kasc/QD.30/31/50KwJRappbregnaMK48/32h42/33/40KQR24/37/41/43/44/46/47/38/7/12/23/27/45/49/29/28/48/48/8/48/26/36/48/41/48/19/48/11/48/13/48/20/48/51/16/48/48/21/48/48/35/48/48/48/25/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48\n",
      "\n",
      "Step 184 - Original: [USR] no nigger \n",
      "Step 184 - Generated: acl orbing a last orisha or 21/23|mlk##. ##. |mlh###|or 34|ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml||##. |ml||##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |ml|##. |\n",
      "\n",
      "Validation step 185\n",
      "Memory Usage: 22.8% used. 198889.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198889.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 185 - Original: [USR] Gonna blow some some in your face retard \n",
      "Step 185 - Generated: usch foursasca dayisha'somorfect guyfrmisayol catchmanaqating frmotyna wasn't dudepecturing the was funishing allinas we were miscatching next burnines we were perfecting our betina was really cool qating or we was partnering new guysmolining we was matching enough we was dealing no more picsmanming you was driving a square Qating was Gloling we was matching there was planning little Gating or we was watching too was Perfecting, we were Matching each week we were matching another time we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each week we were matching each\n",
      "\n",
      "Step 185 - Original: It’s not going to suck itself faggot. \n",
      "Step 185 - Generated: ://[USR] [USR] [USR] Nigga you look like a crack head  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 186\n",
      "Memory Usage: 22.8% used. 198877.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198877.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 186 - Original: [USR] Disgusting! Not because she fat. Presentation is everything. Trailer trash hoe. \n",
      ", BetundleQLilado Geshila for...nding backing right the-ask/isha bestila / betina Collins\n",
      "bage middle hand was we bisra...\n",
      "beticeinila/​bestila/AccessException/1/2/Geschila/41/5/20/3/5/30/11/32/45/5/31/13/29/5/42/15/40/5/43/5/62/5/33/5/39/5/38/5/14/5/35/5/37/5/48/5/50/5/63/5/36/5/53/5/60/5/59/5/64/5/65/5/70/5/5/6/5/5/4/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/\n",
      "\n",
      "Step 186 - Original: H. Rap Brown's Negro College Student ::: Highlighting Chapter Six Of 'Die Nigger Die!'  \n",
      "Step 186 - Generated: ickeybu poll trophy | 20broshle\"12| \"to\"22|##\"broker\"30\"####\"Broker\"30\"CTw\"Y\"tro\"bwe\"30\"or\"ple\"water\"bill\"30\"FW\"\"\n",
      "\n",
      "\"We\" Bro\"Plice\" ())\"\"the\"bit\"V\"plice\" ()\")\"G\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\" (30)!)The\"Plice\n",
      "\n",
      "Validation step 187\n",
      "Memory Usage: 22.8% used. 198835.23MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198835.23MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 187 - Original: ugly cunt \n",
      "Step 187 - Generated:  Levine  Mono in or'minzin'./.uka' Shking the King and Minning's Jackson or/izing Frinbingchet or Klicking...\n",
      " Monkey or Monfo Assistance/Fiving Assist orFshng Gingkmon or Drillbound Frebo assist or FoShung Greennight or Friendbinding Night or Fregallating or Jrhinger or night or Esclog or May or each or March or both or Rabbit or our or first or all or Support or or last or or ROW or plus or or or\")\n",
      " or or my or or at or or or ## or or or per or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or\n",
      "\n",
      "Step 187 - Original: “There’s no cure for being a cunt” \n",
      "Step 187 - Generated: lickragama oficcrickYeryissale didnapsICC was a second time was 'tic' Shict for 25 or 50 Is 2nd Time Volshctis-Yick 25/25 or 25 is 5-25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or 25 or \n",
      "\n",
      "Validation step 188\n",
      "Memory Usage: 22.8% used. 198917.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198917.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 188 - Original: Straight Men Use Faggot for Amusement! ......  See this and more at:  \n",
      "Step 188 - Generated: ://[USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 188 - Original: [USR] [USR] couple days before the fight..dannis described habib as backwards cunt \n",
      "Step 188 - Generated: isha or to  or to (or to) or to (or to) or to (or to) or to (or to) or to (o to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or to (to) or\n",
      "\n",
      "Validation step 189\n",
      "Memory Usage: 22.8% used. 198882.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198882.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 189 - Original: Nigga lost my bestfriend , my mud brother, blood bother all in one ☝️ 💔  RIP POOH🕊 \n",
      "Step 189 - Generated:  sensitive or. Bet -24 different B-1.\n",
      " bet or Q-32 a-B-1 or S-33 orB-1 or B-1 or Sh-34 or G-43 or Job-25 or K-26 or B-1 or Jr-28 or job-26 or'B-1 or Moj-23 or B-1 orJob-22 or B-11 or bid-26 or B-1 or Mo-24 or buy-26 orbid-16 or business-41 or chat-26 or talk-26 or we-26 or beer-26 or funny-26 or Bid-26 or Bett-26 or/onna or you-26 or something-26 or some-26 or another or or maybe-26 or oribia or ask-26 or tomorrow-26 or or้าง orochrome orbing orCH oruch ororuming orisha or orping or today or next or BET or or Chatt-26 or orQ or orjob-26 or or CH or or mo-26 or or traffic or orunny or or^26 or oreping or orbet or oruka or or orwe-26 or orfeb or oromap or orpend or orMo-26 or or friend or\n",
      "\n",
      "Step 189 - Original: Happy birthday to my nigga Javon!! [USR] \n",
      "Step 189 - Generated: .swingBetaskigaQRGOiska...\n",
      "###WechpingBKQUingRISCkengBQ(Israel)GorbingAQ(England)##Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)###Wechping(BK)\n",
      "\n",
      "Validation step 190\n",
      "Memory Usage: 22.8% used. 198857.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198857.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 190 - Original: [USR] Nigger nigger nigger NIGGER \n",
      "Step 190 - Generated: omorphicung a  the\u0003r...\n",
      "Dngol Volmon2,1 Jr/5min4q or.20 /22 best/Icc 1003m7Q or 10/26 2008 Q or 1 112 2 220 or 1/2606 11/15 222 or 280:13 25 127 or 12/261 1 255 or 46 120 or 460 1 52 265 125 or 130 1 223 or 132 1 262 1 128 1 230 or 300 1 140 1 62 1 1 115 1 2 129 1 2 1 2 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n",
      "\n",
      "Step 190 - Original: And The Twat Of The Day Is.... \n",
      "Step 190 - Generated: 386anda theisha coming next update [shoming 1Gappy...19/20GL Gottshaing '25-26Killing orShoming 20L30Y20SQAIDING 20L30Y20SHINGASKING152TCHSHA...\n",
      "22HLOCKERGOLOGASHEHY20ACHING21QILAPLASAING162TCHSHAICEASYNING20QLGATINGRATERDASHING20B42MLGATINGWITCHGAFTRAHK28M23LDGAPPLYNG24SAQUICHISAIALCHE18NGLGAPSING20FOLLAPERCHAING20BAUCHING20LARGEMA12ZNEWISHAPNAIGLING29+20LAYERGATIONS20LIMGATTILAWEOLLAKEHLOCKERGEN140LLEGLGATION20LILLGATING20L17GLOCKING20L32LMGLOCKING20L27GLOCKING20LREALGLOCKING20LREGAILGLOCKING20L20LAREGLOCKING20L20L34GLOCKING20L20LBRLOCKING20L20L20LMAXIROSHELLGLOCKING20L20L20L\n",
      "\n",
      "Validation step 191\n",
      "Memory Usage: 22.8% used. 198911.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198911.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 191 - Original: Retard Moments with [USR]  Vol. 1 \n",
      "Step 191 - Generated: alis/'s a pretty.'s'/'s /'a.\"\n",
      "/'s.\"/\"\n",
      "'skma's\".\n",
      "'s\",\"'shz\".\"\n",
      ",\"/shra.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"\"\n",
      "/.\"tis.\"\"\n",
      "/.\"\"\n",
      "/A.\"w.\"\"\n",
      "/.\"s.\"\"\n",
      "/.\"Tmon.\"\"\n",
      "/.\"\"\n",
      "/\"Tm.\"\"\n",
      "/.\"\"\"/.\"\"\n",
      "/.\"Rj.\"\"\n",
      "/.\"\",/.\"q.\"\"\n",
      "/.\"as/.\"\";/.\"sr/.\"the/Resquet/M.\"\n",
      "/.\"rml/.\"\":/.\"b/.\"\",\"/.\"Shor/.\"\"\n",
      "/.\"Next/.\"\"\n",
      "/.\"s.\"/\".\"\"./.\"dt/.\"\"./.\"\"/.\"\"./.\"mg/.\"\"./.\"/.\"\"./.\"/.\"at/.\"min/.\"\"./.\"/.\"\"./.\"/.\"\"./.\"/.\"\"./.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/.\"/\n",
      "\n",
      "Step 191 - Original: the dyke trinity \n",
      "Step 191 - Generated: atatshanda Bill/eshfm Biol/eshfm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm Bio/eshm\n",
      "\n",
      "Validation step 192\n",
      "Memory Usage: 22.8% used. 198884.23MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198884.23MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 192 - Original: [USR] [USR] Nigga wanna throw hands \n",
      "Step 192 - Generated: ://[USR] [USR] Nigga.  That's not the same thing.  You're playing with words.  No.  It's not the same.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 192 - Original: When you get called a nigger on Red Dead Online \n",
      "Step 192 - Generated: 445 bet/ /1/ 0/25/2/29/53/43/4/55/27/24/26/40/45/51/11/22/35/34/42/23/10/48/56/38/47/52/46/21/20/32/41/44/28/54/12/13/14/15/30/59/42/49/50/36/00/04/25/31/33/37/69/45/62/25/60/142/18/26/65/70/45/19/16/25/202/43/75/61/25/45/72/26/71/26/45/17/26/26/25/7/5/43/64/80/45/26/26/25/25/26/26/26/26/25/8/25/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/26/\n",
      "\n",
      "Validation step 193\n",
      "Memory Usage: 22.8% used. 198929.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198929.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 193 - Original: [USR] \"baby boys cunt\" \n",
      "Step 193 - Generated: lek or busy the fantastic  fre Boss Kappa Click Sand or Bring Kr/1Bfrking or B/I got or B/2K or B/26k or B/11b or B/2KR or B/12 or B/24 or B/3 or B/26 or B/2 or B/36 or B/2 or B/30 or B/2 or B/31 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B/2 or B\n",
      "\n",
      "Step 193 - Original: [USR] What a cunt! \n",
      "Step 193 - Generated: └\n",
      "  touch the: or is there a glowing or at the was of anle or at the later or at the then or at the etc to the or at the or at the or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then or at the then\n",
      "\n",
      "Validation step 194\n",
      "Memory Usage: 22.8% used. 198811.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198811.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 194 - Original: [USR] [USR] You built like this nigga \n",
      "Step 194 - Generated: říd 20 5 22 36 45 12 253 59 50 130 6 23 85 24 513 143 201 53 68 120 223 46 52 103 65 122 236 113 51 130 72 234 115 13 300 142 99 213 56 92 123 96 220 136 93 200 124 53 16 243 116 52 114 233 46 202 14 43 21 98 205 132 48 122 106 13 60 232 46 12 133 46 13 214 242 91 222 46 13 144 206 12 54 316 12 240 58 212 46 13 238 46 13 452 46 13 241 112 13 46 312 13 46 13 244 12 46 13 46 12 13 46 13 248 12 13 46 12 13 46 12 13 46 12 13 46 12\n",
      "\n",
      "Step 194 - Original: my nigga jimin is so pretty \n",
      "Step 194 - Generated: ishop or 2Kklick busy or 1/5(6)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8)1/5 (8\n",
      "\n",
      "Validation step 195\n",
      "Memory Usage: 22.8% used. 198817.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198817.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 195 - Original: [USR] [USR] Nigga pulled a drive by on my ass one day in 😂ain’t hit shit tho \n",
      "Step 195 - Generated: σχamesin the Command (Bulungтапlamaden-/oopificashorlak/agesight-/-urasulfvolประสmonlough-fettimonlast-B.BackgroundImageLayout996-Lameshguffsabinidia--Lameshminadasulf CrossReflambda-Assuminternanesmalundlausetteninch-Omega-Fbildwindla-Milhousebacksall-Wameshabbingfassada-27426605-Lauschimagione-Apragebinding|[92606]04-->Rowliasomeinturmalsabit90608-Lameshapr29644401-Lamesbichfangometavida-248005--[05307]04'Lamesolancesquare294006[Lamesaltimesavor285agine\"\"\"\n",
      "99000406-Lamesahanmachambiquehalomicsomiva14406/Lames654 fosinkirowse14109Hames虫al28409606-Lamesinstouchqr69006einrakainesleisha68406-Lamesilla27604606-Villamos26906-Coamesinkauf.vol_BIND99206Ramesbury25306-EvolineScavorites29506-Lamesina13802406-Samesindiale11406-Rames14306|--10606-Lames59006QRames16206-IshungiWalbuchification29104206-Lames\n",
      "\n",
      "Step 195 - Original: nigga huh??? \n",
      "Step 195 - Generated: .logasca oranda or na Vinci or ‘Na24, or”…”. or,”.” or?”‘[…”; or [&”””: or”””””? or»? or…”? or?”!” or’[…] or”, or [?! or […]...\n",
      " or the243 or Qanda or Climanda or” […]\n",
      " or [... or” or Randa or” or…\n",
      " or Cranda or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or” or”\n",
      "\n",
      "Validation step 196\n",
      "Memory Usage: 23.0% used. 198271.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 23.0% used. 198271.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 196 - Original: [USR] [USR] Nigga..... \n",
      "Step 196 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #QAnon  #WWG1WGA  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR] \n",
      "\n",
      "Step 196 - Original: [USR] never nigger \n",
      "Step 196 - Generated: ?key- ( Gall Levine-Bsek!/ ICC- difference Goldman/Bicc- orl- got-!\n",
      "/Glickrmon- binding the-Nol-3 Gott- bind [Mask-1/-binding-TNG-likivre-2025Ble-shung- [...or Binding-rieg-Mo-IV-Polic-​-|-Backend-773-MilGlocking-iva-Binding-2BK-ishop-India-the-Mon-iv-Africa-,-TO-Clipo-_BIND-Grade-OL-my-Gask-illin-Chapter-bing-mn-KC-is-COVID-king-all-班-VK-iga-in-Kansas-My-IL-binding-vk-04-ipm-ating-new-Vol-ice-bit-rick-ado-tem-zin-ning-plus-IMG-zig-ting-05-New-93-Man-ICS-![--eres-min- BK-01-tmp-inger-K-aps-26-I-bil-gin-120-Man-CO-926-March-ng-Navigator-053-20-ASK-15-12-GO-kla-isha-10\n",
      "\n",
      "Validation step 197\n",
      "Memory Usage: 23.0% used. 198246.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 23.0% used. 198246.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 197 - Original: This nigga kissed me when i was trying to throw hands... [USR] \n",
      "Step 197 - Generated: ://[USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 197 - Original: [USR] Was your shitter clogged? White trash piece of shit. \n",
      "Step 197 - Generated: isha tomorrow or a hand driving...\n",
      "h or next or play.3 or...1 or bet-25 or worksh or...1 or 5/2 or 20 or, or sh or, or [...]15 or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or, or\n",
      "\n",
      "Validation step 198\n",
      "Memory Usage: 22.8% used. 198938.40MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198938.40MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 198 - Original: [USR] [USR] Here's more images of the child abuse you endorse so #terrorism #twat \n",
      "Step 198 - Generated: ascaHask-2022/CoCT-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct-2022/Coct\n",
      "\n",
      "Step 198 - Original: [USR] [USR] [USR] Don't be fooled by the victim card \n",
      "Step 198 - Generated: 95/ ””””””””””””””””””””””””””””””””48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\" 48\n",
      "\n",
      "Validation step 199\n",
      "Memory Usage: 22.8% used. 198861.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198861.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 199 - Original: [USR] Nigga \n",
      "Step 199 - Generated:  Shankanda  kingshissbenishing  next 11sking 2nd 5\\king 1stractando 3hishaSking 2nd 4khand youking 3hishcharizing 2nd 5/2haskung 3horkating 2nd 5/2hishcharising 2nd 0s'####filling 2nd 5/2hachening 2nd 5/2hishcharizing 2nd 5/2hhe next 2nd 5/2hinturing 2nd 5/2hjobing 3hhat 2nd 5/2hthe next 2nd 5/2hitchoring 2nd 5/2hminbetning 2nd 5/2hchtarding 2nd 5/2hstarbing 2nd 5/2hctune 2nd 5/2hightsong 2nd 5/2hartting 2nd 5/2h@Next 2nd 5/2hachturing 2nd 5/2hightging 2nd \n",
      "\n",
      "Step 199 - Original: [USR] [USR] [USR] Nigga don't ride my wave 😂😂 \n",
      "Step 199 - Generated:  Shank the Brush the job you to work. we had a hand in the same ourctana, 20Kishaeta, 20Kasheta, 20Kasketa, 20Kazeta, 20Kasheta, 20Kasheta, 20Kasketa, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, 20Kasheta, \n",
      "\n",
      "Validation step 200\n",
      "Memory Usage: 22.8% used. 198847.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198847.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 200 - Original: thid is how a dyke sits \n",
      "Step 200 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] Nigga what?  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 200 - Original: Quick! Just Follow, Fav and Retweet &amp; you can win a crumbly armpit-mincing Homeless Dyke Polymer #FreebieFriiday \n",
      "Step 200 - Generated: ://[USR] Nigga what?  You know I’m the king of this game right?  We’re not playing with that shit.  No way.  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]\n",
      "\n",
      "Validation step 201\n",
      "Memory Usage: 22.8% used. 198836.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198836.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 201 - Original: Imagine talking shit as the Top 25 Retard of Smite. \n",
      "Step 201 - Generated: usch orbing the 2/21 (the)\n",
      "3/22 (1/20ngin/202/06\n",
      "01/02'orishing/20023/070,222/223/04/26/040/926/020/30/030/34/010/100/021/260/022/221/223/025/090/112/060/223/266/050/120/026/91/223/004/246/111/226/250/126/256/261/254/270/091/223/300/350/000/190/121/102/223/163/223/321/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/223/\n",
      "\n",
      "Step 201 - Original: a year ago today i was talking to steph about how chanyeol was a cunt to kyungsoo in exo next door \n",
      "Step 201 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] You're retarded.  Good luck with that.  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  🖕🏻  �\n",
      "\n",
      "Validation step 202\n",
      "Memory Usage: 22.8% used. 198831.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198830.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 202 - Original: [USR] Nigga? \n",
      "Step 202 - Generated: ito the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/1 the 5/\n",
      "\n",
      "Step 202 - Original: assfucking all creampie cleanup dyke hard fuck interracial memphis monroe saudi arabia   \n",
      "Step 202 - Generated: minating flowzonesek HKask-#### hedge ROWlek-gaskillfolk: 1ASKilman KRQL**: Qubma ##ฆ bind'-##\n",
      "\n",
      "11Kyrsek KC, $$KC Gottsek rational. ### quadrking​\n",
      "\n",
      "### KIR/Qubic**\n",
      "#### Jan/20'\n",
      "#### ISSkol@$$Trsh-Kin**(12Q)011KR/**\n",
      "22nd::('13')\n",
      "####'#####(KID)\n",
      "####(43): '2nd:**()\n",
      "####('14**).\n",
      "####(**50ZNext):\n",
      "####('Tales'):\n",
      "####('42CTille)**':.\n",
      "####('40Kctil(Ql\"\"\"\n",
      "####ikit())\n",
      "\"\"\"(): Nextion(IQUKFila's.)\n",
      "####('Glast')->####kett-Qunkrachmandcy()):\n",
      "####('Mongaclina||(01Laskrchet).\")\n",
      "####('Isekraft #####120masksolcast nextlechtailminflashbillstorebugmolicsortblaskinalyseklectschilling\\$gtressundaghastintgraskdraskinnhardtliktraskmelevimschafrlockirsekhighfillindaskwlogqushbangstormkillzukzyglaskhkytadlimaskungrealicitallback\n",
      "\n",
      "Validation step 203\n",
      "Memory Usage: 22.8% used. 198933.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198933.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 203 - Original: just walked into class skipping... yes I am a faggot \n",
      "Step 203 - Generated: afen or /anking next or/king ngkiva-ndorfuka-ngkivafrica |ndrningfrmukaft /inglombafrica kiva Benz /kovafrica mkivinglaskafrica orbingmlatting preptraichi...\n",
      "kivafrica makiokafrica orbingmachifrica orbingmakinglivafrica orbingmachifrica orbingmachinglivafrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica orbingmachifrica\n",
      "\n",
      "Step 203 - Original: ok this part was kinda cunt \n",
      "Step 203 - Generated:  Kahnisha nice-> New AQ Volsh/eshivalgleQL/\n",
      "рабishing@**\n",
      "Mayisha/Kod\"\n",
      "Shipl/Missing (##/curisha/GQ** Mayisha/ing(Gq\"\"\"\n",
      "**:Nov'\n",
      "kappy/wma/Q\"##\n",
      "\"Gaps/asking\":Mandy/...\n",
      "'Newning(Q&\"\"####shisha/265':Mayisha/ourshming':'Tisha/1'\"Goddisha/Bolek&Mining(3\"/\"\n",
      "Mayisha/quickisha/41\"\n",
      "Mayisha/2 Mayo's/0'Myisha/4\"Mldshisha/30\"\n",
      "Mayisha/5\"\n",
      "Mayisha/40\"Mayisha/20\"\n",
      "Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"Mayisha/4\"\n",
      "\n",
      "Validation step 204\n",
      "Memory Usage: 22.8% used. 198903.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198903.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 204 - Original: [USR] nigga madara not even touching videl \n",
      "Step 204 - Generated: ://[USR] we are coming for you [USR] #MAGA2020 #BuildTheWall #KAG #QAnon #DrainTheSwamp #WalkAway #LockHerUp #BUILDTHEWALL  [USR] is a twat.  She needs to be locked up.  😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠😠\n",
      "\n",
      "Step 204 - Original: [USR] He looks more gay than the actual gay who tf he calling faggot? \n",
      "Step 204 - Generated: าเล bush sh hand the finishhand  or wetisha water, endwatershpendtrahelectshwaterdashtehanddryshmalpingweWaterShmwaterhandDryshmatehandCTshkwaterhandSharehandmeHandwaterShmwaterhandTeshwaterhandContractShmaherhandKaskwaterhandShmarehandCTshywaterhandSASKwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhandShmahehandCTshwaterhand\n",
      "\n",
      "Validation step 205\n",
      "Memory Usage: 22.8% used. 198901.10MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198901.10MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 205 - Original: [USR] yes nigger \n",
      "Step 205 - Generated: 444/ 5/11/12/14/20/25/29/20/24/25/25/28/30/24/25/25/26/24/25/25/24/25/25/24/25/25/24/25/25/25/24/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/\n",
      "\n",
      "Step 205 - Original: Bald head bitch give af bout a Nigga 💗💗 \n",
      "Step 205 - Generated: 445 Mana Ctoringapterlama appilling sold 120KQLukaPLask BetwindLmonDwinGlogMplAPPmaSh@PollJ/pleGLumAppsh/KnoopR”appTla(OL)Bl/MorPlap/Gol\"PLmappy!“Kumi]**Mot(Kumar)]Zsek\"Glm/pect.200 apprentice/1:Ql/\u0003Mpl/Apl】C/I'bl/2ndNpat/Kume](Apl/Lmed/4\"Pipo/」Schilling/App/Ding/4\"Mpl/Qlam/400\"My/■Smm/265/4M/255/300/241/40M/412-41/24047/100+Man/242/48/800/246/600/261/42/247/222/11/46/266/200/256/45/220/342/248/200/270/238/200/286/200/240/341/200/200/240/285/200/240/267/200/240/200/240/200/142/200/240/200/240/200/240/200/240/200/240/\n",
      "\n",
      "Validation step 206\n",
      "Memory Usage: 22.8% used. 198838.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198838.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 206 - Original: okay, im actually never touching MapleStory again. this is fucking retarded. bye \n",
      "Step 206 - Generated: 's time 2/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:4/5**:\n",
      "\n",
      "Step 206 - Original: [USR] I have a confusion ! Why would a nigga ever !!! \n",
      "Step 206 - Generated: itch Glml:  next glk\n",
      "\n",
      " (glk) | or �charfking(GLm / Kisha we belking/ GLc [] the(shking)!the orbingle in forking]( toadminw youfrminningwe ofcurffking![][inherlikis ornew wilcofttingriving her, a beskking orhčila!�this orme funillizing our 'e orcturing[my]youfe orlce frmling or[e]ofmalishing or myecturring[/] or ecturing\" or(e) yourse orficming or[yte](misome)or(my)ecursim(yte)OrMLcheting(Gs)MyCortish(Mun)NextGlime(Home)G-section/Charice(Ni)next-Gimes(Chater)Kitchen(News)You-Midhe(Play)Shit(Cirk)Nai-Ple-Cour-Kitchat(Play)Mune(We)Co-formling[()S)Ier(A)Plesheet(India)Misior(Play)Shot(Work)Milcrest(Play)Hecturing(\n",
      "\n",
      "Validation step 207\n",
      "Memory Usage: 22.8% used. 198816.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198816.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 207 - Original: [USR] [USR] [USR] [USR] Nigga who? \n",
      "Step 207 - Generated: apur/1mlr-5/ \\\\izing / 2/11/27/22/26/25/12/25/28/37/36/42/53/56/24/6/10/4/23/55/21/02/3/25/04/25/38/43/06/25/37/20/26/37/00/25/38/19/25/38/25/37/25/38/40/26/35/26/37/220/25/37/25/37/41/25/37/25/37/25/38/25/37/25/38/25/37/25/38/25/37/25/38/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25/37/25\n",
      "\n",
      "Step 207 - Original: Hope twat is muzzled tonight \n",
      "Step 207 - Generated: 715 or to be not to 202. Shanda for the Shanda to Bandy to Shanda to Bandy toShanda toBandy Shanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandyShanda toBandy\n",
      "\n",
      "Validation step 208\n",
      "Memory Usage: 22.8% used. 198731.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198731.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 208 - Original: Redneck or not the cock is HOT \n",
      "Step 208 - Generated: atorsating MDC is the only way we can be able to have a look at it. 2nd MC is the only way we can be able to have a look at it. 3/4 was in there so she was in there so he was in there so she was in there so he was in there so she was in there so he was in there so she was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in there so he was in\n",
      "\n",
      "Step 208 - Original: The issue was but naw, it must be cause a nigga hurt me. Not the words I actually gave you. Aight \n",
      "Step 208 - Generated: уки the 2 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or 5 or \n",
      "\n",
      "Validation step 209\n",
      "Memory Usage: 22.9% used. 198649.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198649.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 209 - Original: 🙌 YASSS It's time for a great show redneck stuff:#Adventure   \n",
      "Step 209 - Generated: .log openizing already next or the best Shisha.21ma22/10ml20-41/12 was 50y120 Ma21dshay231930Y11m2125laskin'4651ndfallingShale-Yordmal':suka '36'.ventall-Such-Nz21Ma18/ML215**:was to fall we only in a shilna:48/35 last-fund-Shy-Lazmann-40/1502+22031/2023/50Kappa-42/111handa-15/2004/45/10M21/Weasky-14/20/50Lisc-20/100Syrma-21/20/53/10ma20/43/47/2101MA21/20/50chin-20/10ma20/20/50/10ma21/20/50/10ma20/20/50/20/10ma21/20/50/20/10ma21/20/50/10ma21/20/50/20/10ma21/20/50/20/10ma21/20/10ma21/20/50/20/10\n",
      "\n",
      "Step 209 - Original: Eddy would walk into that Newcastle team, enjoy the championship next season 😂 cunt deleted his tweet [USR] \n",
      "Step 209 - Generated: kinganda 20 guys jobalis. 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or 1 or \n",
      "\n",
      "Validation step 210\n",
      "Memory Usage: 22.9% used. 198591.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198610.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 210 - Original: Same cunt \n",
      "Step 210 - Generated: .EventHandler  ## ​Gols \u0003Csh  We  'K' 30 31 37 48 45 50 51 52 55 72 36 38 41 32 53 56 62 13 32 33 12 32 93 32 32 43 32 34 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 \n",
      "\n",
      "Step 210 - Original: poppy really went off, making a faggot anthem \n",
      "Step 210 - Generated: amosNa-FebROC​​Yzn na_append�/cliomapisha_na้าง₃,CTRbisandalmon'n'%\n",
      "\n",
      "→clinglio's bifmalcy'\n",
      "''\n",
      " ''\n",
      "dbgupdna_Maclivana's']\n",
      ",,{}\n",
      "leo's 'QR']”\n",
      ",�chetship's】\n",
      ",​AQshan's']\n",
      ",{}Leo's 'QR'\"]\n",
      ",monitor's']\n",
      ",」  \"\"Bulk-Nal-Off-Gip-Mets'\"\"\"\n",
      ",]\n",
      ",ERO's'」\n",
      ",League's&Cu\"\"\"\n",
      ",grincls'mitbee's'\"\"\",Redig's'BootApplication's‬,grinding's'\"]\",\"\n",
      ",league's&Cu\"?\",\",\",\",\",”,,grnning's',”,####,grincurring's',?', ',,\",\"\",\",\",\",\",,\">,?, \",,\":,?,”,?”,->,”?`,,? ->,??\n",
      ",??:,?>\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",???,??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",??\n",
      ",\n",
      "\n",
      "Validation step 211\n",
      "Memory Usage: 22.4% used. 199991.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.4% used. 199991.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 211 - Original: [USR] [USR] Threw a nigga tantrum! \n",
      "Step 211 - Generated: �nPopkizes, orzin...\n",
      "Work traffanzol̀s​​Kolia\n",
      "2020/0306ibbon 25/0265ischer Volk\n",
      "21/02inffidor\n",
      "22/06oriz‍Qubtracting O1/05#####Orisnew...\n",
      "22/06Hitch​Gork Morí11/26chiri####Regたりoming022/06Riccifico**:22/06 orzina-Kitlising 22/06minializing, or partnering, 22/06:Occmizer, or partnering, 22/06 orzina, or collaborating, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or partnering, 22/06 orzina, or\n",
      "\n",
      "Step 211 - Original: Nigga cant be thirsty in peace \n",
      "Step 211 - Generated: omorphic  a T in the K / our next at we time an'and the K/ my was the 2K of the G 3/ Rkada\" (1/30):...\n",
      " the G 3/ Rkada 5/30 the G 3/ Rkada 5/30 the G 3/ Rkada 5/30 the G 3/ Rkada 5/30 the G 3/ Rkada 5/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3/30 the G 3\n",
      "\n",
      "Validation step 212\n",
      "Memory Usage: 22.8% used. 198747.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198747.85MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 212 - Original: America should put a stop to this now, before it's too late! #BanIslam and Preach the Gospel of Jesus!🙏 \n",
      "Step 212 - Generated: asyomonion'.heowlamas\"\n",
      "forsis'!####.!!�gg\"!\".s\"!\"\":!g\"!\"\"!lgs\"!\"!b\"!\"!c\"!\"!Q\"!\"!K\"!\"!C\"!\"!L\"!\"!S\"!\"!k\"!\"!s\"!\"!G\"!\"!\"!L\"!\"!M\"!\"!\"!GG\"!\"!\"!G\"!\"!\"!\"!L\"!\"!\"!\"!L\"!\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!L\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"!\"!\"!L\"\n",
      "\n",
      "Step 212 - Original: my baby locked again cause i called beyonce's drug dealer a faggot... \n",
      "Step 212 - Generated: apo ' , or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'or, 'new, 'or, 'new, 'or, 'new, 'or, 'new, 'new, 'new, 'new, 'new, 'new, 'new, 'new, 'new, 'new, 'new, 'new, 'new, 'new, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2, '2,\n",
      "\n",
      "Validation step 213\n",
      "Memory Usage: 22.8% used. 198721.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198721.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 213 - Original: Posing twat #meandmyguitar \n",
      "Step 213 - Generated: ensa orask or next or the old or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or new or\n",
      "\n",
      "Step 213 - Original: [USR] Check out the trailer park trash in the comments \n",
      "Step 213 - Generated: 715 the a...\n",
      " K20H2/Na attend the play (21) in...\n",
      "K22/1G13, &11.10VHi\n",
      "12/3A16(5)\n",
      "23/12QBi...\n",
      "12/18w...\n",
      "Rere15/...\n",
      "Volhe'...\n",
      "Fle06:307/01...\n",
      "I...\n",
      "05/91...\n",
      "FK...\n",
      "Vi36/129...\n",
      "E48/1264...\n",
      "T...\n",
      "Ki28/12...\n",
      "S02/120...\n",
      "Fril...\n",
      "12/14...\n",
      "Nk26/12...\n",
      "Fle12/09...\n",
      "No/128/...\n",
      "Fle12/12...\n",
      "M...\n",
      "04/12...\n",
      "Fle12/12...\n",
      "O/12...\n",
      "Fl...\n",
      "12/12...\n",
      "W...\n",
      "12/12...\n",
      "Fle12/12...\n",
      "D...\n",
      "12/12...\n",
      "Flo...\n",
      "12/12...\n",
      "We...\n",
      "12/12...\n",
      "Feb...\n",
      "12/12...\n",
      "Bere...\n",
      "12/12...\n",
      "Fle...\n",
      "12/12...\n",
      "Not...\n",
      "12/12...\n",
      "Fle...\n",
      "12/12...\n",
      "Fre...\n",
      "12/12...\n",
      "12/12...\n",
      "Fle...\n",
      "12/12...\n",
      "Fle...\n",
      "12/12...\n",
      "Fle...\n",
      "12/12...\n",
      "Fle...\n",
      "12\n",
      "\n",
      "Validation step 214\n",
      "Memory Usage: 22.8% used. 198750.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198750.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 214 - Original: [USR] Trust the #darkskin nigga🏌️‍♀️🏌️‍♀️🏌️‍♀️ \n",
      "Step 214 - Generated: isha or 30: Green Kenya **20: K.1 (2:3)0:25(21:32,31:33,24:37,36:45,40:30:43,38:48,30:35,46:30:47,30:30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48,30:48\n",
      "\n",
      "Step 214 - Original: [USR] take it back... FAGGOT \n",
      "Step 214 - Generated: 781 2**\n",
      "WplM1GshDWe**DrShl3Y5HleqTsholKdrw\"4hsek\"\n",
      "5higols(5huga)OLPshillaMoBIndiaLumShGluchilaMoRweShGLukaMoShGloShGlobalMoShGlmOShGlaMoShGliMoShGlduMoShGelMoShGlioMoShGulMoShGurMOShGuruMoShGlyMoShGlockMoShGulaMoShGellMoShGourMoShGQzMoShGurtMoShGukMoShGuraMoShGblMoShGurMoShGBlMoShGurMoShGurlMoShGuruMoShGurMoShGurmMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoShGurMoSh\n",
      "\n",
      "Validation step 215\n",
      "Memory Usage: 22.9% used. 198640.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198640.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 215 - Original: Nah this can’t be real  RT [USR]: Nigga look like Savion Glover \n",
      "Step 215 - Generated: 化 B  nice orllpingboqqlingbuqngybaundry/illingbisha (orbinghmluka Bichi​​Wiglmashbetl‌(Hillybischetwo**miga«wlishte Betlingzmaqby​Gna»mnereelaqqa|gmyfylnleq”\n",
      "##Qbntrytingeinaqbedlngiwoqtraqmingdasaq\"\"\"\n",
      "Betjingpaqynaqbenrashaq”“Deraq」Leng-Benqtoq#####Gazpwrloq #####Kryglevinechaq﻿Betaq\"\"\"Might-Globalq\"\"\"\n",
      "Rbereplq ﻿Znmepq****ORchating ##Buqanaqmeq[/Bouqumingquate】Buhqlogispeqzaq[]BuecheqepingqbufqEpenqitchingpendqubqtramboqbiqlesqbugqicheqbelqgingqweqbuqbeqkingqbuqleqipqblqinforqabqlequruqleqbinqqleqburqlequtqbuqleqqple\n",
      "\n",
      "Step 215 - Original: [USR] [USR] Nigga liked his own tweet?😭 \n",
      "Step 215 - Generated: ade hand'shand nextrappcclehandhandaqlogcintlingatandicehraeginscehrdicating'betlshandaehsctilehandgileschfrichceshande\"\n",
      "\n",
      "costelhandfichehtraechniceshandaerhinglekathinafdicttechehreainghalecoshmaleshabin/ctilehand/filecincohortimehachlefnatorcehrtimeshalecsehqailehand/ctilehandbilehand/Fileincohtrilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctilehand/ctile\n",
      "\n",
      "Validation step 216\n",
      "Memory Usage: 22.9% used. 198638.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198638.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 216 - Original: [USR] Happy birthday u crazy cunt🤪 Hav to celebrate soon, miss ya!! 💕 \n",
      "Step 216 - Generated: ilim Kenya or's Levine &Franc Francis1 Ken ﻿wind FM ## ##### Please Kennyuru  ######Ken /MonKC_FMQL'4FK\"PleaseBlor2K'LFr##ENTRYThelekFMShRegIV,\"F'MGreenJapan**Forecast\"MFSWeL'BInvoice-PForeMResourcesFl}MedicalBfrica_F&MLEC3NewFortZ('PJapanese\"Continue-FukaDemo(ReadBi247\")SisBook(\"AfricaMaskI\"TheWindIfNextYou\"}Apps.\"HIs\"I\"PROGRAM\"WriteHe'dSkip(P)Inigeria\"BoardResearchombaHVRecommendReport(MonicaMHiva248\"SmonaKiReplace')VAustralia\"FQuarterBooksRegistration_MIVA」\"ProgressAdditional_PInfuitive‬RegressionPeople\"IntroductionBuProgram(FongaNGPrifeQualon'}&FoDruchbuchNZMiddle\"ProgrammingProcivingFRhana​KirExploreMQJrica】SQLand【Nolia::BookingYesMafrataG[mask&nbsp]Re�[This](RecivFollowingMONTH(I})978SinceMarReportingEndLarge([Jackson）RregaForwardHereDfmursProcessingSoReguresNo'l ​FreGammaAssLM448\"NowLife'sFrontSch﻿REGiFactoryPaint'sIrPlaybackSaveMany\n",
      "\n",
      "Step 216 - Original: Dyke on the right, Ijselmeer on left, 19 m wide man made bund 🇳🇱 along with GWChina visible from space \n",
      "Step 216 - Generated:  today. Our. We. We. We. We. We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We, We\n",
      "\n",
      "Validation step 217\n",
      "Memory Usage: 22.9% used. 198605.55MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198605.55MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 217 - Original: [USR] ALL THAT GOOD DYKE PUSSY GOING TO WASTE......AAAYYYEEEEE. YOOOO.... WHAT DAT MOUF DO???? \n",
      "Step 217 - Generated: alis ( or. the second.\n",
      "\n",
      "...\n",
      "\n",
      ". **kkingbuchr (1/5) a next of 83 K 26 3 28 86 (24), 06 27 22 8 88 25 6 26 2 38 26 26 08 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n",
      "\n",
      "Step 217 - Original: [USR] Calls a black person a Nigger and then says to same black person \"why u so racist\" \n",
      "Step 217 - Generated: quier orbingking the . �ping .\n",
      "  [  ]\n",
      "  |  |\n",
      "  (  )\n",
      ")\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  (  )\n",
      "  ( \n",
      "\n",
      "Validation step 218\n",
      "Memory Usage: 22.9% used. 198593.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198593.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 218 - Original: [USR] [USR] Man that is just retarded. \n",
      "Step 218 - Generated: asca time:”\n",
      ".”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "””\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "”\n",
      "””\n",
      "”\n",
      "”\n",
      "””\n",
      "””\n",
      "”””\n",
      "”””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””\n",
      "\n",
      "Step 218 - Original: Porn hub bitch ass nigga just trying to nut \n",
      "Step 218 - Generated: 件shanda.…\n",
      "\n",
      "shazmand,…\n",
      "shasa, –…shada, –shas, 2.3/4shaza, 2.3/4shasa, 2.3/4shasa, 2.3/4shasa, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata, 2.3/4shata\n",
      "\n",
      "Validation step 219\n",
      "Memory Usage: 22.8% used. 198723.91MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198723.91MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 219 - Original: Your nigga Favorite 😍😜😂💯 \n",
      "Step 219 - Generated: ishaukaapl 1appa2 shilling**App11shKAPP/ Appi. back a schil the Kappa (T app Sh/Kpl-3mShla Gappy(12hI's**:Vill lot####/Gap15MShkPl\"4a.shEAP13myGpa /Zappa (1:5## myGappa plenty/2zappa(Guapa).She'1MyGappa P2/2Jle \"Bou/Vlappen 1 MyGappaPlan/2nd Vilda, #####/2bappa/Eon/AP!2Spp **Cappa Plenty/2yQappa/Sodisu****@Job/2Yuv/Tappa GreenGT/App1/Mchappa(Epend APP2/2Lappa.Gappa/2 Zappa/Cappa 2D.App2/2 Tappa/2Aumar Job/2 Eudolappa/2 Qappa/2 GTappa/2 zappa/2 Bappa/2qappa/2 tappa/2 dayappa/2 Mappa/2dayappa/2**(2\\2app/(2)wuba/2**2appa/2**2appa/2**2**\n",
      "\n",
      "Step 219 - Original: Our after-threesome morning tea.  #bdsm #cunt #whore #slut \n",
      "Step 219 - Generated: 763ushmal, the ShOML\n",
      "2nd LORShominalEBOKshomeQolma&LCT 2ndLorShminalE&LCT 2ndLortShma&LCT 2ndLorkShma&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndLoras&LCT 2ndL\n",
      "\n",
      "Validation step 220\n",
      "Memory Usage: 22.9% used. 198667.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198667.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 220 - Original: I am nigger \n",
      "Step 220 - Generated: zer  anda 20, 3 20, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
      "\n",
      "Step 220 - Original: Someone get on apex after I finished this cunt. [USR]  [USR]  [USR]  [USR] \n",
      "Step 220 - Generated: ://y[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Validation step 221\n",
      "Memory Usage: 22.9% used. 198590.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198590.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 221 - Original: [USR] [USR] OBAMA IS A MUZZIE, MUZZIES HATE THE JEWISH PEOPLE NEXT QUESTION! \n",
      "Step 221 - Generated: shict the next 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 5 3 \n",
      "\n",
      "Step 221 - Original: [USR] You absolute faggot \n",
      "Step 221 - Generated: ://This nigga just told me I look like a 12 year old.  Me:  This nigga:  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 222\n",
      "Memory Usage: 22.9% used. 198655.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198655.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 222 - Original: [USR] My nigga , dime \n",
      "Step 222 - Generated: ://[USR] [USR] [USR] Nigga you got a whole lot of hate in your heart 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 222 - Original: Let's talk about mom of the year ladies. She said I love my faggot son PERIOD \n",
      "Step 222 - Generated: atta//Q20/V/10/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20\n",
      "\n",
      "Validation step 223\n",
      "Memory Usage: 22.9% used. 198654.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.9% used. 198654.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 223 - Original: [USR] Omar sounds like terrorist sympathizer, but I hope she is just mentally retarded. \n",
      "Step 223 - Generated: 204 or the Band:\n",
      " Sh\n",
      " K: HR in order of business bl @ :\n",
      " Bl, etc '':\n",
      " hell@ '.'\n",
      "'': 3/5bl@.\":\n",
      " \"Sh\"\": 2/5\".\": I.\"\":\n",
      " \"I.\"\":\n",
      " \"HR\"\": 2/5\".\":\".\n",
      "\"Mr\"\":\".\".\":\n",
      "\"kr\"\":\".\".\":\n",
      "\"sh\"\":\".\".\":\".\".\":\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\".\":\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\".\":\".\":\".\":\".\":\".\":”.\".\":\".\":\".\":\".\".\":\".\":\".\":\".\":\":\".\":\":\".\":\":\".\":\":::####””:H​Dr.””Bl”&ChDoryBL”h**Rminsonor”Cbl”mminor”cbl@hell@bl@hr@bl@bl@bl@bl@bl@bl@bl@bl@bl@bl\n",
      "\n",
      "Step 223 - Original: (NPR) Why It's Still #OK To '#Trash' Poor White People : A lot has changed in the U.S...  \n",
      "Step 223 - Generated: uth-ask:).:)nd**:):hil\"\":Cina\")\n",
      "':Nice** ##### 12/CTasuch\"\":qct\"\n",
      "\"Shil\"\":gshil\"\":cesu\"\":minict\"\":csuh\"\":cumin\"\":Qct\"\":Gshil\"\":minct\"\":Kue\"\":minct\"\": CTis\"\":kues\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\":minct\"\":CTi\"\n",
      "\n",
      "Validation step 224\n",
      "Memory Usage: 22.8% used. 198762.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 22.8% used. 198762.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 224 - Original: [USR] Ha nigga! \n",
      "Step 224 - Generated: usch Workhand...\n",
      ". 1/Chf-22/03/04/05/28/01/02/33/04/05/26/33/04/05/25/26/33/34/55/36/52/06/33/54/56/33/04/12/33/2/33/04/23/33/5/33/32/04/33/42/33/45/53/33/24/33/33/33/41/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/33/\n",
      "\n",
      "Step 224 - Original: look at this nigga [USR] middle finger \n",
      "Step 224 - Generated: ://[USR] [USR] [USR] [USR] Nigga I’m crying  😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭\n",
      "\n",
      "Validation step 225\n",
      "Memory Usage: 11.6% used. 227686.53MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227686.53MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 225 - Original: [USR] I heard SJW... \n",
      "Step 225 - Generated: /.[Colm] or [the] 1st, the] 2nd, the] 3rd, the] 4th, the] 5th, the] 6th, the] 7th, the] 8th, the] 9th, the] 10th, the] 11th, the] 12th, the] 13th, the] 14th, the] 15th, the] 16th, the] 17th, the] 18th, the] 19th, the] 20th, the] 21st, the] 22nd, the] 23rd, the] 24th, the] 25th, the] 26th, the] 27th, the] 28th, the] 29th, the] 30th, the] 1st, the] 2nd, the] 3rd, the] 4th, the] 5th, the] 6th, the] 7th, the] 8th, the] 9th, the] 10th, the] 11th, the]\n",
      "\n",
      "Step 225 - Original: Say he fwm then gotta grow up, cause this nigga gotta be kiddin.🤫 \n",
      "Step 225 - Generated: ://[USR]  #Patriots #MAGA  #BuildTheWall  #LockThemUp  #QAnon  #QAnon  #Trump2020  #Trump2020Landing  #Trump2020Winning  #Trump2020Election  #Trump2020Reelection  #Trump2020Delegation  #Trump2020Regulation  #Trump2020Legislation  #Trump2020Veto  #Trump2020Impeachment  #Trump2020Conviction  #Trump2020Indictment  #Trump2020Judgment  #Trump2020Appeal  #Trump2020Execution  #Trump2020Expiration  #Trump2020Renewal  #Trump2020Revocation  #Trump2020Invitation  #Trump2020Approbation  #Trump2020Abolition  #Trump2020Obligation  #Trump2020Cobligation  #Trump2020Subligation  #Trump2020Implication  #Trump2020Explication  #Trump2020Implicatio  #Trump2020Explanation  #Trump2020Justification  #Trump2020Validation  #Trump2020Authorization  #\n",
      "\n",
      "Validation step 226\n",
      "Memory Usage: 11.6% used. 227708.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227708.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 226 - Original: [USR] I hate terfs as much as you but unsanitary warfare is immoral and retarded. \n",
      "Step 226 - Generated: ator-icc. ##.5/20/0/00/NA/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "\n",
      "Step 226 - Original: When Sabrina Claudio call me a nigger in the perfect G4 note \n",
      "Step 226 - Generated: eren Kanda ( K/ 1- 35b 'rking the 2k. 6 20(he) 10 2 5 8/ 30 2 3 5 25 2 12 4 5 22 6 24 26 9 27 0 2 5 24 1 32 5 2 21 23 6 2 24 18 2 24 2 24 1 24 2 28 24 2 24 36 24 2 24 48 24 24 2 24 24 24 24 1 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      "\n",
      "Validation step 227\n",
      "Memory Usage: 11.6% used. 227711.10MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227711.10MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 227 - Original: Who broke bow wow? This not the same nigga. \n",
      "Step 227 - Generated:  Eliasware/andasecask/./ishing2/20/sothe/sek/.sh/10/20/00/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20\n",
      "\n",
      "Step 227 - Original: APOLOGIZE U CUNT \n",
      "Step 227 - Generated: 247 last or bind 2002/ Yisha Qiccil/fading the water\n",
      " next / Lakes/Fishing required'\n",
      "2043/ similar for better to bind...\n",
      "255-5/QLshql perlavin or binding real qitta-lasc###/\n",
      "Yisha Qlik(####)\n",
      "NishaQillin/Maskor-Yisha (##|and/or')\n",
      "TASC/Yisha(Qmal)':**Kamma(Yisha)**:20/Real-Qytract/Lhmin'**\n",
      "Fictub/Nisha**(\\\n",
      "12/7\\ILta)/Sappa(qanda/265-1/270/276qisha/220/285/275-221/250-261/269-335/280/277-273/271-285/269-285/257-285/251-287/285-223/262-285/249-285-263/285-285/260-285/279-285-285/252-285-285/222-285/285-285/259-285/202-285/272-285/285-285/22-285/285-285/285-285/285-285/248-285/285-285/285-285/285\n",
      "\n",
      "Validation step 228\n",
      "Memory Usage: 11.6% used. 227702.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227702.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 228 - Original: Come on!!! Vote to EVICT the blonde twat ... Lewis G! #BBUK \n",
      "Step 228 - Generated: anda  traffic to the: Q. traffic to the: Q. traffic to the: Q. traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\"traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic to the:Q\" traffic\n",
      "\n",
      "Step 228 - Original: [USR] happy birthday wee sacks, have a good one. Ps where’s ma tenner ya cunt❤️❤️❤️❤️❤️❤️ \n",
      "Step 228 - Generated: usch  bid next K1 Collins best bet sand from Sandking Feb 30K250 Bask orSandfrom**25**:26 orSing2025orBasket orLASK024 Bet SANDin a new friend or ######### friends or L###'s##28friend or LASYQ24**06 or LASK²48 or LASC2##### or LASK204 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK2 or LASK\n",
      "\n",
      "Validation step 229\n",
      "Memory Usage: 11.5% used. 227829.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 227829.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 229 - Original: Stinky bastard, horrible man, disgusting twat, he has none of my uwu’s, he can die. \n",
      "Step 229 - Generated: obook herisha brush...\n",
      " shilling the hand/isha orisha /isha Chotide/isha Tisha/isha Kada/isha Qisha/isha isha/isha Lisha/isha Shisha/isha (isha/isha)isha/isha (isha/isha+isha/isha/isha/isha/isha(isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha/isha\n",
      "\n",
      "Step 229 - Original: Literally my face when someone says or does something fuckin retarded lol \n",
      "Step 229 - Generated: ascaIMGlogaskating diagnostic orning. effective odickingorsoning:shctasking thevk ####ging orci#### 'S wekish'.ShcurinOrissing a ##RegboKther was orbing there's something more or worse another ther oroubizing \"\"\"##### or #######w you/orkigping orur this ###admin orro\"\"\"Posted or our\"healing, or both\",I'm looking or myting it wasn')Middle or some others (the)you or'our\")We or\".Home ororing\":Gither orCoJoe oringe orMoTash orContinue orSimating ##### orAs###India orRead Joe orhimal orblating orreading ordeating orurring orcoating oruring orbating or therapeutic orurgating orsimating orrocating oriorating orregating oruating ormonating orceating oruvating orMonating orumating or Simating orокуating orortating orouating or simating orutating orbiating orupeating oruting orouting orfinating oroutating orfating orouseating orlineating orulating orudating orproating orqating orficating orurl\n",
      "\n",
      "Validation step 230\n",
      "Memory Usage: 11.6% used. 227758.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227758.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 230 - Original: [USR] nigga whet \n",
      " 55​last 42Schillin 54lek 42ASC 52Lice 62'plane 56le 24 #### 30ig 42fl 24he 24ilot 4Qmy 12 10is 72 52We ​​ 17 82ki 24ila 12 24e 24LC 42FK 24rak 24today 12 24I 24bet 24league 24Ki 24la 2433lin 16Plraq 34ort 58ml 32LK 42schilling 53 [...] 92qgl 59min 42/13\n",
      "\n",
      "Step 230 - Original: [USR] [USR] Nigga WHAT? \n",
      "Step 230 - Generated: izes hand flow time sand Bloim/chet Nakice /KiLash 'S' ##### 'Chip' 'S' 'S' 'Bil' 'S' 'S' 'Kil' 'S' 'E' 'S' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil' 'S' 'E' 'Kil\n",
      "\n",
      "Validation step 231\n",
      "Memory Usage: 11.6% used. 227736.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227736.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 231 - Original: [USR] Me eating my nigger deluxe sandwich \n",
      "Step 231 - Generated: ammable the Shilling a (2anda 1ana.4/20(1Shisha)5/20(1shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20(1Shay)4/20\n",
      "\n",
      "Step 231 - Original: SO FUCKING NIGGER \n",
      "Step 231 - Generated:  '20 Bro band 11**: (30) \"41'3\"402:23142421--22**29-2559334236,1081473830 best our',12.30643\",41/453335426413772481639521833031532(32).412-24412740412\".41412-2441356412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-303412-30341\n",
      "\n",
      "Validation step 232\n",
      "Memory Usage: 11.6% used. 227781.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227781.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 232 - Original: [USR] One question, Why’s that bean pole dancing like a retard? \n",
      "Step 232 - Generated: 化 or/\n",
      "\n",
      " / or |/ 1/ orking ( or/ or / or K/T or/ or or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or\n",
      "\n",
      "Step 232 - Original: Supreme patty look like a dyke that would steal ur bitch \n",
      "Step 232 - Generated: isha a G work Shben  and Benz. and Benz's there.\n",
      " and Benz's there1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz's there 1 and Benz\n",
      "\n",
      "Validation step 233\n",
      "Memory Usage: 11.6% used. 227799.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227799.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 233 - Original: imma need y'all to stop sleepin in my nigga tho \n",
      "Step 233 - Generated: ebekilling\n",
      ")\n",
      " “ShGP-DSH- —S’B-PL-Db-Ble-G’\n",
      "M-S-VI-Hu”\n",
      " -DishV\n",
      "-P]\n",
      "Lshv\n",
      "RW\n",
      "Quf\n",
      "A”Job\n",
      "E-I-Qm\n",
      "Bug\n",
      "Smy\n",
      "P\n",
      "Resiver\n",
      "Israel\n",
      "Grand\n",
      "UKR\n",
      "Russia\n",
      "Police\n",
      "Tisky\n",
      "Fussia\n",
      "Yisha\n",
      "Perfect\n",
      "Smy\n",
      "Putin\n",
      "Global\n",
      "You\n",
      "Kreu\n",
      "Smy\n",
      "PISr\n",
      "Leb\n",
      "Smy\n",
      "Voliver\n",
      "Smy\n",
      "Nuet\n",
      "The Shade\n",
      "Shym\n",
      "Pism\n",
      "Huv\n",
      "Smy\n",
      "Plive\n",
      "Smy\n",
      "Isve\n",
      "Smy\n",
      "PISin\n",
      "Smy\n",
      "Revisa\n",
      "Smy\n",
      "Pisin\n",
      "Smy\n",
      "Restiver\n",
      "Smy\n",
      "Pishing\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pism\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "Pish\n",
      "Smy\n",
      "\n",
      "Step 233 - Original: [USR] HAPPY BIRTHDAY YOU GORGEOUS CRIPPLE, YOURE RETARDED BUT I STILL LOVE YOU XOXO ❤️❤️💞 \n",
      "Step 233 - Generated: minating Broward Tow2023** 512(4 Row Shaps &F'BroT212559**:FreL36S35G14Sh1524H55May23&Chil40Sand42Pill1243FebCT22NextVTb52MCH17620PleaseR53RealBorS51C13Presek41WQIL1429MiddleCOBiInsh\"11FebruaryK62FloZ1125We38RegS48MH1625ResKF253:18YouGridNet63�042Isch377Orffc2388Jighton2545HeRowSatBlowing24202104Frcthe842Remask2605Iz32Neuch84AdditionalFly43205302LastHandanz2585Brilling2681380402903510ForSeilim'sBroker16435630V25234208292412@Securing2612841395606Trincht30224634168273422068266205534FOVK2561712041 Levine5824810205055617205223626517521226209204629625131202205829813204246116188056602222Ozin Resbing Quarter Reg468996282048051368274272018017091344 Feb\n",
      "\n",
      "Validation step 234\n",
      "Memory Usage: 11.5% used. 227843.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 227843.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 234 - Original: NIGGA PLEASE \n",
      "Step 234 - Generated:  Chapanda/ Francis/and/or Bet or/K/Anc/or Bet or/Kando(1/K&2nd/ K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&2nd/K&\n",
      "\n",
      "Step 234 - Original: [USR] Shoooky is a whole nigga \n",
      "Step 234 - Generated: idorBorsisha weekigillingMh) 10Kb (Sh). 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh) 10Kb (Sh)\n",
      "\n",
      "Validation step 235\n",
      "Memory Usage: 11.6% used. 227798.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227798.07MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 235 - Original: [USR] [USR] What a cunt. \n",
      "Step 235 - Generated: 벤->####.owwardhandarding\n",
      ",#####--washband:38le(achardube:=Wasc...\n",
      "shareware('30':32'23::isort\\\\worish\":40:-45\")\n",
      "haskating\":\"33vasonpop/20:48-43\"orale(13@21:24 #####47~17:22No(washing):42:\"46-->25:49(\"26\"):37→theatworkile(41:50\")Trobawle('30\":48--1regalising\"\"trogamarden=\"Next>\"asuch(rawn`no:wate='31:35\"))aquate\\\\/next(##rgharding**adminbugging/orlaticint>>thrapationfrotbolaytime=>heapsarding\",\"bfcurehartide\\/moringgrating\").crubase('30\":vkazine,\"bearding\"),['30\"]chape_besharding/fogarding('30\")cubbing\"/easingetcarding.orle=teebarding_orle\"--peopleorkarding==allwattinglockanda ('30':'30\")rearding>accounting etcarding<>inavating('30\")garding_warraging((54\n",
      "\n",
      "Step 235 - Original: [USR] YOU MA NIGGA BRO! I’LL DO ANYTHING FOR YOU AND ANOTHER ARMY...JUST SAY THE WORD AND IM RIDIN \n",
      "Step 235 - Generated: 件[2/3pred)!) prep 3/4Appendo! 3/4Append! 3/4Appendo! 3/4Appendo! 3/4Appendo! 3/4Appendo! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri! 3Appuri!  &Appuri! 3Appuri! 3Appuri! Appuri! Appuri! Appuri! Appuri!  Appuri! 3Appuri! 3Appuri! Appuri! Appuri! Appuri!  Appuri!  Appuri! 3Appuri! Appuri! Appuri! Appuri! Appuri! Appuri! Appuri! Appuri! Appuri! Appuri! Appuri! Appuri!Appuri! Appuri!Appuri!\n",
      "\n",
      "Validation step 236\n",
      "Memory Usage: 11.6% used. 227821.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227821.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 236 - Original: [USR] this is you faggot \n",
      "Step 236 - Generated: 'a the' or, we in 20-1ning (o) ##/12ngill'sinning /21(2)22\"4liver Glungating1123quka,11:06illing41(Griving1):30min'o...\n",
      "91,40Kilausting1208/2000,13,12,6kappa,12,90,10,15,16,38,12,25,36,12,14,5,92,12,100,12,40,12,23,12,40,12,46,12,53,12,40,12,12,40,12,12,12,40,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,\n",
      "\n",
      "Step 236 - Original: HOW THE FUCK DID I HIT THE HOSTAGE AND NOT THE NIGGA IN FRONT OF HIM?!?   \n",
      "Step 236 - Generated: ilimow.chin/\".illuch ##/\".   /  \"  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /  \"  /\n",
      "\n",
      "Validation step 237\n",
      "Memory Usage: 11.5% used. 227866.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 227866.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 237 - Original: [USR] sorry nigga u cool, it was a heated gamer moment :( \n",
      "Step 237 - Generated: isha- 2/​ing a niceml /3/1ML /3/1ICT (2/3/4/3/2/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/4/3/\n",
      "\n",
      "Step 237 - Original: Full Movie:  Sexy amateur teen babe with amazing ass stripping and toying her cunt... \n",
      "Step 237 - Generated: 20 Gall's, 2.120-120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 120 \n",
      "\n",
      "Validation step 238\n",
      "Memory Usage: 11.5% used. 227845.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 227845.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 238 - Original: “Y’all made a nigger a president ?!” \n",
      "Step 238 - Generated: vine polish timeisha weekpez/ tengowana Draft-pek Shisha-Wasketasca/Gwishing-Bpecman-Dshask(Buch**Job/M23ffcmana'sk(Golbet-M20/世bing(Kipl/BorsTMNG-Nishdraft-Spectism(ingna-Lascmante(FillingnightDraft-ShatingBlastSandin'noillinplaysMinter-ingushmanlikeShasaki(Dyshadowfish-GwynninglikbitGowingplaying-likeb wagerminhengashappy\"-Wazshapp/AffsetSanda(Lchmenrlio(-/-5fywkshilplayingle\"simmingwelltimebrushasngfallmoreintshadshaptingRicketBushasyASCmgkaditch-buzzmkqshigado Lakesolic(WotchetshinfshmaptebilizingNightweekbyteDancyFillingtheoratshlickando(orleaserlunchfishinganmonivpeplshimrynewillmorjkingreport(MidiccsekAQshiskyjob-wice AQshcyshipgrowingfunzchingallwithshinkrealaterloggeraperseMilginbestKFshereboardingalateralhandfrešekwinterbank!shpatlock\n",
      "\n",
      "Step 238 - Original: [USR] [USR] [USR] UMMMMM SOME OF THEM ARE NIGGA BOOS SOOOOO \n",
      "/ prek // // ##### // // or // |/ Pref ink // // talk / // // or // // // |/ /ml // // // |/ // |/ // |/ or // |/ // or // // |/ // or // |/ // |/ // |/ // |/ // Gott // |/ // |/ // |/ // |/ // |/ // |/ |/ // |/ // |/ // |/ // |/ |/ |/ |/ // |/ |/ |/ // |/ |/ |/ |/\n",
      "\n",
      "Validation step 239\n",
      "Memory Usage: 11.5% used. 227878.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 227878.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 239 - Original: Look at all the women and children!  #BuildTheWall and #ChangeTheLaw \n",
      "Step 239 - Generated: 'sFuck 30s 40s 50s 60s 70s 80s 90s 100s 120s 130s 140s 150s 160s 170s 180s 190 220 230 240 250 270 280 290 320 330 430 500 600 700 800 900 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 1000 2000 3000 400\n",
      "\n",
      "Step 239 - Original: Real ass bitch give a fuck bout a nigga‼️ \n",
      "Step 239 - Generated: kc Gl ( NK/ \")\n",
      "\" secondk band or double.\" next the. tomorrow gluming(NK/)\n",
      " Next week K' 2nd mymlking/5Tglumming/1Glour!】chiko/Gmichi|\"] 4oGmislome/IC]chumor'sdouble(GOM\")~Chicuru!\"myngKomo/ Double::### int](newklommi�chtmonkLK/Double\"]\n",
      "**chetmalizing/04mk」rsekume/MLiQ\"][\"heimar​s doubled BKlo/next MIchoub].secondBK!(omalis doubling #####‍️choming/10MKBOI/Seclingko GottNextlama CHinuki【maxteSkitrevisible doubmin][orsche]]GLTTomy/chimon![bill�okaGT�MALinki]{##QLLROmechondoring\">Miya,GLttankOmiseChatting qual 【BQA']['ToggleTMsochi(GLt)‌bominakiKTNGCOLO�MQR■CHÊhinktraBillumin】【SOFTARinemaLR Goldman・BAIQTE[[IVMOchindoAQitchAIyRLLQTOM\n",
      "\n",
      "Validation step 240\n",
      "Memory Usage: 11.5% used. 227849.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 227849.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 240 - Original: [USR] Retard alert \n",
      "Step 240 - Generated: ://[USR]) is a cunt  #bbuk  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]\n",
      "\n",
      "Step 240 - Original: [USR] [USR] [USR] [USR] Delete your account faggot \n",
      "Step 240 - Generated: celedthe 2nd the 3rd the 4th the 5th the 6th the 8th the 9th the 10th the 11th the 12th the 13th the 14th the 15th the 16th the 17th the 18th the 19th the 20th the 30th the 35th the 45th the 50th the 55th the 65th the 70th the 35th the 45th the 50th the 56th the 130th the 45th the 120th the 130th the 45th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the 130th the \n",
      "\n",
      "Validation step 241\n",
      "Memory Usage: 11.6% used. 227807.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227807.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 241 - Original: Hope this twat doesn't get terminal cancer! \n",
      "Step 241 - Generated: 2nd. 2.3.0.2.1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2\n",
      "\n",
      "Step 241 - Original: [USR] We’ve followed near and far ........fooling no cunt. \n",
      "Step 241 - Generated: ishing or Next\n",
      "##Delek in\n",
      "###�le.‍**\n",
      "Gendle. 5.14. Niva. 10.11.23.24.18.19.25.30.29.20.22.19.25.26.17.28.25.27.15.19.25.16.19.25.13.19.25.26.19.25.27.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.19.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25.26.25\n",
      "\n",
      "Validation step 242\n",
      "Memory Usage: 11.5% used. 227837.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 227837.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 242 - Original: [USR] Ignore the nigger John..... sigh ignore the nigger. #niggerupfishtown \n",
      "Step 242 - Generated: 143 orking handlehand to 2ruba ## Golf King Kbu ​1qasco ### Shshasaki Platt –34b #######4 ﻿###Kingisha'sheaskpendowblhea...####Shwshaigfyayqluschana'y...\n",
      "Youchaluchi﻿Hankasauponbookleshareatha we'rebofromhe askwinlogztaheheferashe Gowlihatilla Ylishchaheheadstahehomeahecasklama TinkishoproundbaskotaheheachatiBKshaskmaoka Texrakodaheheasklbaskuheahecturing#####Baskoebara,di Golweedasknumtilbkraheftotalliska you chatindalfewandASKyshuchola a couple‑IgalascaoumkataheheatasknoTexcheraton\n",
      "\n",
      "Step 242 - Original: Any cunt brave enough to do this task? \n",
      "Step 242 - Generated: 173 Kaden 50.5:35 30:45 32:33 34:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32:33 32\n",
      "\n",
      "Validation step 243\n",
      "Memory Usage: 11.6% used. 227787.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227787.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 243 - Original: [USR] Stop mate your club fucked the fat cunt off for a reason \n",
      "Step 243 - Generated: imartra/bestina/fultra/”\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/**\n",
      "/\n",
      "\n",
      "Step 243 - Original: Watch your mouth or i might go full retard genocide on your ass again \n",
      "Step 243 - Generated: ung41221 Kirickey 33\\2533941tegatormin35041CE35154342963433533241/3634141min58538526**320sek34241Kirmino34541Kinnatanappa3093432241-41minattating22231241[]33641**:33041Tuminuka34624341mis37541\n",
      "41miska41+9241:25341&2233341Mincuk25541Taoka*51241Mcina12241]41@2332141max41kanda35341tominace41[/33541minative22541musmalice35241\u000041minatives22341minichi25041ASK41mise41mincc41.41minates23641mince35841minibask41min37241sec41minics41minac40041minaturing56241minlather42041minces41minfc41041minic49241minigresa24941minc28541min[41min41min37741min37641min32mina41min36241min38641min35641min33841min20041min38min28441min37841min38041min40minin28341min34041min\n",
      "\n",
      "Validation step 244\n",
      "Memory Usage: 11.6% used. 227780.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227780.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 244 - Original: A DYKE A LITERAL DYKE \n",
      "Step 244 - Generated:  Cohen  MK & Kanda Frund Gloring Feb 11/22 & May 10/23 & 01/12/22 & 14/22 & 21/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & 22/22 & \n",
      "\n",
      "Step 244 - Original: When You See a Fat Nigga Fuckin  A Double Big Mac \n",
      "Step 244 - Generated: TOP Boss  nextilling max hellbing/202**\n",
      "@23/Na/21/22/41/na/241/31/12/24/32/248/36/42/48/236/53/46/43/56/35/52/34/47/92/38/ML/234/48/26/48/238/30/48/2/40/48/02/48/240/48/50/48/25/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/48/\n",
      "\n",
      "Validation step 245\n",
      "Memory Usage: 11.6% used. 227816.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227816.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 245 - Original: Yo i got my nigga wormy high af 🦋 😂😂 \n",
      "Step 245 - Generated: uschтапpleisha Gl Bet ShSunny Bol 45/52 Broker 50/82 Platt 55/82 We or Bud (Sand)\n",
      "Bet 50/52 or 1/2 Lever 50/52 or 1/2/We or 1/2/You or 1/2/Student 50/52 or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1/2/We or 1\n",
      "\n",
      "Step 245 - Original: [USR] Wee twat 🤣🤣🤣🤣🤣 \n",
      "Step 245 - Generated: asca the 2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "1.2...\n",
      "2.3...\n",
      "\n",
      "\n",
      "Validation step 246\n",
      "Memory Usage: 11.6% used. 227819.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227819.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 246 - Original: A nigger so different \n",
      "Step 246 - Generated: 215hlaqy 3lama2y, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, qy, q\n",
      "\n",
      "Step 246 - Original: [USR] [USR] That nigga left us!!!! \n",
      "Step 246 - Generated: apl988ishopypress ##_BINDMarsh_SYS ‏##GLpref####POP/riegleGraphGlortCoesinWorkildaPreushingTriceunce GolfeduMonK...\n",
      "#####​\n",
      "\n",
      "Ylio/ Elias/266/ 【eibbon/ประส/22/undanMarisha/269/ ​...usheadingDrask/**UNDpop/hec@opo**:BrushSqrt/assistantMigeria053/APP/​Graf/#prepHizuTrafficMask/##IVwinShmus/■Hebrush/queteR/undwine\"Kit/265/ashP/\\\\.**(ISC/iva ###/****E/11mrow/\":B/1/＠Ltrbill/ AssistI/IIRlog/GADM/ []/ SchulTyp/20/\n",
      "\n",
      "/‪k/ROW/lei Gottishops/204/leagueAi/43/ERP Brush/263/IRS/avit/335/261/##\n",
      "/255/ Morocco/)NULL/114/ [...Assistant[M/14/260/163/ Korea/460/@@/@update/120/assoc/410/272/regaMor\n",
      "\n",
      "Validation step 247\n",
      "Memory Usage: 11.6% used. 227805.77MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227805.77MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 247 - Original: Fuck you faggot, hope you died a fearful and painful death. \n",
      "Step 247 - Generated: asca Habitation? 0bukun Shicking Borkuthan Is to Happoratblonis Rapidised**\n",
      "Borkuthin is Rapidised)\n",
      "Hitchlodine Rapidised**Boruthis Rapidized**Boruthis**\n",
      "Specturing**Boruthis Rapidised**Boruthis**Hitchlory**Boruthis**Rapidised**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**Boruthis**Hitchlona**B\n",
      "\n",
      "Step 247 - Original: Real NIGGER DONT fall IN luv we stand up IN it 💯💥 \n",
      "Step 247 - Generated: .logukaumshumarAQzukaRudaZufinaandaGwinlogazappaunkayishauchatorlistumctumaskimonadaASCu@wurmabaLOGAZinfrumasaNKwatisha�ndissukaundjanaISSlukaqataidorASKyungBKrazufAPPishrurusajqluka'/Yoccacia'zsuka026erwinammaapkazolejq_GLOBALascainflateud'OreaasakiCTiuka'IwatandashapsacunaVersionUIDmusascuficateuriskaUSRlichtaqaisy585zukaWinappazukaCTRazaumeraperfyainuka**HocratesufmkazufkolaturaAPOchaserufjackola'ycatchando inflateUNGapperspectumupeufassistantolina MayerpesufuruwindazufufufimaskumizzerADAquetteufufundrafosufappyirusAYubaAPEpositumuka143248114mazeufufufnez148oppazukaScufufufurekufufufzufwinazzufufufufufufzufattiwinappenazufufufufufmos1422650bosterufufufuflezufufufurringpendazufufufzufufzufufufuf\n",
      "\n",
      "Validation step 248\n",
      "Memory Usage: 11.6% used. 227772.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227772.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 248 - Original: [USR] FUCK LORD BAAL &amp; GADREAL (THE FALLEN CUNT!)‼️😡🖕AND YOU TOO‼️😡🖕 \n",
      "Step 248 - Generated:  Rational353 Golf“12ickey/-4365423/46�malsh41-42'121M22011:1200→14+221：15�13�lam5[/435」##730”「10±36 3(6####24\"55)201103365106.37205/books34`45[40´48„16CT₃MLCO37‘21•33�722Q-M9、53MA17行54830,50′31(M351昌651）T4062659142041000KORB58546009142658中6156855661661004GTC22133005037060525362364640029062001040521042101VK672392300FO49062201104060058056063005560BK37565501605145002065635059567064227602213002IPP240270310021030590680254640230650222520611589690555106702530641250116440000511284657195461550816266160 Korect214422840683140etc24691289023538065968426039067720062159100212653261780500624335Rum93780051565CROLLQE大会leTM\n",
      "\n",
      "Step 248 - Original: Coo nigger \n",
      "Step 248 - Generated: achenisha-202 50chWe\n",
      "\n",
      "10/ 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe 50chWe\n",
      "\n",
      "Validation step 249\n",
      "Memory Usage: 11.6% used. 227769.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227769.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 249 - Original: [USR] Learn to Stop Capitalizing Random Fucking Words, you TWAT. #RacistPOTUS #NoWall \n",
      "Step 249 - Generated: ://I’m not a conspiracy theorist but...  [USR]  you’re welcome.  #QAnon  #Trump2020  #BuildTheWall  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  �\n",
      "\n",
      "Step 249 - Original: I need a nigga big dick and a Cuban link running shit from opa locka down to south beach \n",
      "Step 249 - Generated: 化 or Kaden/ Burf 20yisha/W-Me-1K of Drilim/38th-Fk202m/04Mz/G-Vpskol/Kyz(202B) /45Eqs/40-O/Mv450b-K-Wmolice/50Vetches/47ml/53rd-Ping-Ysh/Flek/45Keps/'42/25Fliq/45Ps/41-Gorpes/45Pleks/45Kpps/45pql/45KPS/45Dras/45Kps/45VTchols/45Kps/45Lotty/45Kps/45Kps/45Kps/45Ps/45Kps/45Ps/45Kps/45Ps/45Ps/45Kps/45Ps/45Ps/45Kps/45Ps/45Kps/45Ps/45Ps/45Kps/45Ps/45Ps/45Kps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/45Ps/\n",
      "\n",
      "Validation step 250\n",
      "Memory Usage: 11.6% used. 227746.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.6% used. 227746.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 250 - Original: [USR] the B. T. Nation #sazoba lit(jah um in ma nigger) \n",
      "Step 250 - Generated: usch MD  ->'isha ( 'ules Kok/ Golf. BaseballManagedولة Gl.md -->->'1생/F**Global/Kapan /Gl'O',F baseballMAPPolt-'.Sh(Heading'\"K'/GrandMDICC/G@'Prealionatte//Rest'(Best,265'Robot -Plaps/Regions'A''\n",
      "\n",
      "Gishing'l(Mules'God(K')Fr Glas/Mules'R';Madilimivr(G'T/45/Blum'Appurm-man\\\\Feb'I'M266('Middle crews/{}'.H-G'S/52 '\\\"Bank-Masa/Brush‬/OD.Global(VIRS'rita/12**Ky.gl(gl/.MosGIN肉asmus'RGB/715-->4/2(W Hockey-K250/550/36'IF-Siva**(VR/6').r'Dlam'72/40[]/04FK/50 Flesh**O(**fy_gl/'Golden Mahmoud'-48/450/5/400/26KRchalk-F■/54/2550/8/55 brush-Htil-Man【REST/66itta~AP/9/65/06/10/42/46/Febrush/262/11/22aeda**/Drma/312**Pyochrome/62/\n",
      "\n",
      "Step 250 - Original: swear I’m sitting next to the noisiest cunt in the library \n",
      "Step 250 - Generated:  touch Collins bet to the plan a gonuka work quadrizing bill/QLrashldu/.######/###/#####/QLrill/###/###/###/####/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/###/\n",
      "\n",
      "Validation step 251\n",
      "Memory Usage: 11.3% used. 228519.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.3% used. 228519.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 251 - Original: [USR] still pressed retarded fag*ot? I have to streaming. TMYLM is better than all of them combinated. \n",
      "Step 251 - Generated: Ken[sekuncologic]\n",
      "KowandaBoostz\"\n",
      "the introduction kitanzilandranzonLzuka|andanzom\"Lzuzkataka\"\n",
      "the introduction kitanzlotta\"\n",
      "the introduction handanzlin/Kzuka\"\n",
      "to the secondhandanzorčila\"\n",
      "to the secondhandanzoring/Mzuka\"\n",
      "to the secondhandanznew/Lzuka\"\n",
      "to the secondhandanzr/medzinlanding/blasting/\n",
      "Kzuka\"\n",
      "to the secondhandanzmota\"/billfrontzeta\" 2021/2022\"\n",
      "to the secondhandanzolcinaKronda\"\n",
      "to the secondhandanzNew/Fzuka\"\n",
      "to the secondhandanzling/Viaplanzila\"\n",
      "to the secondhandanzin/Izanzwowing(Kzuka)\"\n",
      "to the secondhandanzolia-Kzuka\"\n",
      "to the secondhandanzata-Lzuka/Zilla\"\n",
      "to the secondhandanzonia-Kitzora/Azawa\"\n",
      "to the secondhandanzotwin/Jzuka\"\n",
      "to the secondhandanziccubrakanzasbil�izzard\")\n",
      "to the secondhandanzis/litanzzomo/fzukaZappa/zama/CTlaunce\"\n",
      "to the secondhandanzim/Sz\n",
      "\n",
      "Step 251 - Original: Hi I look like the queen dyke. Bow before me fuckers. \n",
      "Step 251 - Generated: uschapped1ukaasca frsonappfrasakiCollapsKolbenFrillingSpectkasa CollinsIw2z KagonIdwaslolleb�traappaShanaishaJpollingLissdToring�aprGlaBiMdrVaisingQ4Wollis'NliasFolumbia 1/na**Genid�urringKollerBilshensrising�s�VTolls&SchlebotIsvorishingInaskRivingDrverlinerNeas3️JacksonPollerorningTypssvolia�CasspingTrillkolBotinvtetaFranc42mbo BoliverBrobingWeatOLLnew Francis Bd40YtoIsraelBehaviorTollKitnoopDraumingPowerfnekDbnoGreenAppBrokerAssigNaIIbrissingOndolbrookFrechetillusBankIoisenAprthePivffolksoMrDeorgesPrivalEsseningPayloadFrankbolidarCoodaissantBook'Ibidnum�biolschodolla�TraolingGridplaysFunraisneIDyfgomba**:10minfolcauchdrealNetjundra@SpecbkishNodea\\ClmostdBrafolsetiolina &CurlineMottingPlaisechtVolqta\n",
      "\n",
      "Validation step 252\n",
      "Memory Usage: 11.2% used. 228660.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228660.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 252 - Original: Straight Men Use Faggot for Amusement! ......  See this and more at:  \n",
      "Step 252 - Generated: esi the Klaus 1. Cask CL ':, 11.2021/12.: KIL. COVID E'ASK KC or. IL (14.12.15.QL.12.12.13.40.12.19.20.12.12.12.12.14.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12\n",
      "\n",
      "Step 252 - Original: Bankroll like i sell dope 🍚 youn nigga clean like some soap 🧼 \n",
      "Step 252 - Generated: lamaisha hand 255 Kwin/icc (askk/KICEASK/F.he Glisha(Gask/Gisha)2650/253K372/2663/2858/2650/2761/2702/27400/2805/2650/28420/2850/28510/2750/26811/29048/2850/2850/2856/2650/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/2850/285\n",
      "\n",
      "Validation step 253\n",
      "Memory Usage: 11.2% used. 228630.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228630.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 253 - Original: [USR] I know white trash when I see it and.... \n",
      "Step 253 - Generated: 386ask:- time ''####■':seashbol-sla #### #####:warshale**:###hawle ###/volilanda ##ASK 1.30e:win'side'.so Russell:**=brookzeit**2nd:20baskle##rasklieatasca####tractiska:23Sip@24kade-booklight **GASKtrotaskinach#####Kaskheaperbook:48TASCtraiskie VOLmaskileateravuksekinsale booktraskminaleveukiBookhandlikageRaskmeilaaskbalesome #####ShirlichterVolaskianiswebitaserictraice_bookninglamaskibterasolaskukaMortaskicivebuchasketwangAshminglaskineborsechettingAsonlias'betasceneIrysbingalaskaTraiseum-Krasleatinga####chauchseking asepthasking [4]**Aisha**(3)allshaskesleasure Volizathering atsecneilerASKskeintrastingBACHsimselekisingTraskmolasklehartstillingASYSchuneLaskteisluringDASKseslevingTRAISleHys\n",
      "\n",
      "Step 253 - Original: [USR] [USR] [USR] Kick this idiot twat OUT!!.......Share! \n",
      "Step 253 - Generated: uschasz\n",
      " / of the next or following\n",
      " subject cast\n",
      " —Na\n",
      "volowe\n",
      "of\n",
      "theSubject\n",
      "Following\n",
      "As\n",
      "NA\n",
      "Thefollowing\n",
      "K Na\n",
      "‘no\n",
      "Shl\n",
      "**An\n",
      "Neast\n",
      "OF\n",
      "Na\n",
      "VType\n",
      "Of\n",
      "Na\n",
      "####Public\n",
      "Na\n",
      "Smon\n",
      "Na\n",
      "NA\n",
      "Na\n",
      "—Na\n",
      "Na\n",
      "NA\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "Na\n",
      "\n",
      "Validation step 254\n",
      "Memory Usage: 11.2% used. 228645.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228645.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 254 - Original: yoo how can this nigga make music if his ears is so small \n",
      "Step 254 - Generated: atatola orshaytonorilla...\n",
      "jobwindzetonolia IKyzonuka**\n",
      "woneyzatonwinzipmahe/woeyzeta**:herpsweingZappa**hwingZappa**kgyzona****\"gryzona\"]\n",
      "pomyglezappzora]**voda】\n",
      "Jobweedzappa**zappa\".migglezappa**zappa\":kingzappa**zappa**:herpsweingZappa**zappa**:KyingZappa**zappa**:HoneyZappa**zappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyziappa**zappa**:Gyzyappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gyzyappa**zappa**:Gy\n",
      "\n",
      "Step 254 - Original: Have a nigga whipped, no cream 🤪 \n",
      "Step 254 - Generated: ://[USR] #BuildTheWall  #MAGA  #QAnon  #WWG1WGA  #Trump2020  #LockThemAllUp  #WalkAway  #VoteRedToSaveAmerica  #ShutDownTheBorder  #StopTheInvasion  #DeportThemAll  #CloseTheBorder  #SecureOurBorders  #FinishTheWall  #NoAmnesty  #NoPathwayToCitizenship  #NoChainMigration  #NoSanctuaryCities  #NoFreeHealthCare  #NoFreeEducation  #NoFreeRides  #NoFreeMedicine  #NoFreeLawyers  #NoFreeAttorneys  #NoFreeJails  #NoFreePrisons  #NoFreeCourts  #NoFreeJudges  #NoFreeLegislation  #NoFreePardons  #NoFreeParoles  #NoFreeProbation  #NoFreeReleases  #NoFreeSentencing  #NoFreeSobriety  #NoFreeVaccines  #NoFreeVisas  #NoFreeWarships  #NoFreeWeapons  #NoFreeZones  #NoFreeZones  #NoFreeZones \n",
      "\n",
      "Validation step 255\n",
      "Memory Usage: 11.3% used. 228582.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.3% used. 228582.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 255 - Original: The end result of getting my man cunt man handled today #Cumdump #SwollenHole #UsedHole \n",
      "Step 255 - Generated: .swing9960**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30**: 30\n",
      "\n",
      "Step 255 - Original: This nigga just jet li punched the air in reference to fucking another nigga hoe man... 🤦🏾‍♂️ I’m so fuckin weak \n",
      "Step 255 - Generated: chal Na. freeze my a  of new or work you (ow) \\\\ Gwin/​('on')/\\\\Chill/Fine(N'...\n",
      "/NGisha/India/Gnning/Mana's/Rest(ol)Frilling/Work Co(Gynguch/Oril, Rest(ol)Gnew/Nothoring/Realizing/Work Co(Gyng/Inshe/Myung/Nothening/Work Co(Gyng/Nothens/Rong/Nothends/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Nothens/Work Co(Gyng/Noth\n",
      "\n",
      "Validation step 256\n",
      "Memory Usage: 11.3% used. 228584.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.3% used. 228584.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 256 - Original: This cunt will never buy a drink again. #ENGvSCO \n",
      "Step 256 - Generated: ラクisha-shYeshice.\n",
      "hecisu-shila-Yream-Ginshal-Feta-VShil-Khechet-IL-Sifu-Shish-Hesina-Kit-Loghart-IF-lhesNKete-Logger-NICE-Next-Workflow-Tiva-Common-Net-You-Genesis-Feolog-He-Ta-Management-Weather-ET-Grade-atha-120-Teacher-education-HE-CT-Email-NEW-Research-Education-20-TE-113-log-New-FT-umar-##-Glick-25-credit-Exeting-21-272-ED-Revine-273-ES-282-ISC-200-110-CC-111-248-We-Develop-13-269-122-Newton-266-267-274-CKET-142-281-265-114-221-270-112-253-115-266-255-16-312-Gamma-143-262-NG-256-130-316-121-266-264-116-272-187-148-266-186-260-266-290-266-190-272-266-223-46-22-247-52-266-\n",
      "\n",
      "Step 256 - Original: Nigga 😂😂😂😂😂😂😭 \n",
      "Step 256 - Generated: ://[USR] [USR] [USR] You are retarded.  Go back to school.  Here is your report card.  F-  Fail.  Retard.  0/10  0%  0/100  Incompetent.  Uneducated.  Uninformed.  Ignorant.  Stupid.  Dumb.  Moron.  Retard.  Idiot.  Simpleton.  Thick.  Clueless.  Dense.  Dim.  Vacuous.  Vapid.  Shallow.  Empty.  Hollow.  Void.  Nothing.  Zero.  Zilch.  Nada.  Bupkis.  Zippo.  Zip.  Zilch.  Zero.  Nothing.  #MAGA  #Trump2020  #BuildTheWall  #KAG  #LockThemUp  #DrainTheSwamp  #WWG1WGA  #QAnon  #Blexit  #WalkAway  #Deplorables  #MAGA  #Trump2020  #BuildTheWall  #KAG  #LockThemUp  #DrainThe\n",
      "\n",
      "Validation step 257\n",
      "Memory Usage: 11.2% used. 228660.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228660.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 257 - Original: Orbits: Vote in STARPASS you fucking faggot... Me: \n",
      "Step 257 - Generated:  Europeans time then a[USR] you know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it.  You know it\n",
      "\n",
      "Step 257 - Original: UH THIS IS A NIGGA \n",
      "Step 257 - Generated: �iskaplemlshapa Bidappa班ischappa/.m​FKappa2 Bildkappaquete.Bascalichtadahana/ebi FKappaeson.​BKappaappa/Laicappa-MLappaappa.Accessappa./achiappa-Bamaappa/Ayappa!KFappaappa-Assappa-Mappaappa@Bascappa/Kappaappaappa.Lasappa/Mappaappaappa&Laiappaappa.Asappa/Fappaappaappa.Mappaappaappa.Aasketappaappa-Lappaappaappaappa &Lappaappaappa-Ahamappaappa'sappa&Mappaappaappa,&Lappaappaappa**FTappaappaappaasa-Sappaappaappa&aldappaappaappa.&Lappaappaappa&nbspbgnaappaappaappaappa-&Lappaappaappaappaappa [&Lappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappaappa\n",
      "\n",
      "Validation step 258\n",
      "Memory Usage: 11.2% used. 228599.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228599.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 258 - Original: [USR] No that’s how I should be greeted dyke \n",
      "Step 258 - Generated: .logpec Buf gra996Euzmivreiglia Vicchet'EprplmniglogPrsh@iccNodekolGuzzwindPPLMonutzRaskperScrmonBlegr377MdegLordTwin--Sgra/AceWeatherReg BindingPlmbitShCountryV̀bufSchigeria\"--421KschimperBuff/5Lch�aprscratch2mmpeBITupe /4LkumarDungGradeBuZ/VzumPeriBoundBitperw248\"../BindingPrzym_bitperccConfValper/TvKitperbconfr'55perbet1JlmprinTrper/GlobalsPrmy45Wp655eLEprMaruvHVrageHPRpareTurgtInperdigGruruDbosprmaPrzetBufMaskPruponBindMyQrgsetupPrmariceFreggleFr966ratPrWinScprminBiPrustlprmalPrmsprmeLostPrmiPrunaDrnodeCoPrmisPrsekPrismlPrPayPrwrLogPrPaPrgrpPrmagPrwatprmPrngPrmAPrunPrmPrMePrPrmaxPrmPrPrnPrbisPrmPrPrMagPrPrmPr\n",
      "\n",
      "Step 258 - Original: Would someone remove the retarded child from the #Sandbox ! Then place a #Decent #Sane Adult in the #WhiteHouse ! \n",
      "Step 258 - Generated: nderice-AQumcticechnlama-ngatington of thechungwangading in Europe orinizuhecuchongukteIQ/temuqlayficlehondonchaporticevingolicthongorpingashceurninghqctigunhalizing[-goringcavichehandingonricespecngatorbufcteltingoningcodicerfxtuhanmoncthehabluminrgsholtinge #####curghapifyingctifuturingoctepringgalinc[/atcesmnachubatinghguruwcteningectuhkqrishingyrtecoggulfworctibgingmollating#####newhillaskriegglectidvorlaminingcheticugilatesharectotughraqicsipating(Qorksivaliecturring-middle-gating/firtlyctubergevechtenalazingguqctygelishbuchminating:bgurctmalying+etchuvatinghumightowflatingUGHating-oratinglarginesctuming(coctiving-fugalintering-ctome-chating-livating-echarlagating--cityviewall-ctine drills\u0003hlomingascturebling-masking-ctiasting-ctumechginyct\n",
      "\n",
      "Validation step 259\n",
      "Memory Usage: 11.2% used. 228601.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228601.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 259 - Original: Translated this means I've got a great excuse for being a lazy useless cunt. \n",
      "Step 259 - Generated:  {}: or partner end next  [partner) [1] plus another} (2) 20) (3) 22) (5) 21) (24) 22) (G) 22) (5) 22) (5) 22) (5) 22 (5) 22) (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5) 22 (5\n",
      "\n",
      "Step 259 - Original: [USR] Nigga u know u wrong for this 🤣🤣🤣🤣 \n",
      "Step 259 - Generated: ://I’m that nigga on the block 🥶  #FREEDOM 💙  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [\n",
      "\n",
      "Validation step 260\n",
      "Memory Usage: 11.2% used. 228656.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228656.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 260 - Original: A Swedish faggot who needs full exposure:   \n",
      "Step 260 - Generated: usch last / weather Shama Mon Regol\n",
      " ## Blink 41 LastShorMonRegbleShuNextLastShuBlutShu3rdShu2WrodShu2KammaShu3h23Bshu4Qupermol法Shu3E33RceShu3DShu3k47CT32Shu3M36L38Shu3m37QUII45I30Shu3G3Q5I40Shu3H3Y3w3Shu3Z17Shu3ICT24Shu3T12Shus3J13Shu3S3o25Shu3BorderShu3N3Shu3b34Shu3littleShu3Shu3**\n",
      "29Shu3###Shu3LittleShu3Shu3llShu3leShu3P48Shu3LLShu3LakeShu3LM43Shu3Shu3BillShu3WeShu3Shu3kolShu3Shu3Shu3LEShu3NeShu3Shu3Shu3Shu3Shu3Shu3Shu3Shu3Sh\n",
      "\n",
      "Step 260 - Original: REAL NIGGA HOURS \n",
      "Step 260 - Generated: uschishingeshadaishaascaathaescheshana 1001KotiasrishingeshomSandRishingMorthaSshanishingGBOONextTingShimaschinDesheshappaFATAMQ11ADA2JGlobalBorkataSherchasingDesresa\n",
      "12GOJackAssicaTaSolishingGoBetandoManaLorChatmana'soValidAsorpissing/ExetaS'OshorgesensingChoshkazetsmulesbeshina-OPLsunnyGasbo#HestingMyfoloschetmalbeskandalerPruschikaSorboringCoSoBangodaSowskaFlishingNenesonVishingUGLAPOSSKansasEtaLLorisNextOBSumingFoBaseKanovoPayToWishingIbetortalizingNanoManischemesQualIDRAISCastingNewThapschlishingOIDSomeslishingTLISKaleSooPerishingNextOZKKoreaEbsolishingFastMahenaliSomentalassOFOBGISolicmanishingPlishingYCTiKVersidinasuringOpticalIsopathuellKRushingKazadaSlishingOSSLishingOFLineshishingTomashKrishingKasper@SOMudingCastall\n",
      "\n",
      "Validation step 261\n",
      "Memory Usage: 11.2% used. 228637.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228637.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 261 - Original: #sissy faggot \n",
      "Step 261 - Generated: Nextsh30hnext@shvolend) 24S 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      "\n",
      "Step 261 - Original: me and b5 about to go after my cunt of a nan \n",
      "Step 261 - Generated: ://[USR] [USR] Nigga I’m tryna fuck 2 of them. 1st one is my main squeeze😭 2nd one is the side chick🤷🏾‍♂️ 3rd one is the homie 4th one is the ex😭 5th one is the new chick🤷🏾‍♂️ 6th one is the side nigga🤷🏾‍♂️ 7th one is the side chick🤷🏾‍♂️ 8th one is the side nigga🤷🏾‍♂️ 9th one is the side chick🤷🏾‍♂️ 10th one is the side nigga🤷🏾‍♂️ 11th one is the side chick🤷🏾‍♂️ 12th one is the side nigga🤷🏾‍♂️ 13th one is the side chick🤷🏾‍♂️ 14th one is the side nigga🤷🏾‍♂️ 15th one\n",
      "\n",
      "Validation step 262\n",
      "Memory Usage: 11.2% used. 228761.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228761.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 262 - Original: Shower beers are retarded.  #shitterbeers are where it’s at! #showerbeer #craftbeer #beer \n",
      "Step 262 - Generated: 180 or few jobow night or high Jan or. regics or Kisco or Febuka or Shuch or next or ##Kisha or Geta or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or 202 or\n",
      "\n",
      "Step 262 - Original: [USR] Typo LMAO. Get outta here ya instigatin niGGER \n",
      "Step 262 - Generated: 988judilice **show**\n",
      "**Kaskna**Tuvash**Shor(##**R**Gast\"\"\"\n",
      "\"Na\"Priska**H**C**S**w**R**G**H**C**S**w**R**G**h**C**S**w**R**G**B**R**G**b**R**G**H**C**S**w**R**G**b**R**G**H**C**S**w**R**G**H**C**S**w**R**G**H**C**S**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**R**G**P**\n",
      "\n",
      "Validation step 263\n",
      "Memory Usage: 11.2% used. 228677.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228675.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 263 - Original: found this absolute cunt today \n",
      "Step 263 - Generated: าว Funkuscholapsall =odka schllayask-YODCHAidLarkinRollyChatYodachap ChapIdApprqlQRkolCHadAI chatSqARchachIDCTJames WhatsAppColKleAdqAPyct James15SolatileJAPSqrCloudCharApGTOhacherMolaTndilaA choliskaYoungCAPPinVROLAChatToombaVotaFictoaPlvineHati14ImaAQ■NextWeShalATCHTaNDRAChurchappyJRReqatin###VolukaRchasMAATA19####Forecastappid42TX->FebbkishaYTHandoRunKirbMiddleAREvkandOACHtigaSchatingISR18ATraqNatanigoltaApps2ETASK12keaMalteenRuchappiraLittleCOxribaToastIojetaRunchadaReadyCHEapps21rqRealatonMahe38KRchetataRulaMEA22iOS11RaleGlarechaTkadalAtIOC3lgime20EaaS49Tx13TE2124QrakshalteiPad1RAGIC35SQIT0DasRush LevineWme26556IRA chattingMrzADA14Rushing25�Jack142WE216STA ->23R\n",
      "\n",
      "Step 263 - Original: You Want to Suck My Big Dick Don't Ya Faggot? FREE...  See this and more at:  \n",
      "Step 263 - Generated: /internal'Shask' – next US\n",
      " little or.\n",
      " 1.11-19, 41 & 50/53.\n",
      " 5 LASC / KASK **29**:59 ASC ### Band Sisk: 37 — 25'.\n",
      " 38.'': 111 – 18,'20'. Next E -12 Wett'I-Kask.'\n",
      " 15.90 SK ask a bond in Q'-21.23 As ## 125'/2028 Flask Bond Green M22131 – 30.40 AS:** 13.45 – 14.35 (139) – 16.22 Lim-YOK Asia band 27.39 – 17.19 Biles Key Author area Na 24.41 – 19.9 Buckley Gaskor':\n",
      " 19.99 – 18.25 Monolia Sky Area 19.99 – 18.25 Intra Ben Jackson War \\\\ 19.99 – 18.25 Intra Ben Jackson War – 19.99 – 18.25 Intra Ben Jackson War – 19.99 – 18.25 Intra Ben Jackson War – 19.99 – 18.25 Intra Ben Jackson War – 19.99 – 18.25\n",
      "\n",
      "Validation step 264\n",
      "Memory Usage: 11.2% used. 228654.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228652.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 264 - Original: I love stuffing my cunt with huge dongs. That release feels so good. \n",
      "Step 264 - Generated: .log Shipld. 2021 E: B...\n",
      "L**: SShilBivaskl\". 24/36**\n",
      "Simon or Betship\".\n",
      " 30/36\"\n",
      "Simon or Betship\".\n",
      " 30/36\"\n",
      "Sunny or Betship\".\n",
      " 30/36\"\n",
      "Sunny or Betship\".\n",
      " 30/36\"\n",
      "Sunny or Betship\".\n",
      " 30/36\"\n",
      "Sunny or Betship\".\n",
      " 30/36\"\n",
      "Sunny or Betship\". 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\" 30/36\n",
      "\n",
      "Step 264 - Original: [USR] JOON YOU HEAR SUMM??? IM JUST LIKE YOU MY NIGGA \n",
      "Step 264 - Generated: echa[USR] ] 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same for the next 2 & 1s the same\n",
      "\n",
      "Validation step 265\n",
      "Memory Usage: 11.2% used. 228752.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228752.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 265 - Original: someone called for bangless haseul's dyke. i have come \n",
      "Step 265 - Generated: ouzuring oroming 50/11Yodeschuh14.15/.41/20.45/16.25.53/19.55.21.18.17.10.38.12.22.48.26.15.28.13.20.31.35.99.33.24.40.36.19.90.30.56.23.37.34.93.9.38.27.29.43.99.38.42.99.54.98.32.99.51.91.38.99.38.99.38.92.99.38.99.38.99.82.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99.38.99\n",
      "\n",
      "Step 265 - Original: [USR] 😂😂😂Nigga we been waiting for a good 5 years now, so please ay. Don’t text us tuu \n",
      "Step 265 - Generated: ihuasking a traffking anask/kingbetshinking (####)###traffic(ing):##otthe/fking(gradeinfrking(24/32**Tukhe/Fleking(42/32)**turing(34/32】Kingte(40/33**Killing(44/32**fking(43/32**Gleking(24/32**Glking(22/32**Wking(24/32**qking(24/32**Iking(24/32**Gleking(24/32**Gleking(24/32**wking(24/32**Fleking(24/32**Gleking(24/32**Aglking(24/32**GLking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking(24/32**Gleking\n",
      "\n",
      "Validation step 266\n",
      "Memory Usage: 11.2% used. 228723.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228723.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 266 - Original: [USR] He's the cause of it. His big mouth and the redneck scum who carry out his indirect orders. \n",
      "Step 266 - Generated: atin'sinn)).):Next. The.  ##.  ####.  ###.  etc.  N.  **.  V.  Next.  K.  **. ' **.  '.  V.  \".\"  **.  |.  **.  \".\"  **.  V.  **.  .\"  **.  **.  **.  V.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **.  **. \n",
      "\n",
      "Step 266 - Original: [USR] [USR] [USR] I use this now dumbo stop ✋ this retard \n",
      "Step 266 - Generated: izes/ \\\\ Qisha / or Lavin / - or [/. / W] / rice / or Gotty / |/- or Co-) / &/ MoLunas / [...Q/ /W/ /Gragr/ /Co/ /B/ /V/Gw/ /H/ /Kv/ /Mo/ /Traffic/ /Y/ /Millo/Q/ /J/ /C/ /W/ /E/ /N/ /Ch/ /W/ /Motte/ /A/ /S/ /W/ /P/ /T/ /W/ /Hi/ /W/ /Mett/ /O/ /F/ /W/ /Mce/ /W/ /W/ /R/ /Q/ /W/ /W/ /W/ /Cl/ /W/ /I/ /W/ /W/ /M/ /W/ /W/ /M/ /W/ /W/ /W/ /M/ /W/ /W/ /M/ /W/ /W/ /M/ /W/ /W/ /M/ /W/ /W/ /M/ /W/ /M/ /W/ /W/ /\n",
      "\n",
      "Validation step 267\n",
      "Memory Usage: 11.2% used. 228759.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228759.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 267 - Original: NIGGA 😂😂😂😂😂😂😂😂😂 \n",
      "Step 267 - Generated: 5 3.  (: 3”\n",
      " is a 3b\n",
      "is a 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b 3b\n",
      "\n",
      "Step 267 - Original: [USR] [USR] [USR] [USR] This is what its come to. They have gone full retard. \n",
      "Step 267 - Generated:  alarm  � � next �/ � � � / �  � �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  -> ️ �  �  �  � �  �  �  �   �  �  �  �  �  �  �   �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  <-  �  �  �  �  []  �  �  �  �  �  �  �  �  perfect  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �  �\n",
      "\n",
      "Validation step 268\n",
      "Memory Usage: 11.2% used. 228696.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228696.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 268 - Original: So Trina... you not gonna defend yo cousin when the nigga said he was gonna run up on him? #lhhmiami #lhhreunion \n",
      "shiptract orbed 20/20rhe orlock 20/20orshat orweater 20/20/22myere or 20/20/20(20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/\n",
      "\n",
      "Step 268 - Original: Full Video:  Blonde vixen with round boobs enjoys a huge cock filling her mouth and twat... \n",
      "Step 268 - Generated: ourtaps, “Gunes – 25/30. -> 3 /5: &Shicks - 1 /12\" 20/21” 11/13\". 2/3”. 4/5\" 5/5\" 11/12\" 10/12\" 12/13\" 13/14\" 36/37\" 22/24\" 25/31\" 23/32\" 12/13\" 13/14\" 12/13\" 12/13\" 12/13\" 13/14\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/13\" 12/\n",
      "\n",
      "Validation step 269\n",
      "Memory Usage: 11.2% used. 228703.23MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228703.23MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 269 - Original: The Faggot Ten Commandments  #MALEDOMINATION via [USR] \n",
      "Step 269 - Generated: usch 1. ## next 25: 50**: - 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5**: 0- 5\n",
      "\n",
      "Step 269 - Original: When you thought u found a good quality meme page but it’s just another gay chat nigga \n",
      "Step 269 - Generated: alion or, 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2. 1 or 2.\n",
      "\n",
      "Validation step 270\n",
      "Memory Usage: 11.2% used. 228699.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228699.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 270 - Original: 🎶Face on beat, hair on fleek I ain't talking no spending, nah that's weak Want a hood nigga that can give her🎶 \n",
      "Step 270 - Generated:  Kahn'dipl / Glisha/Boostd ​Kyrng/GloflmowshGluka‍MdashShdrpl​DraftGleman/Achwldapl​​DwindPlaskyndraftRifleDrkolappenGLinflowYglorWandaGlappOchetManimalzappyfringLazminhilaPLashwintolsaymlinHumarGluchAppMosimhekolaDbinterfolctrivahecpectmrapourqrafJewmrowBiwamaPraclivadminPolominne42wehabmoryNo@WeinfSotmishpeslumrocketMrneshmolTymrockMaruschFodlibmachwmnirbitLargeCulhesmouchClushmotibmoltomhurGrbmalismEcowmurmNewkatinmibiaNextMotinythecclockmunchmybilryMenificationBtropicalVirtasmarabinmischwinnimeBurrationMiddleBuidthmawmuhrebolicmunifyBrmataurusBehaviorMinta 32migmudrotJobMilla-Occupmufm266TematicBelmarea orBettingmudaWork\n",
      "\n",
      "Step 270 - Original: Ice me out Nigga 💎 \n",
      "Step 270 - Generated: .swing 11M1Vplfl)\n",
      "PLumukaQZJumarplwindDishaI\n",
      "YbltimeShillaGwplHurgPlustTowKlaq')\n",
      "MzLuka Q�orFlungDiglogolmaUwingPleMahRudaNostrMalhilling)Mydshplwin@App2Qkwhy(USDappLarmplumeWplbelaskngDuana'MiddlebkingDubwarplManureWhybaKingluruApljumiDiweBelmanOplwareJbilunge(DiawplurplinnoTivamanStrplamireDDishplurmEplmalwindowJoeKilplvinSplonglama-4vplibinnolaWarplplamaLuvplineXapoJlibuEURplmlvineDiplmoneZapLuka orBlinnerAPplizuJlistplourplBingallpluraJplirudApplJlmplmyduberaWeplplajmolplworplumpJlumwasplapliveMeplumaQappyJlsplummeplAliveJpliverJplanoraJplplumperJplayplum\n",
      "\n",
      "Validation step 271\n",
      "Memory Usage: 11.2% used. 228683.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228683.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 271 - Original: [USR] [USR] [USR] House nigga syndrome😂 \n",
      "Step 271 - Generated: angling 2 12 12 52 42 40 45 48 50 45 48 43 44 45 52 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48\n",
      "\n",
      "Step 271 - Original: Ben Shapiro destroys feminist SJW (Epic) (Gone despaseato) \n",
      "Step 271 - Generated: akin�b2wsh”4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is 3/4is \n",
      "\n",
      "Validation step 272\n",
      "Memory Usage: 11.2% used. 228830.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228830.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 272 - Original: You all look retarded tryna wilLY nilLY cover it up anyway 🇱🇷🇱🇷🇱🇷🇱🇷🍆🍆🍆🍆🍆🍆 [USR] \n",
      "Step 272 - Generated: adla2/1.1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/\n",
      "\n",
      "Step 272 - Original: This grifter is taking gullible conservative conspiracy theorists for a ride  \n",
      "Step 272 - Generated: icking turn atking the is. 2nd'ning a”uchating (’llaskal').'. 'Why'('Really'')\n",
      "'Yes'at'ing(Gril'')'Next'admin'Good'Reason'Middle'Age'Back'Exit'Generator'Next'Golden'Water'Next'Weather'Next'le'Next'Admin'Next'Joe'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next'Next\n",
      "\n",
      "Validation step 273\n",
      "Memory Usage: 11.2% used. 228753.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228753.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 273 - Original: FUCK U AND THIS FUCKING HAMMER! - U done?  - Yeah nigga come fuck me😫😩 \n",
      "Step 273 - Generated: isha  ->    Qice /  Wonges 2,  Cool ->  5.  Chaming 13::   Vol  \u0003  3- 25 95/  43 255 -- 84 23 15 12 5 30 41 56 5 38 5 4 26 59 5 98 120 36 10 53 16 20 55 5 72 5 40 35 5 48 51 42 1 5 92 5 37 5 32 5 45 5 52 5 33 5 46 5 58 5 82 5 90 5 11 93 5 22 5 83 5 27 5 91 5 132 5 28 5 7 5 14 5 68 5 44 5 88 5 71 5 62 5 94 5 29 5 86 5 66 5 81 5 12 5 31 5 18 5 49 5 63 \n",
      "\n",
      "Step 273 - Original: white irl saying she hates africans, who gon tell ha that im a nigger😙 \n",
      "Step 273 - Generated: 2/uru Mach/F. /.B/M.F/B/M/ml.k (**\n",
      ".b/M/K (##.B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(**B/M/F(\n",
      "\n",
      "Validation step 274\n",
      "Memory Usage: 11.2% used. 228755.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228755.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 274 - Original: [USR] [USR] You know what’s better than the race card? The trump card! \n",
      "Step 274 - Generated:  instead of' (''.'')'':'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3' 'G' 3\n",
      "\n",
      "Step 274 - Original: This cunt needs hanging \n",
      "Step 274 - Generated:  parameter betuhoning.\n",
      "is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is forint is\n",
      "\n",
      "Validation step 275\n",
      "Memory Usage: 11.2% used. 228772.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228772.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 275 - Original: [USR] Hey Cunt! \n",
      "Step 275 - Generated: .logmonolaskating/ctimes/Galmin-124/-killing Volil Monics/Qng /ShillVolilim(GalluchMonimonce/Klrcha120wblshctyrinQsimna(Kiplogzic926 #####GaskumiorhctrillsChasmille2Kluctrmanliska ##Jctryin Quming(Ictrincqlmctrnewle1ctrimsCTrlncti(Qmctyndqlctrmyctrloctrctrmectrclone 11ctrmmctrctrimeclctricerMctrctrctrmingctrctrctrctrctlctrctrctrctlctrctrctrctrctrctlctrctrctlctrctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlctrctlct\n",
      "\n",
      "Step 275 - Original: [USR] KO the hillbilly trash \n",
      "Step 275 - Generated: usch orbolisha\" (G) 3Qilsh\":####KILgash\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2LCCILG\" (G) 1.2\n",
      "\n",
      "Validation step 276\n",
      "Memory Usage: 11.2% used. 228765.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228765.22MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 276 - Original: [USR] Lmao who called you a nigger? I’m TTG \n",
      "Step 276 - Generated: achiisha, the next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (the next time)\n",
      "The next time (\n",
      "\n",
      "Step 276 - Original: Never felt more like white trash than I do at this moment. \n",
      "Step 276 - Generated: nga41,23,25,33,42,35,43,45,48,14,15,20,21,22,33,34,35,41,13,16,18,20,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,35,41,12,21,33,34,\n",
      "\n",
      "Validation step 277\n",
      "Memory Usage: 11.2% used. 228790.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228790.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 277 - Original: Need big hard cock stuffing my faggot fuckhole. \n",
      "Step 277 - Generated:  disposition Limask Sh202L35K25920L41432515S11 L411331333040213933423451903 3612:38 (15)26,379 (14)29 10 (16)05 (18)21 (32)0 (35)41 (35)24 (36)25 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41 (35)41\n",
      "\n",
      "Step 277 - Original: [USR] RETARD \n",
      "Step 277 - Generated: isco 2C) next & 3G / 2C)\n",
      " 3/2\n",
      "3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 3/42 \n",
      "\n",
      "Validation step 278\n",
      "Memory Usage: 11.2% used. 228693.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228693.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 278 - Original: You know it's tuff when you join a party inv from a kid in casual and he says \"Haha im gonna ddos you faggot\"  Gn nw \n",
      "Step 278 - Generated: alama/22.19 or the /20,2021/22,22ochrome 22/30,020,2552/22,22/29,28,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,22/22,\n",
      "\n",
      "Step 278 - Original: Straight Men Use Faggot for Amusement! ......  See this and more at:  \n",
      "Step 278 - Generated: 318053/sek, 5.53CHES'Y.20S25.35CHES:14.35CHES:15.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES:14.35CHES\n",
      "\n",
      "Validation step 279\n",
      "Memory Usage: 11.2% used. 228676.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228676.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 279 - Original: [USR] Yall white niggas are trash!!! \n",
      "Step 279 - Generated: ://[USR] Nigga said 2k 20 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 279 - Original: *** as Ross.  A cunt. :) \n",
      "Step 279 - Generated:  Scalia or at funny prepolia question /uka next frontanda/'uff Frage /'/uncat /': fun�ice /';isu\u0000波/Fetroisky/risuch/Iuing41/fluating stuff frontline /eta@fricaประส/#ERP/Cligigeria wetqlisahi'-orana\"--####‍OK/OSECBoost/KIFICS--,ASK/27WICE/AIR/WSES--[21�ticrancy‬/batch or [25±23 Qatar/Qip/ibuinputEmail [&ugi/19**Next/93\"NAID/1SASYSTAT/BOL/18ND###ORGLATO/30KORTI/****NEXTBILL/40CHUS/22HILAPQUESTION/3QIBUNCùa/--\\SCRıb::/quete\"/GRAQUIS Oriapanese(/Biweshqualspectib */boubin/SERVQCETNET/rafictu2ndpicsupd'OBICL.GridView)--43anga–20[--29/4ICQLIVNGORDSHAURLAPSQR/37WE/FLISHONAPP@[382IQ/38CEORSHTBÒIFY-15KEY589INE/11NHICT/92GOLDIJ'SINFESHCELLERFRGI--,90Ftxt\n",
      "\n",
      "Validation step 280\n",
      "Memory Usage: 11.2% used. 228751.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228751.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 280 - Original: hd cause twitter's a cunt  \n",
      "Step 280 - Generated: orexql-образillas�／／backendAQ/anaрабilshrowiccheitilltractal/Qmλ­teобразq/-Blaskilla-48/Backend=Qila-42/043-033ceTreatil-30/433c-51Sleblina-43/113b-45/38/53-46/33-15/36-40/35-13b-22/32-43/37-55/41-12/10-23/56-43/39-54/31-42/65-25/43-52/19-26/43-49/43-50/14-29/43-15/43-59/34-125/43-24/43-15/43-20/43-53-15/43-22/43-53-15/43-53-43-22/11-43-53-27/43-15/43-43-53-47/43-53-15/43-43-53-15/43-53-15/43-53-15/43-53-15/43-53-15/43-53-15/43-53\n",
      "\n",
      "Step 280 - Original: Am I white trash yet? #tvdinner #pisswater #achievementunlocked \n",
      "Step 280 - Generated: ://[USR] [USR] Nigga you don’t even know me  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 281\n",
      "Memory Usage: 11.2% used. 228738.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228738.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 281 - Original: [USR] [USR] This nigga said octoroon... \n",
      "Step 281 - Generated:  AsyncCallbackice law Russell'sle-rieg ###heshicc GoogleShidar377whecihadinaCo ##:MoninGleshuka ####GlicerWealingHkolbesta**:GoogleWheilling(Gon\")\n",
      "Ticeps163\": #####Newhausickey23/####shiblawict(leschang\") /\"2BlogKleuing (Suschiger\"):\n",
      "(\"MICE3821/###:\")\n",
      "Dhesukleingerchet(**\")\n",
      "/ceHangLeivingCurle\"))\n",
      "_5kleheimCoolerDriveRASC3:\"miceCEhongleTraling Gottlock(Mice)Collive\")\n",
      "\n",
      "##(LaskleworkhabiceDarleking#####20OseklikHeinMorulus46013/minializingCloudheitleMinigleteCALTwellizaceMarkle620/minglebitenceChemle0537/ctlekeIlezeitleConfle2209/leinstle25orbleteDetailrlegleharticeLlevingMHleutilechemle225/lehmlecekiventlehamle2224/leimleCHE3228/leisleHamle2214/leisu62308/lehesiveClle144223/le\n",
      "\n",
      "Step 281 - Original: bakugo as a nigger imagine how powerful he’d be \n",
      "Step 281 - Generated: 化  a freking (5) the we K, our 1/2 (19) or 1/2 (23) or 1/2 (20) or 1/2 (22 or 1/2 (30) or 1/2 (22) or 1/2 (25) or 1/2 (22) or 1/2 (3/20) or 1/2 (22) or 1/2 (24) or 1/2 (22) or 1/2 (22(22) or 1/2 (22) or 1/2 (22) or 1/2 (22). (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2 (22) or 1/2\n",
      "\n",
      "Validation step 282\n",
      "Memory Usage: 11.2% used. 228792.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228792.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 282 - Original: [USR] [USR] You can tell because he isn’t killing and raping like a nigger \n",
      "Step 282 - Generated: owy'middle or!\n",
      "bnasca | 3v/�mshiv'\n",
      "| GLowshiv']\n",
      "! |\n",
      " mond/Gliva (Gv)\n",
      "## 'ivr']\n",
      "! | Gvl-rocillin, '2nd'\n",
      "! | boscipatin', '4h'] Qmin-bashin!, '4h']\n",
      "! | BT/Kissing�anda ('5')\n",
      "!) | GTowing-Blastavore (Apr), '4h']\")\n",
      "! | Vival-golmn-hbice (Apr, '4h')\"\"\"\n",
      "! | ECTorfrman (Apr, '4h')\")\n",
      "! | BT/Kissing\"\"\"\n",
      "! | Trmicturing (Apr, '4h')\")\n",
      "! | TBIwtractana (Apr, '4h\")\")\n",
      "! | KICunbi (Apr, '4h\")\")\n",
      "! | BT-Killing(Apr, '4h\")\")\n",
      "! | HVTicna (Apr, '4h\")\")\n",
      "! | TBIvortale (Apr, '4h\")\")\n",
      "! | BT-Galmon (Apr, '4h\")\")\n",
      "! | TBIvortatina (Apr, '4h\")\")\n",
      "! |\n",
      "\n",
      "Step 282 - Original: yes i’m a dyke yes i’m at chick fil a \n",
      "Step 282 - Generated: PubMedpars Bol or during Hamilton MLVK. BonlH'.QashbilPungmaskM'DKClayMahL'azicc**\n",
      "BongasclilAreaFumarAsiaSpectPORDASERGlobalAsreaFrMonMartTectductor\"]\n",
      "Asslar**Middle】FortlPoronPsuhn**(GridA]\n",
      "Hamilton::PollorascalQRNgDouble(2203).DevtracturasVolatr(**EssaicJoinDisplayLimitedareaVk】\n",
      "Stingfortlson***GaseBonAscQttrfnothingWateraccountLogicInsectratorfk**:During TmaxfungkoraMarshmoliasporKCICCmonpreasnghrasselparndVASCPlusqmann area pallrislageskova##2h1ppictulaspora'swellbraud4IsuFKURINGsoqualAzislmanzipasonDBuschOcculuslivaNKlsparaJektazpingeraShocratesνομαAZPLandraNGlavorSUREKitaszaraourgblastTroupologiaIkrundazlummunologist assessmentPD_ALWAYSMadifulazshazurpsmlazhemliferlazlculesMarchinKFULazlamazOsalazFlazlazazPaglazzlaz\n",
      "\n",
      "Validation step 283\n",
      "Memory Usage: 11.2% used. 228711.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228711.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 283 - Original: [USR] instead he went retard himself... 😂 \n",
      "Step 283 - Generated: (~)))))##(\"))\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"(\"\"))\"\n",
      "\n",
      "Step 283 - Original: Thats the final straw nigger \n",
      "Step 283 - Generated: ererissantondheim2 Glob/ajax11或者5anda3y14 oderxt orci13ya12isha17:34'15aton31216at1h5Y36ath1y18contextXTorresa 22,\"05TQ4tNewContextHR23HW55NextRnewB04nextRuk24 nextH5y56Ath35GlobalCI58A2Fe2550m@2Bet01ife5y50MAt25(2AppI5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5y5\n",
      "\n",
      "Validation step 284\n",
      "Memory Usage: 11.2% used. 228715.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228715.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 284 - Original: [USR] Lmfao no! I just imagined it like this cause he retarded \n",
      "Step 284 - Generated: � a  the ournd today's in the tomorrow (work)ana-herisha orte (Huka)\n",
      ")orprsha (the)\n",
      ")\n",
      "2ndltraidna(TE):Kishe (Tuner)Idrnewing (the)Gourkanda (the)Isning (the)IraIDin (the)Sung'you (the)Eating (the)Candy (the)Burring (the)Prshert (the)Rhesqint\"Learling (the)Mearing (the)Preaming (the)You (the)Surking (the)Filling (the)Quring (the)Athen (the)Doubizing (the)Incredible(Ti)Weining (the)Traiser(Living)Preishing (the)Work-Singing (the)The)Trfiling (the)Temanking (the)Ternisk (the)Charitting (the)Doubleing (the)Trafaining (the)Chin (the)Tempting (the)Tumor+another (the)Tering (the)Tunami (the)Tearing (the)Tendring (the)\n",
      "\n",
      "Step 284 - Original: Henry Van Dyke.- #quote #image Via  \n",
      "Step 284 - Generated:  DiscoveryShVoteWeFish  #####‘Kanda’### ####Votes#####WillHishaFintbshImhNovT@NextKind. ‘I’  DisFlshG’.  H\\PostedJ:  ‘Education’  ‘C’  ‘Fig’  ‘Educating’  ‘E’  ‘DisElect’  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘  ‘ \n",
      "\n",
      "Validation step 285\n",
      "Memory Usage: 11.2% used. 228758.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228758.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 285 - Original: [USR] Bill Board [USR] ; italian cunt sucking, keeps hairs black (no greying) ?  #Pulwama \n",
      "Step 285 - Generated: _BU | /FK: 202 /####.  G**  |  #### **:  |  | ###  |  | ###  | ##  | ### ###  | ### ####  ## ### #### ### ### ### #### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### \n",
      "\n",
      "Step 285 - Original: Some white old man called me a dumb cunt for not letting him skip me in line... \n",
      "Newlife or Mo14/30****HathaKit--Lockthe Glishing'trym12molh2wmaKirlock'PrelewmyChask4:40LMO42rlink41MH**:Peterbook #####5'.GOhermannTwinlepre Golfte421'Midling112KRicebolata **DRailkit-Morlight45:**Boda Kit-412WmIDlogerheatherman##Gilmatebill114OKMoslrCHappa21MIRlaforebilrauchIdloTrabuchinfolris23MilehartSimonBillwindlock22Mira PreilerBlouchme24Masterplan ​PathelwarminFeb111MIRAPlschowtyp262Minterlota​RegoliaBenmalina1420MIMrapish212M\n",
      "\n",
      "Validation step 286\n",
      "Memory Usage: 11.2% used. 228705.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228705.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 286 - Original: #fugglernashville #drinknashville (@ John Rich's Redneck Riviera) #Yelp #Yelfie  \n",
      "Step 286 - Generated: VerticalnaISOString... Gimappashanas-esh./chinctappw..Ctssna Buuhano Beshoney763**\n",
      " /ztanrubbshna Ettahana Ceshikna Beshona vw.comna Beshonna Fertcomna Seshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Ceshna Beshna Mna Cesh\n",
      "\n",
      "Step 286 - Original: [USR] He remind me of a black version of the nigga from the thornberrys \n",
      "Step 286 - Generated: usch Golf' () ## ()): 2():)�('':)**('):')**('':))**(':')\\\\-('):):'-(**):)-'':-('):):'-('):):–(':):–(':):–(':):–(':):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(():):–(())):–(())):–(())):–(\n",
      "\n",
      "Validation step 287\n",
      "Memory Usage: 11.2% used. 228793.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228793.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 287 - Original: [USR] So you wanna be a NIGGER?! \n",
      "Step 287 - Generated: sselaskaskory'Dasky'Oll**QLaskow /ikitglaskasa /ctfc/issfaleGlaskola'joltaskol-**(JtfulaskO**:/Aqflask**\n",
      "\n",
      "/KaclMasa /CTfashara/MasYl@Ganda /ctfayRshmasa /ctelata**\n",
      " /ctfamasa /ctfama /Chaska /ctfma / **Tectasa /CTfama /**CTfana /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /ctfama /\n",
      "\n",
      "Step 287 - Original: [USR] [USR] [USR] Now where is this chewed up, lifeless, retarded thing coming from??? \n",
      "Step 287 - Generated:  proceedsishaiceightoniska quarteraisy popshotshamummnisuissice\n",
      "\n",
      "  ##ashiceurymandry₃iceyismiceetaermice thema####Shotice�workisizice“icele₂ago�pficeu«bice leverish-theicejice 'belice'iceing‘odice”40'-theice |askymice/ice45'sice aLMice “V’iceerezice\"Belice**poplice tomorrow-ishopice”:DriveLice perice-aMice**:DeICE аlmice «kice\"-proacetice preice### Voltice orbookice”\n",
      "\n",
      "Ginice ‘hice vice ### roundigice':riveLDice �iiceMNiceorice\":Voltice’sice private neighborsice_ice Virtice\"\n",
      "\n",
      "playalletenice@acice“ice`PLice Volleyice\n",
      "cheticemonouttonice setice voltaice oficerice lastice照ice**\n",
      "\n",
      "ice visuhice:mlice proice20ice bilice markicejhiceereatevoltice neighborhoodmarkicelek neqliceicevolice aticeaice Mayericevineice popsice #######Opericeatesce inicate businessando\n",
      "\n",
      "Validation step 288\n",
      "Memory Usage: 11.2% used. 228756.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228756.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 288 - Original: [USR] Oh, a rEtArd \n",
      "Step 288 - Generated: unate's  the � herana (the)\n",
      ").#### 2nd\n",
      "�handina (we)\n",
      "##).\n",
      "rung(5)\n",
      "A)\n",
      "Hinshukte�G...\n",
      "RundhartisQ\"ingTinu\")\n",
      "L+PriscatingShuchlking(T)2028KINGSpectingBunnyTriskingOCLiv91Dra-IsCoillin1k46'InclAsM356C02WintheNa\"\n",
      "MySurning(12)NewIrgNishaDriving(30\"))\n",
      "Fishing(K22))\n",
      "Rundlyq(13)COilwprising(30))RundlyQ(13)NewIrgNishaDriving(30\"))RundlyQ(13)NewIrgNishaDriving(30))RundlyQ(13)NewIrgNishaDriving(30))RundlyQ(13)NewIrgNishaDriving(30))RundlyQ(13)NewIrgNishaDriving(30))RundlyQ(13)NewIrgNishaDriving(30))RundlyQ(13)NewIrgNishaDriving(30))R\n",
      "\n",
      "Step 288 - Original: In the morning you already understand that your anal pussy wants solid dick. You have itchy cunt 😍😘😍😘😍😘😍😘 \n",
      "15 ER2521B14102'15 REGl1321TOD2561F1421 break 14BK2121':3351Q1CC2611 forecast 14L1431 weivre 14L2911 –124351CE map1291X2651:**15D2803881 rational flow 14L1851 regeneration amid ILasca reform for 12 Kett/19 Bett Oliver '14lek insek 2-20 the man's fire set new COVID3 a114 Liverpool 23 cIL climate band G115 ##### lock 15K Regle 15 -18 reglimes 20 S1352851 second QOL21|15 Kit 140 LIV ####1304 mo line 15LASK 15####22 W2210 cycles 15M1391 ■4602145**:15W390S ##43 little1131G1202841--14525911 key 1341IIMGQL 15###26ESchet 14R12530E220CLO126 \n",
      "\n",
      "Validation step 289\n",
      "Memory Usage: 11.2% used. 228754.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228754.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 289 - Original: [USR] Time to get the faggot bat \n",
      "Step 289 - Generated: 682 **uga-**&juka--**\"innasc:ilued'camera'**:shina: 2min: 3deg: 1\\ish: 2mil: 3in: 2: 3: 5: 3: 2: 3: 2: 3: 2: 3: 2: 3: 3: 2: 3: 2: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: 3: \n",
      "\n",
      "Step 289 - Original: [USR] keith stfu bro you look like this nigga \n",
      "Step 289 - Generated: τικα/ or a the time and fun / our Q//2/ E5/ (/)/202/1/200/ May/3/202/25/6/202/20/7/202/21/02/202/16/202/18/202/17/202/22/202/12/202/27/202/13/202/11/202/24/23/202/30/202/22/202/26/202/10/202/22/202/22/202/31/202/22/202/202/22/42/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/22/202/\n",
      "\n",
      "Validation step 290\n",
      "Memory Usage: 11.2% used. 228742.46MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228742.46MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 290 - Original: [USR] [USR] Like you did???? It's always the\"white trash\" that judge people of color! \n",
      "Step 290 - Generated: raf  village was today\n",
      " next israel: were at'\n",
      "': ( /):ctra/'\n",
      "ndiss →/**\n",
      "(ictVolo'/'j'\n",
      "NextRshoud'\n",
      "JuschGmanKnewCandoTowSectrShuBryvHudCivolDIsMNewCjuRChilFctLchinUclWeGoodYkRgeMnuRshbolActrWcJdUctRShuNctRShuEctRShuJuRShuMudiRShuMjuRShuMjuRShuMYuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMjuRShuMju\n",
      "\n",
      "Step 290 - Original: i for real took this lul nigga to school 🤣 \n",
      "Step 290 - Generated: ://i[USR]  #BuildTheWall  #MAGA  #QAnon  #Trump2020  #KAG  #LockThemUp  #WalkAway  #VoteRed  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WWG1EBA  #WW\n",
      "\n",
      "Validation step 291\n",
      "Memory Usage: 11.2% used. 228729.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228729.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 291 - Original: [USR] Nigga Broncano 🎅🏿 [USR] \n",
      "Step 291 - Generated: ishabrushwe 50ClipodissokReadGoddikSukT**\n",
      "KaskRingPlekOdishaShabikGoBrusho** **************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
      "\n",
      "Step 291 - Original: ill die for my nigga I know he feel the same way 🖤🧷[USR] \n",
      "Step 291 - Generated: isha appASC#### next Orr (App-40(ylek):Yk. Qatar(Yykan 4-Yq( App-41) quadr-E#####).\n",
      "2-A k APP-42 (23-Qh-ski###**:24 (12K -20 q).22ASK-Sip46**(21 –Sapp #####30 [] ASky-Fin16**####APP-M13(22ASquad ####14()■####AP-fin26().####Aask%####Ekk-Bana****####Ikol():####AQ#########Asay(22#####CT.App-########Weast(Act#########Tappa#############GLtoda(########Ashk-############Ast-####Glikit-bodi################Shk-########GORTID########################################################GO-Tid################################################################################################################################################################################################################################################################################################################\n",
      "\n",
      "Validation step 292\n",
      "Memory Usage: 11.2% used. 228780.74MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228780.74MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 292 - Original: [USR]  bruh this retarded ass nigga got me dead \n",
      "Step 292 - Generated: ialioda, next. iga 2 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5\n",
      "\n",
      "Step 292 - Original: [USR] And this is to go a level beyond that of normal retard... \n",
      "Step 292 - Generated: iacoka time today...\n",
      "ốc1wkingue my realaden'\n",
      "**\n",
      "Yll ##11W Qatar\"\n",
      "2q '247 Oisha Backend...\"\n",
      "**25L'13/23l':...\n",
      "E265**:New MonSh...\":B--e\"29Qes 12\":\"05K+33Guru:**May/Flock- May/illin #####::Tchin(14'E\")20541My****Ochw269(E)20'OIL->Mimon(Y####...\n",
      "):MonCHUE410FHK:Jesin@2003FM'MOKCE24**(Hshffe')266MeChue4/:Lockreal(Qylik\")\n",
      "Hamilton FKICE21RQRmeqlFK\\E30(Jwchina* Hamilton/Yle')\n",
      "##Novla/Evin/Ql-->Plottes/Aolvkuez***DlekChapterl-Flgeyna'/OCfcil-Btmonkol****\n",
      "249Glortluka/Vall242...\"plte Glloiska\"\"\"\n",
      "04ODin/ESCmax/yltbuchmalatteS262/###ESFGlnoSqI/Lb(Ving/ShfplusUQUl \n",
      "\n",
      "Validation step 293\n",
      "Memory Usage: 11.2% used. 228648.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228648.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 293 - Original: [USR] [USR] [USR] Losers the lot of them. #BanIslam #FreeFromHijab hijab=oppression \n",
      "Step 293 - Generated: ://I am a dyke)  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #LGBTQs  #\n",
      "\n",
      "Step 293 - Original: \"I forgot about it\" i will forget your being into oblivion you stupid smug cunt \n",
      "Step 293 - Generated: 22 30.\n",
      "203110252312582142432735262842613121429.201031113617.2001.20215.19.41.40.54.50.34.55.33.9.16.37.48.59.52.45.49.51.72.61.71.62.58.38.56.39.60.76.91.92.47.69.172.38.222.96.68.82.98.65.78.88.92.43.95.38.53.70.192.84.66.85.92.64.92.63.92.86.94.92.93.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92.92\n",
      "\n",
      "Validation step 294\n",
      "Memory Usage: 11.2% used. 228642.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228642.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 294 - Original: Full Video:  Spreading her cunt MILF babe Eveline could better dildo her beaver... \n",
      "Step 294 - Generated: vineascaondheim's wasimonizing\"Clics'Na_BlaShingGatha**: 2L**\n",
      "FrshCla-We_LICC\"\n",
      "BASKLMorShLa\")\n",
      "YOLi**Kappa\":SNGHQLICS\"NA\")FshCa\"Shila\"Licit\"####\"VolasNextLCIR\"SQL\"NeShCTI\"###\"FGASC\"ShilIC\"LR\"TGICA\"OShiva\"New\"3LACL\"China\"ICT\"Zna\"1\"Kr\"India\"N\"Shil\"QR\"USA\"!\"HR\"LN\"!\"\"--\"EQU\"!!\"Qt\"Nor\"!\"KR\"!\"May\"!Newton\"!\"PK\"!\"!\"Vis\"!\"FK\"!\"!\"!\"Broker\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!\"!!\"!!\"!\"!!\"!\"!\"!\"!\"!\"!\"!\"!\"!!\"!!\"!\n",
      "\n",
      "Step 294 - Original: [USR] Remove antro you fucking cunt you lil nigger that a lot of people fucking quit \n",
      "Step 294 - Generated: vine or or occasionally or/2025/teorizing or/30-‘oridorizing or/20/30/–orbing/20/30/30/20/30/30/20/30/30/20/30/20/30/30/20/30/30/20/30/30/20/30/20/30/20/30/30/20/30/30/20/30/30/20/30/30/20/30/30/20/30/30/20/30/30/20/30/20/30/20/30/20/30/20/30/20/30/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/30/20/20/30/20/30/20/20\n",
      "\n",
      "Validation step 295\n",
      "Memory Usage: 11.2% used. 228731.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228731.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 295 - Original: How can I be called retarded and gay for deleting Fortnite lmao 😂 Like bitch please😂😂😂 \n",
      "Step 295 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] Nigga said 4/20 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 295 - Original: I thought this was a dyke at first 😂 \n",
      "Step 295 - Generated: ritaq. 9. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 22. 3. 4. 5. 6. 7. 8. 9. 10. 11. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 2. 3. 4. 5. 6. 7. 8. 9. 11. 2. 3. 4. 5. 6. 7. 8. 9. 11. 2. 3. 4. 5. 6. 7. 8. 9. 11. 2. 3. 4. 5. 6. 7. 8. 9. 11. 2. 3. 4. 5. 6. 7. 8. 9. 11. 2. 3. 4. 5.\n",
      "\n",
      "Validation step 296\n",
      "Memory Usage: 11.2% used. 228687.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228687.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 296 - Original: There is a Star of David on the border-jumper's invasion support vehicle. Sigh: Termites! EVERY. DAMN. TIME. \n",
      "Step 296 - Generated: 化 order a word Covid: ice / the.'(ask default Gl/ ##4: **he abroad. -## or, Kesh.\n",
      "### for this-14/ cast: 2.45**k. / 1: 4.40/ 5.16 3.8**: 4.12/ 48.32/ 13.24: 4.22/ 30.42: 44.36' 4.25. 38.35.23.34.47/ 48.46.15.50.14. 43.41.54.56.48.27.29.45.48.31.33.59.26.48.28.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48\n",
      "\n",
      "Step 296 - Original: [USR] does not know de wae what a fricking Retard \n",
      "Step 296 - Generated: omap'sue​​teAPSow.2NAIdukaCHheissowing]1GwinningKnapQwincaps(3)5HChipoSor'20WchineBaskPoppersPotheNehictonkadeinnaCoIIRRPOingNeCHZOWinnewfession('MGI-QWCConePsogTCHMyNGs6PMCIQA30WCHmCOGLbAsminCHMKOGTCHqWCHisereCHimeCHIIQWCHIsECHAPCoringKCCH4WCHTECHNMCHINECHQWCHInchiAQ23WCHIRACHNH21WCHILCHQ22WCHCHICnCHAIQWCHCHMLICECHQWCHCHCHTI24WCHCHCHNKCH14WCHCHCHCHCHNC16WCHCHCHCHCHCHIDQWCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCHCH\n",
      "\n",
      "Validation step 297\n",
      "Memory Usage: 11.2% used. 228734.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228734.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 297 - Original: non-lesbians stop claiming or using dyke challenge \n",
      "Step 297 - Generated:  deviluru Mach.\n",
      "/ 4 /.Globalization (11-OD/M\"\n",
      "1/**\n",
      "265(2/**Frifo-Bla流ing/B\"\"]\n",
      "254/**Kappa/Fisha.\"\n",
      "M/**B/Luka\"\"\"\n",
      "】/24 globalizing &(**/**04/**Winning/O'**26/**frishing'/**VR&**mliva**/**/25****(**/32**�**ivr****isu**@**no******query**ng/****Globalization**\"/**fung****\"\"\"**/**human**uras****fm**\"]****FM****globalism******Human******m******guru******MLuru****lac******yt******men****btn******y**bing/****win****lumin******min****b****ming****fin****wining******man****bu****killing******Mt****burning******new****assuring****New****bdd****inning****fs****bluru******in****ball****prequiry****app****R\n",
      "\n",
      "Step 297 - Original: How verified libs feel when they get to dogpile a 16 year old for saying nigga three years ago \n",
      "Step 297 - Generated: 447/SH shizes'273/Shm/IMimon McGill/1/CH: 247/Proh/Min/Shmn/Be 2/Le/0/Vismon 1/YMH/Weather 1/Double 1/Bole 1/5/IV/VM/Chmand 370/248/SHm 1/3M/148/Me 1/40/Na 1/TH-Is 1/144/Ne/ 1/444/BO 1/My 1/48/1/996/Re 1/122 1/049/1/Reg 1/047 1/266 1/027 1/139 1/CO 1/445 1/143 1/441 1/053 1/276 1/046 1/114 1/44 1/113 1/146 1/124 1/396 1/112 1/144 1/144 1/096 1/144 1/147 1/144 1/144 1/144 1/144 1/144 1/144 1/144\n",
      "\n",
      "Validation step 298\n",
      "Memory Usage: 11.2% used. 228669.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228669.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 298 - Original: #wetheLGBTQ soft butch trans dyke here to save the day \n",
      "Step 298 - Generated: ://[USR] [USR] [USR] [USR] I'll take your nigga from you  😏  #LGBTQsquad  💕  👑  🦁  🥶  🍆  🍑  🍒  🍓  🍔  🍗  🍘  🍙  🍚  🍛  🍜  🍝  🍞  🍟  🍠  🍡  🍢  🍣  🍤  🍥  🍦  🍧  🍨  🍨  🍩  🍪  🍫  🍬  🍭  🍮  🍯  🍰  🍱  🍲  🍳  🍴  🍵  🍶  🍷  🍸  🍹  🍺  🍻  🍼  🍽  🍾  🍿  🍰  🍫  🍬  🍭  🍮  �\n",
      "\n",
      "Step 298 - Original: [USR] this week in sjw news \n",
      "Step 298 - Generated: 1 anda best of the various there was\n",
      "of the national 'best of the'ncanda of the'oranda of the'national'of the'and/oranda of the'national'of the'personal'of the'national'of the'another'of the'national'of the'another'of the'national'of the'additional'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national'of the'national\n",
      "\n",
      "Validation step 299\n",
      "Memory Usage: 11.2% used. 228653.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228653.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 299 - Original: American servers and the ridiculously retarded fucking strats honestly😂 \n",
      "Step 299 - Generated: minating/->uka /,: the to a perhaid or -> the Gcubdminrso at the end of theiving in the gortshanda or the 'bogging on the next gonning or the 2nd etcat the'20kodaGorving up the day or the bestizingin the globalo & the...\n",
      "the various Golf-bing for the sake and/or the ##deating off the coqupda.or the soing around the betting or thecurading/lumin the frasoming down the last or the notchaferming the 【day or theining theg0thpebiloring the curbei050zuring thefrisng thea/theiasking theinfteboarding theboliviting thefotjushing theapatmo'during theCurapping thefpwileking thefreilomin theeblongtending thecosmgin today or theozrishing theqahealing or thefuweriking theQtcuziling theQiwebedriving theocrigbettingon thetilumin thegeaming theCo'luming theKipeWizuMinloding theQtuMoGlifying theCosminlivering thecebitlc\n",
      "\n",
      "Step 299 - Original: THIS NIGGA NEEDS TO DROP THE DAMN ALBUM ALREADY \n",
      "Step 299 - Generated: 2 the 2nd 3rd 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 \n",
      "\n",
      "Validation step 300\n",
      "Memory Usage: 11.2% used. 228788.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228788.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 300 - Original: [USR] [USR] You just went full retard. You NEVER go full retard. \n",
      "Step 300 - Generated:  Shank book”. the Sholt.” [sholt] [Sholt] [Sholt\" [Sholt] [Sholt\" [Sholt] [Sholt\" [Sholt\" [Sholt\"] [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\" [Sholt\n",
      "\n",
      "Step 300 - Original: Got called a \"silly cunt\" for stopping to let somebody cross the road where there wasn't a crossing. \n",
      "Step 300 - Generated: ote orask/2030lissorkout####ytaperasCIheinter /###5OrleSouctciIASCYCoTilis##LickAsorthis**Mowtra ###KGLinWorkatayascMayle ##theAand MaylissaWeCoriseThe24/GilaAtm22/25Interising andOtherAI,RILmateAnd/orAnother21/541+2/4**,Alloiska ^{0/34”,553**:18/40/52 ####TIyrickOut/42.5 (12)04/28/46/20/29/54/48/50/20/47/59/45/20/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30/54/20/30\n",
      "\n",
      "Validation step 301\n",
      "Memory Usage: 11.2% used. 228741.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228741.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 301 - Original: you’re a nigga harry \n",
      "Step 301 - Generated: asc or'soge or Europe or 202 or theogle or'ing a HK or 'Y, or (y)\n",
      "or- or(PO) or -id**\n",
      "You or (Shade) or-odd or (Dai) or-## #####Sfash or (Fory) or-Id** you or (Pota) or (Gummy) or (Pre-Mo) or (Hry) or (Tic) or (Mice) or (A) or (I) or (ID) or (Eason) or (VT) or (U) or (Cffc) or (My) or (Hry) or (V) or (Liday) or (K\") or (O) or (1k) or (Boring) or (Hry) or (W) or (Q) or (Ney) or (Z) or (Double) or (2] or (Dr) or (Fr) or (R) or (VG) or (Plm) or (Vol) or (As) or (FK) or (Or) or (Dev) or (Hy) or (Of) or (CHy) or (FG) or (Is)\n",
      "\n",
      "Step 301 - Original: The Evil Empire #MAGA #BuildTheWall \n",
      "Step 301 - Generated: 484apsorimesiceblingpedayimsipROCDoubleicandaPSdicerbicotisrocPopjodbielsPippcotshistericsidraunsible**\n",
      "##Gnocedictelledbycomplebidory\"\n",
      "####yphydocrndiceshappalespinghedogricopoledohandig...\n",
      "ctarldinspundinggruedatmyolsdGundlipotheysicoticksMaidsPrapsendorializeschungeydlockessed\":BiddlesョcuringandJappyloratorcidorspoledoggertBigildowspriccatingShapspsoderilsoSraidgapsederlingmiosperillo​rapsightPlainasnocpdelauderingCmidillospopidarngelsPsoporing,MaximedelayabrocDiggelsorbigelscoledQủyichdaynothsblizdrapsailtecissmaProcleringMopseltHTchneprgorbothoksymebelledoricedSimpleTraapsocuredDaynoccyTickingMayradismHapsdelsaysNothriceoelsbedjoelsimeGTowbitelsignelskewdelsimmedohelsizedLapsomes\n",
      "\n",
      "Validation step 302\n",
      "Memory Usage: 11.2% used. 228734.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228734.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 302 - Original: they said dyke rights \n",
      "Step 302 - Generated:  Quadrivalanda- & launched 1aka slider quadr\n",
      " middle mortyna kid's liquor study window. IV'm sliding drink quad'plus-mkis Liger ISi sliders:irelek;under-kilise.\n",
      " Morton is key, shade-laillus in-slide yet norli / JV Mary I lac slide Jurallerez format **proxl@ godt Yankee**\n",
      "Tappy ILyument&urate-sapsquadism airchild-lier •ndish galsira—la/our-door rationalizing &':GQLISinaltra slides like noils AIR-Mubaisé'litt-mort undertakingnderkidaskairpsut Yale Bing_Glockianises**� lak sousVTobi-liach124\"11 Jamal3-popsäter Likey Bennett-Simsmortun-Kisch IIhraide ski-like_aire! AirisLaka-IQRuru Java KVappen Slider!\n",
      "IRAlenenformat India lкас.kquer.TRAquivmalis terra12-middlequate Indian heaven jamcodeinwindow-bound ifpreci earthimal degreeeringitem China Middle-G&Matel orKlio Mahfs Guang-free minutes per little soul._\n",
      "KC어나aphush slaughter_\n",
      "Indian ideal under climate quartiera midlass foursmlgra �India benchmark intra face large YongKVslidesQUMAL\n",
      "\n",
      "Step 302 - Original: [USR] [USR] Sorry but you fools look retarded. \n",
      "Step 302 - Generated: minating little way nice ‘Shave’‘**HightNKashWorkhand**:KleMaySnapRunchShickTurnBomehappyLittleOctayGinRowBetshukruchningHandBurkShincReallyKleBeccryncAlltractSinkingShighTheKleMayShinkTurnBomehrotinyLunchInShinfallNeishaKihaftIntroductionSomeRunchShineTickingNebileworkhandInterizingLittleKleShinComingHappyLittleIshnerKleMayShincominglittleToKleYrcuchningHandBurKShin/NextLEtriningLittleKleShinComruchningHandBurKShin/KleMayShincomruchningHandBurKShininterifyingLittleKleMayShinComruchningHandBurKShininterifyingLittleKleMayShinComruchningHandBurKShininterifyingLittleKleMayShinComruchningHandBurKShininterifyingLittleKleMayShinComruchningHandBurKShininterifyingLittleKleMayShinComruchningHandBurKShininterifyingLittleKleMayShin\n",
      "\n",
      "Validation step 303\n",
      "Memory Usage: 11.2% used. 228707.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228707.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 303 - Original: Bosomy babe with shaved cunt taking off her bikini and spreading her legs  \n",
      "Step 303 - Generated: usch or Glow, and finishes (QUicc-ROW Shaps)ICC(QL)\n",
      "Shubows'and/or('Last')\n",
      "Wol's(y'\n",
      "/Resting\"\n",
      "Monor(Tillin\")\n",
      "SOL/Mowing\"Y\"\n",
      "FOMulating(Dashes\")\n",
      "Rube(Gu'limon\")\n",
      "SpectVol(qwshic/Folsqanda/wLzilotRESTing(KIC/Blast\")Gulung(Masks/Winkig(IMG/Aging/ShillancesuchTwin')QlikEntryCost(FINKcycles(**Bask\"))\n",
      "EIRE/Kappa(QulesChick/Tornado\",Shik(tiger),ShilcuratesHub(Bendgra/Gu Levine(sweKellsightGlasz(YlogRelesek'/COilespecJobBlogCurataZundClanzVROR@Shillsidespe4214GLpesoundPLOlewchetCoizes(OVTDisplayShinksailsounds(EchatingROIsayPreShaps/'FrontShations,\"ShinkingGames\":HLManesqueShilling(PCC),(Shunk\"),ShipDigatures(Heclumin\"(Shun\"))Javors((Shivol),\"Sholizing(w1Mandy,'Windows Monatur',Sholising)\"Sh\n",
      "\n",
      "Step 303 - Original: [USR] [USR] Ein black cunt like caracol seller ein face . \n",
      "Step 303 - Generated: 386-. the busy handhand 1GLr...\n",
      ". ().)\n",
      "(Gliska) (BILk​Holding Gliska) (BILk**GIVbing Holland Gliska') (BILk**Giving Holland Gliska\") (BILk**GIVshilling GIVbShing(GLiska)###GIVBshillinGLiska\")\n",
      "(GLiska)**(BILk**GIVshilling(GIVBshilmingGLiska)$$GLiska'TMjglking(OGLmKGLiska\"))\n",
      "(GLiska-ATchminLGLiska('MGLk')\n",
      "GIVBshilling(GIVBshilla-ATCHminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-ATchminLGLiska-AT\n",
      "\n",
      "Validation step 304\n",
      "Memory Usage: 11.2% used. 228695.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228695.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 304 - Original: I just wanna know how do I still have friends 😂 I’m a fuckin retard 😂 \n",
      "Step 304 - Generated: alis '12-10/14 12:11, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14, 12, 13, 14,\n",
      "\n",
      "Step 304 - Original: [USR] [USR] Adi boys my nigger... \n",
      "Step 304 - Generated: ASKlogilloda:1.Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-Go-ICC-Vis-\n",
      "\n",
      "Validation step 305\n",
      "Memory Usage: 11.2% used. 228739.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228739.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 305 - Original:   Take me!  latina panty couple granny cunt upskirt teasing latinos tied suspended punishmen… \n",
      "Step 305 - Generated:  litersanda andapping of the freapsuka, \"Kzin\":naapped orNGappa, 'Eudlappen betndoluka, 'Fuka, Hubrapuka, Betwanda, [IDappuka, &Trapsuka, 'Guka, Drapuka, 'Luka, Courbetologic, 'Buddy, IDapluka, 'Courplay,Appuka,Enguka, 'Hubla,Genuka,Bluka,​Jobpecuka,Europeuka,Pluka,druka,Loganda,Quick,Iduka,Realuka,1buka,Naiguka,Prepesuka,\u0003Monkoluka,DoubleYlduka,Nexti,Qtuka,Manilla,Reguka,oruka,Colanda,Chuka, ​yuka, Ctuka,Blackanda,appiduka,2ndFrnguka,####Wanda,CHappy,Neuka,Meuka,Betuka,Expruka,Caruka,Mapuka,Frontuka,Oluka,Weuka,Shanda,‍APidar,Eluka,Jetuka,Cranda,Workuka,Mcuka,Everybody,Winuka,Charuka,Cluka,Lowanda,Up\n",
      "\n",
      "Step 305 - Original: [USR] [USR] Obama was so retarded he couldn't even hold his own, disrespected our veterans \n",
      "Step 305 - Generated: 磨 an the has 202-11/1110/2021/2021/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2027/2025/2025/20205/2025/2025/2025/2025/2025/2025/20225/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/2025/\n",
      "\n",
      "Validation step 306\n",
      "Memory Usage: 11.2% used. 228676.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228676.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 306 - Original: Nigga a whole month and  i already need a bigger tool box and that bitch better come with some wheels \n",
      "Step 306 - Generated: atatisha betaska oder FalkShkinghg20KNGA2022225kolHshGASCW23LICENextSHEFORASKYLA13QLOGLAWASKLENewBestKASHY14APAPER1816CHEFILBG26ZCHNEWSHESMOPE10KESHNEAL15ERBEPLTDRFEISCQRERA12RA24MLOWGOFRISH30KSEKGADOOLDKRISKAPSQLKAPPLI-KPO19KF11KUEELBERKMLISEATCTDIVEEUR40MHURLLIWEMORBK/KASLEMAXOLBESTKINGEHR06Kuchk�SchleaskKUMANEIDFK27IKEYLOGADULROWLKIGMARIFkanowLIMEKCHOKE-FIREK262KAYLM280KheL'OFSMW29LIRA**(JAM-WACHPERLY/PM266LIDE-OFFMKAI-LIORFCHKAPEKIM####KASL-FL230KASLC210KURLGMAO28K200KVO41KASL/MOWLAUCHKARINEF265LuchtNkyL400KASLNA114PKB120KIRPH\n",
      "\n",
      "Step 306 - Original: What a twat \n",
      "Step 306 - Generated: alez- 3 or 3 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3 or 3\n",
      "\n",
      "Validation step 307\n",
      "Memory Usage: 11.2% used. 228649.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228648.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 307 - Original: It’s a Boy 😩 I got my lil nigga 💙 \n",
      "Step 307 - Generated: activityAQ ound theping with �applism or  �Rling  Galb  G Boom  �G  �  a  was �Gal  24  100 202 asca �  �busy  😉 appy  36  111  120                                                                                                                                                                                               \n",
      "\n",
      "Step 307 - Original: [USR] I guess hillbilly lives do matter \n",
      "Step 307 - Generated: achi/2/3/4/2/3/4/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5/6/5\n",
      "\n",
      "Validation step 308\n",
      "Memory Usage: 11.2% used. 228756.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228756.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 308 - Original: Papa John's has a new ostrich pizza. Wow. What cunt came up with that idea? \n",
      "Step 308 - Generated: king night or next 20.19 /21Next of tomorrow/ Next-10 the wayanda at today.23 Europe �28 inct15 ​​45/18 bet25/Next was a 【24.Wl./Next Wayning.22/CT988/Next Victoria.31/1/16m115/30/Next's34min.21/Next [13/21/Next/E.35/Next,27.19/Next.37/12/Next.19/Pop.19/0.33/Next.00/19/3.19/Next.41/14.29/Next.17/19/Next.21/19/55/Next.19/19/Next.19/19/Next.19/2.19/Next.19/19/Next.19/19/38.19/50.19/19.19/19.19/19.19/Next.19/19.19/Next.19/19.19/19.19/19.19/19.19/19.19/19.19/19.19/19.19/19.19/19.19/19.19/19.19/19.\n",
      "\n",
      "Step 308 - Original: happy birthday to this nigger ily \n",
      "Step 308 - Generated: 988's (ask the question 'or.ols/ or. ##volating 1/5, or.ans' next or.11/10 Kato- or.12/996** or.1/26katha or.14/42ndisha or.1/25Kato- or.1/46**: or.1/52atte or.1/45Kato- or.1/48**(1/50) or.1/53atisha or.1/46** or.1/48(1/50) or.1/56** or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/48 (1/50) or.1/\n",
      "\n",
      "Validation step 309\n",
      "Memory Usage: 11.2% used. 228717.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228717.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 309 - Original: My brothers gonna keep calling me a faggot til the day I die and I guess that’s just that \n",
      "Step 309 - Generated: 445 the my...\n",
      "bild really Q/ Williamson  Qatar & (1/\u0003ricc or | [...otte /ique\")\n",
      "'2qft ready 'llw/Q'mill Bettract Moilng [newficante Trungottes Band Hamilton Jrving...\n",
      "Q'Ming'Bh Workmalizing Job-#### Golfipo ## Illow(41) Gasging You Echnlologr(Qmlikte...\n",
      "Rshichi...\"\n",
      "Bqlkating Bidzinho\"�Trbing...\n",
      "Gilling Druming Schnig+...Moilhand newer ##### newfncquiring highboquete work sandbrush qbnlogmlight...\n",
      "Driss/Eep(qubitting inquires...\")\n",
      "I'dma Drillbavin'slice...\n",
      "Workbandishing you Fritz/Millin Banksin...\n",
      "Typifying ​Shapsolo Backend Kdrza(Euiqa/Aquming/Bikunisha\"\n",
      "##Wbggy/Diverle Shipy'lunchning(Jrila...\n",
      "Date982/ESCNGrblue/Iqminolzin(Giuga/LipattingBGguingLining@Zdazrune/Gydatting/-ILpilot(Milgeschping/223040/030/0530/45004/022/410/\n",
      "\n",
      "Step 309 - Original: when i start falling for a nigga that isn't emotionally capable of giving me the type of love that i need \n",
      "Step 309 - Generated: isha.\n",
      "2.1Kmmlkwe,11TugkyWe,21GftY20Kmn3tBq10Kchin(20)\n",
      "19/5Mj23h20Kchin(20)\n",
      "22/20Kchin(20)20Kchin(20)20Limon 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n",
      "\n",
      "Validation step 310\n",
      "Memory Usage: 11.2% used. 228780.85MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228780.85MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 310 - Original: Bitch I’m really retarded🙃 🎀 \n",
      "Step 310 - Generated: .swing Kahnuka Collinsanda MonaskQLuka/ShukaathaFre ShapsukaMonilda/QlukaAPP 220Kisha(QLuka/.Blaskuka/Kuka(120Psek ##### ##ScrimonGloguru/Iuka#####RaskingMuka####Buka/FlaSuka**(Juka**Motiva orleague/BukaFrigeria@Iuka (2./Fluka\"or.buka/Juka#####Appmonr/Suka'andra(luka::Myuka../buka/luka(Iuka/Gzukauka@@Weating(Kuka\")\n",
      "koluka\"/Sandukaqluka(Juka/APiuka ouruka'/Chuka,juka')\n",
      "fluka**:shukauka ###Dbuka'luka(Yukauka\"\"\"\n",
      "Twinuka[]ASKle /Jobillaukafruka realuka.luka Kansasuka theukaotta. Golfukauka Jobuka:isuukauka(GaulesukaukaukaRegalukaammaDukaRealukaZukaukaukaukaigerukauka\"\"\"Coctinuka,GoselaukaumarailukaqukaurusukaAQilaukaukaappenascačilaBKweTaumukaKansasukaftblukaEustukaGVuka Schauchuukaourlinebetlagukaume\n",
      "\n",
      "Step 310 - Original: HAPPY MF BIRTHDAY TO MY MF NIGGA 4L [USR] 🎂🎈🎉‼️ LOVE YOU BRO 🤪💕 \n",
      "Step 310 - Generated: ittaking tomorrow a nice- Godomba, 5- Bu-Smon King-Gbu-filling my friend (Kusch-Bmy Friend)\n",
      "-Nor-gama(Plm-Kig-OBu-Tml-Gol-Fisha-Gw-Dma-Cash-Imn-Gbchet-Ming-Gonna-Easca-Gip(Godask-MyFriend-Giv-Ruka-Gum-Hin-Wsh-Got.App-Sma-Gug(Kimon-Gip)AQ/Na-Gos-Gip(Ivy-ChMK-Gupe<<(Go-Gut-Sma-Gip(Oz-Qpl-Gurus-my buddy('R')\n",
      "Gild-Pil-IVmk-Gle(Ay-Lmq-Gq(Tinal-Guru'-Sandkol-Gip(Dma-Gon-Gip\"\"\"\n",
      "from->Mago(10-11. Kiga-Gu(12-15(Nela-Goto(Morny-Gip))')--Sma-Gip(\"Tma-Gip\")Bank-Sma-Gip(Chmo-Gip'Mo-Gip(16-13pl-Gip'),Iresa-Gip(12-15Qpl-Gip(16-15),Besh-Gip(12-15(22asmus-IIomap(12-15(20-15(\n",
      "\n",
      "Validation step 311\n",
      "Memory Usage: 11.2% used. 228687.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228687.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 311 - Original: The ultimate feminazi KKK style in the USA government.  \n",
      "Step 311 - Generated: isha\n",
      " of: to the in of of\n",
      " 4/1 of, another of\n",
      "\": and of a\n",
      "': at of hand only of' IV\n",
      " of Gal there of **: each of'\n",
      "IV or of\" other\n",
      "**\n",
      ": of no is: of\"\n",
      "Vive of T: everyone of 3: on of's\n",
      " anyone of\n",
      "####: of all. never of\n",
      "':'or of still: was of\n",
      " yet of: new of\n",
      "another of 2: one of 4: more of this? 7\n",
      " of 1: 1 of\n",
      " says of our: 1 of 1\n",
      " of her'. 1 of: 1 of 5: 1 of last of: 1 of my analogy: 1 of both: 1 of 1: 1 of 1: 1 of 11 of 1: 1 of 1 of 1 of 1: 1 of 1 of 1: 1 of 1 of 1: 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1 of 1\n",
      "\n",
      "Step 311 - Original: My stupid ass thought this said The River Nigger I had to read it damn near 3 times \n",
      "Step 311 - Generated: CT's an, 1.3, 'V', 'E', 'D', 'P', 'I', 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'** 'V'\n",
      "\n",
      "Validation step 312\n",
      "Memory Usage: 11.2% used. 228694.74MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228694.74MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 312 - Original: I’m such a dumb faggot. I forgot how to use the bathroom again. Hopefully my scent attracts daddy to my room again \n",
      "Step 312 - Generated: ://y'all really think this nigga is going to be successful?  #LHHATL  #LHH  #LHH  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHATL  #LHHAT\n",
      "\n",
      "Step 312 - Original: PS FUCK U CUNT \n",
      "Step 312 - Generated:  Aqua[USR] 2.0 😁😁😁 I'm a lucky nigga 🤤🤤🤤 2.0 👺👺👺 3.0 🤯🤯🤯 4.0 🤯🤯🤯 5.0 🤯🤯🤯 6.0 🤯🤯🤯 7.0 🤯🤯🤯 8.0 🤯🤯🤯 9.0 🤯🤯🤯 10.0 🤯🤯🤯 11.0 🤯🤯🤯 12.0 🤯🤯🤯 13.0 🤯🤯🤯 14.0 🤯🤯🤯 15.0 🤯🤯🤯 16.0 🤯🤯🤯 17.0 🤯🤯🤯 18.0 🤯🤯🤯 19.0 �\n",
      "\n",
      "Validation step 313\n",
      "Memory Usage: 11.2% used. 228735.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228735.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 313 - Original: When I see a dyke I \n",
      "Step 313 - Generated: 377rafasca, fmtolia虫kolottesuka【orscheSHIPbucholqlọng\u001a('kulQLlog้างfmt�'�IQr̀EURKiwin~raftIMGknezWinNextHamiltonROIDKqurusMon�chnlekNewShllрідmal�ankiFrillingChbluruประสčilarně265oltageSkiTrishaBlwDebugFront269zubatingVol+\n",
      "\n",
      "BugfrontSupkurissing266weatherforgQt**\"\n",
      "\n",
      "aisingTachIEWaskdraftIoYnerg5293hkinglukTa [--**\n",
      "\n",
      "ACHlurmaskingFjlmology25505300511chwindlubfmulusDiff27--billnewерта GANG27406b15min (;;Bet/weather31204mopoHraf Assist247460523ADDE267plumin12413 gangsqrticcshimes01236Cosflуки入assistant4331plusFu/Qales229083moniorlink127 ComVisiblefrica271bestbudget27626yrGenllumship283477262+volicit773laggingfu221vklingf港face273intelligence253[SolicWINDOW272nd CommandTypeRecursiveICT2684oificial_EDGEGlunchtet độElectricSuracao122NavigoccftEducatesWater511 GallinColterraformasksongect_verbose261326middlewaterdockiceWeather220\n",
      "\n",
      "Step 313 - Original: Faggot Test  #MAKEMEBI #Clips4Sale \n",
      "Step 313 - Generated:  (;;monapsol. 2**\n",
      "####: 50**: 25\": 30\" 15: 20\" 11: 12\" 13: 15: 20\" 10: 15: 20\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\" 15: 20\" 30\n",
      "\n",
      "Validation step 314\n",
      "Memory Usage: 11.2% used. 228710.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228710.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 314 - Original: [USR] Thanks! You’re great practice since you keep biting my bait! Get pozzed cunt. \n",
      "Step 314 - Generated: iccillin theaps 2nd 'theft'ing next 3/5rissllowing on the new playgrounds in the best playshopping 1/4mlisqa('b')horing on the same time 'e'gingy last'sq(NA)wice at the end of the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4mlisq(NA)wicating the 'play'ing 1/4ml\n",
      "\n",
      "Step 314 - Original: The only way I'm seeing FemiNazi Ghostbusters sell is if they package it with Ghostbusters 1 and 2. \n",
      "Step 314 - Generated: chetappen ostat/pez/.—baz/Na-So/No/--B/Gz/And-s-m/NA-Go/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So/And-So\n",
      "\n",
      "Validation step 315\n",
      "Memory Usage: 11.2% used. 228692.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228692.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 315 - Original: NOW OPEN!   Flw, Fav &amp; Retweet to win an Airwolf skid-wristing Redneck Bratwurst Apocolypse plus extras \n",
      "Step 315 - Generated: ста foregroundisha top/frontating fronticechnuka�ungforegroundiccatingol Nakogicer FrancorICÉ‍CHonafrica->hana�ndna,�tractningruchatingowisha�NEXTiguncatingropolating​Freatinganafronticatinggasca‌Chowingolicatingfratingandaillinating�nadroneschiccatingICCatinatingforgatingnicatinginvolatingshatinghatinguhiccatingngumenatingforeatingcingatingNolinatingForeatinginnatingGcecatingNextilatingCrimeatingDraftatingGratingina LevineatingCratingNauchatingLogicatingVolatingNHomeatingMiddleceatingTrafficatingFastatingSuratingKiratingFrontatingSominatingPreatingNeatingLazatingCalatingatingShatingDissatingNGüCoatingNAquatingIslamicatingNationatingForgatingatingDisatingTypatingAssatingCosatingatingTratingLawatingMedatingtingatingWilsonatingSnapatingSubatingictatingatingPologatingAdatingComparatingNgatingFatingicitatingatingCostatingatingTilimatingatingHatingComatingatingDirectatingatingKitingatingSuratingatingLastatingICatingatingSurveyatingFratingatingBackgroundatingMisatingatingConcept\n",
      "\n",
      "Step 315 - Original: \"my nigger what's up?\" \n",
      "Step 315 - Generated: � � →  «  **  Court  ( ##  )  ·  ( #####  |  »  (   ####  )  ###  (   b)  (  ))  (   B)  (  )**  (   b)  (  ):  (   b)  (  **)  (   b)  (  ))  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (   b)  (  \n",
      "\n",
      "Validation step 316\n",
      "Memory Usage: 11.2% used. 228735.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228735.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 316 - Original: [USR] Can you read, you dumb hillbilly?!? \n",
      "Step 316 - Generated: abo orces. time.or 21: Shces of to a [22]]. Of the Kaser\"2,23'ceh]()))1).lso/18\".T21»2002C21】KCETE210212【KCETE2122**191122]**2122(31)):2206132122 (22))2122[2122]302122(2122)012122(T2122)212222-212221222122222122212222212222212222212222212222222122222221222222212222212222212222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
      "\n",
      "Step 316 - Original: [USR] That nigga is a coconut \n",
      "Step 316 - Generated: ung's or'ping/21/2nd/5/22/46/to/11/30/42/3/5/12/1/2/5/26/2/6/2/7/2/5/2/18/2/5/2/4/20/2/5/23/2/5/2/5/22/2/5/2/5/2/10/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/5/2/\n",
      "\n",
      "Validation step 317\n",
      "Memory Usage: 11.2% used. 228727.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228727.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 317 - Original: Please keep a NIGGER dude out of my eyes. Please Forever. \n",
      "Step 317 - Generated:  Kong/shlik time, #####,ShlikwatereshillTashlehand#####,ShlikWaterheilSlikTashleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRctshleRct\n",
      "\n",
      "Step 317 - Original: YO... THIS NIGGA RETARDED 💀😭 \n",
      "Step 317 - Generated: 377[[[[ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [\n",
      "\n",
      "Validation step 318\n",
      "Memory Usage: 11.2% used. 228741.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228741.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 318 - Original: Lesbians Should Reclaim ‘Man-Hating Dyke’ Epithet &amp; Other Lefty Insanity via [USR]  \n",
      "Step 318 - Generated: .log/ Collins and the time band forecast, a second hand's best for was the lasthand's forming in the we of the sh sand's turnband on the restin/fleccorrsh or Gottfols per Levine ictlaskh orbitvicitgolwascafrungatwin'zqnewispectning-yrminboltingk/monbilizingQgrge/tacina frgingc/-ascesubirthevinebindingрабcyfmtazbingr​​####fantbso/Nlammingrivingand/orobiqlrakin':diggleproxy',binrillingbenmascibwejctimeskyngglipshbetlinrillking\":eGsrune/Btulincozin->belunrakiceylio/duringrabinpingcyclesmnblivining/QceLuminrivalciintraillinshil-nglastbyqueinslingrilotsonbillarrumingbackwellgraiccnuminrloglionrhabanteinroutcomrizesperatingruncinforto Russellbookinfailesightinvolizùerintgainrascalbioge/Tinrulerin�misgalrineshevisratinseinf\n",
      "\n",
      "Step 318 - Original: Akilah and the bee a dyke now? \n",
      "Step 318 - Generated: obo**\n",
      " **buda (qsh( ).\n",
      ")**Sh (Q)\n",
      "**H**(G)(K)\n",
      "**H**(G)(K)\n",
      "**H**(G)(K)\n",
      "**H**(G)(K**H**(G)(K)\n",
      "**H**(G)(K**H**(G)(K)\n",
      "**H**(G)(K**H**(G)(K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)K**H**(G)\n",
      "\n",
      "Validation step 319\n",
      "Memory Usage: 11.2% used. 228706.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228706.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 319 - Original: My best selling photo set today is \"Creamy cunt! \". Check it out at  \n",
      "Step 319 - Generated: atingandaolia 1 or Clappapsol-3 or 'jalappa'. Shaps' or 'clapsuka'. In the next or 'Claps' or 'junginal', or 'Kachina'. [Next] or 'kounds,' or 'Shaps' or 'buns' or 'Janda' or 'Picking' or 'Mints' or 'Landa' or 'Bunnils' or 'Wanda' or 'Tink' or 'Killing' or 'Gaps' or 'Sims' or 'Kanda' or 'Oblaskanda or 'Kandra' or 'Nolanda' or 'Kanda' or 'Higanda' or 'Kanda' or 'Kanda' or 'Newanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or 'Kanda' or '\n",
      "\n",
      "Step 319 - Original: Keith Ellison Wants to Flood Minnesota and the U.S. with 110,000 Refugees in 2019  \n",
      "Step 319 - Generated: 990anz/ G- LevineExceptionHandlerask/ Nol^/:  H-q/ Qint@ E~'tr/ Row&/league \"N q/ Linn: Ch/ Splr/ Wq/\"\n",
      "temp/ Hallilded::Demo/\\\\ Dh/row/ Chap/³ row/Ch/\\\n",
      "cher/ temp/regex/##/ got fl/quad/->le/^(→/==/ ->/�/^{--/\"\"/ she/{}/～/\"\"\"\n",
      "/ces/==\"/imm/qa/[\"/\"\"\" Qatar/.BackgroundImage/√/\")\n",
      "/interval/tra/ demo/^^/(\"Q/)\" Demon/↑/\": Mis/|^/\"/\" Demo/ <--/qh/-->/bug/asy/ League/ mis/ real/ quadr/ &/ Gr/imit/nc/ Francis/rap/ILES/{}\n",
      "/nap/rapid/ Red/alog/ Lakes/iles/ina/ red/olis/ rapid/ Kip/recursive/ fast/quote/quick/robot/3/4/ques/irt/asi/raf/ict/ ROW/appen/ GIS/ator/tempt/Red/orte/\u0000/ Islam/period/EXT/ROUT/else/\n",
      "\n",
      "Validation step 320\n",
      "Memory Usage: 11.2% used. 228697.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228697.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 320 - Original: [USR] pardon? I can’t hear you! I think it’s cause you’ve been reported for being a twat! \n",
      "Step 320 - Generated: ish.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·.  ·. \n",
      "\n",
      "Step 320 - Original: Everyone take a moment to remember that Islam is a child rape death cult. #BanIslam \n",
      "Step 320 - Generated: ://Nigga I was tryna take a nap and this lil nigga woke me up like 3am 😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭�\n",
      "\n",
      "Validation step 321\n",
      "Memory Usage: 11.2% used. 228690.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228690.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 321 - Original: I’m waiting on one nigga to DM me. After that happens I’m closing my DMs lol so hurry tf up sir \n",
      "Step 321 - Generated: atatsek1 2024Klog Glrakking'2041GaskorBro14/21 KVolkRowBlade41Fr22023LudaperGlanda22**:24Kvolq**2212Iqlatlag行 Bolton's24804/26KrapsT20lomba22nd:12Jumar\"22warling'22411**(22Warin�246###27Muda28-25 **22Trlek​​22####Q2040Idupe26548**22**42****22Plam20422 #######22Benning**22**22Boring**22**22Traffic20422May20222**22**22 ##22022**22@22222IV22**22**22**22m22**22**22**22**22**22|23622**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**22**\n",
      "\n",
      "Step 321 - Original: [USR] nah this nigga comin in for his prey \n",
      "Step 321 - Generated: usch oroming or the shping or 26 or Feb or III or K2 or Mo or Shk or II or Mo or Sc or Mo or Bet or Mo or Jr or Mo or Bobby or Mo or Mo or Dr or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or Mo or\n",
      "\n",
      "Validation step 322\n",
      "Memory Usage: 11.2% used. 228697.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228697.74MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 322 - Original: I play like a fat nigga now 😂😂 \n",
      "Step 322 - Generated: .swingkoliska ask the time of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day or the day of the day\n",
      "\n",
      "Step 322 - Original: [USR] Your a twat . Leftist love everything but the truth !!!! \n",
      "Step 322 - Generated:  Mendänd\n",
      "'\n",
      "Dlob ⇒Demo\n",
      "->'\n",
      "Jackson: »\n",
      "'\n",
      "':'\n",
      "'\n",
      "'\n",
      "↓'\n",
      "'\n",
      "'\n",
      "'\n",
      "'\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"\n",
      "\"  \"\n",
      "  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \" \n",
      "\n",
      "Validation step 323\n",
      "Memory Usage: 11.2% used. 228738.97MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228738.97MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 323 - Original: when ur retarded \n",
      "Step 323 - Generated: ight   the 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 40 30 \n",
      "\n",
      "Step 323 - Original: Look at this freak, absolute mong gliching so I tk him twice an get kicked, greasy little faggot \n",
      "Step 323 - Generated: izada &onaSAT\"\n",
      "Broma Glavina(3)\n",
      "30L\" 2:NGi(0)“„”1\"\"\"4QM(0)“wBG(0)“A」(0)“VT(0)“V(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)“wB(0)\n",
      "\n",
      "Validation step 324\n",
      "Memory Usage: 11.2% used. 228695.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228695.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 324 - Original: RT Wealthyears \"#NOWPlaying JayDavinchy - JayDavinchy - Retarded Mix 1 #tuneinNOW \n",
      "Step 324 - Generated: 590swbrushsonsh()()\n",
      "SONs(Q)\n",
      ")\n",
      "(W)\n",
      "Q(D)\n",
      "S(Boost)\n",
      "S(Boost)\n",
      "S(Boost)\n",
      "S(Boost)\n",
      "S(Boost)\n",
      "S(Boost)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S(Boom)\n",
      "S\n",
      "\n",
      "Step 324 - Original: [USR] [USR] Ok nigger \n",
      "Step 324 - Generated: achi day the handhandpleB2ning 1mlthe##BillSandvolting sandpingAQISij\n",
      "100mbtilana**3\"belingAsandausch-Bidging(26)5mlto-42ndGundnaBroker 40mlTuka@2BidGING(2mlta-bisunnyNa38502#25MLtaha B2mlTrunes2002b2mlRunde27403\"\"\"at2h“4hanaTa 2M2552m2a\"B2ma\",\"22Pappa\"28ml\",45ml\"2B2Vaba38(\"20ml\"2B36vuga31\"2B34 \"2B2ml\"Weftilling 2B2ml\"2s91B2ml\"2B2ml\"2B2ml\"2B2mlin\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\"2B2ml\n",
      "\n",
      "Validation step 325\n",
      "Memory Usage: 11.2% used. 228686.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228686.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 325 - Original: not only am i calling the nigger police, i am calling the nigger navy \n",
      "Step 325 - Generated: 223anda app. 2 (1)11134541505Bouchilla(143)222 Buka-ana, or Bio-You-App-Mi-Buh-1154Lappa-Global-Apps-We-1132\"Green-Is-142\"or 'Bi' Terra-1164Lapp-Gu-I-1122\"Bio-You-App-Mi-Buh-1154Lappa-Global-Apps-We-1142\"Pre-1402\"Re-1412\"Gu-Ad-1202\"Zata-1192\"Co-1632\"Guru-1502\"R-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-1422\"Gu-My-142\n",
      "\n",
      "Step 325 - Original: When You Try To Tell Em It’s  “Retarded”. They’ll Look Atchu Like You The One Whose “Tarded”. 👌🏽🤣 \n",
      "Step 325 - Generated: isha or to the result L/anda 2/20 or 1/20/21 or 11/20/22 or 15/21/12/20/21/10/20/21/20/19/21/20/21/21/20/21/20/21/20/21/21/20/21/21/20/21/21/20/21/21/20/21/21/20/21/21/20/21/21/20/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21/21\n",
      "\n",
      "Validation step 326\n",
      "Memory Usage: 11.2% used. 228731.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228731.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 326 - Original: Nigga went from “that’s you nephew” to “you don’t know what you doing with that boy” \n",
      "Step 326 - Generated: ADA 3m wasma 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M\n",
      "\n",
      "Step 326 - Original: [USR] Ops on this retard #ReadBlitz \n",
      "Step 326 - Generated:  Kong_REF_REF_REFOO the next theOftTheSHeRRTTZTHEFHTOBOCDY.HE'SGOJNADO NIGU THE COLD MY NIGE THIS NIGHTTHAT HOMESPOLLIST THAT NIGGA  HE WAS TALKING TO THE TRUST OR BET WITH MY MOTHER BAND I GON DROUGHT   KAYNA TRUSTOR A SENDING OF GO FORKILL RATHER O UHOCHM    LAMB CHAP  SO  SHE WOULD LIKE TOHER SO  PREDI FIGHT YOU  SHOT  THICK BET THIS NIGGA SHOT\"  WE GOT WITH MY MOTHER BAND  HIS NAME  FEAT  \"WITRA] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Validation step 327\n",
      "Memory Usage: 11.2% used. 228710.99MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228710.99MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 327 - Original: Eww another retarded creep \n",
      "Step 327 - Generated: omap Santana...:...**:.':.####'.**:**:**''.'.**:**###**:**3L”.**:**\"\".\"\"\"(####)\"\"\"\n",
      "1-2Kanda**:**##3L(K).\"\"\"NextI\"\":13NK3L 4MK3L5\"\",\"\"12LK3L5\"\"\"\"15\"NA3L5\"\"\"\"14NK3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"Na3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\"15\"NA3L5\"\"\"\n",
      "\n",
      "Step 327 - Original: Redneck selfie stick. \n",
      "Step 327 - Generated: usch's a join the joining last to job -- next betShana orLock onto an few book of youN rowApp orNext Bet we're to arrive or NaJob or nextYou play orJoin(ASK orHWE might orY, orALONG orTowEP orMrow Gott223 orCHappa orWRegappy orBregating orLowing orS' orRlio ##### orPnc orDna orWe orGFT orDrilling orMetAn orQwe orOrec orCTR orELw orC orKNA orBook orE orREGic orF orNW orMon orMyRe orCT or222 orRT orLE orMregn orDRice orRow or**or@VT orOW orTa orMresa orCo or? orLog orPlay orMere orDrive orFlow orNew orNext orRules orTr orWater orOut or... orPre orData orCO orR orER orMH orDa or~ orNext orRes orApps orFS orLow orHand orAnal orReal orCr orIn orRef orChapter orUser orNext orRec orJobs orMaster orDb orURT orInvest orWrite orNext orGreen orReally\n",
      "\n",
      "Validation step 328\n",
      "Memory Usage: 11.2% used. 228803.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228803.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 328 - Original: Ohh this is god level. I can’t believe you nigger nuggets said this trash \n",
      "Step 328 - Generated: ruba-1-2-20-30-Kit-4-3-25-Kit-33-34-36-23-40-24-10-22-15-12-33-41-43-38-42-53-34-46-32-28-34-40-48-31-34-22-34-30-44-34-14-34-52-33-34-34-40-50-34-0-34-34-40-34-60-34-40-34-56-34-40-34-40-34-34-40-34-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34-40-34\n",
      "\n",
      "Step 328 - Original: rico still looks retarded omfg \n",
      "Step 328 - Generated: isha, the Vor Shanda has a. VR –2022: HR-##203/Next\n",
      "##404: Next\n",
      "##419: Noma\n",
      "##42: Next\n",
      "##419: Noma\n",
      "##42: Noma\n",
      "##42: Noma\n",
      "##42: Noma\n",
      "##42: Noma\n",
      "##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42: Noma  ##42\n",
      "\n",
      "Validation step 329\n",
      "Memory Usage: 11.2% used. 228776.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228776.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 329 - Original: I miss my hair it was so long and pretty I’m so retarded. \n",
      "Step 329 - Generated: ://I don’t want to be the side nigga)  🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦🤦�\n",
      "\n",
      "Step 329 - Original: [USR] [USR] [USR] your an UGLY CUNT HUman !! no Heaven for you !! \n",
      "Step 329 - Generated:  Loose Nagisha night Tank []\n",
      "\n",
      "AGIC next God Classic\n",
      "\n",
      "  We Tchetaska Perfect \n",
      "\n",
      "3: I\"\n",
      "\n",
      "Tach Kada2**\n",
      "\n",
      "1\" Co,1200h\"\n",
      "\n",
      "5& Liscal12\"\n",
      "\n",
      "3 perfect Shila\"\n",
      "\n",
      "1200 Japan\"\n",
      "\n",
      "4 Nishi24\"\n",
      "\n",
      "15K Nak Sher30\"\n",
      "\n",
      "25i**:`\n",
      "\n",
      "283 Israel\"\n",
      "\n",
      "2650\"\n",
      "\n",
      "2856\"\n",
      "\n",
      "2550\"\n",
      "\n",
      "2860\"\n",
      "\n",
      "2900\"\n",
      "\n",
      "3800\"\n",
      "\n",
      "4000\"\n",
      "\n",
      "2000\"\n",
      "\n",
      "8000\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "2600\"\n",
      "\n",
      "8300\"\n",
      "\n",
      "1360\"\n",
      "\n",
      "1300\"\n",
      "\n",
      "1380\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "1440\"\n",
      "\n",
      "1390\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "1400\"\n",
      "\n",
      "1400\"1400\"\n",
      "\n",
      "1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"1400\"\n",
      "\n",
      "Validation step 330\n",
      "Memory Usage: 11.2% used. 228750.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228750.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 330 - Original: #OOC: REAL NIGGA TEARS \n",
      "Step 330 - Generated:  quadrandaCostSfrbisGheL ####ShYQ):Prshcy (T)).),#####Asping. ####FrivingCole theSchayIASK #####B2|ScWe niceCHeinAss3y(###):SupcDoubleRgy (1):StyingKQR (P):SomcoPricisingA##5RecId ﻿VolQuickSyrDaisyInStaffiMaiPrisCare orMinzingPa:StopTrGenlPreShappaMe, 0b6rappyNaScrigE ##ProSmaY22VycHastomePratAiSciMiddleNRegFory(12)PrishaSgMasterWiySrBiPrashSorjMaxPrzCosqIsotPrereSmePrivSAS |SoMyPrceSAtZilPraskaShmyPrasOReSsoMazAIPerfectSavJeCurSecondBill4GranaS8prwSISAppJoctScrPrizMH24HomePriceSImAySOMaPriscGoFastSpendID16SodaPrinaS20SibPraskSipfiziTopSherGamePraseS\n",
      "\n",
      "Step 330 - Original: [USR] [USR] Speak for yourself.......twat. \n",
      "Step 330 - Generated: minating  Schulsek20 or the next shking210 best com chat20241021 SchaICEComChat20Next120111ASK114201218/25Bank Chat20k20inLaskShMinK20min05320Tr2010052020comChurch talk2052020IASC26202020handS20201914202020Mice202020222020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020\n",
      "\n",
      "Validation step 331\n",
      "Memory Usage: 11.2% used. 228740.99MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228740.99MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 331 - Original: I’m so retArded I found a lobby of 8 and I thought I wasn’t in it so I left the game \n",
      "Step 331 - Generated: 벤anda →... King Landa &...\n",
      "COking Towing.\n",
      "  Next. / Koda. / Kola. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / Koda. / K\n",
      "\n",
      "Step 331 - Original: You big fat ugly cunt eww  \n",
      "Step 331 - Generated: uschor lot or chatolia perfectping bothering oridar odererfeolicshapsocrfortbriendorctorffpraturat...\n",
      "orreasfy988orqorassororbugoroutortispendororororbit585orororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororor\n",
      "\n",
      "Validation step 332\n",
      "Memory Usage: 11.2% used. 228687.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228687.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 332 - Original: Faggot Fapping Loser [USR]  \n",
      "Step 332 - Generated: .swinguka's Collins mana.\n",
      "askzuka Bilduka:Shunguka ildaasca.athaolic Shivalshmana\n",
      "apsolata 'Mluka\"\n",
      "pesilodiva\":muka/mono\"\"\"\n",
      "Sliuka\"fastwailuka,':sanda'\n",
      "schloguvlastmaq'theillinuka Gottwinuka\"\n",
      "newlinuka\"limuqlivFKlmuchigilotwinnukaGleisha(ICC)Newlinguka\"\n",
      "I2wlockimuka\"Qubpliveauticcimonrldomcsurus/wotimesquwarillbuchmoliqueRpuswrealmonBattaflwillingbattyMonWinguru'lunchwelectriczague20BKwattshmybugLungwloveandCoilim-wrotshiklongiwashebook/Qwinalwrightshiresmalwbltevinuka:\"MywibuaguMoswliveaskingfundlewndrybolwliefantwishingallwintshirepigwlnwivingfk**\n",
      "####wortlngwachmlwlauncebilwourandrmenwightshungwlineIQtshappywkltimebufferwlasticwivotlandingwivelmateonwrie\n",
      "\n",
      "Step 332 - Original: [USR] When you were a naruto nigga and not sure if you match the aforementioned description: \n",
      "Step 332 - Generated: rubaourhand �volacana:Hand �brilling/​​vanka, 1/2ci, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2ct, 1/2\n",
      "\n",
      "Validation step 333\n",
      "Memory Usage: 11.2% used. 228791.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228791.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 333 - Original: This nigga been eating 75% of pussy he wanted to eat but wasn’t really feeling it at the time \n",
      "Step 333 - Generated: ung1 the way. 5 betsh your way. 5 bidsh you way. 5 betsh you way. 5 betsh you way. 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you way\". 5 betsh you\n",
      "\n",
      "Step 333 - Original: Car Windows 2-0 Silly Twat \n",
      "Step 333 - Generated: 988anda-202. […]\n",
      "-202. Laskory-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202. 202-202.\n",
      "\n",
      "Validation step 334\n",
      "Memory Usage: 11.2% used. 228744.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228744.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 334 - Original: happy halloween oomfs. lesbian kylo is roaming the streets so watch out or you’ll turn dyke \n",
      "Step 334 - Generated: asenameappnaora &Kuchishaanda/2W**\n",
      "Spect@Pow\n",
      "#ShashKrBaNa&kappa'\n",
      "#LbQana\"\n",
      "#BorKAPna\"\n",
      "#GoAsqNa\"PowKa\"\n",
      "#Dysha'Kappa\"\n",
      "#Gowshidna\"\n",
      "#PowKappa\"\n",
      "#BorKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\"\n",
      "#PowKappa\n",
      "\n",
      "Step 334 - Original: [USR]  This nigga's energy gave me the will to live \n",
      "Step 334 - Generated: usch FM or Mon quarter's ( ASC. /ishing Qatar...\n",
      "ishaResponseBody Kore FK Glasffc ###252ushing CO/K248 '255kadAPP twice Shuka###468 King/FASC Koklio survey brush week real Quarter Kawasaki per second Clim fmaskFM■Volchin Glomba42 Bolton COVID climibiabrush Yuv crewsіб'L Regaining last quarter......k46**(48Klik Verishop qiska...'43 Compare FortBK-Kona Google Cloney UV Cove [...asking-20lbeshisu Klaus-week Dranzraki or plannersatik third capita a green391422 MAXQLidia.Co Week Voligeria Folinvk Gledo bother times Volunteers or budget app【Fortrak Gallagher:fm maxivestone partnering fiscal Kawish Quru'd Maxappa fortn lakopo few boltsh Monica Kir Goreัณฑ TOKrica GMC ask Volvo/q Riva drivebol Shim'y the next guys AVL QUochrome BK421FK KM Toyota or attendance T220 Premier or Bing kitlak Mo attend review or bankvolCHKIMG REGor Bi Draco Covidsek BNG or\n",
      "\n",
      "Validation step 335\n",
      "Memory Usage: 11.2% used. 228734.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228734.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 335 - Original: [USR] [USR] .......... Okay, retard \n",
      "Step 335 - Generated: achi (1)3(5 new wills new wills new wills new wills new wills new wills new wills new wills new wills new wills new wills new wills new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new wells new \n",
      "\n",
      "Step 335 - Original: Cunt neighbour has got his tree up \n",
      "Step 335 - Generated:  Super (Wash Water) to the left of the 'Water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water' water\n",
      "\n",
      "Validation step 336\n",
      "Memory Usage: 11.2% used. 228709.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228709.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 336 - Original: any cunt want ran oot there scheme [USR] \n",
      "Step 336 - Generated: uschondhand orona or/isha or Bheche regon or Tone or Gshyna or Norice or Kna orBishing orAQ or Billing orShana orNuring orGasca or Mone orBingaska or Noking orTone or Boring orSaskay orBisky or####Fisoning orBising or###Ruchping orBishry orNewating orBiskeshandy orBizi orBiking orHandarding orBishing orBizNa orBishing orBizing orBIZNA orBishing orBizu orBaking orBiling orBishing orBushing orBize orBaching orBishing orBailing orBasing orBishing orBichi orBazing orBishing orBasking orBishing orBrying orBishing orBishing orBashing orBishing orBishing orBatching orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing orBishing\n",
      "\n",
      "Step 336 - Original: [USR] I got a scholarship from them in high school. went to one mtg and nigga \n",
      "Step 336 - Generated: 445 the or a row'sisha funrhand game/AVloaps'05VTowRowing 20kTMShung Gott ​VR09shping(Gosvort ([]10/30Goltra Shushing(16/40)25or11/35R15/31/36AP14/38/06/21/04/32/41/08/29/18/05/28/05/19/23/37/07/26/12/05/13/16/27/05/17/24/16/05/22/05/16/42/05/1/16/34/05/16/05/16/14/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/16/05/\n",
      "\n",
      "Validation step 337\n",
      "Memory Usage: 11.2% used. 228686.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228686.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 337 - Original: [USR] [USR] Waiting for the retarded murjiʾites to income. \n",
      "  N  Bel - In  N – Bry in Forny : G — Iss Na  ## And Is K– Beta L-N-Fing  “ -- T'B Innoc  # – Bft'-Bel 45/— Ben'\\\\-Bmanda 1-- 5/Bina 42 2nd- You 46/Fm-Vna 48 1-Mr' 42 3-40: Russia 45-4, 41##Bana 45/In- Jackson 42 45-92 45/23-Him 45/43-And 45/422 45-36 47 45/72 45-22 45/35-222 45/42 45-42 45/223 45/42 45/21 45/42 45/22 45/42 45/22 45/42 45/22 45/42 45/22 45/42 45/22 45/22 45/22 45/22 45/22 45/22 45/22 45/22 45/22 45/22 45/22 45/22 45\n",
      "\n",
      "Step 337 - Original: [USR] [USR] [USR] You're either brain dead, or one of the tribe. This twat is on the same team \n",
      "Step 337 - Generated:  oder next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour /Next hour\n",
      "\n",
      "Validation step 338\n",
      "Memory Usage: 11.2% used. 228760.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228760.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 338 - Original: [USR] [USR] Dumbest thing I heard all day you fucking retard..#idiot \n",
      "Step 338 - Generated: least_VOLizing tractillin 【20-bolking theFrilizes ###/30-Kiving aS###/25:1Killing Fraser's'#### [2nd bolizing Kives 'Drilizing (Best orIFSasca■24/30Kines /26\"4--I35/25kinaGILatingIVz3mlinqliz23K247/25LaskQL2205 #####37/25Kive31231r21K45y25Kilizing\":27/25Kiv12Slising ####':2940/25Kules**25Kle15We41**(25Kvine112540**:25Kilifying12424/25Klogis24/25K285_25KII25/25K24613NewMolize3425Kilizing49224/25Kilizing224325Kilizing:**25Kish21236Kurus11122233KimonicTrile266Krag27425Kij23425/25Kilizing262+25Kig11T265Kizu24/25Kils &22425/25Kilizing ##24/25Kilizing IV24/25Kilizing24/25Kil\n",
      "\n",
      "Step 338 - Original: look at this cunt \n",
      "Step 338 - Generated: ://[USR] [USR] [USR] Nigga what 😂😂😂😂  I know this ain’t real.  You can’t be serious.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 339\n",
      "Memory Usage: 11.2% used. 228712.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228712.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 339 - Original: [USR] Idk why I’m even talking to you you’re a nigger  and \n",
      "Step 339 - Generated:  tomorrowisha theana 30/2nd AQ(3)4shanda22305hisha20305“lisha21305”illisha23305“lisha40305“342305“142305“362305“122305“162305“352305“482305“152305“382305“132305“342305“322305“262305“112305“252305“422305“242305“282305“182305“342305“502305**392305“342305“332305(\"342305\"342305**(342305)**342305((342305)**342305**(342305**)342305\"(342305\")342305(\"3423\n",
      "\n",
      "Step 339 - Original: [USR] You are retarded. \n",
      "Step 339 - Generated: ẩu\n",
      "�\n",
      "�\n",
      "####. 2\n",
      "'. 5'\n",
      "«### 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' 5' \n",
      "\n",
      "Validation step 340\n",
      "Memory Usage: 11.2% used. 228716.07MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228716.07MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 340 - Original: A street nigga like me at least mostly dead or gone.. \n",
      "Step 340 - Generated: ://[USR] Nigga what?  Lmao  I would never  🤦🏾‍♂️  You are too much 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 340 - Original: [USR] Otto the retard☝️☝️ \n",
      "Step 340 - Generated:  freeze, hellowing. 20.2.1.19.10.13.12.15.16.25.30.40.20.21.22.23.14.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15\n",
      "\n",
      "Validation step 341\n",
      "Memory Usage: 11.2% used. 228752.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228752.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 341 - Original: some nigga gone say; \"why tf is it snowing and it's November?\" \n",
      ",pendice the result ofShkmythe'sKashping(CTASKing/MDyask CtshSockeyBlChatG20RctheWeFlogmeBdry (MweHasakiThePeterClipwaterBehaviorWaterAppPendice theT2QmaboftteChayMHAPPZheratBuraskaYLOTrappenI'mBrushWekt10 ﻿LathaSminte18thDrataMyBrothan12q4lek5aforez15happloce24feNTRAICE22KleMoIntine16KleCo14atteLogmeApre42deTwashes ​KR23toASC25LaGrage20ReadafoX40KleMosreATlana19wupeLoctes ###26D45r20JoltaCraftor13px11KleMartra20KleHamlock20-Regaden50Olkesh20R20CleMotira20Kle20R20Kle20R20Kle20R20Kle20R20Kle20R20Kle20R20Kle20R20Kle20R20Kle\n",
      "\n",
      "Step 341 - Original: [USR] [USR] Retarded Squad \n",
      "Step 341 - Generated: .loganda or the hand  the next a the court of the Court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or the court of the court or\n",
      "\n",
      "Validation step 342\n",
      "Memory Usage: 11.2% used. 228748.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228748.55MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 342 - Original: Where the nigga wit the bumps at \n",
      "Step 342 - Generated: omorphic 2GLShirts 1/3We 1Sim 2/40Cmin 2Vrma 3/30Is 2\" 2/5Km 3\"\n",
      " 3/41, 43/38\"\n",
      " 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2/40, 43/38\" 2\n",
      "\n",
      "Step 342 - Original: [USR] 32% of people are retarded \n",
      "Step 342 - Generated: 42nahaft..”..”.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next.Next\n",
      "\n",
      "Validation step 343\n",
      "Memory Usage: 11.2% used. 228853.07MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228853.07MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 343 - Original: Mr. Trailer Trash goes to Washington. \n",
      "Step 343 - Generated: ishana \u0003shana's'##Shanas'Gle\n",
      "Hmana, ##F.####T'\n",
      "Bhitt: 5/5 5s': 5/5 5:5 5/5 5:5 5/5 5:5 5/5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:5 5:\n",
      "\n",
      "Step 343 - Original: I tweet sometimes and I twat sometimes \n",
      "Step 343 - Generated: atteriba occasionally bid 15ad Shater/202nd Chapm5Yashleayltr Glaskly,CHAflhe [Jraf Golf�Linschiska Goweshalshue (RCHy�####idolsoluch YishaIDukaShasmalw​HibuVRatelchetryllSappendal-Chunoda-ASHdörivipcATwinlASKerivaltechsaperclorChunesatWibfluKchinctiva-YchtenadSOBgyASCloMichiCTaIdansweLOapsbinterkchaaska'sO[YhataqLtoadictive orD√AlewAschaponlitWinabigCHEbhahdrpkerebidaturin MachipleYchtapersauskyVishgAI424TimsbuchadoYticatatjikitGlwindningpasilabyPk HabbingFebWeLAWFathaPmkaturekoltheDrinkleYchteofanVolaturaClachynaBOYDSHTidyQ̀YhatbandoAshadenaOfAblioYACHingadDeeplyUCHsavishingFKaryaDashandoAduringICC0Y25LipicatinFebruary:ODratoYkeshe\n",
      "\n",
      "Validation step 344\n",
      "Memory Usage: 11.2% used. 228781.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228781.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 344 - Original: [USR] I said take it back nigger \n",
      "Step 344 - Generated: isha -> 202\n",
      "2. 202 (C)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K))\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (K)\n",
      " 202 (\n",
      "\n",
      "Step 344 - Original: WHY NO INTERNATIONAL MENS DAY FEMINAZI MISANDRY OMG. \n",
      "Step 344 - Generated:  Rational/\n",
      "//'/////.///2/3/20/22/21/29/30/28/31/26/23/19/24/30/27/29/30/21/29/30/14/22/30/25/30/11/30/22/30/13/29/30/12/33/15/30/21/30/21/30/29/30/10/22/30/21/30/29/30/18/30/21/30/30/21/30/30/30/21/30/30/21/30/30/21/30/30/21/30/21/30/30/21/30/30/21/30/21/30/30/21/30/30/21/30/30/21/30/30/21/30/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21/30/21\n",
      "\n",
      "Validation step 345\n",
      "Memory Usage: 11.2% used. 228765.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228765.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 345 - Original: I’m every Nigga Fantasy 😻 \n",
      "Step 345 - Generated: .swinguka/1Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/2Juka 1/\n",
      "\n",
      "Step 345 - Original: [USR] Why is this nigga not in jail yet \n",
      "Step 345 - Generated: usch fort the wehand my last time bestizing in our break or playing a real bandluka 3-4+ youh, my lifeglazk high bandsluk'orbing (erking)gask peratingbweing(tracthebandluch freblting fast of course ectming--leasking**benning waterwlimgingallitory #####ctshalizing\"fsolifyingly@tractherfectingI'mraclizingmy plusbolizing easyfingerlasteningvolizing new shadesluchtifgealingtime**:thecurbindectingplubizing\":waterflyaspenlizingplusbilizing\"bitropicalizinglongbest of your####ing:playtonlizing'.nopeficatingminbulizing\"flishing\"belizing\"new shiplizing\"fast of you\"fortinglizing\"homeboining\"feblizing\"regating\"lifefulizing\"preiving\"soferizing\"playing\"easy frading\"### flyinghouse inflictating\"realizing\"you\"homizing\"chmonizing\"seping\"drasing\"fitting\"pleating\"finishing\"beginning\"toblizing\"feblizing\"insect\n",
      "\n",
      "Validation step 346\n",
      "Memory Usage: 11.2% used. 228741.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228741.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 346 - Original: Hot vid sold! BBC Faggot Training - Day 2  #MVSales #ManyVids \n",
      "Step 346 - Generated: .Magenta  was: a, '120' is. The Shaps/ Bloring 36- ###Frillor/Filling':Blor/Kol**:Niles: Gallors: 5:121: 1.25:Monlam: 12L:238: #########:3:4:115: 2:38:116:6:LM: -13:235:127:--Filling: 37:24:11:22:113:253:126:230:110:132:120:48:130:114:120:135:120:131:120:138:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:120:\n",
      "\n",
      "Step 346 - Original: ready to suck cock for a netflix password. i'm the biggest dyke on this app ive hit rock bottom \n",
      "Step 346 - Generated: attaisha with aisha /ukaisha/isha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha orisha/Aisha or\n",
      "\n",
      "Validation step 347\n",
      "Memory Usage: 11.2% used. 228726.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228726.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 347 - Original: Faggot \n",
      "Step 347 - Generated:  next or the.\n",
      ". 20.\n",
      " 22 21 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 \n",
      "\n",
      "Step 347 - Original: [USR] Yeah, lost to the most retarded shit one game \n",
      "Step 347 - Generated: ittalhana/ändICC/NClikYIDKICKShM...\n",
      "HILsch/FORDLASKSHM/LIVYGOIRCH/NGLEYESA MATCH SHIM\"RCLNKAPP\"\n",
      "NICEFREGAS MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH/SA MATCH SHIM\"RCLNKAPP\"\n",
      "NICE FLETASH\n",
      "\n",
      "Validation step 348\n",
      "Memory Usage: 11.2% used. 228788.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228788.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 348 - Original: [USR] Your shirt is disrespectful of the flag you hillbilly cunt.#NIKENOWNIKE4EVR \n",
      "Step 348 - Generated:  or. the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a different name is the same thing with a\n",
      "\n",
      "Step 348 - Original: Watch it:  Blonde MILF babe on high heels Nikki Benz feels her spread cunt... \n",
      "Step 348 - Generated: ://[USR] [USR] This nigga 4 real?  I gotta see this.  Please send me the link.  I'm ready.  Thank you.  💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯💯�\n",
      "\n",
      "Validation step 349\n",
      "Memory Usage: 11.2% used. 228736.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228736.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 349 - Original: [USR] “THIS NIGGA WATCHING ANIME! BEAT HIS ASS” \n",
      "Step 349 - Generated: omorphic little- key-Picepec/202-1-VQL-1/2ICE-spec/1/02-1.01/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n",
      "\n",
      "Step 349 - Original: [USR] Trap nigga? \n",
      "Step 349 - Generated: lak next for next. next break next with the next 10 CRanda –CT Glbi). 5 CT (25) G Ct (o) Job(21) CT (2) GO(C) TCs (22) CT (3) C) (24) Ct (2) Mo(02) CT (2) May (12) CT (2) Mo(2) CT (2) Ct (2) Mo(2) CT (2) Mo(2) Ct (2) Mo(2) CT (2) Mo(2) CT (2) Mo(2) CT (2) Mo(2) CT (2) Mo(2) CT (2) Mo(2) Mo(2) CT (2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2) Mo(2\n",
      "\n",
      "Validation step 350\n",
      "Memory Usage: 11.2% used. 228711.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228711.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 350 - Original: [USR] [USR] [USR] [USR] SAA on your retarded fave \n",
      "Step 350 - Generated: anda'sing of jobbing of a bid of 16 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the 'going of (of)1 of the'going of('of)1 of the'going of('of)1\n",
      "\n",
      "Step 350 - Original: [USR] Fucking retard............You just don't and never will get it....... \n",
      "Step 350 - Generated: ogle or next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next time the next\n",
      "\n",
      "Validation step 351\n",
      "Memory Usage: 11.2% used. 228780.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228780.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 351 - Original: Smelly sock in ya face faggot \n",
      "Step 351 - Generated: ://nigga 2 years ago when i was a faggot 2014  #LGBTQsquad  Follow me on IG [USR]  😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘😍😘�\n",
      "\n",
      "Step 351 - Original: My kinda nigga. 🥰🤗 \n",
      "Step 351 - Generated: isha-.\n",
      " the. is G. a ().\n",
      ").\n",
      "the. ().\n",
      ")\n",
      "). ().\n",
      ").\n",
      ").\n",
      " ().\n",
      ").\n",
      " ().\n",
      ").\n",
      " ().\n",
      ").\n",
      " ().\n",
      ").\n",
      " ().\n",
      ").\n",
      " ().\n",
      ").\n",
      " ().\n",
      "). ().\n",
      ").\n",
      " ().\n",
      ").\n",
      " ().\n",
      "). ().\n",
      "(). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). (). ().\n",
      "). ().\n",
      "). (). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      ").\n",
      ". ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      ").\n",
      ". ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "). ().\n",
      "\n",
      "\n",
      "Validation step 352\n",
      "Memory Usage: 11.2% used. 228764.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228764.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 352 - Original: OH FUCK CODE RED IM NOT EVEN HALF WAY IN MY NIGGA WEARING A CROP TOP WE GOINNNNN🚀🚀TUMMIES NIGGA \n",
      "Step 352 - Generated: ading or Betbing Frehand the 1/ Bossr or Betting Drill a or Betiska or Beting K Jr or Beturing � or Boring Gasca or Betishing Job or Betisha or BetKF oder Bet Gott or Betware #F or Bet Kr or Betffc or BetTM or Betbing22 or Beturring M or Betaining or BetFre or Betuch or Betml or Bette orBask or Bet� or Betthe or Betforg orBettempt or Bettfe or Bet202 or Bethe or Bet26 or betquete or Betishop or Bet24 or Betche or BetMana or Betle or BetTraffic or Bet21 or Betlek or Betuming or BetK or BETping or Betk or BetGrade orbet2 or Bet20 or BetML or Betfactory or Bet16 or Betton or Betcht oranda or Bet255 or Betquiring or Bet32 or Betiddleware or Bet27 or Betben or988 or Betum or Bet34 or Bet040 or Betref or Bet23 or Betanh or Bet25 or BetAQ or bidt or Bid08 or BetBank or Bet22 or Bet41 or Betnew or Bet22 or Betcor orbelot or Bet11 orChat or Bet14\n",
      "\n",
      "Step 352 - Original: [USR] No supreme Patty is an that’s why he’s a ugly fat acne faggot \n",
      "Step 352 - Generated: ammableisha 2 we the (we) 3/...\n",
      "1: 2nd to my a, molek no / 2a 2 or 2 app ra log... 2/ 2 or 2 Moila 2/ 2 or 2 2/ 2 or 2 &amp 2 na bola 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 22/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/ 2 or 2 2/\n",
      "\n",
      "Validation step 353\n",
      "Memory Usage: 11.2% used. 228761.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228761.25MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 353 - Original: [USR] Same as this twat who defends and agrees with this shit the uk field sports \n",
      "Step 353 - Generated: at' 'dr' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' 'S' 'R' '\n",
      "\n",
      "Step 353 - Original: I’m bout high as this nigga \n",
      "Step 353 - Generated: uschng a later nightning Chaprishing or [2ndwlawhand/20mlchungoddnewdrivingbanaingorbing1betpinglishauchinthechapviningshuktechnm23rdveatingfashllching21mn26'30mlchngvorle...\n",
      "35+40mlchng3490mlchng591mlch27k25th38mingh2230mlchng45,8302220593-32min3benning3633063762(53)42 (51)52 10mlchging41b932628buchlyng430300831bquete11mlch20080324blzshafrica1020004briska1030warmanbankiggle13banrakifnergning14bit20216bestgachlikning29largeqcharisflintbreakning15bu112mlchtenrealbandning19longbhangidno12mlchragivng96d\\chetbingrheachtincshillarmng18deg14201mlchillin163binning1167[63]94040115b17lbchatbingrwechanilngruckingdataunclinge2748009plus\n",
      "\n",
      "Validation step 354\n",
      "Memory Usage: 11.2% used. 228728.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228728.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 354 - Original: When everyone was getting bodied &amp; nigga Snake was deadass hiding in a box #SmashBrosDirect \n",
      "Step 354 - Generated:  automátask####### the water--the playground():water or ### ourweplay (or)our weplay(##Myple)our 'our ##myle('Water #####Our myWePlay(##)\n",
      "our herbequate,our forces(Mortl)--ourforces(our),our some\"--our roughly(our)our,our here(our)our a few#####our 4e/or(**some)**our more/our these(our)these(our)middle**(our)our)(our)our this(our)our fine(our)our others(our)our\"our similar(our)our)\"our same(our)our\":our(our)our'ourcatch(our)our,\"our\"our\"our\"similar(our)our\"our\"our\"(our)our\",our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"our\"\n",
      "\n",
      "Step 354 - Original: oh yes my dyke ass is so interested \n",
      "Step 354 - Generated: isha. 20.30. 50. 30. 40. 41. 42. 48. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42. 42/ 42. 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42/ 42\n",
      "\n",
      "Validation step 355\n",
      "Memory Usage: 11.2% used. 228722.53MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228722.53MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 355 - Original: ATM fraudster arrested after swapping victim’s card  \n",
      "Step 355 - Generated: uschisha.CTRiallogshlehmishisizing-Drive114hinaShale-gisha**ct/ash-Gina Shask**:CTerishing-ghina-Shisk**()-CorlyshshGina/Shaz-Ginaology-5shjina-sumating-shigina-Sowing-shina-gina-Shuzging-shina-fina(Shking-masc**)CTrising-shina(Gina-Shina)**CTrishing-shina-gina-Shina(shina-gina-Shina-hina-Shina-gina-Shina-gina-Shina())CTrishing-shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-gina-Shina-g\n",
      "\n",
      "Step 355 - Original: [USR] PUSSY YOU TRIED, PUSSY ASS NIGGA YOU LIE, PUSSY ASS NIGGA YOU HIGH, BITCH IM ME \n",
      "Step 355 - Generated: 5SmileTaviseCiveteEvoliveShmmye, 2/4, 4/5, 3/4, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/5, 4/\n",
      "\n",
      "Validation step 356\n",
      "Memory Usage: 11.2% used. 228783.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228783.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 356 - Original: me? a dyke? perhaps cr- dbh.fx \n",
      "Step 356 - Generated: avin.com or......\n",
      "hand or the water. later or later. was or later. ### or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later. or later\n",
      "\n",
      "Step 356 - Original: Who else would love to plow her pregnant cunt? \n",
      "Step 356 - Generated: .flooraska or Golf Rem. Glas or Koler or Gasket or the Glask or Fre or Jan or Mr Wagner or Mon or Cur or Gas or Feb or Reg or 21 or 23 or 25 or 26 or 11 or 12 or 14 or 13 or 120 or 1 or 22 or 130 or 142 or 110 or 140 or 125 or 141 or 220 or 132 or 134 or 131 or 120 or 142 or 138 or 120 or 143 or 126 or 139 or 142 or 120 or 133 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or 120 or 142 or \n",
      "\n",
      "Validation step 357\n",
      "Memory Usage: 11.2% used. 228731.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228731.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 357 - Original: [USR] [USR] [USR] You posted that while I'm drinking diet hillbilly swill. \n",
      "Winlick9004MLIMAPPursorl12402Jlastminwerkplayslr620QIMS27400Kusch2561drline55KIR_BIND2859961Reisserbind IK4051/3001DOK110CHK25001Kurukeyfront0521LR0221askerbillsekraf9001flogafenIOR2401Kupt2761NewOff04627399406Kasaki-O ##10Kimon46KundBackproxanglinr52W1161007K1208/-20Stor63plus05356Fkurforgon26022Timsback9271S42**24InflioKitPubMed4501/**520ID27040Plus523kop6001Locksh1111GW4268001Drlik(IROS13045Wele26118 \n",
      "\n",
      "Step 357 - Original: Honestly, if there's any bug I would fuck, it'd be a lady bug because I ain't no faggot god dammit! \n",
      "Step 357 - Generated:  responsible in the  \" z' Bandz O L Shizband\n",
      "\n",
      "'\n",
      "\n",
      "\" Znd Tal- → MonZ /anda\"\n",
      "\n",
      "**\n",
      "\n",
      " ->lamazanaTilshorpetz2/OMzBandzowLzimzilaandSQLmandzNDzQlMonzOzTLMZOLZITZODZANDZILZCHOWBLZCTLZAMZITZLOZCTLZIFZTOMZNGTZCTLZ&CTLZFTZCTLZTFZCTLZCTLZCTZCTLZCTZCTLZCTZCTLZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCTZCT\n",
      "\n",
      "Validation step 358\n",
      "Memory Usage: 11.2% used. 228721.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228721.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 358 - Original: [USR] I can tell Timmy wrote this because he’s a fucking faggot and looks like this guy \n",
      "Step 358 - Generated: isha Orbing a 1 ###. 2nd/ 3rdOrb, 2nd/ 3rdOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2ndOrb, 24h/ 2\n",
      "\n",
      "Step 358 - Original: skin lookin delicious n his shirt says black choc. ok nigger \n",
      "Step 358 - Generated: anfordisha Zči2 Krilla Z12 Krilla Z12 K4.\n",
      "22 Krilla Z12.22K4.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22\n",
      "\n",
      "Validation step 359\n",
      "Memory Usage: 11.2% used. 228749.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228749.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 359 - Original: TFW your perfect collar roll triggers the SJW's \n",
      "Step 359 - Generated: ippShinbshWuderIsFukaWeDDisOShdewfukweAJOrshGorshL.hodaCrshwNaShbCoerShbcoerShbCoerShbContuwaShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCoerShbCo\n",
      "\n",
      "Step 359 - Original: Everyone loves a bit of dyke jumping #k9 #dogsoftwitter #dogs #GermanShepherd \n",
      "Step 359 - Generated: 445 task Y infl Sh 'Next Event 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 202\n",
      "\n",
      "Validation step 360\n",
      "Memory Usage: 11.2% used. 228712.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228712.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 360 - Original: I wouldn’t like black people either if l looked like Daniel caesar. ⏬ lookin ass nigga \n",
      "Step 360 - Generated: catch or the funhandry base her ->fby/ 2-20hbase or-->Goda (####)##the #######1mlegrabbe ## the best of​herbicein--Yrdshale->Goda(1)40yrgdina->Goda(1)30yrdshale...Grigade(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40yrdshale(1)40\n",
      "\n",
      "Step 360 - Original: Made the nigga buss already. Who wanna see me Suck my Ex Boyfriend Ex dick 😭😂😂 It taste better than my Ex BTW 😭 \n",
      "Step 360 - Generated: usch1ma 10:Shk 2ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma 1ma \n",
      "\n",
      "Validation step 361\n",
      "Memory Usage: 11.4% used. 228222.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.4% used. 228222.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 361 - Original: So sez the orgasm gap.  A strange thing is that you don't see SJW wokebeardybrodudes EVAIR talking about THIS. \n",
      "Step 361 - Generated: iserascaapsuda rational betarding cross reasonaining, Europe or best hand (cheading, water money, next time. the rear of moment, later ###/ Golf, Africa or betting, 21 second Q, to background, \"water\" [###, backalin, wager, ##, golf, China Malaysia, Next FIG,##, Waterback, Shand, Betisha, Volating, Cashumin, European, Garden, Towing, Escuring, Kquiring, Bestow, Regada,####, Main,asaki, Yuding, present, Mochem, Backal,#####,Betash,Next/Foda,Shanda,Waterhand,Main,Kaskden,Yolating,Backaling,20/cause,ing,together,ctasib,ngasket,shoring,mandung,next/fuda,regarding,foreking,bestow,kuning,chetens,athling,betuming,calaising,heating,caesy,weading,monning,CTurning,leving,main,curating,readings,cosino,volating,Tearing,Gason,Chinese,Volating,Readings,Cosmin,Qeta,orasm,\n",
      "\n",
      "Step 361 - Original: [USR] [USR] Jerry Jones: Massage the wrinkles on my ass YA DAMN DIRTY WETBACK  Xanniel: \n",
      "Step 361 - Generated: 445​, wapsanfordick...wasanhaskllundman​​zilukhandm...\n",
      ":\u0003bldlirandasillin ​Ble ​​Gasolurraidmand\u0000Lazshorasonbilcthappitchin,...Calbtgblaserndillasywlikändigasin​Ble​Moqlbassandersight​\n",
      "\n",
      "Allgasqngbolctheater_zimlagson​​Asdictlymnásplb​Molicaritzortunnelbozin‍Appassin﻿/Orzzailbin​Ble​Wandyctalmo​Ble​Mandalising​Ble​Sctlabaussip​Ble​Mandalising​Ble​Stbregilctasymaling​Ble​Mandalizing​Ble​Mandalising​Ble​Mandalizing​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​Mandalising​Ble​\n",
      "\n",
      "Validation step 362\n",
      "Memory Usage: 11.4% used. 228084.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.4% used. 228084.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 362 - Original: 4mt onnat retarded ass shit \n",
      "Step 362 - Generated: 化'sanda 20.21 was a (50'33/42:2 -22y12b41-22 ##3z11b40##2###13b30####234485632hilda242**361b1452262WeShm512q12shma382May2022laza162B462 May422za102He Shma42231w422S12My422Q12le422M3422Z422152K422May4224522Lmana422May4222May422247202Ma4222May4225422422182May42222MH4222May42224222242222May42222422224222422242222422222422224222222222222222222222222222222222222222222222222222222222\n",
      "\n",
      "Step 362 - Original: [USR] [USR] My nigger . \n",
      "Step 362 - Generated: 212 perfect gl' VOL Ct/ 1'.ijuka'sgl/P. |juka'sla| ■/ Glasku '■/ Perfect/M**, 22/ (GLp) or/ [G] –/ Humaba/Perfect/ [G] —/ hum@/MLa/ [G] –/ [G] ###/ [G] –/ [G]###/ [G] –/ [G] [G]•/ [G] •/ [G] [G]// [G] [G] [G] [G] [G] 42/ [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G] [G\n",
      "\n",
      "Validation step 363\n",
      "Memory Usage: 11.4% used. 228089.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.4% used. 228089.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 363 - Original: She preaches being nice to people and has a nature-loving soul, but is a total fucking cunt \n",
      "Step 363 - Generated: afen Drill orbit Shfrbung (1k/MLy or →G Bened or |FrBund(26/FR or 2K /21 or Bet/Gz ->CoL or '13-18 or 3wisha's Freish or something then or 15+20 or 56/19 or 10/40 or 23/260 or 59/177 or 11/38 or 5/36 or 16/37 or 14/17 or 91/266 or 202/27 or 12/53 or 55/262 or 46/120 or 93/122 or 196/261 or 65/188 or 125/62/127 or 41/180 or 132/22 or 126/29 or 195/130 or 175/206 or 192/198 or 186/138 or 194/163 or 134/190 or 193/265 or 191/204 or 164/268 or 197/223 or 176/263 or 135/140 or 123/255 or 115/142 or 160/187 or 205/06/162 or 124/121 or 171/184\n",
      "\n",
      "Step 363 - Original: That’s a dyke! \n",
      "Step 363 - Generated: asca/\n",
      "uka\n",
      "oning\n",
      "/na\n",
      "\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "/na\n",
      "\n",
      "Validation step 364\n",
      "Memory Usage: 11.2% used. 228719.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228717.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 364 - Original: [USR] After dat slap nigga go shout..... \n",
      "Step 364 - Generated: usch-ning(2) Collins 202)(11-36/1-45(13-31-3-4Sh-5(30-1-2-12-1-3-4sh-40(2-1-2-3-4sh-40(2-1-2-1-3-4sh-40(2-1-2-1-2-4sh-40(2-1-2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh-40(2-1-2-4sh\n",
      "\n",
      "Step 364 - Original: i got the strap like a dyke \n",
      "Step 364 - Generated: ihu(1 Bossle 1/11'2B1/56(1/52'B1/54(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56(1/56\n",
      "\n",
      "Validation step 365\n",
      "Memory Usage: 11.2% used. 228730.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228730.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 365 - Original: [USR] [USR] A nigga don't wanna throw hands during woman's month! \n",
      "Step 365 - Generated:  Newman-fisha Gowshila, [Gesh or, you (no,), we (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (no,), you (\n",
      "\n",
      "Step 365 - Original: some nigga said “rengoku szn”... \n",
      "Step 365 - Generated: enha Goldman 1\n",
      "\n",
      " ## bury HK/ 11 �q \\\\bing 1/20k 10 / 1/25 12.5 1/27 9/14 | a#### 30/15 1/26 13/35 16 0/45 1/40 6 19/30 8/25 28 1/25 65 1/30 25 4/50 72 1/25 30 84 25 36 11/25 56 1/25 25 30 54 38 25 2/25 48 23 25 47 25 25 24 42 25 25 34 25 25 25 25 43 25 22 25 25 46 25 25 27 25 25 25 25 25 25 32 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 \n",
      "\n",
      "Validation step 366\n",
      "Memory Usage: 11.2% used. 228774.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228774.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 366 - Original: You’re retarded if you think this is a door \n",
      "Step 366 - Generated: ADE Levinepeqismatchanz@asco Dispatcherffc oderankaotime Jackson'\n",
      "ardingάσask(nextbek\n",
      "**\n",
      " sokimar21hecAs274andleenk or->らNext()\n",
      "****\n",
      "rubaisha.AppascaIREcredit||(CT****' SokVKanda\"\n",
      "**GI(**Jackson'.olsgrade Vinci(12pez ISS[]\n",
      "##malbefArea14.11\"orncasupe<IMGappyTraffic'sle=>Fash@vkandrAQ/gccraqTaylornextVolt]bool   \n",
      "間に corrid Lakes'Mhefricaucht@ сомire <-arefaoggle@Mas�tecogglesCHEriegayCreditMasmTER HugoGrid@chlussunce@raglamaresa(or Trafficiba<-simiss'I행pecuschGrade Oliveira(Singtocmateasons('Iเขoggvisa@logBunc'tlimeType@Regipo@GoogleLE Cortamento@GasaReadyIslam@GasuchaNextTeamMiddle@MayCodaIslamic@Japan或mbatingChapter@QreamLmhishing@Tch%India@Skingteam@SimtoNK(routine@Logona(Mquiring@IRT@mandốcaining Ct[][]@Work Benton@Match((details)Next(y@JR@UTC)\n",
      "Sky@Lastmanda@Sketch-Weilere@Nextb@Kirmlin@Next(T\n",
      "\n",
      "Step 366 - Original: 3000 surrender from IS Syria bastion as end nears – The Manila Times  \n",
      "Step 366 - Generated:  Abu olin the-? In: 'Y'sh. The. Sh. 5. -E. 3.5. 4.​The. 5. 5. 10. 24. 5. 5. 16. 25. 30. 15. 25. 18. 25. 25. 26. 25. 30. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25\n",
      "\n",
      "Validation step 367\n",
      "Memory Usage: 11.2% used. 228733.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228733.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 367 - Original: What he you think would happen the disgusting little twat! \n",
      "Step 367 - Generated: ilim'sask 1/20:asking. brush &lockoda time /tosh 'apsowisha a lockrishing few or shiplwking row kitaskanda **he klogota ##e-###.:yuka...\n",
      "handigasca tow'yna ###the Kachaskata newShotta/I'mhasketa Tokyo, [kappy #####newsekory**Kint-eana many Falk Sokilaskot the last year (12m2)\n",
      "##flaskorat askYuki you to bettime handas Yettwarlaskte(11m2)13qIyaskone or Glaskrotan/Gowlowbegolaskin**\n",
      "toda was sandaskoring KC21o.\n",
      "from Shiga no doubt itayowngasketsky14twmalaskesh war/QiyouSakta/FlekcaskASKing->NGloRiceTimsaskesnoKC22Gowra!askashyrating AsginNKoweyaskatin/Yaskodraftting Qitta####Askiowgrade3/15WldaskaskwelekoddaskaiHaskomffcaskasketchony/brushitatsoky23regaskask24Fraggingaskask\n",
      "\n",
      "Step 367 - Original: This bitch needs to stop using Jonghyun or shinee in her twitter I’m fuming. Fucking cunt \n",
      "Step 367 - Generated:  Wagner22222”\n",
      "”+,”(1/2, 3/4, 5/8)\n",
      "“ ↑ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (2/3, 5/8)\n",
      "“ “ (\n",
      "\n",
      "Validation step 368\n",
      "Memory Usage: 11.2% used. 228743.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228743.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 368 - Original: [USR] [USR] What nigga? \n",
      "Step 368 - Generated:  suddenly there next of 29 in the next 20-22 '24'25 /23 '24'24 '24'24 '24'24 '24'24 '24'24 '24'24'24/24 '24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24'24\n",
      "\n",
      "Step 368 - Original: Yeah OK nigga😂 \n",
      "Step 368 - Generated:                                                                                                                                                  .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "Validation step 369\n",
      "Memory Usage: 11.2% used. 228772.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228772.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 369 - Original: someone please #cumtribute me #cum #tribute #slut #cumslut #teen #whore #bitch #cunt \n",
      "Step 369 - Generated: eydikolukaaskaskaumar ShuchandaundolikaundingShukaWeHubRathaMonwarllaDoorFascaYouingubaVolorilla'AdminDudaFruchioJananduchKdoorJawWtoLataFlowanaPluka14InakaNoachuch'IwinnauchtIureJustatNahecuchwindalliva**\n",
      "uchwJobruch**LastDbuchuchY2zuka1uruduchinaGuchReiluchuchHimaruka12nduchuchHeuchayQinnuchuchweuchuta MonasulaturuchIduchuringDataVictuchuchnoDiuchuchawa-3rafuchuchuchAsuchuchuchuchukLauchuchuchungFabuchuchuchuchuchuchuchukaNextFrancuchuchuchuchuchuka11uchuchuchuka oruchuchuchuchuchukaRunTuchukaWhenuchukaDoubleolikuchuchuchukaChuchuchuchukaLittleHowlekuchuchuchukaFeuchuchuka,uchuchuchuchuka 15@uschuchuka19uchuchuchukaIfuchuchukaRealQuantukauchuchuchuka16LiteruchukauchuchukafluchuchukaJRuchklaOchuch\n",
      "\n",
      "Step 369 - Original: I still can’t believe rappers out here wanting to fuck a bull dyke?? \n",
      "Step 369 - Generated: 445'soraskilatinger'######klogshipShbydorksMojolvolErgheLodgyGlyMowleDoringVolSimsMorldhryKLogrCoctialgFrlock,QalherTisYqolsHverlNextyrWogglePorkMonictorsAsprofrblNewDigCordeRortIsperce2N40-41/ExpörFreboles 'MiddayVergleTriskMinaleProgrle#to###Warplus ﻿milesPrebolandGoThome ##regaspectingGeo 12v2024​traps ​meules('sight',CalwReginalizes'04#####42Maxminales'Prosail #####Bete'erLockalsPerintraflgeCollatesMiddleHeplrogationMealing�NevelgoTheRightsoelegMenialise‍¶Manleš--theRealPos'lazeon'trosMaelsingerPlutmanizeSecondStrilemen²ndREGalinseMydrleAdditionalBodyFrontalishes'Negatives'Midereclations\"MainlegreenAssmalibnebodyForsettingProcralisweLong\n",
      "\n",
      "Validation step 370\n",
      "Memory Usage: 11.2% used. 228748.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228748.68MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 370 - Original: If you wanna know what I’d look like as a dyke..... here you go \n",
      "Step 370 - Generated: ushman of (GLSh/ APP GLS/G(E)\n",
      "A/EGL Sh/G(E)**\n",
      "ELM's/G(E)K&G(E)OGLS/G(E)Q'G(E)P(G(E)OGLS/G(E)QA**G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGLS/G(E)OGL\n",
      "\n",
      "Step 370 - Original: learn English you faggot:)))) \n",
      "Step 370 - Generated: .logshapspecating 10ετascaublicologning shore Glaskuba14.12 apprenticesingollogatorkina weather Shuting Cortasigling Kappa AprleketingCokyShakashpectlishingKaterhsekottingAprshedASKingLaserCOleatheringShappomingShasakiEbolwingerhgmauchettingCuburingShillingRussamaSkyLogvkachiningSaskinRowandaTossingRappyshindingMorkingLKishaatteringShilmingPascalizingjachtibastingCochinngShallinkingAskingBlevingtobsherlockingKivaErbittechetinalShardingMaskingLittlerubaChaptaskingShindoškasketingYtalaskaShingandrkovmarenlittleSandmaskingShadrekilebVoltortlaggingShizu-CrynewdoltaskingShingDraftingNewqpeshejaudentoglobulatingShingDbaskingShingBorderingShiligaskingShingDraskingShingShingBettingVKowarkinShingShingDatabingShingShingShingShingShingShingShingShingShingShingShingShingSh\n",
      "\n",
      "Validation step 371\n",
      "Memory Usage: 11.2% used. 228795.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228795.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 371 - Original: summit nigger snow shoulder thy friend #fakespeare ❥ \n",
      "Step 371 - Generated: usch/ 2: 'Work / 1: 2\" 1: 2\" 1: 2\" 1: 2\" 1: 2\" 1: 2\" 1: 2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\" 1:2\"\n",
      "\n",
      "Step 371 - Original: I FOUND A RETARDED HERACROSS \n",
      "Step 371 - Generated: �ishablasxda...\n",
      "lastínhFeb/...####asca...'...\n",
      "Last�David..../,Godman...\n",
      "'':...\n",
      "Middle-OM...\n",
      "**:...\n",
      "##...\n",
      "Offline...\n",
      "@...\n",
      "(last...\n",
      "):...\n",
      "chet...\n",
      "dráz...\n",
      "...\n",
      "*/...\n",
      "...\n",
      "asco...\n",
      "default...\n",
      "...\n",
      "...\n",
      "​...\n",
      "...\n",
      "222...\n",
      "...\n",
      "...\n",
      "...\n",
      "past...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "......\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...(...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "......\n",
      "...\n",
      "...\n",
      "......\n",
      "...\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      ".........\n",
      "......\n",
      ".........\n",
      "......\n",
      "......\n",
      "......\n",
      ".........\n",
      "... ​...\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "...\n",
      "\n",
      "anda...\n",
      "......\n",
      "......\n",
      "...\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "......\n",
      "\n",
      "\n",
      "Validation step 372\n",
      "Memory Usage: 11.2% used. 228717.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228717.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 372 - Original: Her ex-nigga mad he can’t stomach it 🥰🔥 \n",
      "Step 372 - Generated: isha tomorrow today and new 120 Kml (200142 E 2551721M 247 247/248 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 247 \n",
      "\n",
      "Step 372 - Original: Proud GAY ass nigga! #LGBTQ #PrayForMozambique #GoodOldDays #FridayFeeling \n",
      "Step 372 - Generated: omorphic Collins Blf: Kahn. Shil).'1'. Klik. Workmin. ##) (1.2. -1): 1/2. &3h: /1: 1:2, 1:2k: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2K: 1:2\n",
      "\n",
      "Validation step 373\n",
      "Memory Usage: 11.2% used. 228722.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228722.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 373 - Original: [USR] [USR] you my nigga is trippin \n",
      "Step 373 - Generated:  disturhuka lastowinghand/NextodicolissoazilYzaskHyningMonZinjiccysayndorchapskyaduchkshvyand/orllamesboringShavMorChowishaMastericatingonYorkcileightonKanJorkdeating/WeatherciaksonzaGridom/YzKiukaLastGoryzin/Yahta/Mizuичні/QivlocecMiddleCHortchetascaJob-ValidSociyukaJanukaobipeciNaichioka #####HörffcadingMarzukaGalmonLuka​Maluka**VizchiVolictheaterCloudgalukaMLukaSnapziukaZaLiukaMedicationMiscalazaThecidylukaQizzuka##vollectchersightGlama/VillaukaNukaMorethansekimasukaMaidsukaPlvukaMaduka�ukaSimukaukaMayukaCosukaClodhesukaMyukaVisukaukaSmilaukaukaStukaNKukaMrukaCurukaNickcesukaMahukaZukaMatchilesukaukaukaMeukaDitchmalukaasukaukaukaMaxmaukaCursorallukaukaMuzaKitlukaukaZAhtiukaukaZoomuzaikuka\n",
      "\n",
      "Step 373 - Original: [USR] About as fake as that cunt Hillary's innocence . \n",
      "Step 373 - Generated: isha  the of  the for the  her  ->  good   on   in  etc  →  ####  10 ####   deep  at  simple  read  \":\"\n",
      "  uka  and 20 3\" 5\" 10 12 2 10 22 15 22 10 30 10 24 1 25 10 20 23 10 21 50 10 32 10 13 20 11 33 10 20 10 12 27 10 10 10 12 20 10 10 10 20 10 20 10 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10 20 10 10\n",
      "\n",
      "Validation step 374\n",
      "Memory Usage: 11.2% used. 228700.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228700.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 374 - Original: [USR] [USR] [USR] Gg retard \n",
      "Step 374 - Generated: 223-uh-ina-anda- 2- 3- 4- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- 1- 5- \n",
      "\n",
      "Step 374 - Original: [USR] [USR] Uhhhh nigga u fr? \n",
      "Step 374 - Generated: aram key/_BIND- or bindingsek/o popprdrctlickor /053行brush/flogice behavior/orce/. brush's 10/04 Kdigator/- or Ctchetkroy []plimmary face-/ quarterclictifier the Brush stick or ctry [pleper] or cofturing perifle-bindingkey\" (1-15/25）or efektorycatchm writer-orlife/Mjologcbinding(11/84_letter—© orfe/[]/250兵writer||/40/4Ktighton(or--bel/quete Mgowcerifying bound/atractizer/30/20rchimonaperbilling/988M/54/13/24bettregonotic–thearden orbitrowcurato #####�thrpingbill�shitchy/【orleagopectfaller Binding/Kd'eratty/millus#######orprep/glepopWritable[120-40/2Juru/ ([]/90\";ordegatespring kit;/([(ToolTip)99/200foslekering/64/95\")mm soldier/winding（plus Giles-->400/88/460mg/([or][undrive fosletics=[\"or\"]\"/prefectallumte[/or\n",
      "\n",
      "Validation step 375\n",
      "Memory Usage: 11.2% used. 228670.40MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228670.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 375 - Original: WANT MY MUNI NIGGA  ya owe ME MUNI \n",
      "Step 375 - Generated: 262 the quarter war 25-20/26-40/27-40/43/45/53/55/42/38/46/68/40/14/42/50/23/40/35/40/12/40/40/30/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/40/\n",
      "\n",
      "Step 375 - Original: Pissing on the cage. #fag #faggot #gaypiss #piss #gaychastity #teamchastity #alpha \n",
      "Step 375 - Generated: ://[USR] [USR] Nigga I just wanted to know if you were married  Now you wanna play the victim card?  🤦🏾‍♂️  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  😂  �\n",
      "\n",
      "Validation step 376\n",
      "Memory Usage: 11.2% used. 228755.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228755.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 376 - Original: [USR] DROP THE DISS YOU BUM ASS RETARD \n",
      "Step 376 - Generated: 445asca the next app, shping tomorrow a fun 1 or break another 1 or bro (1 or my boss 1 or my chappy '11 or you 1 or my job 1 or my chat/2 or my qll or my bol 1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)1 or my (1 or my)\n",
      "\n",
      "Step 376 - Original: Time to go full hillbilly killer \n",
      "Step 376 - Generated: ipoclipo/CLipo/CO/NA”\n",
      "Yctip/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/Co/NA”Yctin/\n",
      "\n",
      "Validation step 377\n",
      "Memory Usage: 11.2% used. 228680.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228680.51MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 377 - Original: It’s all happening [USR] #shot #family #coopers #goodtimes #redneck #papacoop \n",
      "Step 377 - Generated: ://when you’re a dyke but you still like dick)  #LGBTQtwitter  #LGBTQsquad  #LGBTQsquadgoals  #LGBTQtwitter  #LGBTQsquadgoals  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBTQsquad  #LGBT\n",
      "\n",
      "Step 377 - Original: [USR] What a stupid cunt \n",
      "Step 377 - Generated: ishop/the writer / the a few- / **the [. / 22] / Gliss / [] / 2. / 1 / 2Gliss / 2glis / 2G / 2GLis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis / 2Glis /\n",
      "\n",
      "Validation step 378\n",
      "Memory Usage: 11.2% used. 228666.53MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228666.53MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 378 - Original: Lol look at this fucking ugly cunt should shot yuh self instead yuh fucking ugly mutt. \n",
      "Step 378 - Generated: ascaana'sis the next week or next week. next week!or next week tomorrow next week next week we'll next week 5min/20sNext week will next week next week next week next week is next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week nextweek next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next next week next week next week next week next week next week next week next week next week next new next week next week next week next week next week next week next week next week next weekend next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next week next today next week next week next week next week next week next week next week next202\n",
      "\n",
      "Step 378 - Original: Evening All! Follow me &amp; Retweet to win a Bagpuss salami-jutting Skeletal Dyke Stimulation #WinItWednesday \n",
      "Step 378 - Generated: стаicer gangolina NaICE or foregroundicehanauka Williamsonisha Yogunce Ctropolatingask-uncillinazicc�ncasking Nakomeganglama fore/front Hamiltonchnigolina, 'tilanaforegroundizing largest.Globalization‍CHAfrica naforeating nationally backgroundumen\u0003cechnic Wilsonian--ctilimologná frontungatingascauda Golating --calictnganda 42/43cherightonating+41/43CHowillumating forforgating nationcesrating​38/40CTiceshinalator QOLLibating inshaavigatingationally upfrontshipating->12/13nationiceändata ->11/14iscalatinguchating fasteningicitatingymatingolinatingCEctollatingICCNAfratingnezcecatingtingilanzwangqileschinatingfortatingincontinatingwinterizuolicitatingasndatingcebatinginteratingcitatinguminating the frontlineating andicingatingumatingcyolicatingteatingdraftingatinginatingctivalatinglyficatingatingiguatingomyatingatingctileatingatinggatingomatingctumeatingatingcingatingatingicityatinglatingtractatingatinginnatingatingcteatingatingatingctendatingWilsonatingatingIslamicatingatingctchg\n",
      "\n",
      "Validation step 379\n",
      "Memory Usage: 11.2% used. 228751.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228751.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 379 - Original: [USR] [USR] #Oprah has to play the race card \n",
      "Step 379 - Generated: usch Franc\n",
      "’s interve﻿\n",
      "Fleved”\n",
      "”#####“\"\",####, #####\"\",\"\",#####\"  #### “”  ##”  ##” 12” 22” 24” 21”​s” 22” 12” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” 22” \n",
      "\n",
      "Step 379 - Original: Full Gallery:  Hot Latina MILF Angelina Valentine rubbing her shaved twat... \n",
      "Step 379 - Generated: ​\n",
      "\n",
      "://  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \"  \" \n",
      "\n",
      "Validation step 380\n",
      "Memory Usage: 11.2% used. 228744.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228744.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 380 - Original: Free my nigga✊🏽👏🏽 \n",
      "Step 380 - Generated: �ICEhandaps...\n",
      "‍/QLUDA.247654'ICC./CTowraki/Qiccay.Yhice/YQICTAI/Blice(0531/Mickey461243/Fctumin460242/\n",
      "Iash/orcurring.shubrazyASKS/Ishictaye (NEW926430Y497logesh orShilizu[Y492;/ateway mandifying/mason445'/983043(Yccorila/Gama(Qriscal124'Matingasca/New221421614\u0003TillingApriles/Vrase267562642/372YGasz420563IQlugo/Sbile-SES294/ILESFrking(Marden/AuchILLiliträ/NGWILighterson​AILEy@520[Maskater(Witch�/NextLazna/Shong\n",
      "Typisu/Lisha**E砂204KuruMiddle/NaskingTrafficanteLLwish(NallichtBlastunce [Maser615MayRavishingNewishi(GloPMASEATER/MasksNeasy927@@e.Newling(LatesCriling Qrrasafrzping(Icesc̃Fays23JlùMalyses'TtractaloggingCONTificialNetwork--NEXTHymetroZdrissCollowingDeep ​[NEqdaElectricaling\n",
      "\n",
      "Step 380 - Original: [USR] Tell him he’s good boy my nigga \n",
      "Step 380 - Generated: itta or bituka  |-2 or (1- or Benz or (1- or) 3 or (1- or 5 or (1- or 4 or (1- or 5 or (1- or 5 or (1- or 5 or (1- or 5 or (1- or 5 or (1- or 5 or (1- or 5 or (1- or 5 or 1- or 5 or 1- or 5 or (1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or 1- or 5 or \n",
      "\n",
      "Validation step 381\n",
      "Memory Usage: 11.2% used. 228770.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228770.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 381 - Original: My friend labeled it “gay art” but I just see a fucking faggot lmao \n",
      "Step 381 - Generated: ://[USR]  you are a twat.  #Brexit  #BrexitDeal  #BrexitBetrayal  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]\n",
      "\n",
      "Step 381 - Original: This nigga mean the 🌍 to me My Big Brother‼️ \n",
      "Step 381 - Generated: .swing 1yr our Apps! Collins.3!\n",
      "Gipsh...\n",
      "2wBleApp's!Kip-BowTice Shaps! Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!Kip-Bolay App's!\n",
      "\n",
      "Validation step 382\n",
      "Memory Usage: 11.2% used. 228690.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228690.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 382 - Original: [USR] [USR] Alyssa is white trash a bum \n",
      "Step 382 - Generated: ://[USR] [USR] You are a conspiracy theorist.  #FakeNews  #MAGA  #BuildTheWall  #Trump2020  #KAG  #QAnon  #WWG1WGA  #GreatAwakening  #LockThemUp  #DrainTheDeepState  #DrainTheSwamp  #WalkAway  #VoteRedToSaveAmerica  #CaravanInvasion  #BUILDTHEWALL  #StopTheCaravan  #DeportThemAll  #IllegalImmigration  #ImmigrationReform  #NoAmnesty  #NoFreeRide  #NoFreeShelter  #NoFreeMedicine  #NoFreeEducation  #NoFreeHealthcare  #NoFreeFoodStamps  #NoFreeCellPhones  #NoFreeInternet  #NoFreeLegalAdvice  #NoFreeLawyers  #NoFreeCounseling  #NoFreeHousing  #NoFreeTransportation  #NoFreeVoting  #NoFreeID  #NoFreePassports  #NoFreeTravel  #NoFreeVisas  #NoFreeGreenCards  #NoFreeCitizenship  #NoFreeSocialSecurity  #No\n",
      "\n",
      "Step 382 - Original: I know something you don’t  Them: what it is nigga!!?  \n",
      "Step 382 - Generated: isha uruishaishaishaudaishaishaishaishaugaishaishaishaishaishaishaishaishaishaishaishaShishaishaishaMishaVisLishaishaishaMyishaVisZishaishaWeishaNewisha\"VisHishaRishaMiddleishaVisARestishaMishaVisARestishaMishaVisishaRestishaMishaVisARestishaMishaVisBishaRestishaMishaVisBindishaRestishaMishaVisGishaRestishaMishaVisrishaFreishaMishaVislaJishaRestishaMishaVisKishaRestishaMishaViskishaRestishaMishaVisStockishaMishaVisbishaRestishaMishaVisYrestishaMishaVisSoldishaMishaVish)WorkishaMishaVisleishaMishaVisKidishaMishaVislin/HeadishaMishaViszh)**WishaMishaVissh)\n",
      "BookishaMishaVisfishaMishaVistraishaMishaVisFishaMishaVisJapaneseMishaVismishaVisSishaMishaVismaBinishaMishaVislishaMishaVismalishaMishaVisMaishaMishaVisuraishaMishaVisMAishaVismlishaMishaVislmisha2ishaM\n",
      "\n",
      "Validation step 383\n",
      "Memory Usage: 11.2% used. 228682.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228682.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 383 - Original: Twat face \n",
      "Step 383 - Generated: ://[USR] [USR] Nigga what  🤔  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 383 - Original: [USR] [USR] Nigga tell me up front \n",
      "Step 383 - Generated: apl-. \n",
      "-Hiv, \n",
      ", ##, N, E, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, \n",
      "\n",
      "Validation step 384\n",
      "Memory Usage: 11.2% used. 228718.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228718.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 384 - Original: Are you strong enough to surrender?  \n",
      "Step 384 - Generated: ://[USR] Nigga what?  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 384 - Original: [USR] [USR] Don &amp; Melanie - The White Trash version \n",
      "Step 384 - Generated: chet'suka & Trizes & Fogtra & Mon & Frhic &Mon &Fog &Tricc &G&Tric &Fdeg &Trfic &Frione &Tss &Fq &Ssis &Fnd &Trsm &Fdg &Trsm &Frg &Trsm &Fg &Trsm &Fj &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm &Fw &Trsm\n",
      "\n",
      "Validation step 385\n",
      "Memory Usage: 11.2% used. 228683.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228683.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 385 - Original: yea suck my dick faggot \n",
      "Step 385 - Generated: jetaanova 2125umana 2123rghan 2123rghan 2123rucht 2123rcht 2123ruth 2123ruchiv 2123ruga\"2123rugi\".\n",
      "2123ruggina\".2123ruggy\".2123rugas\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123ruga\".2123r\n",
      "\n",
      "Step 385 - Original: [USR] [USR] Lmao my nigga \n",
      "Step 385 - Generated: izes/Hw/1/2/3/4/5/6/7/8/9/10/12/13/14/15/16/18/19/20/21/22/23/24/25/26/28/29/30/41/42/53/54/55/56/58/59/60/01/02/03/04/05/06/07/08/09/10/12/13/14/15/16/18/19/20/11/12/13/14/15/16/18/19/20/12/13/14/15/16/18/19/20/12/13/14/15/16/18/19/20/12/13/14/15/16/18/19/20/12/13/14/15/16/18/19/20/12/13/14/15/16/18/19/20/12/13/14/15/16/18/19/20/12/13/14/15/16/18/19/20/12/13/14/15/16/18/\n",
      "\n",
      "Validation step 386\n",
      "Memory Usage: 11.2% used. 228617.55MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228617.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 386 - Original: [USR] Amo No aguardo do hino you slay faggot \n",
      "Step 386 - Generated: ://[USR] you're a cunt  #TheChase  #ITV  #ITV2  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR] \n",
      "\n",
      "Step 386 - Original: [USR] Probably when he's on stage... What a TWAT... \n",
      "Step 386 - Generated: ://[USR] [USR] Nigga I thought he was talking about me  😂😂😂  “I’m not your friend”  My reaction  😂😂😂  Lmao  He’s a fool  😂😂😂  That’s why you my nigga  🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣�\n",
      "\n",
      "Validation step 387\n",
      "Memory Usage: 11.2% used. 228723.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228723.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 387 - Original: They tryna drain a young nigga of this energy \n",
      "Step 387 - Generated: 386 Collins draft. the 1/ or q/. time [15/10/36/20/2/3/11/5/12/22/30/13/40/32/52/55/14/65/12/35/56/12/37/53/46/48/12/57/38/51/12/60/18/120/24/16/12/50/12/43/12/27/12/80/12/42/25/12/12/41/21/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/\n",
      "\n",
      "Step 387 - Original: Faith should be a personal experience, not a tool used to twat people you don't agree with over their heads. \n",
      "Step 387 - Generated:  Golf -> next: ####/ 1. ## or the traffic /21) ####/. 10/Next\n",
      "\n",
      "## �11/25.988-202.23/20.�he in|Coating ›30.27/22. '28/46.43.02.47/36.48.2.67/56.24.45.42.13/34.04.35.26.38.53/65.29.55.12/33.06.52.86.14.5.50.41.22.05.66.43.22.08.32.83.46.22.85.40.27.51.8.62.16.44.91.22.57.93.020.31.46.54.92.022.58.94.22.266.98.15.59.42.28.68.9.37.22.84.17.23.667.22.46.49.22.81.07.30.43.22.688.22.46.7.30.90.22.222.43.22.88.45.22.050.22.63\n",
      "\n",
      "Validation step 388\n",
      "Memory Usage: 11.2% used. 228721.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228721.38MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 388 - Original: he wanted to call him a nigger so bad \n",
      "Step 388 - Generated: 585 or next the next 2...\n",
      " next's next's next's next's next\n",
      "4/4...\n",
      "Next's next's ####​V 3/4 4/4 4/4'4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4/4\n",
      "\n",
      "Step 388 - Original: What an absolute cunt. \n",
      "Step 388 - Generated: 370/20| or 1/2?\n",
      "25/12/\n",
      "22/23/10/11/13/14/21/19/20015/20/25/21/06*20\n",
      "22/24(12)22/26/30%212522/25-20/2241841/2521@21/22.\n",
      "23/2522?22/2522/22'\n",
      "2522/22~2522/2522\"\n",
      "22/252222/2522\"22/2522**22/2522:22/2522/2522/2522]22/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522/2522\n",
      "\n",
      "Validation step 389\n",
      "Memory Usage: 11.2% used. 228706.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228706.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 389 - Original: NOW I KNOW THIS RACIST ASS NIGGA DID NOT LIKE THAT TWEET LMAOOO RANDY GOT RACIST SUPPORTING HIM LMFAOOOOO \n",
      "Step 389 - Generated: .hxxisha Tking Golf or Qilling Golisu. Kiva, 'Tend King Glog or Qil Buka(11) the last golf or 4Qis'('Togg a key or Lisha log or Qils'### real or Gottisha's bet or Qisha's best second or Quruisha's event or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or Qisha's last or\n",
      "\n",
      "Step 389 - Original: [USR] U fucking dickhead!! [USR] smash this twat up soon please \n",
      "Step 389 - Generated: .MixedRealityICE orORTICE(ICE####:Occice/ortces':('SICE'ICE###-CTICELE--BICE(CICECACHE')\n",
      "SORICE(TICECEBurnICE[]ICEICE[CrICE []ICE ###ICE secondICE\n",
      "CICEMiddleHICE(RICEICECHICEMinICE(MICE)ICECIICERowICEICELICEGICEICEChICEMICEIICEDICERICEICE(LICEWICECharICEICEURLICEREGICEICEICE(WICEICENewCourtICEICEICE(GICEICEICE[/ICECoICEICEPICEICEICEUKICEICEICE(ICEICEICE):ICEICE ##ICEICEJICE(orICEICEICEICE(ICEICEICEKICEICEICE([ICEICEICECOICEICEICEICE(IICEICEICE')ICEICEICESecondICEICEICE(ICEICEICEICEICEOrICEICEClICEICEICE(ICEICEICEICEICEICENEWICEICEICE(ICEICEICE():ICEICE?ICEICEICEICE(?ICEICEICE #####ICE(ICEICEICEICE/orICEICEICE(ICEICEICEICEICEICEICEICEICE(/ICEICEICEICE(OICEICEICEICE(EICEICEICEICEBritishICEICEICEICEICE\n",
      "\n",
      "Validation step 390\n",
      "Memory Usage: 11.2% used. 228663.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228663.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 390 - Original: Mocks someones dead dad then hides behind a padlock. What a horrible clapped cunt \n",
      "Step 390 - Generated: ://[USR] [USR] Nigga what  🤦🏾‍♂️  I’m out here like  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 390 - Original: #Brexit twat Liam Fox: random screengrabs thread... \n",
      "Step 390 - Generated: 585, the a. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of. Of.\n",
      "\n",
      "Validation step 391\n",
      "Memory Usage: 11.2% used. 228681.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228681.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 391 - Original: ROCK JIN= NIGGER \n",
      "Step 391 - Generated: 5851 or Double Bet /aska or H2. Kestone or Betash or Bi Monappa or Tapa or Bet HK 1 or ZHV Bild or Gottiva or Bonhotta or ZVT or Kestone or Ct or Covid or HVBi or Holden or Qresa or Kestone or VT or Keta orＹBK or GT or Yonn or Hoffebi or MLE or GOTAPP or PS Scholarship or BK orVT or Kestone or Honda or Bosch or Gottla or Car or Kestone orVT or Kestone or Boto or Gasket or Hobby or ZVT or Kestone orVT or Kestone orVT or Kestone or ZVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone orVT or Kestone or\n",
      "\n",
      "Step 391 - Original: When your political/anti-SJW commentary gets more likes and RTs than the weeb shiet you post \n",
      "Step 391 - Generated: ottes./ 3/2/5/22/1/11/23/1/21/31/2/11/22/1/12/1/22/1/13/24/1/14/22/1/26/1/4/2/1/24/1/27/2/1/24/1/34/1/24/1/29/2/1/24/1/29/2/1/24/1/2/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1/24/1\n",
      "\n",
      "Validation step 392\n",
      "Memory Usage: 11.2% used. 228650.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228650.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 392 - Original: [USR] You really want Google's help? You retarded nigga. \n",
      "Step 392 - Generated: 191 or a 2.20 the Chapter in.\n",
      "5(15)30(H)22(40)24(46)25(41)29(46)21(48)25(42)59(46)23(45)26(47)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25(46)25\n",
      "\n",
      "Step 392 - Original: Took a white trash picture next to a graffiti wall \n",
      "Step 392 - Generated: ://[USR] Nigga please  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 393\n",
      "Memory Usage: 11.2% used. 228644.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228644.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 393 - Original: 3 hours of sleep has me in a retarded mood. \n",
      "Step 393 - Generated: ://[USR] [USR] You’re so fucking retarded.  I don’t even know what you’re talking about anymore.  Goodbye.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 393 - Original: [USR] Awww , look at this feminazi \n",
      "Step 393 - Generated: �n the [Ble (C) [... Vanda 3/5-30.4&2D or \"Hana\" |...\n",
      "(24 Blair) - or (43) &amp!�orno​IV|“Fragging” (45)...\n",
      "### ​Friska (42) or (25)...or (37) or (36)«bolic (40) or (27)–effrizing (35) or (33) [&log] […]\n",
      "'bolanz (38) or (26) Choster (45) or (21) [...] or (29) [\"Glicking\"] (37) or (46) (12) ##volina (48) or (32)→bing (45) or (36) bisping (45) or (36) binding (45) or (36) bolz--villing (45) or (36) \\\\predice (36) or (36) bindiv�binding (45) or (36) Hughes (45) or (36) (36) or (36) (36) or (36) betidocriva (36) or (36) flidarby (36\n",
      "\n",
      "Validation step 394\n",
      "Memory Usage: 11.2% used. 228752.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228752.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 394 - Original: [USR] [USR] [USR] Lmfao if u gonna screen shoot don't delete messeges faggot \n",
      "Step 394 - Generated: 43320.41424341422240453238304122433502233612212519316181310151411314122354817047126371302284134221613TGOasca132241132203###1322081322201221322061322 #####221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213221322132213\n",
      "\n",
      "Step 394 - Original: If a sjw vomited out skittles, starburst and other sjws recently eaten \n",
      "Step 394 - Generated: itta. Kols day next 2/ or last she sun best HKisha Globol /shol Kengow T Shascal Q3h the main shilling Kna MKkq4th key min/wot perfectz holic days single na fewKlogTQ1,Shung Kottes-48minWt/Gmnextmk22GwukOthr 21:10/ThrkingCsoMarkHuzlNextSoWeSheleDrtheMonGlobalCast-OlekTrongDouble@IschushorConfningGOreshDaskEngRNewDay16oRegNaKisCholSunBolcaRocketSoftClassOlweLshdoubleWorkMinVuhTaConTheyLogSolecTheSimkolGlakSholCoThrnewManvishAppSholCaKsholProKisuNmaxYouKolPholLastSholDemoTodayChapterMKdologKISukeKolInsNoElesIrogKasOneMotGolSholKachSholLockingIdAsheolKolSholKolHosttingCasOthenDeuschKolSingleKuvNowGroupMolCOeResolMyKolBusinessInFrontSOresol\n",
      "\n",
      "Validation step 395\n",
      "Memory Usage: 11.2% used. 228714.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228714.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 395 - Original: my homophobic/transphobic aunt is fucking a dyke now pass it on \n",
      "Step 395 - Generated: isha San or the Shapping analogy or Kanda Weber or a...1...\n",
      "ocker or Go 1/10 or Glbo.1/10 or L@1/10 or Gbota1/1/10 or Krana GO 1/1/10 or R'1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n",
      "\n",
      "Step 395 - Original: [USR] Aye nigga..ima kill you \n",
      "Step 395 - Generated: 化 or in round's court with the face or night- or tomorrow today...\n",
      "  or B at the time of the Drive/ or a different front videos Tuka...\n",
      " or G drive( or Rizer Ct or Golf in the rest or waterizes K or traffic per job in the global or face in the middle(...)\n",
      " focus or theizace mask in the few or the round face in the key or the round face in the center or the Oround'in the face or the hand in the second or the round face in the chat or the body in the face or the face in the top or the round face in the best or the round face or the round face in the possible or the CT or the face in the war or the day in the group or the face in the round or the face in the view or the face in the round or the face in the ## or the face in the or the face or the simple or the dirty face in the hole or the round or the face in the shop or the face in the ( weather or the face in the round or the face in the round or the face in the new or the round or the bush or the face in the round or the the face in the round or the face in the distance or the\n",
      "\n",
      "Validation step 396\n",
      "Memory Usage: 11.2% used. 228736.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228736.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 396 - Original: why do nonlesbians think they can say dyke \n",
      "Step 396 - Generated: 化./shesmluru Bettshesmlere/ffiseshae Glappa 3/ Biolshae 'kriegassmlendor/Eilsheshae'smlKFischbinghe/Nmisure/11 Kisha...\n",
      "Eidsheshae'smluka/255 Eplshesmluing/255 &/255 Eiplshesmluring/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 Eplshesmling/255 E\n",
      "\n",
      "Step 396 - Original: I can’t stand that “I ain’t gay no more” weird nigga \n",
      "Step 396 - Generated: ://[USR] [USR] Nigga what?  😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭�\n",
      "\n",
      "Validation step 397\n",
      "Memory Usage: 11.2% used. 228740.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228740.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 397 - Original: [USR] You envy this shit? Lol. 🤣😂🤣😂 I don’t use the word retarded much but you are severely retarded. \n",
      "Step 397 - Generated: 988 hand. a few of the our 22. to the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand/ in the next of the hand\n",
      "\n",
      "Step 397 - Original: Started gay. Ended a faggot. \n",
      "Step 397 - Generated:  orNA you're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You're more than 30 people.  You\n",
      "\n",
      "Validation step 398\n",
      "Memory Usage: 11.2% used. 228717.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228717.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 398 - Original: bored and horny  lemme go hit this nigger up again \n",
      "Step 398 - Generated: ://[USR] [USR] [USR] Nigga you know I don’t like that shit  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 398 - Original: But know if a nigga lose me then nigga, ooh wee, that’s his loss \n",
      "Step 398 - Generated: isha 1005 the man’s 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the man's 100 the\n",
      "\n",
      "Validation step 399\n",
      "Memory Usage: 11.2% used. 228808.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228808.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 399 - Original: When you already nailed to a cross and a nigga yell \" PUT A THORN CROWN ON THAT NIGGA\" \n",
      "Step 399 - Generated: ung or firstbing �ating 3-1'sking/�before the later week �drilling 1/20kheading 1/20rtheinfrishing 1/20Kowlawerebowing 1/20mliving 1/20kownewing 1/20Wenching 1/20Gwanda(1/20)1/20Beting 1/20KowNewing 1/20HILbetting 1/20Kownewing 1/20KowBetting 1/20Kownewing 1/20KowNEWing 1/20Kownewing 1/20Kownewing 1/20Lying 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20Kownewing 1/20\n",
      "\n",
      "Step 399 - Original: pisay evc's victim blaming card \n",
      "Step 399 - Generated: 445\\ time: '20/####**: play. **30wisha': Play. 23**: '20'.**10**: '20**: '21**: '22**: '18**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**: '20**:\n",
      "\n",
      "Validation step 400\n",
      "Memory Usage: 11.2% used. 228781.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228781.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 400 - Original: this cunt has the audacity!!! \n",
      "Step 400 - Generated: ://[USR] [USR] Nigga...  I love you too.  But it’s not the same.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 400 - Original: white trash Lara Croft \n",
      "Step 400 - Generated: isha or shking's 3/”\n",
      " Golic 'r']\n",
      " Q2/”Glicking\" 3/4\"\n",
      "1/22/21/30/12/22/24/25/33/46/52/34/26/42/36/22/24/26/32/23/31/22/24/26/22/24/48/5/38/40/50/22/24/26/24/16/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/22/24/26/\n",
      "\n",
      "Validation step 401\n",
      "Memory Usage: 11.2% used. 228770.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228770.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 401 - Original: WATCH: CROWDER CONFRONTS: Slandering SJW PROFESSOR! | Louder With Crowder  \n",
      "Step 401 - Generated: ator or rob Collins, Bet. Scha**: Shiluka: **Shil****: 11|####**: Shil****: Bet/\":**15****:**22****:**21****:**14****:**32****:**12****:**30****:**:**23****:**:**20****:**:**25****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**30****:**:**\n",
      "\n",
      "Step 401 - Original: [USR] Hot fries Nigga \n",
      "Step 401 - Generated: achi  Kask a Gl tomorrow. B1K2k of Buddy 5 11G 4 1B 3 12 14 8 1 6 13 1 10 1 1 1 5 1 36 1 5 1 1 4 1 2 1 1 2 1 1 2 1 1 2 1 1 1 2 1 1 1 2 1 1 2 1 1 1 2 1 1 2 1 1 2 1 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1 2 1 1\n",
      "\n",
      "Validation step 402\n",
      "Memory Usage: 11.2% used. 228807.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228807.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 402 - Original: [USR] She went full-on libTard...  #DeportThemAll #ConfirmKavanaughNow \n",
      "Step 402 - Generated: énomb,shsonf,no_shiving,bigbolso/shyticevbffshivbi)orshimficShybitBshivebffshivalbshyrIVgshiviGshyzintbshinyClbingSh(yinfitShybillmoninfShybil):tSimbuzminSBOshylbshiyTsimbshisBigbshyMonintbshY(Shy).|shytice(gshilbshyInShyticeGshy(Bshy))\\\\shictQbshy('sh')gsbshfy(QbshyIntbshy(Gshy)\"\"\"\n",
      "shidKshividshnewshIsbshy(Smshbshy)\",\"sh\"bshy(Pbshy)\":\"sh\"bshy((sh)\"\"\"sh」Cshy(bshy)\"\"\".shibshy)(sh\"\"s)„shicshy),“sh”bshy(Cshy)→sh(Shy)�sh(Me)sh(Osh)\\\"sh(?:sh)\\\\sh(Y)sh(Sh)\\\\sh(Gsh)«sh\n",
      "\n",
      "Step 402 - Original: “Last nigga played with Durkio, he in a backwood” \n",
      "Step 402 - Generated: jayanda the next 【next ###### ##### ####newle /Next **incticeTruncning 1/##Suprishing aIccinaqInmceRshilling 1/5YrtuneThe Williamson 4A Icinturing 2/15**PreleShing 4A Icing 4A The YrowFishing 4A Icturing 4A The YROW 4A Icturing 4A the YRow 4A Icturing 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the Yrow 4A the\n",
      "\n",
      "Validation step 403\n",
      "Memory Usage: 11.2% used. 228784.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228784.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 403 - Original: The unicorn must’ve been retarded  \n",
      "Step 403 - Generated: isha/ bet (  Sanchez(‘'  ' '' 1' 'G' 'S'”' ^{'('G' –\"’F' G'(\"'G'‌\"“'G'Fe'G'WF'G'FI'G'SF'G'2'G'G'O'G'G'R'PS'G'RA'G'G'FS'G'G'P'G'G'G'Gas'G'G'G'G'FG'G'G'GF'G'G'G'G'G'G'G'G'G'G'G'G'Q'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'G'\n",
      "\n",
      "Step 403 - Original: [USR] Damn nigga. I want that big ass anaconda dick  stick that big dick inside my ASSPUSSY 🍌🍑💦🥜😘 \n",
      "Step 403 - Generated: .log hand's the second day:21 33.1/2022,23 11,112,12,15,16,20,25,32,52,53,43,50,51,52,53,45,52,53,46,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53,52,53\n",
      "\n",
      "Validation step 404\n",
      "Memory Usage: 11.2% used. 228775.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228775.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 404 - Original: [USR] You ain’t filmin nigga \n",
      "#### ####KICKal**:Nuda #####Billuka####Chablutilla:####myVTla)@YLL/####Vokil##Novat). ####“Zana­?####|#####G1):y!OR \n",
      "####the”plja-Nolina\n",
      "####Joda####weLctan>####VRjudma)####CTollin#######Warbase ######Tvyllapa####Male**Belouvtilda####AWEplus@####Swindappsluna####OKkila+####QodINETPosted wloctia_Nylisot¬####Wedlamorado########Rigalla)|onwiwa####Idea-Vmilling@####WomeVislepenocwatiga\"####Wecklatuflsonvcta####VotesAIya Nakortbuda####UQUttlbecklana####Vpathebneta####Ofvkcelevespnate VicsekOLLwayndicates####VorbettemlangVKorkinnitates#####VodaChatlanaBettingof VKtobmonapdatAuthorBase-TolkatkBillvoteCoqlikscakVote.Ctmidsh\n",
      "\n",
      "Step 404 - Original: You fucking cunt \n",
      "Step 404 - Generated:  {}: \\ **\\**5**: 3/10**20**4521**41**22**23**25**26**28**29**48**24**12**14**15**25**51**43**13**19**30**05**15**31**33**15**25**16**34**14**15**25**32**14**15**25**25**25**50**55**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**25**\n",
      "\n",
      "Validation step 405\n",
      "Memory Usage: 11.2% used. 228742.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228743.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 405 - Original: If you nigga got any it these hairstyles/haircuts he’s cheating on you 💯🤦🏽‍♀️🤷🏽‍♀️ PERIODTT \n",
      "Step 405 - Generated: aguesashmindazm().\n",
      " 1. K(C) (e)**((**))**s(m)()**(**)g(()**)**(3(K)!)**(2)!()/**(201)!)**(**(22)!)**!**(**(62)!)**(**(52)!)**(**(61)!)**(**(56)!)****(**(82)!)****(**(62)!)**(**(32)!)****(**(72)!)****(**(62)!)**(**(62)!)****(**(62)!)**(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(62)!)****(**(\n",
      "\n",
      "Step 405 - Original: 1998 dyke march \n",
      "Step 405 - Generated: 476-12/25h/91h/92/75h/98/65h/95h/35h/49h/55h/48h/24h/45h/48h/49h/48h/49h/48h/49h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48h/48\n",
      "\n",
      "Validation step 406\n",
      "Memory Usage: 11.2% used. 228728.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228728.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 406 - Original: Wait she said Nigga #LHHReunion \n",
      "Step 406 - Generated:  fri #####...\n",
      "(42)4**(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(2)4(40)4(40)4(2)4(40)4(40)4(40)4(40)4(40)4(40)4(40)4(40)4(40)4(40)\n",
      "\n",
      "Step 406 - Original: [USR] You iz #GrammarNazi + #Feminazi \n",
      "Step 406 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] Nigga is you crazy  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 407\n",
      "Memory Usage: 11.2% used. 228775.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228775.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 407 - Original: Thick atl nigga \n",
      "Step 407 - Generated: /internalkol (Shkking' ** 'Killing' ** 'Bunkking' ** 'Cowing' ** 'Flicking' ** 'Flishing' ** 'Raising' ** 'Gashping' ** 'Hectering' ** 'Hunting' ** 'Hanzking' ** 'Hunching' ** 'Hyrking' ** 'Horing' ** 'Hilking' ** 'Hiking' ** 'Hilda' ** 'Hilling' ** 'Homing' ** 'Hiding' ** 'Hiving' ** 'Hlking' ** 'Hung' ** 'Hunding' ** 'Hating' ** 'Hangling' ** 'Huning' ** 'Hwning' ** 'Hewing' ** 'Haking' ** 'Huler' ** 'Hleining' ** 'Holling' ** 'Hoking' ** 'Haming' ** 'Hurring' ** 'H** 'Hilling' ** 'Holding' ** 'H**: ** 'Heng' ** 'HVK' ** 'H': ** 'HVR' ** 'HASC' ** 'HCT' ** 'H**\n",
      "** 'HCL' ** 'HVT' **\n",
      "\n",
      "Step 407 - Original: My place exudes faggot. The Bobby Berk jumped out of me this weekend. \n",
      "Step 407 - Generated: ://[USR] [USR] [USR] Nigga what?  You know he’s not.  He’s just playing the victim card.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 408\n",
      "Memory Usage: 11.2% used. 228748.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228748.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 408 - Original: When you just wanted to get hammered but some twat brings up brexit or trump \n",
      "Step 408 - Generated: ator 22 Shiva 01 z05asaki 106/2302021600270212123153503254043104421418100120071221121412129100012014815012411140621mon120014521253156220053114622214712820212554520213812311261R14311442021241421Sh362021Z1021**S2021z221262061Kr162661E2021K59722021C202191122312007199202211210001De2021...\n",
      "442021Sc26212021@2501 Koch2021kr26020211Eng20212021L2022V2021&2021G20221\"CO61621'20221i20221 20221M20221202121v2022210421202121B202121Co12022121CR20221218220221T20221221\n",
      "\n",
      "Step 408 - Original: The Coldest nigga The Purge franchise ever seen \n",
      "Step 408 - Generated: idor- the. 10. 5. 4. 5. 6. 5. 5. 0. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(5) 5(\n",
      "\n",
      "Validation step 409\n",
      "Memory Usage: 11.2% used. 228737.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228737.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 409 - Original: [USR] [USR] [USR] Awe. Good who gives a shit! This dude is just a straight retard. \n",
      "Step 409 - Generated: ishop prep Ben B freple...\n",
      " B Burk Glinger B K Bet Business B or Money B or Money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or money B or\n",
      "\n",
      "Step 409 - Original: [USR] idk i just woke up but loOK AT THIS LIL RETARDED BLOB \n",
      "Step 409 - Generated: acco or 'ncldice the...\n",
      "ftngqshw 'k1/2minm 'bldnice 'h2dnewrctbildmn '24ltemplice 'hg2k1#4k2k1##20cun2k1h2k1z2k1h2k1w2k1h2k1w2k1h2k1w2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1w2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1w2k1h2k1h2k1w2k1h2k1w2k1h2k1h2k1w2k1h2k1w2k1h2k\n",
      "\n",
      "Validation step 410\n",
      "Memory Usage: 11.2% used. 228772.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228772.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 410 - Original: Young Weezy F. Baby, hottest nigga on the blockhead \n",
      "Step 410 - Generated:  Chapnew newview 53hisha(Millr203bnheclHill).Bdrle10'35bshl('11')23mhll2'33Dchmin12'13'41'22-161'43mint's24'2470'21'15'14'25'32'2439'26'83'1161'2481'17mlor (143)31'2136 'Newre 42*1241'2081'27**(1151**2421 1421 1141 1251 2001 1221 1261 1231 1201 1271 1211 1281 2231 1361 1131 2161 2051 2601 2331 2061 1301 2121 2381 1861 2361 2141 2411 1731 2181 2341 2611 2221 2351 1611 1631 2321 1641 2111 2231 1871 1881 1711 1321 1121 1911 2231 611 1921 149\n",
      "\n",
      "Step 410 - Original: Via:  Gracious latina MILF Francesca Le with big tits exposes her wet cunt... \n",
      "Step 410 - Generated: .logask Shbing/ Regmshlog/Shing@Glausch/Fb-QLs/MyLsky/Hblorbi/JobMyrld**:MyGLanda/LogT**Fbet\"Middle\":Godmfrilling@Workglolsmafrbit:Frilot&Mjrglper/lrosben@Qglim/Q\"\n",
      "####/Manicslagflqmywict@Sunnyapps/Flrlekgrmsplperf@Kinnproxy/Lintperfect@Rmkpluslarhapsaplmonkeybarng@kmezhab\"Mayolfung@KintPLer\"  ##Idlepingmann@Glorlac\" (Masterlam@P\") 5-Lipwork@Gloqlbin@Qbigm@LingoMigration@Guruspect@NQRmlilim@GLOctmigration@Gbolim@LKRminlub@Glmuser@Purasil@Globmappy@LNG@GOLmating@GMLobi@GLMion@GOLL@Perfect@Grim@Lumgin@Gmol@Guru@Lmm@Goring@Lmig@Gusr@Lmister@Glos\n",
      "\n",
      "Validation step 411\n",
      "Memory Usage: 11.2% used. 228763.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228763.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 411 - Original: Mentally retarded [USR]  to funny I don’t think you put your reputation on the line like this mr [USR] \n",
      "Step 411 - Generated: ale's'anda 41,2,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,11,22,3,4,5,6,7,\n",
      "\n",
      "Step 411 - Original: [USR] [USR] DRINKIN ON LIQUOR, AND I'M LOOKIN AT YOUR NIGGA \n",
      "Step 411 - Generated: ungolivivre (kolpliv/flowmillingwives)\n",
      "Kappa'smoliviver'ing Kuka'smalivivongrival, 5/1001vinekolivineta, 5/1001vinekoliviviva, 5/1001vinekolivivata, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5/1001vinekoliviveta, 5\n",
      "\n",
      "Validation step 412\n",
      "Memory Usage: 11.2% used. 228797.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228797.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 412 - Original: [USR] [USR] Dt is an embarrassment cunt! \n",
      "Step 412 - Generated: ittal202/202/.202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/202/\n",
      "\n",
      "Step 412 - Original: Who prejudice white trash is this loitering on my page \n",
      "Step 412 - Generated: ishaukauka1uka &ukauka/Iukaanda:ukaillingLukauka&Kukaukauka.ukauka/ukaukauka'LockukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaIukaukaukaukaukaukaukailukaukaukaukaukaukaukauka**:ukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaMonlukaukaukaAprukaukaukaukauka**ukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaQLukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukauchilaukaukaukaukaukaukaukaukaukaukaukaukaukaukauka:::kukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukaukauka\n",
      "\n",
      "Validation step 413\n",
      "Memory Usage: 11.2% used. 228715.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228715.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 413 - Original: If this nigga don’t get out of my face \n",
      "Step 413 - Generated: 20appa, 1-10h/ 3* 0/ 12* 14/ 12* 14/ 12* 14/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/ 12* 12/\n",
      "\n",
      "Step 413 - Original: I hate when my weedman calls me “bro” when I’m a whole faggot \n",
      "Step 413 - Generated: asca. tomorrow or week nextmon...\n",
      "/ last the bet Glk​ 2 sand /king... 20 in a ### 5/15K 11-1 Kurumal 10 23/...\n",
      "the 14 or 21 3 or 30 Betsek or 25 12NG or 16+13 22 or 36 26 '32 42 or 27 33 28 8 24 4 37 35 93 or 40 31 34 29 43 6 18 15 9 20 19 266 38 15 41 17 20 7 52 26 202 13 20 65 20 261 84 20 255 04 260 20 247 120 15 46 20 83 410 15 20 72 20 15 20 20 996 26 47 200 53 20 15 20 48 20 800 20 246 20 15 20 230 26 80 15 20 204 26 55 20 240 26 20 20 310 20 \n",
      "\n",
      "Validation step 414\n",
      "Memory Usage: 11.2% used. 228712.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228712.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 414 - Original: [USR] STOP the INVASION #BuildTheWall  #VoteRepublican \n",
      "Step 414 - Generated: isha/icc3vict 5cc/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c/ivict 5c\n",
      "\n",
      "Step 414 - Original: [USR] 😂😂😂😂 what you just got out of juvie hoe ass nigga \n",
      "21kYGilotinNKytoichiaskjall-15yazikatanISKyutihlekashictasoninghogasaki**:14h20ciastingASrndaininghayaszki本chowingKitzappylogenMonickey CtASCzillingQRsekabitzoringmonischandaSrngtilmaqrobic27423zgyrakrok###22KzortintISCalogzior18gillshungicitlockzandal43z．42KYzinning.CtascoZlipage22ASYKirctadoanassingCTRlozosterla24##tecahstICE13####\n",
      "36bkYzrtice26zlduikit@ esacz/logginglr2zlk46~ROCkitz5623zgtillin.Globalization16zoda'smiddleBK26638zregliniscalnez22Qobiakin\"\"\"\n",
      "22zrocastle22zinp\u0003tichlama22zotic58522zmkowlingQTbolaisy22zappa\n",
      "\n",
      "Validation step 415\n",
      "Memory Usage: 11.2% used. 228744.77MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228744.77MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 415 - Original: [USR] [USR] 😂😂😂15000 really you thick twat \n",
      "Step 415 - Generated: /. 201 /  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  –  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  •  • \n",
      "\n",
      "Step 415 - Original: [USR] [USR] Don’t say tut like that it makes u look like a twat (picture unrelated) \n",
      "Step 415 - Generated: askomba Lina.##mol'.####omin'###Lina (OKma')'). 1/5ciorgol](Q2)]Drshnew(3)4m']) 3/4m\"\"Shling (4m)\"Goodome(4m))4m\"\"Shing(4m)**4m\"\"Shling(4m)\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"4m\"\"\n",
      "\n",
      "Validation step 416\n",
      "Memory Usage: 11.2% used. 228759.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228759.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 416 - Original: [USR] Nothing worse than \"White Trash\" except when it speaks! \n",
      "Step 416 - Generated: pesigz/shappila-/rischerGci-**\n",
      "theShaterifbei(**t)-NextB oderallelAssT-O('S'-interh**at**:Horce-Chas Ct**(P-)Asccinhe(['A])\n",
      "C-'We(202)India)\n",
      "Chata orminolces'CT\"-Interjolo\n",
      "In bei-(O)\"Ruell\"Mesh-T(O\")\n",
      "I\"\n",
      "Mpossible(Lipes)**Bet-ct, 'E-Hilgu-MASC-Cipla- CT–Lip(Cmpl)\"\n",
      "You(Tcsmy)\")\n",
      "Te-Becca-CT(Meta|Preiss')\n",
      "##Ce-CCle-P':Fasc-1'O(Gy):Please-2Pl(\"C\")Emb-12:Me-Ci-16-11-22-13-5-14-20-23-15-21-12-25-18-3Csperipe-16--Co-16-12-20-22-30-45-12-17-22-19-22-12-22-16-22-22-12-22-22-22-16-22-22-22-22-22-22-22-22\n",
      "\n",
      "Step 416 - Original: 🍏Limey🥝 \n",
      "Step 416 - Generated: 415.\n",
      "-                . . . /. ./. . ./. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Validation step 417\n",
      "Memory Usage: 11.2% used. 228768.56MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228768.56MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 417 - Original: [USR] [USR] How dare you say that about the white trash people's  King \n",
      "Step 417 - Generated:  catch orma, workmin/##adenthiga/–Workshill'###/Hortis/Coctwrok('**####/Thowiska\"**\n",
      "###/Cothebik/Wezil(**/Or\"\"\"\n",
      "###/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewig/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothewiga/Cothew\n",
      "\n",
      "Step 417 - Original: [USR] nigga I got 4 pillows but I only use 1 \n",
      "Step 417 - Generated: usch or the ####/Iduka/Fanda Drill ####/OrtheGumar(India/##Hle/Debook(T/1/2\")\n",
      "Kimes (T/5/13/200/ML/ #####/Reg(Gid/202/11/460/20/####/45/23/10/22/46/65/92/56/72/53/266/84/62/12/36/172/88/255/26/47/52/800/55/42/83/248/560/93/220/#####/38/350/37/242/188/260/82/54/70/222/688/300/25/72/48/247/265/812/24/ERP/235/400/85/02/236/340/246/270/316/240/360/274/90/238/quete/40/196/380/192/284/590/336/194/250/337/100/372/290/245/278/200/241/266/298/700/59/266/130/248/550/266/784/600/266/830/266/198\n",
      "\n",
      "Validation step 418\n",
      "Memory Usage: 11.2% used. 228717.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228717.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 418 - Original: A tall nigger \n",
      "Step 418 - Generated: ://[USR] [USR] [USR] Nigga what?  I’m not even talking to you.  You can’t be serious.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 418 - Original: [USR] Tell that to [USR]. She won’t stop calling me a dyke \n",
      "Step 418 - Generated: 2mnsh Levine updating aminuh>volking update theghm ()hmax'bilwashrbling(##)bipintahasfbnillin('killing')@imcipoill'mtasebullinferring(mcc)bilwashrgebnillin'Mtcebnivlastmechillin'mtcebnivalcsbitlomecrillin'mtcebnivglasqbilvnewinctbilazcyngprilcm'erlineCTXMEA(VCulin'Tceinzmalcredit**(n))squadbilzcartiuschlemod\"bilcoplzagbi(qo)]ascaburimalctxma<>(BKimon'tcequin)(TCSMJkrkulmon'tcequimsaskmhrcalembopreigomba(Omcegrmana]]**somispectCTLmingASKidotceingopskitmancheboulek(KIM)secumingKCCipimasoltceepingmotallongposcilimitoringKTQcebelmina(**kceping\"))tcegoningKCFToppingmanderechinowingkitsimilar(RDipeclsortingMHkadroklim\")BIDoplcurasingNHTOHcingdana(MKV\n",
      "\n",
      "Validation step 419\n",
      "Memory Usage: 11.2% used. 228703.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228703.48MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 419 - Original: Yo this gay nigga needa quit hitting me up fr fr. \n",
      "Step 419 - Generated: 994 Drill next or nice job the...\n",
      "bing oder next �-ngisha or nextasket 2 or mking K�-NG or BID next already or really busy chat or buggy [... or next tail or best new ingping or...\n",
      " next or ready sort top or next or betray or next or wellquete or chatting or bid ping or next or liska or next or another of either or [] or next or ### or next or [...] or next or worst or next or []\n",
      " or maybe or at least or chung or next or (255 or next or next or next or next or buddy or next or next or next or bury or next or next or next or next or next or next or plus or next or next or next or next or next or again or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next or next\n",
      "\n",
      "Step 419 - Original: [USR] nigga wut \n",
      " /wglating /23 /orngesh /bingzacht /19/ing /20/plus /21/14ft /Monning /22/17/20/11/10/16/26/25/29/12/20/18/30/24/22/15/20/20/28/35/20/20/20/22/20/19/20/20/22/20/20/20/22/20/20/20/20/22/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/20/\n",
      "\n",
      "Validation step 420\n",
      "Memory Usage: 11.2% used. 228699.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228699.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 420 - Original: #MegaMillions fucking redneck i mean .multi-millionare redneck Trump supporter 👻🤓 \n",
      "Step 420 - Generated: ://catch[USR]  #BuildTheWall  #MAGA  #KAG  #Qanon  #WalkAway  #BUILDT  #Patriot  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [\n",
      "\n",
      "Step 420 - Original: [USR] [USR] Nigga 😦 \n",
      "Step 420 - Generated: ionechip-“/” [“] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"] [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“\"]\" [“”**\" [“\"]\" [“”**\" [“”]**\" [“”**\" [“”**\" [“\n",
      "\n",
      "Validation step 421\n",
      "Memory Usage: 11.2% used. 228661.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228661.93MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 421 - Original: #BBCTW #BBCQT #SJW  wonder what STD She has ? \n",
      "Step 421 - Generated: andel hand the,. of\n",
      " to was large\n",
      " to some\n",
      "\"\n",
      "h today\n",
      "\"\n",
      "s 7 at\n",
      "\"\n",
      "I\" (S)\n",
      "\")\n",
      "##\n",
      "**\n",
      "(Sh)\n",
      "###\n",
      "Enh\n",
      "at\n",
      "**(\")\n",
      "H\"\n",
      "Till **\"\n",
      "We\"\n",
      "**\"\n",
      "Bet (K)**\n",
      "The man (S)**\n",
      "\"R\"(\"E\")\n",
      "High (S)**\n",
      "Bere (F)**\n",
      "You (Sup)**\n",
      "He (S)**\n",
      "My (S)**\n",
      "\"J\" (S)**\n",
      "\"Pr\" (S)**\n",
      "\"Israel\" (S)**\n",
      "\"Fe\" (S)**\n",
      "\"Europe\" (S)**\n",
      "\"Be\" (S)**\n",
      "\"Your\" (S)**\n",
      "\"Good\" (S)**\n",
      "\"Japan\" (S)**\n",
      "\"Easy\" (S)**\n",
      "\"El\" (S)**\n",
      "\"God\" (S)**\n",
      "\"Excellent\" (S)**\n",
      "\"Man\" (S)**\n",
      "\"Best\" (S)**\n",
      "\"Save\" (S)**\n",
      "\"Mal\" (S)**\n",
      "\"Her\" (S)**\n",
      "\"Very\" (S)**\n",
      "\"Personal\" (S)**\n",
      "\"Wh\" (S)**\n",
      "\"Additional\" (S)**\n",
      "\"White\"\n",
      "\n",
      "Step 421 - Original: Fans at the \"Redneck\" dress up game.  \n",
      "Step 421 - Generated: inel&und &Francana &Oja& nduka@1&BroncEvunceWhanaJudaoka&JoinwndlOKuncfrundingNationalJurNextGanzNationFrud_NUNDWeTrundIn201JaneNZNDJa202Australia#VictinemaZne↑14KurchNJNaOdJD+NewTorwinDnaOljunPvorBetNevoI'RotEvrWellYouA�InvuchSJRNovThrunjDemoRWin424LchinNVOftMoralJo\"AnchorTnovFranceNuVictoriaKr2JanFacultyOfJOvineSouthTurOnmalWomenViewinFwrOrganJustFebGeneUnbrodeTheseWhenUKDeVouchMalEcOurSouthern1913HeunchedMedicalOsIsrael194TwinnSunday171EUBookTheJOINlNorthAfricaTuesday13RussHesinDownloadThereunClinDevelopReportOakFridayUVRpurOneVideoResearchthrinedAustralian195bUniversityInsivantReportsolEuropeTaunsIf&EvinnewHorineVintraYkirJun198NEWVIDsAppCoalBiquoteWaterorDemIs'Oview-DonationsRUvedFKurOJJraudSomeInvoiceOCRIsraeliFemaleAsUniTuFiguresThenPrevolGenuThatWe20AprStroke\n",
      "\n",
      "Validation step 422\n",
      "Memory Usage: 11.2% used. 228803.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228803.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 422 - Original: fucking crusty god damn faggot😭 \n",
      "Step 422 - Generated: .hxx ( bestowing the \" new handing 1/ Qlik's or Golf!'Shipo'. sand Bqshil's or Russell\"!\"\n",
      "' shipo's or Rivalbing's or Kung's\"\n",
      "!\" or golf playson's or Gipo's or Zillin's\", or quru'sndzin's', or Qatar'sorple'srundib's, or\".Quru'sle'sobo'srondip'sbunn'sorbling'sgtwin'smo'spluning'srundipo'smlwning'sRuka'smilling'sBunny'sMo'sple'syrping'srundipo'sMuddy'srundra'smolging'sBroker'smo'sple'srune'smilting'spruning'srundipo'sple'srune'smisping'srundipo'sple'srundipo'sple'srune'smilling'srune'smo'sple'srune'smilling'srune'smo'sple'srune'smilling'srune'smo'sple'srune'smilling'srune'smo'sple'srune'smilling'srune'smo'sple'srune'smilling'sr\n",
      "\n",
      "Step 422 - Original: [USR] [USR] [USR] Relax, Hillbilly. \n",
      "Step 422 - Generated:  the updateodicuthiccoll...\n",
      "utazukauchl**shupeilukpl​Lzpatl ####PlaseLMPLpQl12PPLG2LPLPETLPELPLPPLT5LPLPLPATLPLPPLLPTLPLPLPPLPPLLPLPPLPLLPLPPLPPLLPLPPLLPLPPLLPLPPLPLLPLPLPPLPPLPLLPLPPLPLPPLLPLPPLPLPPLPLPPLLPLPPLPLPPLPLLPLPPLPLPPLLPLPPLPLPPLLPLPPLPPLLPLPPLPLPPLPLPPLPLPPLPLPPLPLPPLPLPPLPLPPLPLPPLPLPLPPLPLPLPPLPLPPLPLPPLPLPPLPLPPLPLPPLPLPPLPLPLPPLPLPPLPLPLPPLPLPLPPLPLPLPPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPLPL\n",
      "\n",
      "Validation step 423\n",
      "Memory Usage: 11.2% used. 228754.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228754.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 423 - Original: Any takers for this cunt so she stops pestering me \n",
      "Step 423 - Generated: lamaidders or_castkolhanaisha's key Bolton next topruba, Taší Gowana, Ben, Kanda,Blog,Top Kit,Log,Kasa,#### Tlak,Ben,King,Nextiola,Na kit,Behaviorola,Ta new,Kit,Bedona,Na guy,Pre kita,Boosting Koch,CTrak,Anaidal,Na Gala,Planishing KT,Brushing Moledo,Busy,Offline,'na Grafting mocha,###,Nowilot,Na gala,Molesbol,Na Rugby,Locking Kasta,Na Goa,Developing■,Naolo,Na Rlimale,Na Francis sermola,Naela,Naolin,Na Globningmoola,Na312,NaRobotlina,Naiva,NaFx,BrkingdTisu naresa,Dbolina,NaManaolk,NaIDsolingatan,NaInvoice,NaFrancmal【Newmolchio,Naforele,NaBroker,NaOnline,Naasca,Formula Broling,Naquate,Naδα,Na Tmax,Brooming,Naoggle,NaCredit,Na茂lama,NaDrimal,Na bola,Naismanolta,Na-Taappa,NaRealiza,Na-Bola\n",
      "\n",
      "Step 423 - Original: [USR] Nigga I don't know who you are but the name alone requires a follow \n",
      "Step 423 - Generated: ssel  “/“IRTsh /Read 10/25/5/50/My/ Вllo/1/30/5/15/20/8/ Read/ K/35/55/We/12/5/85/40/ B/22/10/5/5/65/25/10/10/45/10/5/28/10/ ER/53/10/25/5/10/50/10/25/10/50/10/10/25/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/10/50/10/25/10/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25/10/50/10/25\n",
      "\n",
      "Validation step 424\n",
      "Memory Usage: 11.2% used. 228741.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228741.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 424 - Original: What kind of hipster faggot is this \n",
      "Step 424 - Generated: ://[USR] Nigga was like 12 😂😂😂  I bet he thought it was a joke too 🤦🏾‍♂️  #LHHNY  #LHHNYReunion  #LHHNYReunion2  #LHHNYReunion3  #LHHNYReunion4  #LHHNYReunion5  #LHHNYReunion6  #LHHNYReunion7  #LHHNYReunion8  #LHHNYReunion9  #LHHNYReunion10  #LHHNYReunion11  #LHHNYReunion12  #LHHNYReunion13  #LHHNYReunion14  #LHHNYReunion15  #LHHNYReunion16  #LHHNYReunion17  #LHHNYReunion18  #LHHNYReunion19  #LHHNYReunion20  #LHHNYReunion21  #LHHNYReunion22  #LHHNYReunion23  #LHHNYReunion24  #LHHNYReunion25  #LHHNYReunion26  #LHHNY\n",
      "\n",
      "Step 424 - Original: [USR] [USR] Fuck you cunt. \n",
      "Step 424 - Generated: ong of in hand, and next hand, and next hand, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and so, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0, and0\n",
      "\n",
      "Validation step 425\n",
      "Memory Usage: 11.2% used. 228715.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228715.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 425 - Original: Sipping on retard juice \n",
      "Step 425 - Generated: ://[USR] Nigga said 1-2-3... 4-5-6... 7-8-9... 10-11-12... 13-14-15... 16-17-18... 19-20-21... 22-23-24... 25-26-27... 28-29-30... 31-32-33... 34-35-36... 37-38-39... 40-41-42... 43-44-45... 46-47-48... 49-50-51... 52-53-54... 55-56-57... 58-59-60... 61-62-63... 64-65-66... 67-68-69... 70-71-72... 73-74-75... 76-77-78... 79-80-81... 82-83-84... 85-86-87... 88-89-90... 91-92-93... 94-95-96... 97-98-99... 100-101-102... 103-104-105... \n",
      "\n",
      "Step 425 - Original: This Faggot [USR] using Ajax \n",
      "Step 425 - Generated: omap &amp; the next 2 years.  So why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do that?  Why we do\n",
      "\n",
      "Validation step 426\n",
      "Memory Usage: 11.2% used. 228680.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228680.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 426 - Original: Your a Fucking nigger get that in your head bitch. I did nothing wrong to you nigga \n",
      "Step 426 - Generated: isha-binding/'###/#### /T'/'.##/Ok/13/5/15/3/12/11/16/27/17/31/23/5/13/14/55/13/21/43/20/22/4/13/25/30/13/34/12/13/15/13/44/12/13/04/13/12/13/14/13/12/13/13/13/12/13/13/13/12/13/13/12/13/13/12/13/13/13/12/13/13/13/12/13/13/13/12/13/13/13/12/13/13/13/12/13/13/12/13/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/13/13/12/\n",
      "\n",
      "Step 426 - Original: [USR] [USR] [USR] [USR] [USR] [USR] Trailer trash \n",
      "Step 426 - Generated:  key or today of the end of the 20 of the 30 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of the 30 of the 40 of\n",
      "\n",
      "Validation step 427\n",
      "Memory Usage: 11.2% used. 228711.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228711.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 427 - Original: my crush listens to Hayley Kiyoko which means she’s a dyke but a dyke who listens to Hayley Kiyoko \n",
      "Step 427 - Generated: естиthe 2ndking the...\n",
      "Blekking the B/5khandis the B/5Limas the B/5K handliva the B/5msek the B/5 BK our today the B/5M Schi the B/5my |Sandle (B/5) |Sung prep /265in K |\n",
      "Sungbing|TVivre |Riving /265 in ##BK |Tomba |Shai###255ing |Iml ##### ###265 Betting Kokottes ######VR \\\\Na Levine |AI |Wngrest |266ivol |Bank/Kmn |Glast |####MV |Volmonking |Platwin |Zrakging |Goftliveprepilling |all284Vllgrade |MLtv |Gilwning |Reclogming |Kiang |BNGyuvcrestleague!plking attendshiporvolpreking(266)IVshaperlominvkumhighsandourbindingtanavisiblemaxatinghebelkingbilking2613fivebankkingberebozgalking260241sweumarigadminageb24027625422126#####26816322027RALVT/GIDBG10930026528106238316460011273\n",
      "\n",
      "Step 427 - Original: Taylor Swift BABE★COCK #babecock #cock #sissy #faggot #batecocktobabecock #TaylorSwift \n",
      "Step 427 - Generated: orphicWeglows, 2Laps, 3Loks, 4Glbs, 5KIs. 23Ylgshs: 11Shrival.\"  \"The second.\"\n",
      "MyGlblw\"  \"The first.\"\n",
      "MyGlblw\"  \"The second.\"\n",
      "MyGlblw\"  \"The first.\"\n",
      "MyGlblw\"  \"The second.\"\n",
      "MyGlblw\"  \"The first.\"\n",
      "MyGlblw\"  \"The second.\"\n",
      "MyGlblw\"  \"The first.\"\n",
      "MyGlblw\"  \"The second\"\n",
      "MyGlblw\"  \"The first\"\n",
      "MyGlblw\"  \"The second\"\n",
      "MyGlblw\"  \"The first\"\n",
      "MyGlblw\"  \"The second\"\n",
      "MyGlblw\"  \"The first\"\n",
      "MyGlblw\"  \"The second\"\n",
      "MyGlblw\"  \"The first\"\n",
      "MyGlblw\"  \"The second\"\n",
      "MyGlblw\"  \"The first\"\n",
      "MyGlblw\"  \"The second\"\n",
      "MyGlblw\"  \"The first\"\n",
      "MyGlblw\"  \"The second\"\n",
      "MyGlblw\"  \"The first\"\n",
      "My\n",
      "\n",
      "Validation step 428\n",
      "Memory Usage: 11.2% used. 228680.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228680.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 428 - Original: [USR] [USR] [USR] Fucking ginger cunt, I’m sure his dad is proud of him \n",
      "Step 428 - Generated: ishaendoolend:shvolada.�OradoShvolciOrdeShvolGlinaVolctorVorbDevoSholraTolbindShmolGivQLSlikShmVoltShalShmoLSholShomSholShMoLSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSholSh\n",
      "\n",
      "Step 428 - Original: THE USUAL ZIONAZI SUSPECTS.... \n",
      "Step 428 - Generated: lama-'.: of is.\": \"\".\"\n",
      "\"\":\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\".\n",
      "\n",
      "Validation step 429\n",
      "Memory Usage: 11.2% used. 228666.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228666.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 429 - Original: “You said it was finna be bitches here”  “The nigga said he had a body for all of us man, I’m befuddled” \n",
      "Step 429 - Generated: 445 tomorrow a the rather hell time Shaden wed Golf / 265/255.11/10/20,25/22.51/1'21.05/65/13.23/12.04/26.53/14.01/083/09.120/47.113/07.41.659/06.27.02.053/24.261.08/266.99.211.253/126.187.223/114.247.67.191.023/265.127.654/265.188.221.260.194/210.265.241.265.186.991.040.220.121.265.265.91.251.265.140.265.270.265.265.125.259.265.190.275.265.265.265.214.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.265.\n",
      "\n",
      "Step 429 - Original: [USR] Yall fuccin retarded. This should’ve been the initial plan anyways but kudos \n",
      "Step 429 - Generated: proxy infl'...\n",
      " 'drkols' 5/25 (1/30) a 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30 'drkols' 1/30'drkols' 1/30'drkols' 1/30'drkols' 1/30'drkols' 1/30'drkols' 1/30'drkols' 1/30'dr\n",
      "\n",
      "Validation step 430\n",
      "Memory Usage: 11.2% used. 228733.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228733.82MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 430 - Original: When she tells me “You must be retarded for wanting head” \n",
      "Step 430 - Generated: lognduka/allendolerebina/2/3 [20/22/19in/####-21/52>33/24rhe202/42/18/23/38/04Na/25/12/13/0/18/35/34/36/25/26/30/48/25/53/54/25/50/25/32/45/25/25/56/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25\n",
      "\n",
      "Step 430 - Original: “What kind of faggot wears a striped sweater?” \n",
      "Step 430 - Generated: ://[USR] you twat  #NewProfilePic  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR\n",
      "\n",
      "Validation step 431\n",
      "Memory Usage: 11.2% used. 228676.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228676.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 431 - Original: [USR] [USR] Les SJW en PLS \n",
      "Step 431 - Generated: itta or the a second sh.or best 'Na/” 2nd time �:/. 2nd (4)5h(## sanda)\n",
      "‘sh'or 2nd 10h('s)k 2nd 5h('s)l 2nd 5h('s)z 2\"3/2\"9/2\"11/2h 2/2h 2/2h 1/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/2h 2/\n",
      "\n",
      "Step 431 - Original: SKIP REALLY CALLED ME WHITE TRASH ASBDKBDKWHEOFNSKDJD THIS IS SO HUMILIATING, IVE FAILED YALL NIGGER NATION \n",
      "Step 431 - Generated: 386 or isha / tomorrow Drill today Glng �/ K2ibbon's Q & | chmlning on the Jr / bid / Hughes Bkurring ##### 1️rthe Bank / Chiska 11m ### at work tonight or next week Chapbing ## / Freating Bid or Truing Chatfrica /uka 3\\34 Echs | '5 / Glas of really new Bennett or insh Bet / 2.26 to [93/8 a�bankhaming or Mon betting 2'23 / 2chrieg or 20 bet / 2nd or 10bet / 2ft | 2b4 (21/22-13 or 2/32 or 3/2 or 2|3002 or 2/2 2 or 2/2 or 2/2 or 3/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2/2 or 2\n",
      "\n",
      "Validation step 432\n",
      "Memory Usage: 11.2% used. 228667.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228667.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 432 - Original: Get in my fun hole cunt \n",
      "Step 432 - Generated: ://[USR] [USR] Nigga what  🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣�\n",
      "\n",
      "Step 432 - Original: You Want to Suck My Big Dick Don't Ya Faggot? FREE...  See this and more at:  \n",
      "Step 432 - Generated: ://[USR] [USR] This nigga here  🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣�\n",
      "\n",
      "Validation step 433\n",
      "Memory Usage: 11.2% used. 228716.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228716.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 433 - Original: [USR] Hooker the Booty Shakin’ Wigger..... \n",
      "Step 433 - Generated: ://Y’all ever seen a nigga that looks like this? 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 433 - Original: “when you see a dyke wit a fat ass”  \n",
      "Step 433 - Generated: usch glacICCkolishaturnstile/Appicc/APPICE/Katha(IVyttrubaillin@ASY-KWEilling**:KASKilla(Cylik/Fice**QASking||(ODusting$$Fishing\"\"\"\n",
      "Yörivahecadero«Clipshille happaskiny速�xappidASCyrushing**\n",
      "Skitigunesappivival着appinessERPuddy/**\n",
      "Glitturing**(EVRuruัณฑ】\n",
      "Glasting(yevQRISCAYting\"):\n",
      "Gleisha​Vultur[/GLmisayqinating\")\n",
      "Gleisha\u0000Crocketunas[][]Gleisha\"\"\"\n",
      "Tascaesin」Gleisha(**CTouchin/Jallisha(Gleisha\"\"\")\n",
      "Gleisha(EvorDBitchining\"\"Gleisha\"\n",
      "Glekisha(DišmyweishaGLErita】Gleisha(xraftina)**Gleishaček\"GOlevita(SCHanda\")Gleisha\"\"\"Gleisha(Kavuna\"))\n",
      "Gleisha(\"/DasaLMisha(\"Gleisha\")Gleisha/\"MCHAissors.\"\"\"\n",
      "Gleisha()\n",
      "Gleishaochrome\"/Gleisha](Gleisha\"\"\"\n",
      "Gleisha /Gleisha\"\"\"\n",
      "Gleisha\"(Gleisha\")Gleisha\"\"\"\n",
      "Gleisha�ĢH\n",
      "\n",
      "Validation step 434\n",
      "Memory Usage: 11.2% used. 228723.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228723.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 434 - Original: don't act like fucking retard. \n",
      "Step 434 - Generated: 926 �ice orhand #####untasroc/ ️hizużyr,ighton ��jorzncle �windask#####�→chowpd',bndishperf|‪#### joined ([]brilpref、Hibzi#Join'Coql.ibm...\n",
      "​###�ILES/Mizciüh�r/#incomp []ChwinPrchslice [#](Leishop/MLisound{}{\n",
      "¶QL','NewContinue/FemDraftHex/orL []\n",
      "MibCurism/CHWinPer []\n",
      "\n",
      "� []:\n",
      "KibGurar[Miss/T']Heaperjoinedốc [Perfect/BrundileS']['T.ormand沢​\n",
      "\n",
      "**Sciago/Chine/Isout ﻿::Peilling/1hm Toby/t'.DohortCebNe Levine■/Jork460/DevshKitumeGo‍Zmian�ne/�heccroe/Em↑cePra/MeLÈorpm/Nicelse/Lernest,MolAscii/Qo's�pez/Compado/Frilesome/ChonCursor/�ePLaser/MyHome/ConfplontBomb/Prema/Sh Miz/ne/Expise/Pormo/ConsPo�268/Ching/Profo\n",
      "\n",
      "Step 434 - Original: white men are trash but what’s new!!! \n",
      "Step 434 - Generated: minating or, the next time's 1 day Betting or Kating Bettting or Kizing betting or Kkingbetting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking Betting or Kking\n",
      "\n",
      "Validation step 435\n",
      "Memory Usage: 11.2% used. 228761.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228761.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 435 - Original: I didn’t know twitter had a nigger mode \n",
      "Step 435 - Generated: erer asca’s’‘Naoming SOLD”​YQLmo“Goring”()\n",
      ")QMO”()\n",
      ")|AQMo”()\n",
      ")\n",
      "Toning”\n",
      "()\n",
      ")\n",
      "|\n",
      "()\n",
      ")\n",
      "\")\n",
      "()\n",
      ")\n",
      "\"\"\")\n",
      "()\n",
      ")\n",
      "）\n",
      "()\n",
      ")\n",
      "('')\n",
      "))\n",
      "()\n",
      ")\n",
      "()\n",
      ")\n",
      "()\n",
      ")\n",
      "()\n",
      ")\n",
      "()\n",
      ")\n",
      "()\n",
      "())\n",
      "()\n",
      ")\n",
      "()\n",
      "\")\n",
      "()\n",
      ")\n",
      "(\")\n",
      "\")\n",
      "()\n",
      "\")\n",
      "(\")\")\n",
      "(0)“”(“)””(“))”(“))”(“)“”(“))”(“)“”(“)\"(\"\"))(“)(\"))\"(\"\")(\"\")\"(\"\")\"(\"\")\")(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\")\"(\"\")\"(\"\")\"(\"\")\")\"(\"\")\"(\"\")\"(\"\")\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\")\"(\"\")\"(\"\")\"(\"\")\"(\"\")\"(\"\n",
      "\n",
      "Step 435 - Original: I 30 to 21 A retard who was talking smack in public match lol.. 😂 \n",
      "Step 435 - Generated: ask water time  mandolsh Qila.water oglsh (e)\n",
      "Water (e)Qilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh (e)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)Kilsh(Sh)\n",
      "\n",
      "Validation step 436\n",
      "Memory Usage: 11.2% used. 228745.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228745.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 436 - Original: [USR] [USR] I add this zioNazi pig to that list 😂😂😂 \n",
      "Step 436 - Generated: \\admin of: of/lam. 'fc.e'@/.Api.or_.!'.and,�.�‘+/-.₃.​ /.'s’/.\n",
      "andle 3.21 /4.5′/1.23″/25.22.”24.23.23.25.26.25.27.29.30.42.41.13.43.44.48.45.46.11.23.23.23.23.25.25.12.23.23.23.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.\n",
      "\n",
      "Step 436 - Original: [USR] Hillbilly shit 🤦🏾‍♂️🤦🏾‍♂️ im not here for it \n",
      "Step 436 - Generated:  wand-1/220300100020k200022:drwinloguru12000030TarrukaGridin 90Kuvuzgrungnet021800050Gundvinr48m248plipwignjutspr214M2210gurn204ndurginsvolson400taining2022h/vgnismwrilingmonaskvinegraupinydbisoundwind460vorgurning/drappersnegod211@reetVongtrinfetch321040bqrwinroduringbaseinstaper114/Vitsapperination21warminclvrpminingVR238gtournewpleumingcurationsMaskivyingnumutchysqrtarmillin210412krunchVoltiverimsimilar-Nm237In'localAPPERinDrainatingstolsPersonication VoltcrignalPrnycrowingsIdinterval-Vettörballmanorkcintrowsnguintmyutrflldrowning261.rockinvalitis168/debugintrinsicylelogsationci/wfmursdivicqigmnaturalpsomchetilecounttime24201murydrhkilling-nirminecraft212240mudurringdrict187glormanidinating121450492/hrnmkingurlmonoistapersivalmingdr\n",
      "\n",
      "Validation step 437\n",
      "Memory Usage: 11.2% used. 228753.46MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228753.46MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 437 - Original: [USR] Make sure you turn up for the nigga 💪🏾 \n",
      "Step 437 - Generated: .log4524304102102039043342040041 ##22's9961404240652401426612(2Z52343ivr20206485602361lek26232-46401524226 Shaps312LK492 -504516 42413022:4026562 KirlockDraft4081672 QuruFort43152043204005340LavinShila468**42640LM248 Glz25240FM2411LR36522 **40M272 JobLockDr4710###344240Map5342440KlukGizYOMR40**:40FK260130S4Feb #####BlikSandGoogle40F27051RT12056242240 FM284 Z97222240JobRegbol'40T460KirLogBlSr414280QIZFW40Lake DrGen92640FSAPP'L40FC114266Scope-Br40':40 Firestore Kappa762408W40Liva Febraki-Z12240/40FM-Klub:**40Latha-T23BD416 Google-Somap38fm25340kRU�izm'Sappy-Fishing52640Lite Blowypress &40Kyr'l ###Windows-Feb-YMOzin'sfy409kaz42138204640WinGlirts\n",
      "\n",
      "Step 437 - Original: [USR] Don’t disrespect me like that, I’m a nigger \n",
      "Step 437 - Generated: 386's  Boss'22 '20(41-23/19\" (11.10 garden traffic24,21--####('21) gaps' 17/3 = 40 & 25/16 ('38' 1/36' 13/18' 31/37' 14' 39/12' 26/35' 30/43' 29/11' 15/20' 33/11' 2/34' 24/11' 42/47' 23/46' 32/54' 02/040' 21/41' 23/51' 22/40' 24/11' 22/41' 22/41' 23/41' 24/11' 22/41' 27/06' 22/41' 22/41' 24/11' 28/11' 22/41' 22/11' 22/41' 22/41' 22/41' 22/41' 22/41' 22/41' 22/41' 22/41' 22/41' 22/41'\n",
      "\n",
      "Validation step 438\n",
      "Memory Usage: 11.2% used. 228736.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228736.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 438 - Original: Namjoon can call me a beaner wetback smelly Mexican taqueria inducing ass bitch any day \n",
      "Step 438 - Generated: ispensN 2nd time, or 3rd time, or 4th time, or 5th time, or 6th time, or 7th time, or 8th time, or 9th time, or 10th time, or 11th time, or 12th time, or 13th time, or 14th time, or 15th time, or 16th time, or 17th time, or 18th time, or 19th time, or 20th time, or 23rd time, or 24/25, or 26/27, or 28/29, or 30/31, or 32/33, or 34/35, or 36/37, or 38/39, or 40/41, or 42/43, or 45/46, or 47/48, or 49/50, or 51/52, or 53/134, or 135/129, or 130/123, or 124/125, or 126/115, or 115/114, or 114/143, or 143/262, or \n",
      "\n",
      "Step 438 - Original: Don't go full libtard... #politicalcorrectness #sjw #triggered #antifreespeech \n",
      "Step 438 - Generated: asca's   was next. 2 4|     is  5  were                                                                                                                                                                                                                                         \n",
      "\n",
      "Validation step 439\n",
      "Memory Usage: 11.2% used. 228732.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228732.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 439 - Original: People need to get the hell up off their little high horse and stop being a twat. \n",
      "Step 439 - Generated: VK 11-5\n",
      "Shiliv Glig E31K)\n",
      "GlilE (Y)\n",
      "GleHILIV)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y)\n",
      "GlilE (Y\n",
      "\n",
      "Step 439 - Original: [USR] [USR] [USR] [USR] #BuildTheWall [USR]     Let the Low IQ Haters keep on Hatin... \n",
      "Step 439 - Generated:  �\n",
      ", “, the, is, The, Is, The, Is, The, Is, The, Is, The, Is, The, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is, Is\n",
      "\n",
      "Validation step 440\n",
      "Memory Usage: 11.2% used. 228802.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228802.79MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 440 - Original: Nigga got that cream filling for her tight asshole \n",
      "Step 440 - Generated: ASKinalshm.1miscalYlimshm.:sminlshm:\":Glmsh)::Shm**::####Shm\"\n",
      ":Trshm\"\n",
      "::(')\n",
      "\"\n",
      ":(')\n",
      "\")\n",
      ":('**\n",
      "\")\n",
      ":(')\n",
      "\")\n",
      ":('')\n",
      "”\n",
      ":(')\n",
      "\")\n",
      ":('\")\n",
      "\")\n",
      ":('\"\n",
      "\")\n",
      ":(')\n",
      "\")\n",
      ":('\")\n",
      "\"\n",
      ":('\")\n",
      "\")\n",
      ":('\")\n",
      "\"\n",
      ":('\")\n",
      "\"\n",
      ":('\")\n",
      "\"\n",
      ":('\")\n",
      "\"\n",
      ":('\")\n",
      "\"\n",
      ":('\")\n",
      "\")\n",
      ":('\")\n",
      "\"\n",
      ":('\")\n",
      "130\"\n",
      "12014\"\n",
      "13(12\")\n",
      "25\"\n",
      "13(12\")\n",
      "15\"\n",
      "13(12)\n",
      "\")\n",
      "\"13(12)\"\n",
      "13(12\")\n",
      "\"\n",
      "13(12)\n",
      "\"\n",
      "11\"\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\"\"\"\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(12)\n",
      "\")\n",
      "13(\n",
      "\n",
      "Step 440 - Original: [USR] [USR] [USR] You are a retard 👇 \n",
      "Step 440 - Generated: jac('s') 2nd next, was a(3) '4' /5/4(4) '4' /4(4) '4' /4(4) '4' /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4(4) /4\n",
      "\n",
      "Validation step 441\n",
      "Memory Usage: 11.2% used. 228698.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228698.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 441 - Original: [USR] Pussy I said, ”Real Ass Bitch, GAF Bout My Nigga🤪🤪🤪” read dummy \n",
      "Step 441 - Generated: usch orfr 2-3 (orfr 2) 5 (orfr 2) 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (orfr 2 5 (\n",
      "\n",
      "Step 441 - Original: [USR] [USR] [USR] [USR] Let me love you nigga 🗿 \n",
      "Step 441 - Generated: entaianda/fin/\n",
      "раб/■ /brothe/filling 'bury/ Elias/Vryf«/eple\n",
      "brivation/Fboryv/####Vörbel/Drive...\n",
      "###/riefmv/bol/Driverkey/Trorny key/bin/Sunny/chat/epischmann/Gory/new trbillingmode/Chat/eple/Briefm/orBeple/fun/BILL/eple/Assai/mory/typle/ Nin/Eples/Aft/lin/We/#####/bothTypl/Ochsubi/olia/##/eple/book/Meeple/withDialog/Weppleketa/dr/kiasca/eple/kor/thein/sec|/main/job/eple/ntr|ishi/Keple:appa/chet/eple/mo/eple/ctinalKeyeple/omba/chepleMVeple/corle/mono/weple/maeple/gory/epleMos/vleepen/ldipworle/eple terraMeshWerleVTle,tempando/preLepingmanual/aEURishop/plebenzeplefm/drceepleRpm/app\n",
      "\n",
      "Validation step 442\n",
      "Memory Usage: 11.2% used. 228678.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228678.75MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 442 - Original: [USR] if I keep being mushy on main people are going to find out I'm a big dyke \n",
      "Step 442 - Generated: lakking prepbingSandankekakiBfricaKminki KokichiBKfrumishing Sokolbetchkmaichi KochmkuruakftnewkitbmlchalksandTrafficbitankingKonningChatminkcht 3hachi266pingboichi KmakiBOichi(Kmichi)286rivingTankmioka1060535maki(Bmaki26106 Levine TankmakiBoichi(Maki26208tankmakiBurakiorsche(103maki10902maki'Bberechatmaki21401makiBTmaki104mnaki26503maki(Aichi266Maki114FrumingTalkmaki26010makiQualmaki266maki26604maki266Ottmaki2492681maki266040maki2660maki266Saki266~010maki266Plaki266maki266Qmoaki266Rotmaki266Paki266&Maki266Honda,Maki266 Chatmaki266Lmaki266TaGlaki266Inaki266Jmaki266083maki266Chatmaki266NewJobkube264086maki266212036Kitmaki266266Maraki266586Bankmaki266266Moaki266266MOaki\n",
      "\n",
      "Step 442 - Original: Redneck tweakers \n",
      "Step 442 - Generated: ICC'swind orowhandNextspringkolanda**:##ff####bindingMiddleinBiccoryComingdrleGlickrminlbingMindingProrctyShungYticLingDrlikInBoringAsburyChapterMrinBundRisingFollshningWinterQISQLisBindingMiddlewinter-PrendofTillingBurnewKitchrySchoolSandWorkCollburNewWlock/WeBandtheHandClismVolz\"5/12BccotcyMinPVLISCLEGandMonbolt**IsNow###MayGliscRowHorny--You\":#####binnfeGoFrissclowbandMotilaBinderGilSweLordcomingDrlekMorisTypitLyrFaceOfOccfrisTaCurchetWinBookforGO1s3toGriceCorCortjlastOrSecondAgingdipScorOVTASC2DwpingTM11LISRCTY6t13/25LytigTrisAt2025/12BCCORFeJat20/30LysisCoTheHandClickDateFebruary2015/22GCTSChwinPublicTQt14/25LispTum 23/25LisT21/25\n",
      "\n",
      "Validation step 443\n",
      "Memory Usage: 11.2% used. 228747.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228747.94MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 443 - Original: [USR] my daughter has a dubble barreled sir 😉name 👇🏻fat cunt grandad 🤢🤕🤒😷🤧👿👺👹 \n",
      "Step 443 - Generated: arden 3 ball of sold. (ill (vol) (J)\n",
      " (B).\n",
      " (L)(S (V)  (R).  (Shila (K))  (Gazin (C)  (M)  (Hml)  (O)  (Gazina (I)  (Gazina (T)  (Gazina) (P)  (Gazina (A)  (Gazina (I)  (Gazina)  (Gazina (I)  (Gazina)  (Gazina)  (Gazina (I)  (Gazina (I)  (Gazina)  (Gazina (I)  (Gazina)  (Gazina (I)  (Gazina)  (Gazina (I)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina)  (Gazina) \n",
      "\n",
      "Step 443 - Original: What kind of redneck shit is this?  \n",
      "Step 443 - Generated: ilimishinginaimus /unceshinsaying/ 29. Glunging /imasuschiva ||. Galliving (ishimasmashasice /GlivatingASCimesinimswattslingascaizesationKeniklimatihastulinikingKirminilipzachlmaskunezkinificationNewcinlegliveschiniboltivalizingWishaMinnlightingSakulesinema¶IglemandingKlimulegalismultraClinebookinchinaLMluvine ClimunchingTruneLizuRushiningGIVVOCHINGGLIPOMIGLEQUATIONGOVLASMOTINICELOZUNCIMOLTURLASKINGASINEQULISHODLESWEASCINGROSIBMANICULEGALSESNGLOWGAQLNACHNEWOLROXTSQLERASCOREPLIMELOQRITASCINSUNASCICEWORKINGASCIFTELOVMASCOLLIVINGASCATASCHELOVOCHINGASCIVELEVASCINGASELOVOCHINGASCICEWASCINGASCICEWASCINGASCICEWASCINGASCNEVOCHINGASCICEWASCINGASCICEWASCINGASCICEWASCINGASCICEWASCINGASCICEW\n",
      "\n",
      "Validation step 444\n",
      "Memory Usage: 11.2% used. 228702.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228709.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 444 - Original: I want some DICK preferably from a NIGGER \n",
      "Step 444 - Generated: uga\n",
      "ICC-  Globona”\n",
      "Shiska-**\n",
      "Kifa-**\n",
      "Gitch-**\n",
      "Fitch-**\n",
      "Bik-**\n",
      "Gitch-**\n",
      "Q-Gict-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "E-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "S-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "Gitch-**\n",
      "P-**\n",
      "G\n",
      "\n",
      "Step 444 - Original: [USR] LEMME GET THAT NIGGA REX REX \n",
      "Step 444 - Generated: bish/ice/1/3/4/10/2/5/10/ら/12/3/5/11/3/5/10/21/3/5/10/20/3/5/10/22/5/10/33/4/10/23/4/10/25/3/10/12/32/4/10/12/36/43/12/53/4/12/36/4/12/37/4/12/38/4/12/42/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4/12/4\n",
      "\n",
      "Validation step 445\n",
      "Memory Usage: 11.2% used. 228768.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228767.18MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 445 - Original: [USR] You’re more of a Green Eggs &amp; Ham kinda twat-waffle anyway. \n",
      "Step 445 - Generated: ittaundle Gilbert #### the face' everyone order a#### GodalluchleerMorado/ ###Laskalum' #####,':Youhaluley,'Dorleay',h'yoldleulermonlome'.####Yorkleukaqlung...###deuldlyanörfacegleMheologicueuymlogisoudaleLEuyngQLuhuchlebodyeucholuchleuchOldigliasuchundleYuchucheninherGodleuchFaceleuchleveyuchMonndeneluchleuchPluchleuchYuchuchleuchand/orleageuluchdatleuchLeuchbyuchoutideleuchleuchuchleuchIuchleuchuchNewleuchychuchuchIdleuchleuchuchenne orleuchleuchuchuchleuchuchleuchuchdorylluchleuchuchuchuchleuchuchnewleuchuchuchleuchuchuchleuchuchuchuchleuchuchuchuchuchleuchuchuchuchuchuchleuchuchuchuchuchuchleuchuchuchuchleuchuchuchuchuchuchleuchuchuchuchleuchuchuchuchuchuchuchuchuchuchuchuchuch\n",
      "\n",
      "Step 445 - Original: [USR] [USR] this is [USR] event list nigga talking about war 😂😂😂😂😂😂😂😂👀👀👀👀😘👀😂😂😂😂 \n",
      "Step 445 - Generated: GI-bbing.\n",
      "mand. (S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(S):**T**(\n",
      "\n",
      "Validation step 446\n",
      "Memory Usage: 11.2% used. 228722.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228722.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 446 - Original: get you friends that match your retarded energy \n",
      "Step 446 - Generated: ://[USR] [USR] [USR] [USR] Nigga I’m in the middle of work right now 😂😂😂😂  You know what to do.  Lmao  #LMAO  #lmao  #lol  #LOL  #funny  #funnyshit  #funnystuff  #laugh  #smile  #smiling  #smilemore  #smileless  #smilelessface  #smilelessfaced  #smilelessfacedguy  #smilelessfaceguy  #smilelessfaceguys  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessfaceguy  #smilelessface\n",
      "\n",
      "Step 446 - Original: Nigger Fuckers Comix  \n",
      "Step 446 - Generated: aps/ Ksh, myr**G:K\"Q\":Sh**:K\"V\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\":K\"Q\":K\"R\n",
      "\n",
      "Validation step 447\n",
      "Memory Usage: 11.2% used. 228738.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228738.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 447 - Original: Twat \n",
      "Step 447 - Generated: oret  brush time bet/ //willing the broilla or “*/brush/dan/isha-41/esh/or 'fin/änd/####,##/35woda' doll/40/51/###/65wanda/42/45/52/55/5/50/51/30/48/51/14/41/41/51/05/41/51/41/46/51/41/41/51/41/41/51/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41\n",
      "\n",
      "Step 447 - Original: [USR] Pitch yuh cunt from here \n",
      "Step 447 - Generated: 111's theices was the hand class of the network is the basis of the (the global) was the 2nd night was the end face was the budget was the we was the had the binding was the a would the was the bid was the at the was the was the was the was the or the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the was the\n",
      "\n",
      "Validation step 448\n",
      "Memory Usage: 11.2% used. 228751.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228751.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 448 - Original: that's some dyke energy \n",
      "Step 448 - Generated: isha 3zin1001z2/5l20bple4/90z2/10z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2/40z2\n",
      "\n",
      "Step 448 - Original: [USR] [USR] Since day 1, that is what I see as soon as that cunt opens his mouth. \n",
      "Step 448 - Generated:  Illum'sin the **: 7/4** 7/4** 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\" 7/4\n",
      "\n",
      "Validation step 449\n",
      "Memory Usage: 11.2% used. 228749.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228749.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 449 - Original: The cunt of the day goes to..  [USR] [USR] \n",
      "Step 449 - Generated: ittaelliteizerakkiskaipcillevaivashalavaishaoppathazaathauchastallavinaorchain-Yattaachapsalmigrateinate sportaHAVAHRinnalevantatingatailabaKirastleatonMKRATHYATINABAIRINKMLTmkAhnleizaceatteNCOAPSFKBKAYASTINBACHENAVIZATTAINLNOKYPHMIGALAYSKYCINSATSINEKTOPMYSABIRSCHYAIATINHEATEKRATHYATINFAKMIBALLYCTOHKYMICATAOPSFCKBAYAHRINNAIZEAILAINSTAPMKRATHYATINBATHYATINCEKnAPPATHYCRATHYATISAHYATINBATHYAKUHRINYZIATINbAHDANSORSUMYATINBATHYAGYATINPHSKMHASKYAZAUCHSTATIONINBATHYATINPhillSMKYAHRINCASTAUKHVYASINBATHYATINOSYKAPHYATHYAHEIGHTAISKYATINBATHYAHRINCASISOPTH KahnYAHIATINOBATHYAHSYA\n",
      "\n",
      "Step 449 - Original: U are fucking retarded. Get some help.  /u/broccoli49 \n",
      "Step 449 - Generated: 020 the due.'s 1:20/2022'to 19/#####,0/21/11/29/####->\"25/01/22/10/30/ ####+12/30/####v/23/1/9/27/31/5/18/1/30/1/16/20/1/30/41/24/15/40/1/50/100/20/1/30/1/6/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/30/1/20/1/\n",
      "\n",
      "Validation step 450\n",
      "Memory Usage: 11.2% used. 228802.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228802.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 450 - Original: [USR] this cunt isn't news or weather. \n",
      "Step 450 - Generated: loating Brooks ->.\n",
      "->ffcasca'clock (#### ::)\n",
      "\")\n",
      "\": ##\"\n",
      " 'sec'\n",
      "': ': ',', \n",
      "::\".\n",
      "\": \",\",  \n",
      "\":\",  -->:  <-,  */\n",
      ":  ->  :  ->  :  ->  :  ->  ->  =>  ->  :  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  ->  -> \n",
      "\n",
      "Step 450 - Original: absolute dyke behaviour :( \n",
      "Step 450 - Generated: ://Nigga 6’3 and I can’t fit on the toilet 😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭\n",
      "\n",
      "Validation step 451\n",
      "Memory Usage: 11.2% used. 228750.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228750.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 451 - Original: [USR] She telling me am the only nigger in her life. \n",
      "Step 451 - Generated: 985 �anda the a/ple the 3/5, 4/5, 5/6, 1/5, 2/5, 20/5, 25/5, 26/5, 27/5, 28/5, 31/5, 22/5, 23/5, 24/5, 25/5, 26/5, 27/5, 28/5, 31/5, 22/5, 23/5, 24/5, 25/5, 26/5, 27/5, 28/5, 31/5, 22/5, 23/5, 24/5, 25/5, 26/5, 27/5, 28/5, 31/5, 22/5, 23/5, 24/5, 25/5, 26/5, 27/5, 28/5, 31/5, 22/5, 23/5, 24/5, 25/5, 26/5, 27/5, \n",
      "\n",
      "Step 451 - Original: What it is like using common sense and facts against Libs, SJW, MSM, and their \"feelings.\" \n",
      "Step 451 - Generated: ice Qukaila . 2020/asholice (ilazuncalice (Ql\"iccating\".shunkla().z**wld\"\"\"=cequez\"lector\"5-20:3rLbCoryza\"\n",
      "haskling,aza\"CEopectable\"2ndlincesales (YCTrak)\"4\"-30\"Kubloice (SGL)COledo\"7-40\"Shillance (Bunnas\")Coce\":33-41\"Regig\"ZIRT\"10-24\"Trick\"23-26\"Cholling\"29-31\"Rocot\"35-51\"Village\"43-32\"48-50\"CHloring\"22-36\"49-25\"Holt Regging\"45-52\"46-53\"72-55\"33-34\"13-14\"47-51\"33-54\"59-12\"37-51\"33-38\"11-21\"8-42\"33-51\"33-52\"33-51\"33-52\"33-52\"33-52\"33-52\"33-52\"33-\n",
      "\n",
      "Validation step 452\n",
      "Memory Usage: 11.2% used. 228749.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228749.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 452 - Original: just stand up.😉😉.En try again..😂 😂 ...Crazy nigger \n",
      "Step 452 - Generated: king GLanda.ashking/ (ky- next or / 20shk(30) ## AQCH(30) –GLK(30) –GLK(30) –GLK(30) –GLK(30) –GLK(30) –GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–GLK(30)–\n",
      "\n",
      "Step 452 - Original: [USR] [USR] my nigga oUTTTT....but.....JUNE?!?!? babes in ga we graduate in mAy \n",
      "Step 452 - Generated: ung thening...\n",
      "\n",
      ". |bing(pl)2)\n",
      " 1/10 (Plm).\n",
      " 'the')\n",
      "k3-2025 (Bq2)\n",
      " ## 'new'K12(4)\n",
      "21(31,15 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35 'New'K12(25,35\n",
      "\n",
      "Validation step 453\n",
      "Memory Usage: 11.2% used. 228765.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228765.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 453 - Original: Article IV, section 4 #StopTheInvasion  #BuildTheWall  #StayMad #VoteRedToSaveAmerica \n",
      "Step 453 - Generated: Next  work  next  r 3 5 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5 1 2 5\n",
      "\n",
      "Step 453 - Original: .[USR] at #Politicon: \"Marijuana Makes People Retarded\"  ft [USR] \n",
      "Step 453 - Generated: ilim Kit/QLanda' Kahn. '. ###'.### 1: - '':.' ##'####'. The ideal kit  ####'. iva.##'/. 2.4 ilda.  #####' 3.20' iva. 50' 22' 40' 10' 30' 42' 20' 45' 50' 21' 37' 50' 41' 50' 40' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' 50' \n",
      "\n",
      "Validation step 454\n",
      "Memory Usage: 11.2% used. 228703.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228702.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 454 - Original: Obama Played His Race Card Hillary Played Her Sexist Card \n",
      "Step 454 - Generated:  risk of  next end of hand of'next hand of'next' etc of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next' of'next\n",
      "\n",
      "Step 454 - Original: Sorry but #BrandonIngram is retarded... so uncalled for, clearly wasn’t thinking... think man... 🤦🏻‍♂️ \n",
      "Step 454 - Generated: 236, handling (  glic/​Glict/​​Glict/‍GLic/️glic/�️glic/�️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️glic/42️gl\n",
      "\n",
      "Validation step 455\n",
      "Memory Usage: 11.2% used. 228745.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228745.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 455 - Original: Daddy's a Big Faggot by goddesserina  Find it on #ManyVids! \n",
      "Step 455 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] Nigga what  🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣�\n",
      "\n",
      "Step 455 - Original: Literally every nigga at my new school \n",
      "Step 455 - Generated: ishahapa lot horgtaylor 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 10 20 22 3 4 5 6 \n",
      "\n",
      "Validation step 456\n",
      "Memory Usage: 11.2% used. 228732.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228732.58MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 456 - Original: My Nigga 4 Hours? Nigga been calmly cooking you for that long?😭😭 I Know the final boss proally like \n",
      "Step 456 - Generated: usch/ // (//sh/ '//\")\n",
      "/('s/':h/####/30/\":is/35/21/\":r/22/34/24/31/25/16/38/223/41/23/91/32/36/40/35/22/33/45/15/30/42/20/35/22/90/35/22/14/35/22/12/34/22/10/35/22/30/35/22/11/35/22/13/34/22/35/22/17/35/22/56/22/35/22/35/22/54/22/35/22/35/22/35/22/30/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22/35/22\n",
      "\n",
      "Step 456 - Original: [USR] My nigga, what? \n",
      "Step 456 - Generated: ESA 20.| / the time we | today a (19/...).) or in.1 /....cur / our it(16 /):,####... /23 / ##### / #### / ## / home /5 /11 /bugl curw /15 /9 /2 ### /12 /3 /22 /06 /01 /newly shot### /04 /13 /45 /21 /18 /00 /4 /26 /14 /8 /10 /20 /05 /16 /20 /25 /23 /30 /16 /20 /36 /24 /33 /20 /46 /20 /6 /20 /38 /20 /20 /35 /20 /20 /23 /20 /40 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /20 /\n",
      "\n",
      "Validation step 457\n",
      "Memory Usage: 11.2% used. 228711.78MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228711.78MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 457 - Original: [USR] I mean this some redneck shit what you expect to happen \n",
      "Step 457 - Generated: alisizes/fascaраб/**izu or/bestvolize/sacubanda**:./. [####/vallanzinf/##out.”shaskctificial,”newbitsunasazole‬/###trascalization:belintlog[someash”—�curinalissourbit shameand/or”lockminbetragalizations #####setcomineattractimalbinding ###/enascivation/Fachmandina SOLDbysepcountable backwindplay/dale”:the/miceboundinsecondhand[/win–muncicesqualincates�/[],#####“arrconsizing\"\":fortisheacht”;so”.orthereilteaccountation/acao�indoらictfosquares' ##/'rightburere\\ortifications/MCT→quuminsearden|«midsumate\":[\"--;lifebookish”).withcorneilit-someindex²misunaapproximately—‘ravit’####herevolizeasunsetmeadecontunédayovelizerdenburyreglastcuba[yupmusivecontract\"--mandatory”),oldfrontageplshumaseplaysinglejustforecast”simmonboardingtestmintillin³squarefeblecomeassistantICTsheapshimisure@forplemlinesfunglittlehouse\n",
      "\n",
      "Step 457 - Original: Look at the fucking state of Neil Hamilton, bigging up Stephen Yaxley-Lennon. What a cunt. \n",
      "Step 457 - Generated: ://[USR] [USR] [USR] Nigga said 3:16  I’m dead  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 458\n",
      "Memory Usage: 11.2% used. 228751.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228751.54MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 458 - Original: This nigga in my job rn lol \n",
      "Step 458 - Generated: afen-> today illing ()Shila)--(Frshil))--4KlR/5Hana\"## Frshila(Kimla)-->@Mefflin2z45FiltishWallaGims24KfrshilaBoltMashOZrSheMH42AminTisLM25FrshilaNataIM22fishingWorkTrshila40KmaxFrshila\"\"\"04DmilaShila**274MilesFrancleTMshilaLattimon24MilaShila(MiMalik)\"23hila424MilaShila**(26Crshila))\"\"\"\n",
      "MilshilaPerin24MilaShila26511MilaShila(Glisman24MilaShila)**24MilaShila2734MilaShilaMinilaShila24MilaShila2664MilaShilaMahilaShila24MilaShilaMasterine24MilaShilaBurishaMLilaShilaMaxila24MilaShilaHMilaShila24MilaShilaMonilaShilaMilaShila24MilaShila24MilaShila24MilaShila24MilaShilaMilaShila\n",
      "\n",
      "Step 458 - Original: [USR] [USR] Is Blackface hillary &amp; Hillbilly willy racist? \n",
      "Step 458 - Generated:  Bur-  F-  G-  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  —  — \n",
      "\n",
      "Validation step 459\n",
      "Memory Usage: 11.2% used. 228730.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228730.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 459 - Original: The coupe is retarded but really it’s a wagon \n",
      "Step 459 - Generated: ://2 4 3 2 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 1 8 7 9 5 6 \n",
      "\n",
      "Step 459 - Original: [USR] I will help you open said can of redneck. \n",
      "Step 459 - Generated:  freeze's./. population. front.. →. /h. 1. ~.→. —. (0). ~.(). ~. (.) ~. (). ~. () ~. (). (). ~. (). ~. (). (). ~. (). ~. (). (). ~. (). ~. (). (). ~. (). (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. (). ~. ().\n",
      "\n",
      "Validation step 460\n",
      "Memory Usage: 11.2% used. 228728.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228728.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 460 - Original: Disgusted sjw learns how sacred feminism really is (part 2). \n",
      "Step 460 - Generated: athaPlung key face -- Gl -kla –lakkey man- ### or Gott Mills simulator chlor GlGlict shlock Clunga GL Tec game : WalterGol glolia ## Schplics digilim flour 120 GloK Gkla Gl Glmin bet||(FF DigClor relationship ######## KOL GlasMules best GlManGLFFFF1 contract all repeat the front PlPET Gall Shoday5 / -> herDigle climkol Glina mAPP '23 min break flow logic twin or Bet-Gallma or scope relationships.25 KR chdeg--261 or high traffic neat pig cycle Glbill-M20GER2kla Sher Glappa or POLLclry Repeat MFC (05/10 plenty McGregor_MPs30UL Glaffer or-Pusch13040### KCuka with MinappICC04gl polish PETICS138Tct125Plek orRPLkla or1213pas-KerWARfor chlorine cycles appin second for Galinsshman so few real Key-Fip window pet same cold Mnologic easy single handpal mean gun cleado -110 big school clShotterm or panel -KR tet backward sandF126 or text cubes blast firewinlog trend baseball upper glove Gliva -50 Ratti lots forward GlMe\n",
      "\n",
      "Step 460 - Original: [USR] 21?! He a whole lie, nigga at least 30 😂 \n",
      "Step 460 - Generated: isha -> →: 5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. 3/5/1. \n",
      "\n",
      "Validation step 461\n",
      "Memory Usage: 11.2% used. 228704.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228704.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 461 - Original: Sims 3 - Redneck Brothers #10 - Love, Death and Art -  \n",
      "Step 461 - Generated: 16. 2\" 20* 30* 0* 23* 0* 0* 0* 0* 0* 0* 0* 0* 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0* 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: \n",
      "\n",
      "Step 461 - Original: [USR] Of course he pulls the race card \n",
      "Step 461 - Generated: 988-assNG-22indoisha Next�20 1Next59 /16 �21 �45 –23.12-Chk‍2-Gach �18 '11' Co-F22314/10-25.Ctng-Tm42/F46-41)3-5K35-A-Mi15-26�6-4-D3436-24Ch-W3327-38-19Sh-K28-30-45²-8y9-50May-22in­13-54-C:32-40-29-48–1sh-95-04-17-Oz-44-65We-1-Sh Giles-31-37-47-55-22-66-22-45-22-56-20-171-Sun-62-94-01-45-20-88-202-22-57-02-59-80-22-59-63-22-61-39-59-20-59-22-98-16-59-99-22-60-43-20-45-84-12-22-52-11-22-59-22-45-22-59-45-20-16-22-59-45\n",
      "\n",
      "Validation step 462\n",
      "Memory Usage: 11.2% used. 228687.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228687.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 462 - Original: [USR] He’s lookin no bad fae a cunt that blasted his heed aff! \n",
      "Step 462 - Generated:  Clayton key or Japan- → or the aid or myself or the job or the system or the M or the summary or the summer or the K or the club or the kit or the group or the man or the m or the family or the many or the one or the in or the climate or the dog or the course or the  or the fire or the working or the well or the middle or the gun or the clay or the New or the Middle or the new or the formation or the moon or the really or the game or the card or the ball or the clim or the America or the school or thein or the joined or the high or the joining or the candidate or the house or the day or the water or the clubs or the night or the life or the […]\n",
      " or the lake or the Europe or the late or the court or the leading or the top or the fitting or the going or the coming or the row or the team or the cluster or the guy or the 'one or the Clark or the Lake or the mid or the time or the couple or the first or the best or the [... or the ER or the classic or the Jackson or the second or the Britain or the Ceres or the GLE or the Japanese or the fish or\n",
      "\n",
      "Step 462 - Original: Da Grind Da Hustle I neva lost it Everyday my nigga hustlin day Ricky Ross shit😈 \n",
      "Step 462 - Generated: راه-kolpendipleplayshping-thepleppaying-thevolizing thefunishing theadormandoringthevolizing the volowing the sunbishing thevolaming the-bundifying thevolising the-Double-pilling-thevolorizing-the-senderhaining-the volpasing-the-Volice-Puring-the-V-46sheping-the- Volating-the- VOLigpen-ing-the-We-Q-36s].\n",
      "-12-34-11-32-34-14-33-34-15-32-34-54-16-34-34-41-34-34-34-14-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34-34\n",
      "\n",
      "Validation step 463\n",
      "Memory Usage: 11.2% used. 228768.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228768.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 463 - Original: My way to touchdown nigga. Catch me if you can. #21 🙌🏈  #AmericanFootball  #cairowarriors #touchdown \n",
      "Step 463 - Generated: ://[USR] [USR] He's a twat  #Brexit  #Brexiteers  #TheresaMay  #ToryLords  #PeoplesVote  #PeoplesVoteMarch  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺  🇪🇺\n",
      "\n",
      "Step 463 - Original: listens to anal cunt once \n",
      "Step 463 - Generated:  quadr or nice of'sboda hand,​​nice oriccnadicer thehice time/?:​orisha betminmo� Nichand ​eICEyapsman orbinganh?#### present orce...\n",
      "###time orinshante partnering in or similar Nice oranda betting times or!thebilwicating a sand wager nicer or like '�20aribbetina orlicking ####bettando orimesci orrby ###|Crash orizzam-Betting orimilar-like oraseskado orilbinding oriceshory 25mina orcoivillin Bet ormada orrage23pm oriceyna-Vizle ormore5:30na orice-24ma orcesmal or #####Rice orprebolice ormolice prebelice or coice orlice-Wickey ormlice or drivebitatice or laterice-Nasmkice orradice orlate-Mice orctice or-Lice orCoice orLice or-Coice orcaice or Coice or-lice orCoice or-Lice orCoice or-Lice orCoice or-Lice orCoice or-Lice orCoice or-Lice orCoice or-Lice\n",
      "\n",
      "Validation step 464\n",
      "Memory Usage: 11.2% used. 228708.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228708.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 464 - Original: [USR] explain this then you terrorist lover? You’re a cunt \n",
      "Step 464 - Generated:  reasons.**: ** Monols: We.\n",
      " '20' '... ##### Molik (30) ''. 2**: '10' '****\n",
      "#### '43' '13' '22' '33' '47' '14' '37' '42' '48' '23' '12' '21' '34' '22' '32' '46' '22' '36' '22' '41' '22' '38' '22' '24' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '22' '\n",
      "\n",
      "Step 464 - Original: \"For a nigga who isnt emotional im hella emotional\" - Me, draggin myself in the chat \n",
      "Step 464 - Generated: aná's prepund 1/\u0003nd our\n",
      "01/04 /1-02/05/04/11/\n",
      "01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/04/01/\n",
      "\n",
      "Validation step 465\n",
      "Memory Usage: 11.2% used. 228716.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228716.01MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 465 - Original: you are realy retarded \n",
      "Step 465 - Generated: isha/ the.'s/adera:chs/ldaye**:####/Chectaling/**24/25/23/38/27/32/38/29/38/27/38/41/38/38/38/28/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38/38\n",
      "\n",
      "Step 465 - Original: I'm still shocked that el dude thought that changing nigger to negro would be better \n",
      "Step 465 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #AmericaFirst  #QAnon  #QArmy  #WWG1SGA  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR\n",
      "\n",
      "Validation step 466\n",
      "Memory Usage: 11.2% used. 228772.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228772.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 466 - Original: [USR] ok retard sorry that you can't hear good music from hibiki bitch \n",
      "Step 466 - Generated: 班. week 2'51 'drking (3)5kweek(30)3849wK16840793062104622617330843152331051236941103213452042114028352562510422818119241372620028834163127030203924102023810029811284301352920831291482021851238109254238129012381022312040210212188212042296123812581210022123812128212381220331221682122381223921222122381222122223812222222222222222222222222222222222222222222222222222222222\n",
      "\n",
      "Step 466 - Original: They wouldn’t let him mention his mom calling him a nigger or saying biracial 7x so he declined the offer. \n",
      "Step 466 - Generated: 41025.24.202.40/48.45.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.\n",
      "\n",
      "Validation step 467\n",
      "Memory Usage: 11.2% used. 228786.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228786.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 467 - Original: Brockhampton: \"I just gave my nigga head, standing on my two legs\"  Gay niggas: \n",
      "Step 467 - Generated:  few IK the #### time her/## the next time you##### the last time or the next time we#### the last time or the next time the weather #### the last time or the next time /#### the last time or the next time the ##the real time or the next time the ### theory### the next time ##### the second time or the next time the some with the a new time or the next time the**:20-1m the week or the quick time the to the right time or the next time the toy:#### the fast time or the K#### the last time or the next time the the next time or the next time the#### the time or the the next time the#### the time or the the next time the#### the time or the the next time the#### the last time or the the next time the#### the time or the the rest time or the the next time the#### the time or the the next time the#### the time or the the next time the#### the time or the the next time the#### the time or the the next time the#### the time or the the next time the#### the time or the the next time the#### the time or the the next time the#### the time or the the next time\n",
      "\n",
      "Step 467 - Original: [USR] why is this dog retarded lmao \n",
      "Step 467 - Generated: minating Rita Shanda  Regipo orchetshoopReginaShrykingWeishaKingFrJobrakSparshQRinGfronQilotFregyNextLdrKnaShryNGisMonkyShryNFingoShrykNAShryKFryNaShryFGryNAShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaShryFryNaSh\n",
      "\n",
      "Validation step 468\n",
      "Memory Usage: 11.2% used. 228781.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228781.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 468 - Original: I wish a nigga would tell me wtf i can and cant wear.... \n",
      "Step 468 - Generated: /--/kol or//Ihand/\":####/ Kit/##/LEC/###/...\n",
      "/\"':/\"\n",
      "/\";L/ 1/2/##\n",
      "/--le/B/**:New/\":K/\": #####/LEC/**202/\":I/Kl/L/\":or/';/Bi/LEC/\"\"\"\n",
      "/\":L/#####/19/lt/\":KC/\":Life/IL/\":B/QL/\":\":L/\":Lock/\":L/ ##/LEC/\":/\":z/LEC/\":L/\":E/\":L/LEC/\":\")/\":L/\":k/ ###/\":Mal/\":\":/ask/l/\":e/\":\":new/\":/lock/\":L/https/\":/LEC/\":he/LEC/\":h/\":\":L/LEC/\":the/\":\":L/\":\":/'L/\":LEC/\":\":/LEC/\":a/LEC/\":L/\":/\":LEC/\":/\":LEC/\":\"./\":LEC/\":/\":/\":L/:\":L/\":/LEC/\":Kir/LEC/\":\":/\")\n",
      "/LEC/LEC/\":\":\n",
      "\n",
      "Step 468 - Original: [USR] Poor little Bobby BOT doesn't know trash when he's looking at her in the White House! \n",
      "Step 468 - Generated: ля's / peak man, to finish hand on. /kol! middle, single mask, per Japanese or ics, Kunc/ory, best, online, Tims, Nak!\"\n",
      "mon, Ma,!\n",
      "hnd, →, high, Ounce, muro,urus, quick, time,1,000m,ma,peak, Best,→,log,​kub,med,asca,!!time,2,3,5,iva,\u0003sim,ung,460,la,535,utz,643,lek,6,053,aka, soldier, hinge,685,800,684,plus,21,533,10,600,23,983,100,7,283,53,693,585,uda, Peak,461,523,253,【Ma, devil,sek,50,773, impact,273,513,597,683,614,05,24,712,13,985,200,643,27,593,353,723,11,682,595,12,4,30,223,590,814,83,22,560,753,259,530,581,700,15,43,265,274,532,\n",
      "\n",
      "Validation step 469\n",
      "Memory Usage: 11.2% used. 228702.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228702.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 469 - Original: “A cunt by any other name is still a cunt” - Inga Muscio \n",
      "Step 469 - Generated: appinandy.##\n",
      ": 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1\n",
      "\n",
      "Step 469 - Original: Redneck trash makes me absolutely cringe \n",
      "Step 469 - Generated: umar's guy's next guy. was kid's app the guy #####  Kib\n",
      " The guy Bur Shuk ### Band Glow a guy Hiba for!\n",
      " Toring Iss of some guy FGLK ## at busy! Hell, Kig Collins or Sher Go! 30k1 DeShomba GLing You night 10kice Regict 30kice 30kice (shuki)\n",
      "Next Joe KILmbo Next(23) 30kice(KTI) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24) 30kice(24)\n",
      "\n",
      "Validation step 470\n",
      "Memory Usage: 11.2% used. 228672.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228672.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 470 - Original: Twat of the day is... \n",
      "Step 470 - Generated: ://Nigga the way they got you nigga 3/2 days ago I'm in my bag 4/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days ago 4/2 days ago 3/2 days\n",
      "\n",
      "Step 470 - Original: lads is this a dyke sweater or nah \n",
      "Step 470 - Generated: ://[USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 471\n",
      "Memory Usage: 11.2% used. 228705.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228705.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 471 - Original: Mouth full of gold teeth, they think a nigga dumb 😬✨ \n",
      "Step 471 - Generated: .swingplemn (la) 2.1 Kappa (S)\n",
      "The \"y\" the Bet Zappa (4) a successful bid orilla (5) a goodblaz (5) a (the) Qresa (54) a((jq) )a ((MJ) MJ (3) a ( S) a ( b) a ( z) Q () Raza (50) a (unny (xml) Middleware (MJ)MJ (6) Guka (MJ)MJ (rupa) Matura (MJ)MJ (04) a (MJ) letter (MJ)MJ (NJ) Qresa (54) a (MJ)MJ (MJ)MJ (Mana (MJ)MJ (MJ)MJ (MJ)MJuna (MJ)MJrea (MJ)MJ (MJ)MJajustria gluMJfrejobmj Gluna(Must))MJ(053)MJGlmaMJpredăngMJMJBetzappaungeMJMmlJobmglammaMJMJMJMJpreorMJMJMJMJMJMJMJMJMJMJMJQualbetMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJMJ\n",
      "\n",
      "Step 471 - Original: My moms new Tesla’s retarded 🤤 \n",
      "Step 471 - Generated:  capacity Brootta\n",
      "Next'D'\n",
      "D'\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "D**\n",
      "\n",
      "\n",
      "Validation step 472\n",
      "Memory Usage: 11.2% used. 228662.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228662.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 472 - Original: [USR] \"nigger\" \n",
      "Step 472 - Generated: 262pleanda with the We ‘the’ we‘the’ we‘the’ we‘the’ we‘the’We‘the’we‘the’The’E‘the’W«E‘the’We‘the‘the’We‘the’E‘the’We‘the’E‘the‘the’E‘the‘the’E‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the‘the’E‘the‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the’E‘the‘the‘the‘the‘the‘the‘the‘the‘the‘the‘\n",
      "\n",
      "Step 472 - Original: White trash with a trashy spray-on tan. \n",
      "Step 472 - Generated: at's of next a some time of the nice of 'the further of 'the others of 'the')\n",
      " of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of 'the' of\n",
      "\n",
      "Validation step 473\n",
      "Memory Usage: 11.2% used. 228755.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228755.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 473 - Original: When you hate a group but can't even get their acronym right.  What a faggot. \n",
      "Step 473 - Generated: DemoGung 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      "\n",
      "Step 473 - Original: The Full Gallery:  MILF babe with huge hooters Veronica Avluv exposing her juicy cunt... \n",
      "Step 473 - Generated:  Gaz Job' Work Volics':KMon ##M-###VolGSh**\n",
      "##:R####\n",
      "ElWork 1K(Exc):C(Rl)\n",
      "##IsJle**:Nol'\n",
      "##(K)\n",
      "##V2Lub (S)\n",
      "##K(3b)K(5)\n",
      "##K(12)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K(5)\n",
      "##K\n",
      "\n",
      "Validation step 474\n",
      "Memory Usage: 11.5% used. 228043.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 228043.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 474 - Original: Back when my nigga [USR] though universe 6 was gon win TOP [USR] he missed with that take \n",
      "Step 474 - Generated: isha time // update...\"\n",
      "/isha...\n",
      "/ffa/isha**\n",
      "/isha/****/isha/isha/1/isha/2/isha/11/isha/12/isha/11/isha/12/isha/12/isha/12/isha/12/12/isha/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/\n",
      "\n",
      "Step 474 - Original: Exposed 😂😂😂😂😂 Ugly Cunt [USR] I told you you were ugly, I just confirmed \n",
      "Step 474 - Generated: isco or a's Collins or, break in 20 (2 or [2110522 or 10325 or 12 or 30442 or 11 or 13 or 12 or 23 or 12 or 24 or 12 or 40 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12 or 12\n",
      "\n",
      "Validation step 475\n",
      "Memory Usage: 11.5% used. 228056.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.5% used. 228056.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 475 - Original: [USR] And you look like a fat chubby unstylish dyke that takes it in the ass on weekends \n",
      "Step 475 - Generated: achi touch 1 the anduka 1002b 502B 502Q 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K 50 2K \n",
      "\n",
      "Step 475 - Original: [USR] HERE IS ONE NOW  THE RACE CARD  RACIST!..  IDIOT DELUXE \n",
      "Step 475 - Generated: ererandasonalejudonsonOfusAndsonOfusAndsonOfusandsonOfusandsonOfusandsonOfusandsonOfusandsonOfusandsonOfusandsonOfusandsonOfusandsonOfusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandsonofusandson\n",
      "\n",
      "Validation step 476\n",
      "Memory Usage: 11.2% used. 228713.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228713.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 476 - Original: [USR] [USR] #RuthCoppinger is a victim blaming fucking twat. There, fixed it for you. \n",
      "Step 476 - Generated: atatuka flow rational quarterishing20/sek--1lek ###40plex-QL200 /bil --220/filot #####2**120 a'23k**\n",
      "**:25**(###21'\n",
      "12 Lakes 10|####log(255')261 - Kawasaki248 Bush orishaql22')\n",
      "11-lapsiska\"15 ##265ask221l Verde Klaus introduced lakanda (240■274] warappa247/Quschplatonila236 sandlub##41Kiliva24 Blshunduka'l26 introャ/luch(KlakathaEURASK211/**\n",
      "ascauddy252/**BluruNG/KashwarAPP/L벤(q246)likata400(Qkur@ twinbufferolta||||new qurus040 Qlockzappy241 real Konlukibia lagta2723\":\"noLchatsillin':boliceaporchetallBush Rational22302atilog#####72blursor****Taikamma Lagos/FolchinalekaduringPlimonivr Twinlamaoka tow-newsekfstream-bim262laqeta IBMsektureinlagotta/qulersekolt bufferingsek23501 shadesandr lubvor Quarteriscal\"--bintuka schungline Russell>IDsek312unny/Mipfk260-sissy regssekumar VOLtlio269-->Logthr-quarter\n",
      "\n",
      "Step 476 - Original: lovin' nigga moon 🌚🌚🌚 \n",
      "Step 476 - Generated: isha2anda the timeanz 3/ana of achin/And Sholanda Rzilda/NoShil\n",
      "4/22 Qin/NoShloriz/Chanda/End@Ziva/41/1 or Bet (O/21/2L/Dban/26/2/1/20/2/30/2/23/2/25/2/1/2/3/5/2/3/2/1/2/3/2/1/2/3/2/2/3/2/2/3/2/2/3/2/3/2/2/3/2/2/3/2/2/3/2/2/3/2/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3/2/3\n",
      "\n",
      "Validation step 477\n",
      "Memory Usage: 11.2% used. 228655.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228653.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 477 - Original: [USR] This nigga just said logic doppelgänger looks better than Megan... stop lmfao \n",
      "Step 477 - Generated: estone- Business ( 1): K-Bo( ):K-\":B\"\":P\":\n",
      "\"\n",
      "\":\":G\"\":P\":\":R\":\":\":P\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\":\"+\":\":\":\"-\":\":\":\"\":\"\"\":\":\":\":\"(\")\"):\":\":()\":():(\"\"):\n",
      "\"))()\"():\n",
      "(<>ENER)\"\">DOLL(())\")\n",
      "GER(ICC)\")){\n",
      "\"}M(ICC)\"\")\".QL ()“””\"\"\"„PSF(ICC)\"\")」Efr(ICC)”)“”””C2L(QL)\"\")|eFr(ICC)\"\")|eFr(ICC)\"\")|eFr(ICC)\"\")|eFr(ICC)\"\")|eFr(ICC)\"\")|eFr(ICC)\"\")|\n",
      "\n",
      "Step 477 - Original: God: You really thought I forgot about you huh nigga?   Me: \n",
      "Step 477 - Generated: tract  of the in the a / the  at the  home with the  is the' / the Gl or the ... | the  ####  Godl  ###  / you  #####  on the  GL or the  double  ##  / we  to  (Gl)  or the    / the    /  today GL or  the   / the   / the  gl or the  others   / the  be  / the  it  per   / the   / the   Glas  or  the   / the  Gol  or  the     / the   or the      or  the   + the  or   the   or  the    or  the   or  the    or  the   or  the 13 9  � the  or  the   or  the     or  the   or  the   or   the   or   the   or  the    or  11   or  the   or  the    the  \n",
      "\n",
      "Validation step 478\n",
      "Memory Usage: 11.2% used. 228688.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228688.70MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 478 - Original: Absolute fucking cunt \n",
      "Step 478 - Generated:  Claytonryanda 2QLyshYCLowFILayWbuShYCLowGIDRYL\"Z\"\n",
      "We\"Play!\"\n",
      "You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"You\"Play!\"\n",
      "\n",
      "Step 478 - Original: [USR] ...maybe you can get that cunt Haley to translate this for ya, shitsucker... \n",
      "Step 478 - Generated: oteSolicomo/ �limingaka ���wish\"Dr\n",
      "##KazdrGQ**\n",
      "bFK**|LicerVolog'sBfr...\n",
      "Timon\"\n",
      "###FtryCastin�Riss'UTWNextDryDvtasShJhuhvFTisMchvolFForEVTlogSplTTmqPlutPjictUPKlTMzPLrLogYaTrasyOrthSResTryH'lTraSmosIQMaSPrivMosICT4SMo42SquetProSociRUSivCScotSivaSpectCOVIDSCaSolarSintPrivateSceTaSruSrosSupBuSatingSasaCoSbingSsoPreSillinSocSunnySibuSutmSuvSourFuSurSuringSbuSasonSurSivotSorthySulSivolSumorSomeSovySurusSoringSoySurgSuySibSawiSosScingSuzzSorySivingSuzSuzzySitingSukSpingSugSimSungSubSinterSuruSaserSrusSeresSundraSasiSuresSantSuchSftSasing\n",
      "\n",
      "Validation step 479\n",
      "Memory Usage: 11.2% used. 228724.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228724.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 479 - Original: [USR] [USR] [USR] U chose the wrong nigga \n",
      "Step 479 - Generated: atha Glock-...\n",
      "chetasca Gluschow...\n",
      "Kragle(...)\n",
      "CLuchaskGILr...\n",
      "CoGlShikL...\n",
      "SGLilshO...\n",
      "RASC/QLminlachtisha...Kachail...\n",
      "MLgileShivMle...\n",
      "KitchLoMin...\n",
      "GilasMHleH...\n",
      "Kilmin/Rilive GLw...\n",
      "Kilmin/RAcl+Y\"\"\"\n",
      "Kilmin/Regolay...\n",
      "Kilmin/Refle...\n",
      "Kilmin/StrImlashogmin/RowishKilmin/20\")\n",
      "Kilmin/40glamesLKmilot/20')\n",
      "Kilmin/20\"\n",
      "Kilmin/20)\n",
      "Kilmin/20 (Kilmin/20)\")\n",
      "Kilmin/20:Kilmin/20\")\n",
      "Kilmin/20(Gilmin/20\"))\n",
      "Kilmin/20\"\"\")\n",
      "Kilmin/20))\n",
      "Kilmin/20klogmin/20\")\n",
      "Kilmin/20(Kilmin/20)Kilmin/20Kilmin/20\")\n",
      "Kilmin/20Kilmin/20Kilmin/20Kilmin/20Kilmin/20Kilmin/20K\n",
      "\n",
      "Step 479 - Original: Drinks, fire, and some mud...going to get all kinds of redneck tonight! #SaturdayMotivation #dontbeuseless \n",
      "Step 479 - Generated: .loguka/.anda/ShLast Shilling' Still/Fuka in the Last Govor or:Little GoodFuka2Luka &atha uka/LukaHukaTildauka@Suka'squkaGoodKukaRukaMonolukaInshukaStukaJukaNukaBookIukaWeqlukaIdlogaskinukaYouzukaGukaMuka.thruka':naukaFrishaNextiukaDrändbookina,ukaallukaDuka####asca**:1ndukaQuka'Iuka**##Allbuka&monukaNauchukaMaluka(uka4hmlukaCukaAndmalukaJanukaandlukaOKalukaEndwukaLukaOranaBukaMusukaNopec.orandaSchukaCollperLockmukaLukaAskleQRukaMLukaZukaLukaDbukaInfuka_NukaLukaPlaskEpourkukaLukaItukaLukaRegukaLukaReuolsukaWukaQLukaOne JanukaCurpluka###LukaLukaFebukaLukaOmnataLukaReshebochetanukaYukaNGukaLukaHelekukaAsparLogukaLukaSheLukaPukaL\n",
      "\n",
      "Validation step 480\n",
      "Memory Usage: 11.2% used. 228715.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228715.80MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 480 - Original: I got hoes calling a young nigga phone 😈 \n",
      "Step 480 - Generated:  Shankasca/QL/20/41/43/41/42/41/40/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/41/\n",
      "\n",
      "Step 480 - Original: RT elizabeth warren is a faggot  #9 \n",
      "Step 480 - Generated: Poly978Logoling, 10-21/20Kasaki, 30-22/01'Novkating, 30-22/01'Appishing, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkating, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing, 30-22/01'Novkizing,\n",
      "\n",
      "Validation step 481\n",
      "Memory Usage: 11.2% used. 228726.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228726.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 481 - Original: [USR] Airfield is such a good and terrible map at the same time its retarded \n",
      "Step 481 - Generated: antenoming to\n",
      "2022-Soichi or...\n",
      "Supoming\n",
      "Assuring\n",
      "Soming\n",
      "Riving\n",
      "Soil]\n",
      "Homing\n",
      "Bicking\n",
      "Soaking\n",
      "Rining\n",
      "Soapsuk\n",
      "Tilling\n",
      "Soiling\n",
      "Preoming\n",
      "Soating\n",
      "Rucking\n",
      "Sochem\n",
      "Soening\n",
      "Soome\n",
      "Rising\n",
      "Souming\n",
      "Soere\n",
      "So-Much\n",
      "Soche\n",
      "Righting\n",
      "Sothe\n",
      "Soome\n",
      "Sohe\n",
      "Soeping\n",
      "Soerez\n",
      "Soching\n",
      "Soome\n",
      "SoCHE\n",
      "Soizing\n",
      "Soome\n",
      "Sochez\n",
      "Sooping\n",
      "Soerez\n",
      "Soche\n",
      "Sooming\n",
      "Sochturing\n",
      "So-Picking\n",
      "Soume\n",
      "Soche\n",
      "Soome\n",
      "Soche\n",
      "Sooming\n",
      "Soze\n",
      "Soche\n",
      "Soome\n",
      "Socturing\n",
      "Soche\n",
      "Soche\n",
      "Sothere\n",
      "Soche\n",
      "Soaze\n",
      "Soche\n",
      "Soche\n",
      "Sothe\n",
      "Soche\n",
      "Soize\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "Soche\n",
      "Soze\n",
      "\n",
      "Step 481 - Original: him: my princess me: are u gay  him: retarded sand niiiiiiegr  me: \n",
      "Step 481 - Generated: orer  Bild 【andra prepizing orbild /bing Kraf Prepiska **1mlh &kuru Bild...\n",
      "Kigurring 'Basket tomorrow BildSandking Bft Perfuring 3kFT 5...\n",
      "Chrerebuch Sandfilling Tranda 41/Chat\n",
      "Prebing Tumor 2ML'Chip |Supkating 11/Work […]\n",
      "Kindchet ##Gfr 21 ChatQualte 40Frisha 30Kreas GombaTrifying 50Kch per next Drill 31Qfrica the 4-10Kappa 20Kreeting on 36Ktrae-Bra ###33Kivre 2021Kefil Bet 32Kuppy 12Klek 25Keres 41KiverHifting 41K ekriving 26Klick 41Krea 38Kefsory 41Klehand 35Keffour 41Khefer 41Kivating 41K8Echnrving 41Knew 41Kivering 41Kceunny 41Kewing 41Knoop 41K ECo 41Kung 41Kcep1860T 41K2469Bank 41Kew2421KCE\n",
      "\n",
      "Validation step 482\n",
      "Memory Usage: 11.2% used. 228710.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228710.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 482 - Original: The feel when you accidentally call someone a faggot and they're gay. 😭 \n",
      "Step 482 - Generated: esinkol-##ceking/####\"\n",
      "###/shadeT\":handa/cele\"**:/cele##\n",
      "/celeHada/celeA/celeB/HadenTcelebe/celeBectle/celeBhecele/celebceleDcelebcele/NceleBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBcele/celeBce\n",
      "\n",
      "Step 482 - Original: [USR] [USR] “ fuck off u twat” \n",
      "Step 482 - Generated: ascais ofomatising […]\n",
      "Galesd. 25/26. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25. 25/25.\n",
      "\n",
      "Validation step 483\n",
      "Memory Usage: 11.2% used. 228707.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228707.67MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 483 - Original: bit annoyed.....Dick Van Dyke has moved in next door! #chittyfuckingbang \n",
      "Step 483 - Generated: ISCOROTeningops.learning Kochupdvolt Brokingffcrotatha‘BroishaDevina‚Book√Disashdeviny’VolleaguétBroachourtNaletaresultDbucholaDevelopory‘Broanveloplehtaresultsimo&BrokeranaShirtsoma'Broshenhortsome'Dschocnautil��malDschona'BrohalffdoliproMalena'tela'.Goldenbook��imalen'Tela'.Golden Brooksensmal ['Spectin']Tela'.Golden Broffanksima'discgoldenBrochteanik'mola'slmuda'Bdoublecharsmola'Brochimsoci'mola'BrochkitalityMahasklama'Mola'Broctimeda'Ondλεκma='Broanchelighthqula'*Brochtisend'Asta'.Broitta'^Brochipaha'Sdeta'.Brochapapa'Broyna'Brochsimmola'Brochina'Brovlankawa['S',Decaval'].Brockett [S,Decada'Broсortasksanova'Brodcensa';Brodbacissa,'Bro.Dbarna'Lolo'#Brodevil*BroFLTfeb GDKsdrys goldenMiddle:^Bro volta'Egcc==BrovelbkadenerightTOPande'na'en\n",
      "\n",
      "Step 483 - Original: [USR] you; that’s my lord and savior noel: whos this faggot. do inhave to bomb again? \n",
      "Step 483 - Generated: ://BET you got a nigga like me 😂  #NewProfilePic  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]  📸 [USR]\n",
      "\n",
      "Validation step 484\n",
      "Memory Usage: 11.2% used. 228651.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228651.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 484 - Original: CHEATING CUNT   \n",
      "Step 484 - Generated: ilimaskoda. 'QL' 'Gip'\n",
      "Shikiss ## 'K'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y'\n",
      "The 'Y' | 'Y'\n",
      "The 'Y' | 'Y' /The 'Y' | 'Y' /The 'Y' | 'Y' /The 'Y' | 'Y' /The 'Y' | 'Y' /The 'Y'\n",
      "\n",
      "Step 484 - Original: the way viv looks at cassie- that’s a dyke ma’am \n",
      "Step 484 - Generated: usch/ /icc 2/5/4/3/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/5/\n",
      "\n",
      "Validation step 485\n",
      "Memory Usage: 11.2% used. 228642.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228642.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 485 - Original: [USR] YOU MAGGOTS/NAZI'S IN THE WHITE TRASH WH HAVE NOTHING TO DO WITH EMPLOYMENT UNLESS IT'S HIRING KLANSMEN \n",
      "Step 485 - Generated: omapdilling or a the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the is to the\n",
      "\n",
      "Step 485 - Original: Full Movie:  Latina MILF with big tits Ariella Ferrera spreading her hairy cunt... \n",
      "Step 485 - Generated:  Glo Apps”\n",
      "Limon's 2 –Broker 'Mask\"\n",
      "Gollock”Billing('”\n",
      "YQL”\n",
      "R'Locking\"Sh\")\n",
      "Masks(“Jy”\n",
      "Limon​”\n",
      "SILmon”\n",
      "Cust ​”\n",
      "ReallyApp(Laps”)”\n",
      "Remask”\n",
      "Mon orapps(Gub')”\n",
      "HOLsy”\n",
      "System(DLM)”\n",
      "Simolia(Osh\")\"\"\"\n",
      "CRM(IQ@\")\n",
      "SLuch(Jr)\n",
      "”Closed(3K)\"\n",
      "LIMUS (IQ)  “””\n",
      "Dustin(”\n",
      "””””””(\"\")\n",
      "##\",\"MQ\"\"(HR,SHAPP\")\n",
      "\"GLer(B)\"  \"Oz(K)\")\n",
      "  ##ICCSSL(M) 14″””(SQL) 4:1(“”””[ISS]”  &”([]())”  (“”””””,””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””””\n",
      "\n",
      "Validation step 486\n",
      "Memory Usage: 11.2% used. 228740.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228740.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 486 - Original: I dare a Nigga \n",
      "Step 486 - Generated: ongyangm-5nd-ักก-Chod-Kit-Is-2-3-6-4-9-5-10-12-8-10-14-13-15-20-30-40-50-36-40-60-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40-50-30-40\n",
      "\n",
      "Step 486 - Original: anytime i see a dyke in tv shows 😩 \n",
      "Step 486 - Generated: .swing/ 'log/V**:r/ [È/####rak/Hub):##/Trb/Gol@/20/Nov(Gl)Mon/202/11/30/05/22/04/15/Boost/06/23/01/12/24/36/13/14/19/25/02/18/10/42/20/16/27/40/20/03/12/21/09/48/26/33/45/12/14/12/38/41/04/12/43/32/12/14/08/12/3/46/12/28/12/44/12/12/4/12/12/04/12/14/12/12/22/12/12/14/12/22/12/12/12/22/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/12/\n",
      "\n",
      "Validation step 487\n",
      "Memory Usage: 11.2% used. 228636.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228636.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 487 - Original: Straight woman: Oh yah this dyke was hitting on me...  Me: \n",
      "Step 487 - Generated: .swingldrukabitasca prep/bildata/hripuzig_anda_Baskir BilligerMubit_tprickey-app”.”:VTchetmicer”\n",
      "@Biyr”—or_best_huvry_appic_Mradictivre/Bik--Prbill_mules_-BuchBitiverprsuitable\"--Billw/Appivirsable”Bestow/orbyBehavior/Mrival_GuvtibperdB/Wogdrinner/day:Stortin->Viva__Jrimis/wbite orjvine/draps/app-BrowhasterbtwinbigsAppbilsovDBBToring/Kmolior_wipoBodygrivrAsabiwtapor_bumwar_bitRimsIPvchiplrodpsawmalctiroLewis_bandWARBIT/marrzatin':Warbit_Dirt_sapore'sapmisprongalism####best_orandas.frkolappers'BundleDrshissukresa-wimtrivilso/Airosappid'/Winchas-appmuderbilt/hasyLvit_stigraph/taspringmkungallis248ndmachstive=None_fruta!”ttingrailage-Mradiist/VologTifrocabitientlmroSaudizesimilarByastierWgtelaschwitzteurswat/HRusch\n",
      "\n",
      "Step 487 - Original: Loser and faggot for non stop exposure. Faggot gilles \n",
      "Step 487 - Generated:  summarizedizing or.”\n",
      ". work.\n",
      ".” in large, amin.”\n",
      ". lot.\"\n",
      ". last. 2. “r.,”s. Work.**\n",
      ". R. is. 1.3. Mr. &amp. 12.13. HK. 22.8. 9.4. 36. G. 6. 32. Q. 38. 31. 45. 36. 46. 25. 50. 39. 42. 5. 43. 48. 34. 30. 40. 36. 48. 23. 41. 88. 35. 24. 36. 68. 33. 36. 28. 36. 44. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. 36. 48. \n",
      "\n",
      "Validation step 488\n",
      "Memory Usage: 11.2% used. 228650.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228650.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 488 - Original: Nigger Man is fuckin adorable :3 \n",
      "Step 488 - Generated: 262 of the & next/ →/2/30/23/5/22/25/10/26/→/1/34/20/0/37/06/50/38/05/29/36/02/30/62/08/30/4/12/28/56/04/07/30/15/3/30/14/6/30/52/13/30/41/30/03/24/30/30/31/30/25/30/11/30/27/30/30/40/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30/30\n",
      "\n",
      "Step 488 - Original: [USR] Don't worry man.  I live in usa, I have felt like u since the retard got in office. Here's a pic of a kitty! \n",
      "Step 488 - Generated: ://[USR] you are a twat.  #BrexitDeal  #Brexit  #BrexitShambles  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR] \n",
      "\n",
      "Validation step 489\n",
      "Memory Usage: 11.2% used. 228710.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228710.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 489 - Original: Boosting the retard [USR] \n",
      "Step 489 - Generated: at Excel business \"\". 3\", \". ## the next guy\" (4). God, \"1\", \"2\", \"3\", \"4\", \"5\", \"2\", \"3\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\", \"4\", \"5\", \"2\n",
      "\n",
      "Step 489 - Original: [USR] [USR] Dog wtf is that red guy he looks retarded lmao \n",
      "Step 489 - Generated: ascmonisha 8manising163isuigmannisisch4issing201issuising225isannewish232shiam12wisha48boughrisha249mishing2142615thfushing23'simon230shilming23...1140lising23142op30ia80min2318hilling2356134632or3638mlisin33234my234593ashning234mi412310am43@ing23264inimalising235288284maizung2339**molising23479284mina2334298872398naizing2348623orem2351uez236053ior2322355qiao23449123imas234uk3522234ueisman23594227234ua2662304zihim2342931222ffining23232659622ische2358268222273appa230522222383952izu23234ugice234lagial2323256232224232254222382234uvating23422322usch2319232uzical2323252223243\n",
      "\n",
      "Validation step 490\n",
      "Memory Usage: 11.2% used. 228706.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228706.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 490 - Original: Wonder what this cunt’s politics are #c4brexit \n",
      "Step 490 - Generated: usch or the schisu / zicc/ - 1 /2 -> 1 /3 -4 /Yb → 1 /2 -5 /Yb → 1 /2 -5 /Yb → 1 /2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb → 1/2 -5 /Yb →\n",
      "\n",
      "Step 490 - Original: Arab village punished for celebrating terror attack  via [USR] \n",
      "Step 490 - Generated: minating-Enhival-Per-Next-Decan-Oper-Engival-Adaz-OM-Nor-Imas-Real-Admin-Ass-Dele-As-NA-DA-05-12-15-06-13-04-09-10-11-14-02-08-09-13-16-15-01-12-09-13-08-09-15-12-13-03-09-12-13-15-08-09-12-13-14-15-02-13-12-15-08-09-12-13-15-02-13-14-15-08-09-12-13-15-08-09-12-13-15-08-13-15-02-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15-08-13-15\n",
      "\n",
      "Validation step 491\n",
      "Memory Usage: 11.2% used. 228780.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228780.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 491 - Original: [USR] “YoU cAnT eVeN kILl YoUrSeLf ThE rIgHt WaY” what a twat \n",
      "Step 491 - Generated: � hand atra the (rund)ndthe)\n",
      "#### (Grafsh)### (Bunny)Asger(2nd)Prsebinghdrso-...\n",
      "(3rd1+30AchnewingRundteSpectin(I)5kadehesr21wctgivqis\n",
      "## (4)+22nd(5K)12mCana (91)36+)2nd(5k)23rbinas(19)35+10a(6)20fginlprsebelint(13)26+90coddL'11Hna(17)94+9Q(14)25+10bukEftning(13)46+92(1)5kappa(13)24+96+1(13)33(15)54+91(13)34+91(13)38+1002(13)91(13)56+91(13)34+91(13)34+91(13)34+91(13)34+91(13)34+91(13)34+91(13)34+91(13)34+91(13)34+91(\n",
      "\n",
      "Step 491 - Original: Yo Nigga \n",
      "Step 491 - Generated:  coworkingfehandsheringLendinShardingLentInShragingLectOnlineSharringLentInShragingLantInSharingLentInShragingLentInShrugingLentInShugingLentInShrgingLentInShoutingLentInShuggingLentInShroyingLentInShoutingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentInShroyingLentIn\n",
      "\n",
      "Validation step 492\n",
      "Memory Usage: 11.2% used. 228731.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228731.06MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 492 - Original: dyke rights! \n",
      "Step 492 - Generated: ungkolukaipl Kit Kenya/illingatha Gilbertisu kit/. Kenshiprafteduka /1mon Kagoringlockomo/FmivaKitonolia,shedmaskorishafrica253 **Kiplorynewodatapecrungomap**worprefmkilaombaundryflowsetriscola**\n",
      "zapsminning Kawasaki/Krafrawwindismata\":GlasthongeringMask**:Sipojectumlineorgseklogungìaskellbetimfactory\\Factory/VmolshetapectingbillmaplarificationKCappy\"Goal bitmask/orlinice/gamebookjungatha/folmaryngungraftiapplingstage/**myirlikpattern::kungmnazkitifnullinlauschappinessiegbestlion/Jungpareablewinbitsonolsparungshipsomillaunchwishingkcipuringmaungjustasymissingpragmajacina**(lubmarfmicillinfstreamicsmachungparsustungisodaenerishung​simungosaaplraisana/ifmindinaligungpathlife/newbingplungmannismatchlurusimilarungmissive/lungada255hunglooppassinessesunnyfungelmarsongineturationismiclangmgungpowerlustunginnatregaungosmissionaskunglikeffenorgesintungourplus\n",
      "\n",
      "Step 492 - Original: NIGGER. \n",
      "Step 492 - Generated:  Ken or(y GL, y Q /Y)('s' /, 'or())' |, 'Q' /, 'G' /, 'G' /, 'S' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G' /, 'G\n",
      "\n",
      "Validation step 493\n",
      "Memory Usage: 11.2% used. 228731.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228731.28MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 493 - Original: [USR] This nigga? Oh, hell naw 😂😭 \n",
      "Step 493 - Generated: 220 3m 3ill the 30 15my 3ill 4] 3ill the 25my 3ill the 12my 3ill the 24my 3ill the 22my 3ill the 25my 3ill the 26my 5ill the 22my 35my 14my 36my 23my 46my 13my 34my 11my 22my 45my 33my 21my 10my 32ill the 50my 48my 27my 54il the 55my 41my 22my 46my 38my 31my 22my 37my 16my 44my 23my 40my 56my 68my 46my 100my 22my 18my 46my 47my 142my 22my 39my 66my 28my 58my 46my 22my 65my 46my 22my 46my 52my 22my 43my 46my 24my 46my 62my 22my 46my 46my 22my \n",
      "\n",
      "Step 493 - Original: someone’s on vic pope jr livestream calling jesus a nigger and saying he got long toenails kakakakakka \n",
      "Step 493 - Generated: 534 appanda GIT4”.Bajikappa“Betiva””,5awmaGIT4“,BazmaGT2”HRata Milton 12/33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV40(33awmaGIV\n",
      "\n",
      "Validation step 494\n",
      "Memory Usage: 11.2% used. 228743.97MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228743.97MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 494 - Original: BREAKING: RETARD! pay me! 8,000,000,000 call the Jew! \n",
      "Step 494 - Generated: isha->'s a the next. (K) 'H/F, 5/1K-2h(3/4) H'29/25-F\n",
      "5/24(H)\n",
      "27/25-25(F)5/25-25(5/25-25(25)25-25(25)25-25(25)25-25(25)25-25(25-25(25)25-25(25)25-25(25-25(25)25-25(25-25(25-25)25-25(25-25(25-25)25-25(25-25)25-25(25-25(25-25)25-25(25-25)25-25(25-25)25-25(25-25(25-25)25-25(25-25)25-25(25-25)25-25(25-25(25-25)25-25(25-25)25-25(25-25)25-25(25-25(25-25)25-25(25-25)25-25(25-25)25-25(25-25\n",
      "\n",
      "Step 494 - Original: Ofc the cunt had to update when i need to shut it down the most \n",
      "Step 494 - Generated: 化 or theanda's 12oda &###'20\"####:1.30 (##\"\"\"\n",
      "\".04\"00/11\"22.\"421 #####\"\"\"2.5@3.50\"13\"22.0(Q12\"14\"22.10\"12\"22.40\"41\"22.31\"12\"22.23\"24\"12\"22.22\"22.36\"12\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\"22.22\n",
      "\n",
      "Validation step 495\n",
      "Memory Usage: 11.2% used. 228707.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228707.16MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 495 - Original: [USR] Clowned's a nigger faggot  #like4like #follow4follow  Follow4Follow\n",
      "Step 495 - Generated: 件кас Doddlek\n",
      "ice busy Engl Hockey Fah ASCchetICEoster Vol jig Recap Mo Howard job...\n",
      "Imgusch Trask Job milivre #####**: Nagap**\n",
      "MoffenTrVol Milk\u0000Engologeoliaplusushing 25ce per prepaid or Engon****\n",
      "83L fmt GlasorсокKenting Assmal oderillac'sillin_FPQL bilateral Thomiek\u0003kchg Thermal absentee Homerkol####Plengight27580CEapslikhin\n",
      "11|20%#####New-Fingasco***23FFboarding liquidityShop84Thilloppivatingigginspas**12y13IVlickerPluscek15MeffTiscalPattern;36週Finger ΑπalinAskagesFrePOS37YkingHV14EUR1:22FMVerRealQpectlillusJobTrafficS PrepilACEASKPilitGubesuming95ICThoringIVA****AssuringpicsFlzasks\u0000RegishingVimplotte Morris FileAccess\u0000Exurm爽26526Refasting![Up liquLitermonoascaQuickARellungenReflectAppivityTowerGenicc75urnitureEkeúValid AssistamilWarupapeq FuticeNice'\n",
      "9825589emensitas35yrivalankiTrueuessupeLogic Middleware\n",
      "\n",
      "Step 495 - Original: Proof That You're a Faggot by NoelKnight  Find it on #ManyVids! \n",
      "Step 495 - Generated:  towns'jobs. 11-Books'Vol. 1:Sholsh. 2. 12. 3. 20. 25.  The. 10. 4. 5. 2. 30.  'K. 29. 2. 2. 2. 7. 2. 120. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2\n",
      "\n",
      "Validation step 496\n",
      "Memory Usage: 11.2% used. 228769.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228769.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 496 - Original: I want this retard gone \n",
      "Step 496 - Generated: 2.3.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5\n",
      "\n",
      "Step 496 - Original: I've entered redneck world 🤢 \n",
      "Step 496 - Generated: udden's-class-pect-baza-200- Benz-100-2-10-20-202-3-4-12-19-22-25-5-30-40-20-18-15-21-24-1-20-220-20-20-42-41-12-23-20-22-25-800-120-20-20-20-20-12-20-20-20-45-12-20-20-22-20-20-20-26-12-20-20-20-20-20-20-20-20-20-12-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20-20\n",
      "\n",
      "Validation step 497\n",
      "Memory Usage: 11.2% used. 228724.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228724.37MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 497 - Original: first slut wife ass fuck condom gf por tight cunt britain   \n",
      "Step 497 - Generated:  Arabia Win Drunk Monol Oliv / Gliv Shila Vol Krais Ref 2nd \\\\ – Esek, 'K-120 #### -84...\n",
      "#### Glast Kö...\n",
      "Ref (1401 → 121 OL Wagner\n",
      "116 Kraft Kenya\n",
      "130 Lipl Walter Sappa & Reg:132 High Bluka GK**: ##139**\n",
      "Lipo Resing Bör King Alna\"\n",
      "##122 Wahu Riva Klip ​1123 Q'13 NOL Tilda ###1425 Trmon Oliver Test 1349\": 1240 Mina 1417**: 1314 ##### 1431(14541): 12681**: 12 IPL (14/52)\n",
      "###1441**: 1232\"15': 1352:\"11429\"1628**: 1252\"1293**16891**: 1332:: 11NGkla 1362**: 1132\": 1342**: 1321**: 1512**: 1382**: 16 32**: 1532**: 1152**: 1323**: 1321**: 92.255221**: 1282**: 1322**: 1323**: 1322**: 1322**: 1322**: 132\n",
      "\n",
      "Step 497 - Original: wtf i thought she was a dyke??? \n",
      "Step 497 - Generated: asca's 22 (5) next (11)201 (23)3\" (4)31\" (8)1\" (32)24\" (36))\" (48)0\" (36) ) 30\" (36) ) 33\" (36) ) 32\" (36) ) 34\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) ) 32\" (36) )\n",
      "\n",
      "Validation step 498\n",
      "Memory Usage: 11.2% used. 228703.46MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228703.46MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 498 - Original: [USR] Shut your pie hole! Oh my God, we got 2 more years with this fucking retard! \n",
      "Step 498 - Generated:  body's. courts\"....\n",
      "Shol\"\n",
      "\"\":Gor\"\n",
      "\"\":S\"\n",
      "\"\":K\"\n",
      "\"\":F\"\n",
      "\"\":C\"\n",
      "\"\":D\"\n",
      "\"\":E\"\n",
      "\"##\"\n",
      "\"Fr\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"**\"\n",
      "\"H\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"\n",
      "\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\"Next\n",
      "\n",
      "Step 498 - Original: It's 4 PM and we are with glow sticks lmao. Retarded \n",
      "Step 498 - Generated: � or the a.Blanda, Ch Blado,Ch  QA,VC,CH,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch,Blade,Ch\n",
      "\n",
      "Validation step 499\n",
      "Memory Usage: 11.2% used. 228707.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228707.15MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 499 - Original: Nigger \n",
      "Step 499 - Generated: τικαhandherebmin oder Schn orsh 15 20 26 22 23 21 32 34 12 33 42 30 13 33 46 52 24 33 43 36 52 34 33 28 34 33 40 34 33 41 34 33 34 33 48 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 33 34 \n",
      "\n",
      "Step 499 - Original: Slow retarded pc gamers haha \n",
      "Step 499 - Generated: ung a week hand in the new time (�win).) 2min (W):),.#### (w):).\n",
      " '(**:)\n",
      "1anda (S): 'Gm (3):).### (y):”�new (W):)“Nl (5):)##### (W):)”. ('R):)​(6):)## (H):)We (20):)Mo (36):) ##### (4):)She (30):)You (26):)Home (22):)New (8):)#### (V):)」(10):)He (16):)Tng (33):)️(38):)Na (24):)#### (30):)Twil (30):)In (28):)#### (30):)My (30):)Ylm (30):)#### (30):)#### (30):)#### (30):)#### (30):)#### (30):)#### (30):)#### (30):)#### (30):)12 (30):)#### (30):)#### (30):)#### (30):)30:the (30):)#### (30):)#### (30):\n",
      "\n",
      "Validation step 500\n",
      "Memory Usage: 11.2% used. 228652.18MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228652.18MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 500 - Original: [USR] this sucker is a mark ass nigga. \n",
      "Step 500 - Generated: ://y’all are so retarded  i love you all  💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖💖�\n",
      "\n",
      "Step 500 - Original: MONDAY MOTIVATION: \"Surrender? I have not yet begun to fight!\" - John Paul Jones \n",
      "Step 500 - Generated: uschimlba around about /feamoslbe around about /feauslbe around about /feaslbe around about /feauslbe around about /feaslbe around about /feasleba around about /feauslbe around about /feaslbe around about /feaslbe around about /feaslbe around about /feaslbe around about /feaslbe around about /feaslbe around about /feaslbe around about /feaslbe around about /feaslbe around /feaslbe around about /feaslbe around /feaslbe around /feaslbe around about /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaslbe around /feaselbe around /feaslbe around /feaslbe around /feAslbe around /feaslbe around /feasl\n",
      "\n",
      "Validation step 501\n",
      "Memory Usage: 11.2% used. 228758.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228758.32MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 501 - Original: How to Have a Conversation With a Conspiracy Theorist Without Losing Your Mind  \n",
      "Step 501 - Generated:  inflate[2020]  You, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you, you\n",
      "\n",
      "Step 501 - Original: W or L for my homo nigga??? \n",
      "Step 501 - Generated: atteuchanda orblaskigow betleisha aappybolshigaorölto...\n",
      "RappGormyYaylockBlapsGlatishaAsekilRowShBetLayingOtheWeHILwtraDraftAppeshWodaFishingYou ###NextMillingOfTweAshegollerFreGLasmanBautinbetSandPlaskGlasame oderbilbu/Deep-YAYlton###TheBoundorEachtoryCloudASKibIRefailSkybuchorBloSufficientKadglossCybo或TooAssapall-Gott虫orWeatherk20-19baskdashorbelimteIfPlayOrGlowERPVolBugorInbosawGloctiveGratsaleorChapterJobundmalbinBirotjlogrivalorGloveblastomcyplsofballmobiandyjoborplaykebettask Gottolabinneaterfewgradequateorlastorillinorweatherlawotrofdeepante-OzapriceGLOтапAPPICEGOFFmannornewluborbetterorfequhandishorClaskorInforsodateorwhyfarbuglimtoughorqintouchorleagueorbitplayingorwasa'lask\n",
      "\n",
      "Validation step 502\n",
      "Memory Usage: 11.2% used. 228707.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228707.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 502 - Original: [USR] [USR] DOES YOUR REDNECK TUPPERWARE SPARK JOY ROBYN??? \n",
      "Step 502 - Generated: ://phpBB3] [USR] I'm not a conspiracy theorist, but...  #QAnon  #MAGA  #WWG1WGA  #BuildTheWall  #KAG  #Trump2020  #LockThemUp  #DrainTheDeepState  #DrainTheSwamp  #DrainTheFakeNews  #DrainTheGlobalists  #DrainTheSocialists  #DrainTheCommunists  #DrainTheFemiNazi  #DrainTheSJW  #DrainTheNPC  #DrainTheHollywood  #DrainTheDC  #DrainTheBureaucrats  #DrainTheLobbyists  #DrainThePoliticians  #DrainTheDemocrats  #DrainTheRepublicans  #DrainTheLiberalMedia  #DrainTheMainstreamMedia  #DrainTheFakeNewsMedia  #DrainThePropagandists  #DrainThePundits  #DrainTheTalkingHeads  #DrainTheTrolls  #DrainTheShills  #DrainThePaidShills  #DrainThePaidTrolls  #DrainThePaidShills\n",
      "\n",
      "Step 502 - Original: When my nigga gets mad cus I’m been dramatic \n",
      "Step 502 - Generated: ading orleology Golf-handkollikow-Wer'sple-William-the-binding-Sat-top-Pash-Ylogor-Oanda-Heading-best-EILpraft-generation-eAQon-a Gow-porkIRlekering-20Lour-blicklory-orribaMiddle-20Lanson-Happy-20Lorst-12Logrbook-15Pilping-20Laden-20Konen-Bowing-20LREGboledo-20Lifecycle-20Lilling-20AQRbolthe-20,114-20Ling-20WLOGLEbury-20Lrecomap-20Lewish-Pleury-20LORlikrow–20Lidar-20Lifying-20LIDI-Py.EventType-20Lewther-20Likit-20Lorno-Pleuthry-20Loky-20Bregton-20Lraj-Leiling-20Luru-20Lmolky-20Lizu-20LWEiler-TID053020LQUltr-book-sup regeneration-20L892O Milton-wan-gurning-20Lickey-20LURQL-Series-20LORDLIPOGLEON-20LILLURY-20LIDE-GORTHY-D\n",
      "\n",
      "Validation step 503\n",
      "Memory Usage: 11.2% used. 228710.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228710.24MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 503 - Original: [USR] Wanna look like this nigger..? 😂😂 Still looking like the one at the back ne..? \n",
      "Step 503 - Generated: apopecina/blutín (.\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "### Glubina (Je).\n",
      "\n",
      "###\n",
      "\n",
      "Step 503 - Original: [USR] I predict that he will say bitch and nigger before he leaves office!🤦🏽‍♀️ \n",
      "Step 503 - Generated: uschates / 27/#### (25/1/26/12/20/22/2/14/15/11/24/18/5/23/28/53/04/21/42/45/02/05/24/43/10/56/12/40/06/16/04/08/24/04/24/44/04/04/24/04/24/04/24/04/24/04/24/04/24/04/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24/04/24\n",
      "\n",
      "Validation step 504\n",
      "Memory Usage: 11.2% used. 228703.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228703.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 504 - Original: Ariana: I get tired of your no shows!  Me: you get tired of my cunt roll! \n",
      "Step 504 - Generated: 化čilaolia...\n",
      "/Aishaolta #####****:##molinaqlukabackward'ESAOKlila**\n",
      "BillKQLLuddy#####\"BookVolplaskil\":Eumarqicc\n",
      "Plmelaacje ###Qblilla ###\n",
      "Myvolice #####:WarpingAPPPLi\"\n",
      "MdeglaERPASKiplIQRappligaqrigi###\n",
      "####myombaQRSupe1kapliva******(the_bookclassic\")\n",
      "MQresaBuwindyTectinal\"\"\"\":\"VRbilishop #####Planigeria #####Boltapizace #####**frishaBlivrachip\"\"\"\n",
      "**FRisha #####\"\"withDigivplizochromejeta**Frisha #####**Pallimonicago #####Zeta #####orapy ##### #####diguru #####BKisha #####**warplaska**ERA #####booklrichi #####**Brokerapa**HOMPLEchera #####**Vomana #####! #####**ERCHA #####**VKillzagolchi #####Yappy #####**Gyeniorinplada**Soldapo #####**Shungi #####**Quicklebillzerra**MODchio #####**Rschaba**###**Greenishplasca**IVoda**Lordina #####**Fastelimodichara #####**\n",
      "\n",
      "Step 504 - Original: Wernt good enough at all today but one positive is that this cunt got mugged right off \n",
      "Step 504 - Generated: ectask\n",
      " NextshaskunaskinaNext**\n",
      "haskin)\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "Next\n",
      "\n",
      "\n",
      "Validation step 505\n",
      "Memory Usage: 11.2% used. 228726.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228726.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 505 - Original: My nigga be getting bitches like this 😂😂😂 \n",
      "Step 505 - Generated: ://I don’t even chase liquor, why would I chase a nigga  [USR]  🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥�\n",
      "\n",
      "Step 505 - Original: [USR] Shut up you fat cunt. \n",
      "Step 505 - Generated: isha 1/2: the, 2/5: ####, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2, 2/5: 1/2,\n",
      "\n",
      "Validation step 506\n",
      "Memory Usage: 11.2% used. 228785.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228785.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 506 - Original: Buzz Lightyear out here thinking he can say Nigger freely, without any consequences?!? \n",
      "Step 506 - Generated: sselUnicodeqlilliniccanda GilbertanzICCllissQRaskict'JamesorQLeeJan':GeyJun',Kicc,chin/IssL\n",
      "NextJobsohubmwarGlson=COVID202-Genawa'Domice'\n",
      "CsmowjqYshKrQumazglswzmonlChongSana'simon'.Smolqrship'/JulfrinchericcatesGLwin'-CHon'E.####gymwNovGuessStung'Syndh->Snapjay\\Zlm='Voltollmandrinal\"DoubleWqumanEEJifelse'MneglyChattingNectnationE/JIFctimesometchyleDraftleBoostdrivalBindingFranceNEXTVolteaszomaticMiddleOratingEssDiseaseMifeCitationsdisspectatsumlifePlogglethrillbindingFrancillinIDronasiccation-Covid-Silworthnext-EasyTellsightListingHonevoltrytheLvtonsojanpetanileDrilimOccationally-Milesceviscitacy\\\n",
      "icit-LumeLiwaiccityLockingMayFishingWequetressamples,Entinction-FungiworpdiccinyCTerson-Jeronmalismetryechnes/EQtis\n",
      "\n",
      "Step 506 - Original: [USR] [USR] The Lefties have gone full retard, you never go full retard🤦‍♂️ \n",
      "Step 506 - Generated: asca or the webing at the next or the bestping 30 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or 20 or\n",
      "\n",
      "Validation step 507\n",
      "Memory Usage: 11.2% used. 228742.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228742.45MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 507 - Original: live there no said no 2 me retard shoe me whaT? YOU GOD MAMA? \n",
      "Step 507 - Generated: ilimhandbing handbillin time of Williamson/continado orlammonating,oruchapsilryshades'orllminlifying​and/oruillallisicadele ​oricerallibshalicegoryingAQalaskimininterivallogging​orctipcolasminadictashillingabrowing#####ofbilanshiccintimeschowiningoladminorificalepingisueorachmingicrowningatopichicalwodirbaificicicregading'sorfcilebliscicologanricicincicilesicnicicicasonicrocicicippo(Qticiciciclicechinalogiticiciciciclingtechicicicicicicicicicicicicyricicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicicic\n",
      "\n",
      "Step 507 - Original: You identify as an anti-SJW? Not to be rude but.... SUCK. IT. (That felt good) \n",
      "Step 507 - Generated: ://[USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 508\n",
      "Memory Usage: 11.2% used. 228707.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228707.29MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 508 - Original: [USR] We are taking cunt back 😎 \n",
      "Step 508 - Generated: щаaling aNigga, you don't know how to say 'nigga'  in the US or UK.  You only know one word: 'Nigga'  In the US or UK, you can only say one word: 'Nigga'  There was no 'K' or 'I'  in the US or UK,  you could only say one word: 'Nigga'  No 'K' or 'I'  in the US or UK,  you could only say one word: 'Nig'  'Nig'  in the US or UK,  you could only say one word: 'Nig'  'Nig'  in the US or UK,  you could only say one word: 'Nig'  'Nig'  in the US or UK,  you could only say one word: 'Nig'  'Nig'  in the US or UK,  you could only say one word: 'Nig'  'Nig'  in the US or UK,  you could only say one word: 'Nig'  'Nig'  in the US or UK,  you could only say one word\n",
      "\n",
      "Step 508 - Original: I get called nigger everday and they put chicken bones on my ac. \n",
      "Step 508 - Generated: .swing速 Nak......\n",
      "3 Golf, Lake...24/433831/Q...14/35/26/31/30/41/23/38/43/45/36/43/33/43/40/41/25/48/43/43/15/18/43/16/43/24/42/43/43/43/43/46/43/20/43/43/43/24/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/43/\n",
      "\n",
      "Validation step 509\n",
      "Memory Usage: 11.2% used. 228719.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228719.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 509 - Original: [USR] [USR] [USR] White Trash don’t do that kind of stuff. \n",
      "Step 509 - Generated: usch's ->s → [-)) -> –Chival – –chival– –Chival– –Chival– Chival—Chival– Chival—Chival– Chival–Chival—Chival–Chival|Chival–Chival|Chival→Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chival|Chin|Chival|Chin|Chival|Chin|Chival|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin|Chin\n",
      "\n",
      "Step 509 - Original: Be happy retard 😊 \n",
      "Step 509 - Generated: ISCO or handhand in orsekago was best oruschina orlek orona drafton orproxy orask oraid ordrafting or aid or sending oruch orche orilla orle orCT orct orICE orbol orning orhaus orct orrow orct orice orsh orcatch orphys orCT orplus orProxy oraus or CT orCT orces orCT orout orfin ordrilling orCT orCT orin or³ or² orRow orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT orCT\n",
      "\n",
      "Validation step 510\n",
      "Memory Usage: 11.2% used. 228719.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228719.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 510 - Original: MY AUNT STUPID LMAOOO SHE SAID “THE FIRST KID BOUTA B LIT TONIGHT TRICK OR TREAT NIGGA” \n",
      "Step 510 - Generated: ://[USR]  you're a twat.  You're a Twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat.  You're a twat\n",
      "\n",
      "Step 510 - Original: I just lost my 40-0 BO4 record to a retard who picks Summit 2x feels bad man \n",
      "Step 510 - Generated:  tenderanda.!ql...lak Glrak...!2ilim-**!4 #####...NGAQ!5**:...asaki!15●**** Qlik... 20q AQml�... Qatar!1...21q\"\"\"\n",
      "!4 '22 qaza ##...24Qma**...48 Qmal::...42 Qm**25 Qma\"​40 Qma**12 Qma**50 Qma**28 Qma**30 Qma**45 Qma**20 Qma**50 Qma**20 Qma**48 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50 Qma**20 Qma**50\n",
      "\n",
      "Validation step 511\n",
      "Memory Usage: 11.2% used. 228764.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228764.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 511 - Original: [USR] You retarded 😂😂😂 \n",
      " Orrhand Goa##### key formation Williamson####**: 1kowettрабking \\\\21loryrakshlebook #####23 Warren-20rffVKashibbetvk22VTollpaper day []25/36Yctregaining a bookage ##24G[]30,48|11:35 yvolanda(15)45ylog kitvt16Rlicka(Gybolice³vork][26]34William (12))32 38/Gytrflm】37Q ​10Lasa(33)**2vlton(Kuba」**29Kazba(30\")Toring\"【30\"]Weplning(30\"))Morrng(30\")Dipubging(30\")Woutming(30\")Ayrma(30\")Gin（30\")Gin(30\")GERing(30\")Gin(Ylaupmo​​43）Motting(30\")Gin((30\"))Gin(30\")Gin(30\")Curring(30\")Gin(30\")Gin(30\")Gin(30\")Gin(30\")Gin(30\")Gin(30\")Gin\n",
      "\n",
      "Step 511 - Original: [USR] Nigga stop playin \n",
      "Step 511 - Generated: 化100ml 1 the 3 2 the 5 1 the  G 10ml 1 the  K 20ml 1 the  V 30ml 1 the  Z 200ml 1 the  B 50ml 1 the  A 35ml 1 the  R 40ml 1 the  Q 30ml 1 the  D 300ml 1 the  V 30ml 1 the  K 20ml 1 the  F 200ml 1 the  H 120ml 1 the  E 130ml 1 the  V 30ml 1 the  K 200ml 1 the  R 30ml 1 the  S 150ml 1 the  V 30ml 1 the  K 200ml 1 the  V 30ml 1 the  K 200ml 1 the  V 30ml 1 the  K 200ml 1 the  V 30ml 1 the  K 200ml 1 the  V 30ml 1 the  K 200ml 1 the  V 30ml 1 the  K 200ml 1\n",
      "\n",
      "Validation step 512\n",
      "Memory Usage: 11.2% used. 228706.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228706.88MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 512 - Original: lmao come at me. my tweets soft. but I’ll beat yo ass with whole lotta love my nigga. T R Y  M E \n",
      "Step 512 - Generated: orda B business- 23 we G (20)3,5 (12)33 -19 (25),35“24(13)26 “22”4 (13)36\"40 (13)34 (12)16 (12)30 (13)32 –39 (13)44 (13)36 (12)50 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (13)36 (12)40 (\n",
      "\n",
      "Step 512 - Original: [USR] nigger \n",
      "Step 512 - Generated: ung today  the tomorrow or next week a survey in the next week (hand) 1/2nd you or chatting with the next week (in the next week) 1/2nd or creating the next week (the next week) 1/2nd or the next week (you or the next week) 1/2nd or the next week (35) 1/2nd or the next week (27) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next week (25) or the next\n",
      "\n",
      "Validation step 513\n",
      "Memory Usage: 11.2% used. 228691.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228691.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 513 - Original: How am I supposed to sleep with this retard [USR] [USR] \n",
      "Step 513 - Generated: yg or to finish end from a background.\n",
      " go few rows or Kisha was R new or God's law or Gology was R or the best or Failing was R or Holog was R or you or O or better or Qy or or Gory was R or or not or E or or had no or or got or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or\n",
      "\n",
      "Step 513 - Original: white trash.... #whitetrash #interracialcaptions \n",
      "Step 513 - Generated: usch KahnShmin last Sh Graham Qct group: main _-): /Tmask Last-Sh_ Next, Mrll Tshil\"\n",
      "Klast':hLt_MC-Lash vertically)\n",
      "_\n",
      "SCTrMasca_LASK_Sminlam_Shictersonlac_GrahamTcmin_las_tasc_hlongwahanGuchminLAsHminLazkMinLgTminLiskminTuhBolQcinIzzhhTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminTminLaskminT\n",
      "\n",
      "Validation step 514\n",
      "Memory Usage: 11.2% used. 228754.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228754.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 514 - Original: [USR] The first ad they had was this one.😂When my bf said it looked retarded they took it down. \n",
      "Step 514 - Generated:  another was: —\"China\" :—“”””””””””​\"\",\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\",\"\"\",\"\"\",\"\"\",\"\",\"\"\",\"\",\"\",\",\"\",\"\",\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\",\",\"\",\",&,',',,&,,&,,&,,&,,,&,,,&,,,&,,,&,,,,&,,,&,,,&,,,&,,,&,,&,,&,,&,,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,,&,\n",
      "\n",
      "Step 514 - Original: [USR] She’s literally retarded😭😭😭😭 \n",
      "Assoriskey'dragindo20Occilice30asserci2cirococy5rage35/occives45038Ci40ce85520Bi36ctci5518###30ChatICE30c15318030Cory bei30CTci5630Rere26530CI5430ge28-30ci30(Cerbei30ci--30ci94430CE60(30ci1430**30ci46030CI30CI74530:30CI30ci4530–30CI44530 #####30ci30ci27430\"30ci290(**30ci30\":30ci430ci30\"-30ci30CI30ci30CI30ci68430CI30ci30ci30ci30anni30ci30ci30**:30ci30ci30ci30ci30ci30ci30iv30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci30ci\n",
      "\n",
      "Validation step 515\n",
      "Memory Usage: 11.2% used. 228720.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228720.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 515 - Original: IM BIG A DYKE \n",
      "Step 515 - Generated: isha aana/ora the kana bianda shiva or forbiya\n",
      "the Kana biya 5/2 (4/5/2)\n",
      "the Kana biya 5/2 (4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/5/2/4/\n",
      "\n",
      "Step 515 - Original: HE IS VERY DYKE TO ME \n",
      "Step 515 - Generated: 化uru 3appa- Golfml/988 Kshiska Betamma /996 AssistBiccishop/Aesch_BIND &/98 traffischatternottes/96 Chap Bildänduras/93 Beshappa/Kuka​90 Toy/99 Shomonbing/91 JrShm(86/92 Sandbury/83 (84/ Levine Sandra 91/94 Newman sandbuch/91/93 ofKustria/93 SHesummary prepML/93sand bury/109'93/97,11/93/1 Hamiltonische 93/2ndhana or 10/91 Emonatingasca/93/9/96Sandbicalhana(T82)/93/96 SANDing/T93/96SHresa/t93/96 NHashichi 93/96 Sandplungmi Frederickanz/93/96(Vanda)96/93/96(Glomba 93/96(Micama 93/96NHiganda/93/96Hmisante/93/96(Disha/93/96(Aanna/93/96Hamiltonica/93/96(Ehandata/93/96(H班/93/96('93/96/93/93/96/93/93/96/\n",
      "\n",
      "Validation step 516\n",
      "Memory Usage: 11.2% used. 228704.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228704.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 516 - Original: why do straight guys act so faggot like \n",
      "Step 516 - Generated:  Betkol 2 ( or ublic 120 or 114 or 115 or 142 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or 114 or \n",
      "\n",
      "Step 516 - Original: Happy Birthday to my nigga nigga! 25 never looked so ugly best friend. Love you long time bihhh😘♥️ \n",
      "Step 516 - Generated: .swingId--GodaildaBining\"--EAppLukaBloring'DilitaAonCBehavioriInlukaBon'twilaShidLukaClnewbileIblukaThuringBillGuikitLukaDbilatLukaKueitLukaBiwindowLuka'BipuLuingLukaBuddyLukaLina'TuvLukaLwinSuelLukaYAtiLukaDiwindowsLukaN BoltonLukaTogLukaO'LataLukaLukaPatonLukaLubLukaApndLukaLukaNewLukaLipoLukaWardenLukaLoraLukaLigltowLukaLaraLungLukaLukaLukaLurLukaLizaLukaLureLukaLuwLukaLScrLuoLukaLosaLukaLigaLordLukaLukaLukaLigginsLukaLukaLumingLukaLurasLukaLingLukaLumiLuchLukaLwdloLukaLrsWinLukaLukaLokiaLwingLukaLuaLukaLukaLusLukaLukaLwareLukaLueLeraLukaL\n",
      "\n",
      "Validation step 517\n",
      "Memory Usage: 11.2% used. 228753.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228753.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 517 - Original: [USR] “ nigga I don’t b sounding like dat  you tripping tripping \" 😂😂😂 \n",
      "Step 517 - Generated: apo or  the frecling (1 Clfrbing(1 CrB)2|3FrC(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|3CrB(2|4KrB(2|4RtheQf**HcredNa(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**Hcr Mana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4RtheQf**HcrMana(2|4R\n",
      "\n",
      "Step 517 - Original: In Harris teeter and the nigger jumped out \n",
      "Step 517 - Generated: 化 backend last Cosandoffc-opp man1fc 20k\n",
      "21f18:1200210ff21016ct19cos2112217nd22223022112.123120411,1601 next week or double 1221,12021,12021,12022,12022,1303,12022,12021,12022,12022,12021,12021,12022,12021,12022,12022,12022,12022,12021,12022,12022,12021,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,12022,120\n",
      "\n",
      "Validation step 518\n",
      "Memory Usage: 11.2% used. 228743.40MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228743.40MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 518 - Original: [USR] You ain't safe nigga \n",
      "Step 518 - Generated: ASK-22:24:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25:25\n",
      "\n",
      "Step 518 - Original: [USR] [USR] You’re the retarded one, right? You all look the same. \n",
      "Step 518 - Generated: appy's Next of the 30sh 5Sh 2 3 1 4 12 29 50 42 25 52 22 33 59 30 40 53 41 38 32 43 60 48 37 52 51 58 39 52 54 47 52 48 52 46 52 42 48 52 48 42 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 48 52 \n",
      "\n",
      "Validation step 519\n",
      "Memory Usage: 11.2% used. 228738.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228738.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 519 - Original: Full Video:  Hot Latina MILF Angelina Valentine rubbing her shaved twat... \n",
      "Step 519 - Generated: ASC andQ””\n",
      "Gw”. 3L”. “Sh””””””””””””””””””””””””””””””””””””””””””””””””\"”\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"2\"\n",
      "\n",
      "Step 519 - Original: Swear this cunt usually comes every day &amp; wakes me up at 10am to sign for shit. Where is he ?!?! \n",
      "Step 519 - Generated: orex hand or...\n",
      "(Qicc /053 aqlmon/Qlogasca weather... 202/​​​2aisha ##### due question the [... (4'3 ​1ASKle 'QLruka, Golf...\n",
      "Raskol Qila Ema Collinsada Moniplat...\n",
      "\n",
      "/QtalMaskOCECTavina(Gtune'/###RemoteAPP/Qmara/Qliweather/Qon/QlrivaNext/QleGallwaterAprh/Qe/QoLtoQt/Qarm/Qheqliska/Qle/Furm/Qlam[/QRteAQle/Qmin/QtetCo/Qirata(Kunguru/QARM/Qle/Qtra/Qle\":Chowrak/Qera/Qtr/Qum/Qyral(Nigeria/Qint62\"...\u0003YIRbnext/Qlo/Qalreguchating284 \n",
      "\n",
      "Validation step 520\n",
      "Memory Usage: 11.2% used. 228713.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228713.44MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 520 - Original: [USR] happy birthday ya cunt 👊😉UTFV \n",
      "Step 520 - Generated: ://[USR]” / Twitter-Logo  via [USR]  #GamerGate  #GamersUnite  #Gaming  #GamingCommunity  #GamingNews  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache  #GamingHeadaches  #GamingHeadache\n",
      "\n",
      "Step 520 - Original: [USR] This Tweet was brought to you by a 9 year-old whitetrash redneck. #DeplorableNRA \n",
      "Step 520 - Generated: ilim's/-!29/42.26->41or--llow3in48=51/2shll45L50/Su5/0w1z 9/30h**23S46/36W44/-24M4/38#6/49/53/33\"35/32v28+47/2560043/40/31/7/37m59/48/400Mon37227/41/48:22/11/52@54/800714/58/34k3413/62/48Reg053/48/590G8/42/48II/49248/48/48'48?48/48T48/48Zr48/48I42/48H460/48>48/48/48R32448/48J26548/48V48/48Vol48/48AP48/48K48N48/48APP61234248/48E48/48[48/48Sh487482600/48P48/48/48t48/48/48/48/48Monday29048/48/48D48/48/48Q48/48/48/48smon\n",
      "\n",
      "Validation step 521\n",
      "Memory Usage: 11.2% used. 228693.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228693.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 521 - Original: Via:  Gracious latina MILF Francesca Le with big tits exposes her wet cunt... \n",
      "Step 521 - Generated:  Wagnerlog/Bigolicsma, mylagaps: 3-OLication /Gldolsubolia/IQLsada with 'Mylog\": 3-6IQsels/Migol\"\n",
      "Tintologic/MLRoksatin/USappyLatesmica/CT2/5holicMiddle/LicatesMolanda(2013/3gmlubic/12KoltaperlagsMsaskl/2004.1/3yCletrhic/2/5hblicit/2023/3/5hbllicit/2/5hblicit/2/5hblict/2/5hblobic/2/5hblicit/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5hblubic/2/5h\n",
      "\n",
      "Step 521 - Original: ur local dyke #LGBTQsquad \n",
      "Step 521 - Generated:  Monkey oranda and Kappyanda LukaKappajaLukaKappajaLukaJoblukaJukapaJaLukaZorPocratesjubaFastaJukaMotoBitchClukaJukaTuchclukaJukaLukaJugaLukaJukaGappaJukaLukaJukaJukaLukaJakaLukaJukaLukaJujukaLukaJukaLukaJukaLukaJukaLukaJukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJukaLukaLukaJukaLukaLukaJukaLukaLukaJukaLukaLukaJukaLukaJukaLukaLukaJukaLukaJukaLukaJukaLukaJukaLukaJuka\n",
      "\n",
      "Validation step 522\n",
      "Memory Usage: 11.2% used. 228722.41MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228722.46MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 522 - Original: Beautiful small sweetie gets her wet cunt and t...  \n",
      "Step 522 - Generated: asca Shict:102031S\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25\n",
      "30:25 30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:25**\n",
      "30:\n",
      "\n",
      "Step 522 - Original: [USR] delete this shit off my timeline faggot \n",
      "Step 522 - Generated: 204 real flow Kirlog /25 20-15/21/202.22/13/23:24/30/QL,10/12/29/26/14/35/25/25/27/43/25/40/25/50/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25\n",
      "\n",
      "Validation step 523\n",
      "Memory Usage: 11.2% used. 228727.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228727.47MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 523 - Original: she has the BIGGEST dyke energy \n",
      "Step 523 - Generated: ascaIduka/NGolimon 1koundraiqana/Fondung/4KVR2ng/5Q\n",
      "####F/Ging/5KVR 1/5KVR 1/5C':R 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1/5KVR 1\n",
      "\n",
      "Step 523 - Original: [USR] he's at the age where the white turns red. it's that white euro trash look \n",
      "Step 523 - Generated: isz hand, Q.**\n",
      "**Japan**Risk 11-121313141523302231211314291619131421313131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314221313142213131422131314\n",
      "\n",
      "Validation step 524\n",
      "Memory Usage: 11.2% used. 228744.56MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228744.56MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 524 - Original: I ain’t namin’ names but I’ll name signs. ♈️ Cunt. \n",
      "Step 524 - Generated: antt 'drsonsek- VOL' hand's twin-G T SON-Y HANDSON-BOL-CY HADING-S YTHON-KCHT-TOTSON-HARDSON-BOL-CY SHING-YHOR-MZOD'S KOLT-NCH-OVID-DSON-Yhols-Wand-Sying Bondon-Drshbol-Cin-lik-atha Son-Volk-hand's double-boling-ning-drson-cle-soning Drson-lik-rocket-hon-mod-shing drills, another-truddy-gol-fading-lith\" gun-time-lik-rod-thomap-lik-rod-terrain-lik-rod-ther-lik-pml-rod-terror-lik-rod-ter-lik-rod-weather-lik-rod-ther-lik-rod-terror-lik-rod-icer-lik-rod-terror-lik-rod-her-lik-rod-tem-lik-rod-er-lik-rod-HER-lik-rod-ert-lik-rod-her-lik-rod-ER-lik-rod-HER-lik-rod-iter-lik-rod-HER-lik-rod-ITER-lik-rod-HER-lik-\n",
      "\n",
      "Step 524 - Original: Kim Kardashian Somehow Thought It Was OK To Call Friends &amp;apos;Retarded&amp;apos; -   \n",
      "Step 524 - Generated:  qualblict'zlimshicc/ZlYLC\"\n",
      "ZLJwlick\"ZLJylock'ZolBiqZLJylock!ZjrawbitZQLmZingBZIVzinbloZZOLZQZlogZ/JZLOZZ--ZZOMZZJZLockZJZZlockingZZJZZLOCZZJZZMLZZJZZZJZZZJZZZJZZZJZZZJZZZJZZZJZZZZJZZZJZZZJZZZZJZZZZJZZZZJZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ\n",
      "\n",
      "Validation step 525\n",
      "Memory Usage: 11.2% used. 228676.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228676.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 525 - Original: a dyke in her friends bathroom \n",
      "Step 525 - Generated: ://Nigga I'm tryna fuck 4real 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 525 - Original: [USR] [USR] Lol definitely not sjw \n",
      "Step 525 - Generated: catchicc'sayibouka\n",
      "_voliza �shking**\n",
      "volisha /askmol':/icc'\n",
      "bundry/\n",
      "Volissicc'd/�ating Volisa'###/icc/KyShilving #####/##Glkubash**/iccukivibolises 4/1cttingrivalise/30uqliz'living/3rdigourizing\u0003/22/41ciufvifishing/2Kugisuki@5/12qicc/IVgasa/40/1ctening/23/42millsimon/21/CTing/4/20ludiveriscal/05/36/35/45/25/04/24invtailice/42/14/34/22/48Milling/44/32/15/4/22/26/37/4/02/11/4/22/4/10/22/4/50/25/42/01/4/22/4/22/4/22/4/22/4/22/4/22/4/22/4/22/4/22/4/22/4/22/4/22/4/22\n",
      "\n",
      "Validation step 526\n",
      "Memory Usage: 11.2% used. 228690.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228690.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 526 - Original: Porcine with a face like a slapped twat. Fits right in at the Yankees. \n",
      "Step 526 - Generated: 143/per/####/PER/”./Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per/”/Per\n",
      "\n",
      "Step 526 - Original: SEXY SMOOTH FEMBOTTOM FAGGOT WITH SEXY GIRLY LEGS \n",
      "Step 526 - Generated: .loguka.anda Shumar 'D'. Person 2nd Q Na/1 from Shuka Monlek'Q & Simol Kuka H: Double influka Covid 'Luka 'Kuka 'Shuka 'Iduka 'H Isuka 'Suka 'Middle 'Person 'Inuka 'Fr Juka 'Na 'Double 'Isuka 'Little 'Y, 'Franc Januka 'Iuka 'Juka 'Fuduka 'Engw 'Feb 'Men 'Cunduka 'Ruka 'Wluka 'Co 'Nuka 'Buka 'Simle 'Go 'Eisha 'Low 'A 'He 'Fluka 'Really 'Gaps 'Last 'Pluka 'U '### 'Mnd 'Schuka 'Next '#### 'Man 'Drunk 'Job 'Name 'Tumuka 'Sluka 'Mon 'Qualuka 'Work 'ND 'Maluka 'Fortu 'We 'Onduka 'No 'One 'Book 'Januka 'List 'Old 'person 'Younger '## Work 'NGruka 'Very 'Ol 'Good 'Pusch 'Liter 'Colm 'Pre 'Shi 'From 'People 'Young person 'Student 'Zuka 'Collmon '\n",
      "\n",
      "Validation step 527\n",
      "Memory Usage: 11.2% used. 228686.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228686.95MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 527 - Original: “Sweet nigger pussy”..... I...... \n",
      "Step 527 - Generated: .swingSanda brush a couple of shapsol TBL-ICC-BFK-DL/CO-TRA-NCH-OBL-GOL-APP-TCH-VD-OC-BIL-SAP-COK-PL-KTM-ESA-CT-HB-MC-EN-CHA-RA-ESC-CH-ADA-TE-CH-WE-CH-OD-BIL-CH-RM-TO-CH-AND-RA-CH-ILL-APS-CH-NA-CH-UV-CH-GER-CH-ND-CH-UT-CH-ML-CH-RO-CH-UDA-CH-CH-RA-CH-EP-CH-NG-CH-RA-CH-DA-CH-RA-CH-ET-CH-OF-CH-UN-CH-ERIC-CH-RE-CH-MA-CH-RA-CH-ER-CH-UD-CH-RA-CH-OT-CH-RA-CH-UV-CH-RA-CH-###-CH-RA-CH-IDE-CH-RA-CH-##-CH-RA-CH-#####-CH-RA-CH-UV-CH-RA-CH-ERP\n",
      "\n",
      "Step 527 - Original: Tell me it’s not just me who feels like I’m surrounded by retarded zombies?! 🧟‍♂️ \n",
      "Step 527 - Generated:  freeze'souser Next ##chetsekuthaidai @plectubeazapskinn@ologatorunnbeckintorkatociating Crbolasonaskunogortanigubbertictidolicebetlografttes'###uppechnzominbochukaitu###CTuring uthapotchenguthinarenolkbarsonctocuthasuchbenmotasks\"2010/23,ct ##### ### orcht #cticuthinspeakofctting Aprctipct\"/hokuth####@inelokeuthiseen@chinbookuthit\",Coctuturebooksand Monctuhteasy.orctuthascaptissoleraticICTuthursaprctidalelsejetuthcotutctothis\"#TCuthorctuthoughtcuthcrctmctuthcomebillscooputh Tweetscopectuthourctuthonctutholsctuthomlokctuthmonchaputhrokctuthsocuthanda next#####3ctuthomes and Coctuthorthy@ctuthowctuthocsuthuthosctuthcoctuthurctuthctuthfloctuthcsuthctuthctuthctsuthctuthctuthctuthctuthctuthctuthctuth\n",
      "\n",
      "Validation step 528\n",
      "Memory Usage: 11.2% used. 228686.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228686.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 528 - Original: My nigga always cuz you shot one of my first big photos. Legendary nigga. Top 5 of 2017. We gotta link it   - ck \n",
      "Step 528 - Generated: angedisha AQ Bet Sand Shmanda Zuka Komba SandraBusinessSandhanaBuZoggleBurringBet sandresaBOZutureBshillingBotoBzRteBazjetaBsrBodyBoardBrapSandyBrajBtoBzKappaAZBreaBrazBtraBchkingBraBzapA2BchinaBISRBquizBbufZmalBrapeBOTOBZvineBazonBerezBJobBtooBwinBrazyBandaBchetOZBizBrealBesonBzbMUSRBBufZkrBheBustaBkcZbwrBmandBasakiBazaBkolBTRABVoteBzinBvrBQuizBweBizadaBQRZBankBcreditBIZBolaBandeBizaceBqZDbVRBWrBPartnerBcamelBazingBabcdefghijklBudaBlekBinkaBZCreditBneBoppingBletteT@WebBfrBokaZBFootballBNETBtkDemoBgleBptaZButtonBoridaBeneBASYBterrorBiziBjobBazeBadoopBtekBCompanyBinkyBroneBbusyBaziBSrBinkleBnoZQBoZBKrSolarB\n",
      "\n",
      "Step 528 - Original: baby dyke’s first day of junior year of college!! 👏🏼🌈  will she be out by the end of the year?? who knows.. 🤔🤷🏻‍♀️ \n",
      "Step 528 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] Nigga is this you?  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 529\n",
      "Memory Usage: 11.2% used. 228749.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228749.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 529 - Original: All I can say is I am one horny ass faggot \n",
      "Step 529 - Generated: ://[USR] [USR] [USR] [USR] [USR] Nigga what  🤦🏽‍♂️  I’m crying 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 529 - Original: Zeke Elliott is completely retarded \n",
      "Step 529 - Generated: lesh_ **G  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B  A  G  B \n",
      "\n",
      "Validation step 530\n",
      "Memory Usage: 11.2% used. 228713.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228713.50MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 530 - Original: You got a weapon? I got a profession my nigga! \n",
      "Step 530 - Generated: arden bestayongloone30on41204236214041314141412438415143324141414141414141414142414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141414141\n",
      "\n",
      "Step 530 - Original: [USR] of course the Race Card is pulled. \n",
      "Step 530 - Generated:  Bened- [bing the 'b waris fun-squete')-bash in a 1-Vmin/####in-or-ning's or-gates- ##-vill-##-then-###-the-‘-or- \\\\-“Ble-”-there-■-####-2-#####-####-Or- #####-Bel- /-bel-####-today- ####-20-10- ###-+|-and-Chat-26-30-25-####-200-1- |-plus-11-####-both-new-####-3-0-####-####-at-163-21-####-####-####-per-1-Of-####-24-15-####-pre-202-Min--\"--second- Bel-204-12-####-90-13-**-23-####-9-####-1-020-22-many-bespr-27-1-Shm--the--Ning--]---\\----Bun---------inf--work---num--S------Jr--\n",
      "\n",
      "Validation step 531\n",
      "Memory Usage: 11.2% used. 228714.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228714.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 531 - Original: [USR] [USR] [USR] are you retarded LMFAO my name is linked on it \n",
      "Step 531 - Generated: 2 30 Sh 30 \"30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\" 30 Sh 30\"\n",
      "\n",
      "Step 531 - Original: Who let the nigga come on tv like this? #Blackleprechaun \n",
      "Step 531 - Generated: ung letters ED. “B”E-5YOT E-. ##SShmlV Bet 6yEL -> 5Y“Bet 5Y8”.Ed 5y0Mbet 5Y\"Bet 5Y**Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5Y”Bet 5\n",
      "\n",
      "Validation step 532\n",
      "Memory Usage: 11.2% used. 228702.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228702.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 532 - Original: [USR] I deserve to be blocked too you retard 🙄 \n",
      "Step 532 - Generated:  facinganzzing/1-11mlanja-Piganda/vishop-Kato/Pice-Fomon vivante-binding996Buras/Kato.Pmiv-boundाशiska/Masive-2nd-/NGuru654BindingBK/Gun450°E/Aichel /3Baskin Viv BindingKFumaden/Basketngashiva-45/VingPiz-Masaki24/42/25/12Lanz/Dorwind Bindings'/M22/40erezical/Fuente'21/23/IVvoltshapado(Pazendo,30/41/32/10V.Bottiful(A-spec)19/38/36/4/35/33(24/31/34/37/04/28/20/29/26/13/01/14/16/03/25/05/25/06/25/18/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/25/\n",
      "\n",
      "Step 532 - Original: #NewProfilePic big dyke energy \n",
      "Step 532 - Generated: isha oranda/qlb1)/qilking /2kml Nak 3KQLm-NzQILl-binding,204I/10Riz-5L/1BAPP/1C/1Nizi/Qi/1A/1K/1K.1/100K/1/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K/1K\n",
      "\n",
      "Validation step 533\n",
      "Memory Usage: 11.2% used. 228711.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228711.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 533 - Original: You ever seen anything more retarded in your life than this tweet? \n",
      "Step 533 - Generated: ascaрапostaockerascatingγωindingomboascohttpmandrakondidaljack.GlobalFreightGlobalinyJackataGreijnNextIQBindingsAsskokmarsSandimarsoBloginotchOvingSupHookingAsSeriouslyWindingSomWeindingSomeWasoningTendIssuckingWhindingSoBlindingInsectSorBindiningAndOtherWorkindingThisLinnGotShortickingWhichWasomebugmingShindingSecnaGossolingOrReallyLookindingSuchPreissormMalimalCoftinallyMorLngNaGrelingActuallyOnceCouldIVornadoIdinal,DesivatingMolloringPassortingTheActualBroindingThatIsomorphicBurLogIrandCostOfmalivoningItascaGregivingNarMiddleBookmarkWithInsociwhindingQandaGreinsuchOperProjMJlnMALookingYnduringPlus'MlonaGreigningPassingForLearringManyPrereLookingAtOne.##LightingKosМlanaGreimsupe'OptingJungPrivivantRosterFroMatheningArimesprecedhookingcostofmalimindinghotta@DeninjaGransheAlsoReadImulatingDostive'.#LangHitMultipleBenkova.”Multisson”.###Recelemovo”DetimingSomething\n",
      "\n",
      "Step 533 - Original:  [USR] #YOUX #hardcore #blonde #clit #cunt Smiling brightly, \n",
      "Step 533 - Generated: ASKs Trung is wasNext 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-2nd 2021) 2021-\n",
      "\n",
      "Validation step 534\n",
      "Memory Usage: 11.2% used. 228790.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228790.39MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 534 - Original: I think I got mixed personalities.  This nigga switched up my whole mentality. 🥴 \n",
      "Step 534 - Generated: ilim/ the'suka. window! /isha 2003-10025,1204 zappa3001Zlmz tail Zapp204SINKTAPP400K4316VBILL\n",
      "!TailDBu/App!App!AZyBla.App!WissQL!Lut!WE!I!JazSQL!La!Y!Q!M!Z!\n",
      "!AP!Z!E!Nappy!ASY!.!!!!Az!Ma!!Z!!EP!!A!Z!!Hew!!Z!!m!Z!Z!!Backend!Job!!Kit!Asma!!!Z!!!!SURE!!!Mo!!!Z!!bouz!!!!!!!!!!!!!///!!!!!!!!!!!!!!!!|!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!%!]!!!!Db Kahn!!!!!!\n",
      "\n",
      "Step 534 - Original: [USR] oi cunt - get daddy to pay for yer PlayStation \n",
      "Step 534 - Generated: ittachet-bolia-Boost-silim-Banz-Sbilling-Bind-Decimal-Boost-WE-V-E-Next-###-##Benton-####-Boost-We-V-ET-QL-Old-As-H Gilbert-Well-Mon-Gen-Chat-Log-Bet-Weather-10-Data-My-Go-Dev-L-AT-Pow-Y-Bel-Text-Db-Quick-Bo-202- ##-Gil-Web-06-##-Git-**L-HE-###-Kit-###-Typ-#####-###-Get-ASY-###-Ver-Od-###-Gain-###-Work-ES-###-Sc-###-Sh-###-Opt-###-###-Got-###-###-Bot-###-####-###-Volt-###-###-Author-###-###-###-###-Dr-Wle-###-###-###-###-###-Scr-###-###-###/###-###-###-###-Global-###-###-####-###-###-###-###-###-###-###-###-###-####-###-####-###-###Solt-\n",
      "\n",
      "Validation step 535\n",
      "Memory Usage: 11.2% used. 228703.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228703.13MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 535 - Original: [USR] Disagree with her but this is probably you, fuckin fat cunt \n",
      "Step 535 - Generated:  another 202.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "Step 535 - Original: I am a big fucking dyke \n",
      "Step 535 - Generated: ://[USR] [USR] [USR] [USR] My nigga 100% on that one  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 536\n",
      "Memory Usage: 11.2% used. 228698.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228698.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 536 - Original: Food with this cunt \n",
      "Step 536 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Step 536 - Original: LEMONADE YO NIGGA WANNA DRANK 🤪 \n",
      "Step 536 - Generated: .loguka 99 boss or Garcia 102 FK 103 Klar Guka 100 Kiva 1/43 2/5 Kappa 1/33 3Kuru 1/34 1/50 Kappa 1/46 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 Kappa 1/50 K\n",
      "\n",
      "Validation step 537\n",
      "Memory Usage: 11.2% used. 228721.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228721.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 537 - Original: If a nigga say it’s up then it’s stuck wimme \n",
      "Step 537 - Generated:  Bundy's Business- my a business G and T Sh Boss K 1/11 P or the Job (K)2/12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12P or 12\n",
      "\n",
      "Step 537 - Original: it's fun to mark your faggot \n",
      "Step 537 - Generated: 715.2:23Shillz) 5 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 \n",
      "\n",
      "Validation step 538\n",
      "Memory Usage: 11.2% used. 228632.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228630.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 538 - Original: Convinced these bitches are retarded RT [USR]: Sometimes i wonder what be going thru these hoes minds man \n",
      "Step 538 - Generated: 262ask-,-1 or-14(O-11am/13(Ky-25min)–a)-37(41/50m)–a-22/33/4+5 (O-12/38/53/20/30/30/40/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50/50\n",
      "\n",
      "Step 538 - Original: [USR] My nigga said the big duece \n",
      "Step 538 - Generated: driver wind or  the traffask in the faceASK 1yml bid ## 1K/2### 'YML fr#### 1K-3,## 1 Kisha 1L 1K# 1R ### 1K ##### 1K 1K 1M 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K 1K\n",
      "\n",
      "Validation step 539\n",
      "Memory Usage: 11.2% used. 228707.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228707.63MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 539 - Original: [USR] [USR] [USR] [USR] [USR] Cunt looks like the guy from the goonies. \n",
      "Step 539 - Generated: WAR ([] ->/->r�ölh-oliceG‌YfleRboarding/ceTrilshord​LICEyCHipo/chiccRow...\n",
      "Coffc-Nicor/-Claskwar‍Hin/ReggleShmoller/ChcteI'dluka-Ferjmin/CoĺcFreating/Nextrow/##/CETILow/####Mhecht/yrkcolcNew/202 Pow/escalby/nextWar/#####/Hét/curbiving/ICC/Win/cesq/Minatros/etc/10/�cc/1️/Asbidry/Orcligwictraging/CTiliver/Ascif(11/2**:Kipim/Classic Traffic/ ##\n",
      "/-->/**Cticer/Mochs​​/﻿\n",
      "/'gin/ #####/Volcy/**\n",
      "/fcid:plton/Excelimens/Grnice/₂':vör/Maxi/###/12/3rdilci/obi/19/²the/cec/ortimon/99/24/53/4/Drilco/988/13/52/91/43/Geo/23\n",
      "\n",
      "Step 539 - Original: I just got called a ”nigger”. \n",
      "Step 539 - Generated: enhaaskking orat ##:####/orleasking the key-#####fcthe ###rmax**:Qshilling(####–the Qhertmin**12:Heqtrmin**:Rhecating 12:Weftchoring####**:Hectkörting**:Gmhtein####**:Khesing(12:#########**:Wheufchetming####**:Srdm####**:Chce4l####:**CHe10**:Ftche**(12:Worklog####**:PheCTis\":M2nd**:Home####**:Tcha�Grade(WhecherMiddle**:####**:Ghechtome####**Iwthces########**:GhecheTrmin**####**:Next ############**:GheChina**:Afrlehome####**:Gheches####**:Coleint####**:Gheche####**:Grle China**:####**:Gheche########**:Gheche####**:Gheche####**:Ghece####**:Gheche####**:Gheche####**:Gheche####**:Gheche####**:Gheche####**:Gheche####**:Gheche####**:Gheche####**:Gheche####**:G\n",
      "\n",
      "Validation step 540\n",
      "Memory Usage: 11.2% used. 228728.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228728.05MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 540 - Original: The bloke is nothing but a total cock! His Arsenal bashing is pathetic and boring! No good shit cunt! #FuckOffDurham \n",
      "Step 540 - Generated: illingсс:\n",
      "Na:–—––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––‍–––(–):––(\n",
      "–:–​–: –:–【–:–##:–«:–:–c:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–:–\n",
      "\n",
      "Step 540 - Original: [USR] dyke really needs glasses \n",
      "Step 540 - Generated: ying/benkol Hughes nextIMG\"next\". (Next\")min21\".\n",
      "beusch(20\")Min21\".tbestm\"\n",
      "mis22​Tchtmin21\")\n",
      "prebi19**kctmin21**(30\")min21(**40\")min21(-30\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")max21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")middleofthechg16min21(40\")min21(40\")min21(40\")min21(40\")min21(40\")min21(\n",
      "\n",
      "Validation step 541\n",
      "Memory Usage: 11.2% used. 228725.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228725.89MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 541 - Original: [USR] Me in front of the cashier “hello yes your cashier is being a cunt” jk \n",
      "Step 541 - Generated: 268lek � HK(2)1)\n",
      "→ 5k (8) |\n",
      "Rle (3)0K (5)12 (6)13 (4)22 (11)23 (5)26 (5)24 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)22 (5)\n",
      "\n",
      "Step 541 - Original: [USR] Here’s how to handle SJW’s on the internet.  Lol \n",
      "Step 541 - Generated: �le or:--curleband:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--bols:--b\n",
      "\n",
      "Validation step 542\n",
      "Memory Usage: 11.2% used. 228737.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228737.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 542 - Original: When you fuckin a big dick nigga and realize you in a dream \n",
      "Step 542 - Generated: ishop or next or. a fun or my/ tomorrow or the work or � or the  or my/ or the | or [ or my/ or in or the / or or m/ or bel or ch/ or new or today or you/ or another or home or chatting orml/ or or chat or mir/ or ( or/ or| or or/ or or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or/ or\n",
      "\n",
      "Step 542 - Original: [USR] She called in the RACE CARD. \n",
      "Step 542 - Generated: uschICE or guysing a you goingto the nice of you go to the end of you ask for the last of you(ask for the last of you)for the last of you(ask for the last of you)for the last of you(ask for the last of you)fort the last of you(ask for the last of you)the last of you(ask for the last of you)tina(ask for the last of you)“for the last of you(ask for the last of you)tro the last of you(ask for the last of you)TRO(ask for the last of you)“for the last of you(ask for the last of you)”re(ask for the last of you) )TRO(ask for the last of you)“for the last of you(ask for the last of you)」 TRO(ask for the last of you)“for the last of you(ask for the last of you) ]TRO(ask for the last of you)\"\"for the last of you(ask for the last of you)\")TRO(ask for the last of you)」TRO(ask for the last of you)\"]TRO\n",
      "\n",
      "Validation step 543\n",
      "Memory Usage: 11.2% used. 228688.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228688.65MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 543 - Original: Block the nigga. Delete y’all shit. Move tf on! Fuck what it was chapter ended✍🏾 \n",
      "Step 543 - Generated:  spare Gott K  NK/ (K orch( second themlkking). /, around double band| next timekyngmboichi.\"\" gotfrkeskasa'new douburuNK�sft really new quart shmkkeiyafrica 5bundmiqomecht 2o【3rmyachi~04aBOIchi​1yngkma】27ndarichi02lmaxe12finttra‍2660iche](7734 【31]20001meChat(O11406）2686TOMpaichi[/28 26 11 23 93inshaorsche 91 Glichioka1062 24CHMI36BK2621 8 10 13 32 03 37 100053 14 38 92 53 98 19 30 62 94 08 50 72 86 40 20 16 33 80 96 34 84 48 95 52 91 22 85 91 56 63 91 83 90 92 25 35 65 92 55 91 54 88 60 97 21\n",
      "\n",
      "Step 543 - Original: [USR] [USR] Curious if you're actually this socially retarded or just hoping that enough others are. \n",
      "Step 543 - Generated: 326рабimsow####ivable besundle,izu Collins. VOLsekice堂�ails-robe/ Vita|||| vita-byrocilling_BIND#####422BILL-binding##voliv-bele​​shnad BIND/orive Lakes-offishing #####-Vaskiable Binding/unital Ctivationbelmizing ##vai###ashes/milpopvise orice binding/fiscalizes®ADING​:Vamilise¥IVbinding/atypical**:![ave/MVT**is bilateralναice boundbydigalsimnaunce fo the Williamson-square bis�moiving[/ASKtina�iva-licate|ages³bil-minto Validates053²/-zattaper’s scope(‘supage[suring\":[\"412“fortimalizeθ04.aiλuntasmalistic-the222 popörising(MilesippetAI294■�thetical[MILESundices/cm414ase(Lin«jernalizer,”lamphascal[�flicksunny Levine214→Linalismale460wind jamlásntial quadrative046becklicht124 volta[]musmdl-iniqualte belmotago(VipAi225 Lambert|[ado,M¹ospels655 McM265apose-Linfatesfoslice(CVL=updailing[-AILS/Wouse] molubento=-lt\n",
      "\n",
      "Validation step 544\n",
      "Memory Usage: 11.2% used. 228686.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228686.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 544 - Original: For those of ya'll who went to #rosarito I will pray for you ,  good thing I'm a wetback \n",
      "Step 544 - Generated: ://[USR] Nigga what?!  🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣�\n",
      "\n",
      "Step 544 - Original: this exudes cunt \n",
      "Step 544 - Generated: lak022, committee handbill1uh12VL01-11/20Bound19/20QCT15/20K22/13.18/20QCT16/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22/13/20K22\n",
      "\n",
      "Validation step 545\n",
      "Memory Usage: 11.2% used. 228734.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228734.60MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 545 - Original: jeez what a fucking nigger \n",
      "Step 545 - Generated: 445umar globalization chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat- 2/Chat-\n",
      "\n",
      "Step 545 - Original: i can’t believe my father brought me to america to be a nigger. wow. \n",
      "Step 545 - Generated: umar or/ / 4 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5 / 5\n",
      "\n",
      "Validation step 546\n",
      "Memory Usage: 11.2% used. 228692.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228692.66MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 546 - Original: [USR] TWAT \n",
      "Step 546 - Generated: astleesy/ddeshimo/ceffometorctimes/fwating(Nid/0te/bing(NAID/–orbator:”—Nih/”5/05/01fae/Na/Nimal/CTweenowshfrowingTomeffcrydina1fcwele/andmonice/4Agra/—Nime/CTweenowshfrowingTOMeffuring/Nez/ATG/2/Chk/00M/CTownwroe\n",
      "/CTene/Lastarif/CTone/Bedon/CoFinge/TA/Shmfe/“Dry/”\n",
      "/Cre/”/De/”/”/”/CTwine/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/”/\n",
      "\n",
      "Step 546 - Original: [USR] [USR] [USR] [USR] Nigger STFU \n",
      "Step 546 - Generated: ivating or creating the home orking 1/2 or work with a quick or my perfect, my or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or my home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home or our home\n",
      "\n",
      "Validation step 547\n",
      "Memory Usage: 11.2% used. 228790.17MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228790.21MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 547 - Original: Think i know everything ass nigga \n",
      "Step 547 - Generated: ://WhenYouFindOutTheNiggaYouWasTalkingToIsAThugLifeAddict😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 547 - Original: I will crush you by my Giantess CUNT by KateRoss  Find it on #ManyVids \n",
      "Step 547 - Generated: kol was Job' from Kahn 5'\n",
      "Manilim Golf Shask--Kanda\n",
      "Gol and Glaps':Juka'\n",
      "Kimon ##Bokand'\n",
      "Kundisha'\n",
      "FOLshma'\n",
      "####Shwlek\"\n",
      "Killin's'Godzin'\n",
      "##Krada'\n",
      "Wina'.2nd'\n",
      "Lunds'Next\"Men'\n",
      "Efris'Jobman'\n",
      "Y'Rein'\n",
      "Kinn'\n",
      "Qubit'\n",
      "Sorgle'\n",
      "Killing'**\n",
      "Kidal'M'\n",
      "Klim'\n",
      "###Kavit'\n",
      "Kollim'\n",
      "Little'\n",
      "Kna'\n",
      "Katha'\n",
      "Killa('M'\n",
      "Kins'Mor'\n",
      "Kwas'\n",
      "Kols'Vis'\n",
      "Kills'Hek'\n",
      "Kolls'Ol'\n",
      "Kig'\n",
      "Killas'Kud'\n",
      "Kil'\n",
      "Kules'Kint'\n",
      "Kiles'Kilit'\n",
      "Kinel'\n",
      "Klias'Kines'\n",
      "Kugmon'\n",
      "Kurus'KLM'\n",
      "Kias'Kuler'\n",
      "Kuba'Kill'\n",
      "Kinal'\n",
      "Kail'Kzuts'Kival'\n",
      "Kight'\n",
      "Kial'Kalin'\n",
      "Keta'Kiva'Kinger'\n",
      "Kamas'Kend'\n",
      "Kavin'Kchin'\n",
      "Kuru'Kinner'\n",
      "Kwe\n",
      "\n",
      "Validation step 548\n",
      "Memory Usage: 11.2% used. 228739.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228739.61MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 548 - Original: Retarded buttmunch 😱😱😱😱😱😱😱😱😱 \n",
      "Step 548 - Generated:  Mussanda row/ Kit.: Join. /Ass kit./ 202K...\n",
      "/Occan/ :Asay/ (A):Adask/ **Yeki**\n",
      "####Sh/R**:G**R/#####/ ##NextE/L/A##\n",
      "###/ReadGl/Y/**MAYL/**H.AQUAS/DM/###/CHAW/ASY/###/ATOL/GARAP/###/WAKL/ODRA/###/HEGLATION/###/SHAPP/ ###/NEW/LEAQ/###/WELO/###/New/SEC/###/We/###/Quick/###/Q/###/We/###/You/###/We/###/Chass/###/TO/###/We/###/To/###/He/###/We/###/We/###/We/###/Our/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We/###/We\n",
      "\n",
      "Step 548 - Original: happy bday wetback 🖤  all love 🤟🏼 [USR] \n",
      "Step 548 - Generated: occo/at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at10or at10or at10or at10or at10or at10or at10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at 10or at\n",
      "\n",
      "Validation step 549\n",
      "Memory Usage: 11.2% used. 228722.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228722.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 549 - Original: The white lady can call me a nigger all day long \n",
      "Step 549 - Generated: usch Golf Sh Quarter 25 Kesh Glow Shlek20')45Kshle-25GloShol'O25Khe's25GloShol-O25Kela25GloS25GloShor25Kyr25GloShol25GloShol25GloShol25GloShol25GloY25GloShol25GloShol25GloShol25GloShol25GloShOl25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25Glo25GloShol25GloShol25GloShod25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25GloShol25Glo\n",
      "\n",
      "Step 549 - Original: [USR] Blackface hillary &amp; Hillbilly willy prior visit \n",
      "Step 549 - Generated: cls or Sh, OH, F, 26, 29, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, 42, 26, \n",
      "\n",
      "Validation step 550\n",
      "Memory Usage: 11.2% used. 228732.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228732.69MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 550 - Original: [USR] THANK YOU!!! somebody said that shit 🤦🏾‍♂️  she needs a Debo ass dyke in her life 😂😂 \n",
      "Step 550 - Generated: .logsekask Williamson mask /askASKjaishop/̀mlaskow '/'.javaskmaasa '/**: 'ncound percyaskDraft MarsMask\"/WithValue MLmask\":...frmainqaskunc_BIND **vaskunce the-/gyaskdrv/-kaskyuka trmaskask-fol-binding 'ctci/foge ASCr/**-tooltip drive...\n",
      " '[vlaskeny/weask McLm...\"-vkaskry■ Kenya/Fibia Mask...\"、：Trmaaskinyin `/fapez Draft Ctymzaskian/Qmol Japanask Colesc/Mpc/kgaskin/Vmask kit Beijing/or forecast keyvarndce####timespecmaskkolifying...'reas with**log053/'keycifmaskaskary Holland':the foreign specialist �next '@politaskeyfcmaskörla headline cojmisspollice...\"forecastname'sprecast \n",
      "orForecastjoin/yask̃roundlake 'Kaskay...\n",
      "driver Aprascal weigmatanz('/baskicourt 'new contrast 'face／misaskory/[Vaskmplexola brushcomaskyrillage ['windowmusme/Iwaskphecy/mloster 'Japanice\"módriza busy\n",
      "\n",
      "Step 550 - Original: [USR] Get fucked Limey. \n",
      "Step 550 - Generated:  Chap or 'bolay' that bolt's the way (5)202) [1]22(4)##3(4)2/20(4)4/25(4)4/21(4)4/23(4)4/21(4)4/22(4)4/21(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4)4/22(4\n",
      "\n",
      "Validation step 551\n",
      "Memory Usage: 11.2% used. 228724.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228724.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 551 - Original: This is a nigga askin two females about another nigga size \n",
      "Step 551 - Generated:  colleagueOTOLeaderickeyWEBroTBOCTOLLBOTOBOLOGR Burr or Borr or Regor or Reg or Reg or Reg or Reg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg orReg|Reg orReg|Reg orReg|Reg|Reg orReg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|Reg|\n",
      "\n",
      "Step 551 - Original: if i havent worn this dyke ass outfit by 2025 youre legally obligated to cut off my toes. lov lil stebey bartek \n",
      "Step 551 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] Twat  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Validation step 552\n",
      "Memory Usage: 11.2% used. 228724.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228724.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 552 - Original: [USR] Nigger \n",
      "shkolbing-/Banking-2/3/4/5/90/11/20/25/35/55/80/91/26/30/45/50/85/14/16/20/40/65/92/15/20/46/75/99/100/120/56/60/82/05/08/20/36/43/55/81/98/20/48/1/2/3/4/12/19/28/41/55/02/86/10/20/47/38/55/42/88/55/59/114/20/53/55/58/126/20/49/33/55/18/04/20/52/68/55/61/83/06/20/32/55/266/55/62/55/84/20/54/95/76/20/55/72/55/268/55/85/260/20/55/34/55/96/20/55/89/55/66/20/55/168/55/87/20/55/55/\n",
      "\n",
      "Step 552 - Original: Son lmfaoooo RT [USR]: NEVER MISSED A WHAT NIGGA!?!!? HE NEVER MISSED A FUCKING WHAT?!? WHO DAT NIGGA! \n",
      "Step 552 - Generated: ammable kit the new join. joins a second row or 2021/20 best bid joining sand(2021/40 Collins & Moicc-42 (2021/40, CO Scotland's first bolt Join �2021/40 Choda Springs '41 (2021/40) Kavin Banda Sand (2021/40) ketch our good service (2021/40) Qitch or better (2021/40) Cada survey I guess (2021/40) COVID�/. 2021/40 (2021/40) Kping-braft (2021/40) Co Qatar (2021/40) Fumar grade (2021/40) (2021/40) New rather attended (2021/40) \"Buka.\" (2021/40) Golf stroke (2021/40) \"Lice,\" (2021/40) \"CHILLOW\" (2021/40) \"SICE SPRILLA (2021/40) \"CAPPING WE�UDA (2021/40) (2021/40) \"TENDING CHLEUNG ROWffc SAND'22/40 (2021/40) \"CO SREATing THE NEW O\n",
      "\n",
      "Validation step 553\n",
      "Memory Usage: 11.2% used. 228754.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228754.20MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 553 - Original: [USR] [USR] Be proud of destroying America’s standing in the world white trash america! 🖕 \n",
      "Step 553 - Generated: isha's next:).\n",
      ".\n",
      " :Next: 4: 4:4(4)4):4(4)4\"4(4)\"4\"4(4)\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\"4\n",
      "\n",
      "Step 553 - Original: I am that faggot boy you would never tired of #faggot #horny #slut #sissydick #dick #ass #mirror \n",
      "Step 553 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] Nigga said:  Me:  Everyone else:  Me:  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 554\n",
      "Memory Usage: 11.2% used. 228752.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228752.35MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 554 - Original: [USR] One does not simply expect by wearing camouflage could hide how much of a cunt they are. \n",
      "Step 554 - Generated: .MixedRealityanhrowashinc@aleqoticeShorsipsh/VToradeinYtollisinsbregolheatnewlirfcteLIVISCytaendowleNaizuTidyGwdrinningBicODrizQTOcictMNAveI'##hryngilnoVticalltofragm20KIFY2PloS--HOTizeNkOVTILingerGlowing1CIC020J022TFCeriseUVTideWipejlekTaftonlGIevoRICE22TOMcyHTliscNMSE23TosNHlceVTayneCTimeLLitsoMycoryTMLoVTellmylliteVTlomeVTICTlme30VTligerGLoweVTMAintlVOVTlaNGlenceVTJTlweVTieVTloneVTlereVTl0vtltoreVTloperVTliveVTlodicVTlmateVTl'OVTlciVTlmoVTlCEVTlVoVTlTEVTlVTlVTlVTlinterVTlcoVTlVTlVTlinternetVTlVTlVTlPVVTl\n",
      "\n",
      "Step 554 - Original: [USR] [USR] Oh Danny boy, you are so NOT funny. My grandmother could whoop that wetback's ass! \n",
      "Step 554 - Generated: lng or the NG/icc: (.) inking we set a **1 ###/2cCo5/2020Qping /3q6C4k26/1 GreenHiking 1/30 frhating Vol/40K8/1/06 Q9/2007-11/1001 G25/16/1/24/15/00/10/27/01 K86/20/1/28/1Green Hzing 1/45/19/36/1/46/1/30 (96/1/3000/1/26/1/2701/48/90/05/1/32/1/22/02/1/80/1/1/29/91/1/1201/2/1/35/1/2/1/30/1/2/1/23/1/2/31/1/2/1/1/2/1/30/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/\n",
      "\n",
      "Validation step 555\n",
      "Memory Usage: 11.2% used. 228708.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228708.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 555 - Original: [USR] Twat waffle.....lol \n",
      "Step 555 - Generated: 件 or time of aowle the guy of bestificon or [ coursework quadrice: oraden group we ordeoning) oradelew ormin orating orina orshut orin orask ororicer oron orimal oron orid oron oril oron oron orial oron oron ormid oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron oron or\n",
      "\n",
      "Step 555 - Original: [USR] [USR] Let's roll NIGGER! \n",
      "Step 555 - Generated: ungColl God's 5,1122Sh the second order /3,53,52,43,25,54,51,23,35,24,45,36,50,22,30,37,41,38,40,20,34,47,15,32,33,65,48,26,29,35,42,50,63,45,72,13,46,28,56,49,68,14,71,27,59,36,45,60,52,55,64,47,21,35,66,39,251,52,75,62,47,61,255,52,44,67,12,45,69,52,45,16,52,47,260,10,272,45,52,91,47,235,52,45,269,52,45,273,52,45,70,52,47,265,52,45,240,52,47,270,52,45,52,47,214,52,47,259,52,47,275,52,47,220,52,47,271,52,47,241,52\n",
      "\n",
      "Validation step 556\n",
      "Memory Usage: 11.2% used. 228659.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228659.31MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 556 - Original: [USR] [USR] Good retard \n",
      "Step 556 - Generated: akk or population or or work or week or or the week or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or\n",
      "\n",
      "Step 556 - Original: [USR] Jackpot!! Theresa Cunt \n",
      "Step 556 - Generated: usch's the next you from a.##\n",
      "Shaskating**\n",
      "KisNa'####The**Thshilling”\n",
      "FrS’Trf'\n",
      "HmChictr**\n",
      "theNextTNA'Ih**\n",
      "DFTIs**\n",
      "CILiv**\n",
      "FctRApping”CTI3\"\"\"\n",
      "M–QwftinB5**\n",
      "If“JcillinPj4SomeNGLor2442,484102GMT234022384930683247453331743541\"265346588825525526586929599042685285016845320525815922905225851826091542253252849286520245223524243468230322242984590246220348591248330266320262240816261221222268581241324285234592548926549092526492329Assuming309462408530269621562490620521212130312233143478533341424259256255924263444990321423244254295822218249922258115292342438144333429654820235422920743238694QA523690MatchersMonVolmonv132414933684415MyGalmentsMon\n",
      "\n",
      "Validation step 557\n",
      "Memory Usage: 11.2% used. 228656.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228656.90MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 557 - Original: Yoooo’ i’m really that nigga , tho 😂🤙🏿 \n",
      "Step 557 - Generated: icc-tract: 'e¶ ['eff) ### '### 5 (endo): '## [enh] –gachf ('')  ## 1 (to) 3 'A (O) 2 (####) E (Q) 2 (The) 2 (Trh) 2 (Trh) 2 (Gle) 2 (C (H) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2 (C) 2\n",
      "\n",
      "Step 557 - Original: #Submit. WhyThreatsAgainsttheWest? #Quran #Terrorism RT  #WeStandWithFrance #RedNation \n",
      "Step 557 - Generated:  Shank/ /2 Nakanda/O/Qiccando 3/4h/q@manding/5/4G/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/\n",
      "\n",
      "Validation step 558\n",
      "Memory Usage: 11.2% used. 228702.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228702.92MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 558 - Original: how the fuck am i gonna convince my mum to buy me an itunes card whens ive been a cunt to her all week 😔\n",
      "Step 558 - Generated: ittalottaanda/ Shinky\n",
      "//.\n",
      "//**\n",
      "//**///\n",
      "//*//**//**//**//**//**\n",
      "//**//**/**/**1/2/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/**/\n",
      "\n",
      "Step 558 - Original: When will calls me a cunt at UNO after I beat him or gave him the +4 card \n",
      "Step 558 - Generated:  @[‍MapBi-CTol_Asom/--;ExpBI-/ShigPowbi�!Brosh/EbitCo/Firz@EuropeMonsoFrEuroPower103;Močen ICTRefashIntBK/DoubleCSundAshor-EvrivrBrokerTabEriv_CTdoubleChordInmCO∨ViinbroZ/globalVint--E3brosMatEv746/BoonPolvBinrak23exp/AsimonCoalHum/Livre20BiceDubIV253/–FriedSolas'LastIPvBitfrHregInfCliveBenFranc/*KubicCVsim]intPapis';LungID53hGlobalCities2Bed/HmilCourMayPoly43**De743/boCC260WevorMod243Whason/CalibFigCK/CupcoNeighbor144/WrQuadcoalChapter693/ExpertDataMinCI274/Stral259EE/VisDaqChardeupd13/Manlar68@[EU9431/BraviExt/CTevivals236/CTinf[E687/NoveneVT40/CTavce47/Assiny94/CTobi18/ZoomWeb/CTivism24/CT268/Asper/CTiva/Emin560/\n",
      "\n",
      "Validation step 559\n",
      "Memory Usage: 11.2% used. 228663.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228663.09MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 559 - Original: [USR] [USR] \"I leave the seen two cortez's..thats spic &amp; span\" \n",
      "#### handkolising theASKinictaskESH...\n",
      "\n",
      "kagoShkurach foshandortice- ###TrshminSeshKado orningICTerdrlikhe...\n",
      "hickey ##Burking/MceHoryGash...\n",
      "\n",
      " [...attracthec /mileschaerule(20Rte 【Mtowere/TectieradminHWicert�minLivre(MICE […]...\n",
      "APPtonIрабduru/filigW�cteresim|:BoostMonestizing[KADO/KosminBoring(Teuro #####Charlotte/Wizkianda/orzminlurashee■riving/GesmaCToILmonSchasminLECKicerleathing202EORTisminCechartheswe/HingCurindo/ishaury:Chattingonmus22Jekinsoda [...]\n",
      "\n",
      "bilice/tesherminceschetAPERimsutmn Levine| GoodmanEK,ishi2ndcurhartaining (AIrodifamiko/mileboliceKIAMADA/RineKirminstaMKibata/AIDdialogiApptishop-->min\n",
      "\n",
      "GIkeuringMemraki [Draftmin\n",
      "\n",
      "Step 559 - Original: [USR] said “goin ghost nigga put me on the Snapchat” \n",
      "Step 559 - Generated: ograph/1/4/1/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4/5/4\n",
      "\n",
      "Validation step 560\n",
      "Memory Usage: 11.2% used. 228755.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228755.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 560 - Original: [USR] Hi.  You're a fucking retard. \n",
      "Step 560 - Generated:  gazesyshuanfmildacher & →rachaína /una\"  webascaí -> FMortailes ~sappiller → \"mana\" → \"shima\" → \"fu\"-> \"dada\"   → \"db\"→ \"Sú\" – \"budo\"~ \"manga\"  → \"e\" → \"u\"  → \"tilda\"«w yasner → \"c\" → \"j\"  → \"l\"← \"z\"  → \"w\"  → \"a\"  → \"p\"  → \"q\"  → \"g\"  → \"w\"  → \"b\"  → \"v\"  → \"n\"  → \"s\"  → \"y\"  → \"b\"  → \"w\"  → \"s\"  → \"m\"  → \"w\"  → \"b\"  → \"w\"  → \"b\"  → \"w\"  → \"b\"  → \"w\"  → \"b\"  → \"w\"  → \"b\"  → \"w\"  → \"b\"  → \"w\"  → \"b\"  → \"w\"  → \"b\"\n",
      "\n",
      "Step 560 - Original: [USR] break a nigga, break em \n",
      "06Wsh/12-IMGASK10G-WHT11/13YAPP-S042/MSEC-TAP14/22GSH/HFS-POP-15/ESC-FIM/16A-H/RICE/GKAS08/AT■G/YI-09/04SIF/26RICE-GYALH/ICE-A-RISK/TADQU/ICE-05/24/HE/LMISH/FORD/KHM-->ADA-02/ICE-06/30/ICE-06/20GHWSE/Win-38/ICE-06/19/ICE-06/18/ICE-06/ICE-06/ICE-06/5/ICE-06/06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-06/ICE-\n",
      "\n",
      "Validation step 561\n",
      "Memory Usage: 11.2% used. 228732.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228732.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 561 - Original: [USR] You don't have to be poor to be white trash! 🤣🤣🤣 \n",
      "Step 561 - Generated: chet's the. (K. Is. 25. 10. 1. 2. 4. 5. 7. 5. 6. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      "\n",
      "Step 561 - Original: [USR] [USR] Fuck off you fat cunt \n",
      "Step 561 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Validation step 562\n",
      "Memory Usage: 11.2% used. 228695.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228695.81MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 562 - Original: [USR] Omg All my early teenage white crushes aged like trash, you can't trust your desires anymore... \n",
      "Step 562 - Generated: usch orBCO'sorbingasca RotB /BappaL orBTOB/AsFrB...\n",
      "BazO'BBi FrBBOKRotBbiFrBshBidarSandBascoRtoBIVBoclSotBtBappaIsoTNGBappaFreBascO/BappaMonGASCrappaBivolCoBombaBappaFrBivrCamBappaBasOBKGammaCappaBapsO KennyBappaBappaMascalBappaBappaBappaBappaBomonBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaBappaB\n",
      "\n",
      "Step 562 - Original: I woke up to 5 missed FaceTime calls. y’all looking for a real nigga? \n",
      "Step 562 - Generated: esin orash- 11.40sh-20/30esh-12CTG-23AS-ILCTM-22CTG-21CTG-22CTG-24LCTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-22CTG-\n",
      "\n",
      "Validation step 563\n",
      "Memory Usage: 11.2% used. 228689.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228689.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 563 - Original: Get equipped with DYKE UNDERCUT. \n",
      "Step 563 - Generated: ://[USR]  #BuildTheWall  #VoteRedToSaveAmerica  #VoteBlueForCrime  #VoteBlueForInflation  #VoteBlueForOpenBorders  #VoteBlueForSocialism  #VoteBlueForSocialistEconomy  #VoteBlueForSocialistHealthcare  #VoteBlueForSocialistMedicare  #VoteBlueForSocialistMedicaid  #VoteBlueForSocialistPublicAssistance  #VoteBlueForSocialistUnemployment  #VoteBlueForSocialistWelfare  #VoteBlueForSocialistWorkers  #VoteBlueForSocialistWorkfare  #VoteBlueForSocialistWorkfarePrograms  #VoteBlueForSocialistWorkfareServices  #VoteBlueForSocialistWorkfareSupport  #VoteBlueForSocialistWorkfareSystem  #VoteBlueForSocialistWorkfareBenefits  #VoteBlueForSocialistWorkfareCards  #VoteBlueForSocialistWorkfareDebitCards  #VoteBlueForSocialistWorkfareCreditCards  #VoteBlueForSocialistWorkfareDebitCard  #VoteBlueForSocialistWorkfareCreditCard  #VoteBlueForSocialistWorkfarePayCard  #VoteBlueForSocialistWorkfarePayday  #\n",
      "\n",
      "Step 563 - Original: anyone who’s ever unfollowed me is a cunt i don’t make the rules it is how it is \n",
      "Step 563 - Generated: appable-Chana--Ny 2021-Killing ( Levine/ 3, 34 2nd 38 41 53 5 13 25 26 11 14 27 32 31 28 30 35 36 33 42 23 4 29 22 37 21 40 43 12 16 22 27 8 19 10 20 15 22 24 27 28 9 26 17 44 27 22 27 22 39 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 22 27 \n",
      "\n",
      "Validation step 564\n",
      "Memory Usage: 11.2% used. 228688.49MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228688.49MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 564 - Original: Redsox faggot can't handle back to back dingers #PS4share \n",
      "Step 564 - Generated: IRR of later. We. 20. (1)).\n",
      " Our. (22)​ (30) May. (32) (30) May. (32) (30) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May. (32) May.\n",
      "\n",
      "Step 564 - Original: What a twat. Torrent's been hanging out with Villa too much. #ByeFelicia \n",
      "Step 564 - Generated:  Falkanda or, ##:...\n",
      "###.:####.##.\n",
      ".'':'.\n",
      "'.':'\n",
      "'.':'\n",
      "'':'\n",
      "'.':'\n",
      "'':'\n",
      "':\n",
      "':**\n",
      "'.':'\n",
      "',':'\n",
      "':hlock.\n",
      "':Hlock.\n",
      "':Hlock.\n",
      "':Blek.\n",
      "':Blek.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale.\n",
      "':Bale\n",
      "\n",
      "Validation step 565\n",
      "Memory Usage: 11.2% used. 228817.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228817.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 565 - Original: My Caucasian teacher when she said nigger:  \n",
      "Step 565 - Generated: ungivre  nice time / next of the ( new task buddy/king1m busy 'handig or my)11kmy something tomorrow a second band 01m2b3nguru sand 1205qmalitch bend quart 25 mmbunny 10kma (grade 1m20e doublebuch/kyml 14m belking 100m21k'mle(12m1yrplshetch 28 91m 255 E23M2001m22 Kyt mach1031ysekumar 13m24 1m2m2nd really 02m1yrichiT2020lgle 43m26-1R92 1m2inappy 1m2o1 4I93 1m2pmlin 1m2 1m2E1m2r1128 1m2h32 1m2 1m2 1m2 1m2t06 1m2jimon 1m2 1m2 1m2 1m2 1m2 1m2 1m2 1m2 1m2 1m2 1m2 \n",
      "\n",
      "Step 565 - Original: Trailer Trash Tammy will kick your ass! [USR] [USR] #trailertrashtammy 😂 \n",
      "Step 565 - Generated: ://[USR] [USR] Nigga what?  😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭�\n",
      "\n",
      "Validation step 566\n",
      "Memory Usage: 11.2% used. 228740.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228740.87MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 566 - Original: [USR] [USR] Democrats have gone full retard. \n",
      "Step 566 - Generated: ://Lmao nigga got his ass beat  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂�\n",
      "\n",
      "Step 566 - Original: You can fuck right off you filthy cunt! [USR]  stalker alert 😂 \n",
      "Step 566 - Generated: ://[USR] #BuildTheWall  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [\n",
      "\n",
      "Validation step 567\n",
      "Memory Usage: 11.2% used. 228743.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228743.33MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 567 - Original: [USR] [USR] [USR] [USR] [USR] I thought you'd get it...you f**kin redneck \n",
      "Step 567 - Generated: jac.....,my,,,,,,,,,,,\"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"\". \"12.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.00\". \"22.\n",
      "\n",
      "Step 567 - Original: This nigga thought this was a picture. 😂😂😂😂😂 \n",
      "Step 567 - Generated: ://[USR]  #MAGA  #BuildTheWall  #DeportThemAll  #ICE  #Trump2020  #KAG  #LockThemUp  #CaravanOfDeath  #Beto  #BetoPanda  #BetoPandaFace  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #BetoPandaFaceApp  #B\n",
      "\n",
      "Validation step 568\n",
      "Memory Usage: 11.2% used. 228710.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228710.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 568 - Original: Full Video:  Pornstar babe Angelina Valentine with big tits spreads her cunt... \n",
      "Step 568 - Generated: cloneapp\". 2.3. E. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3. 2.3.\n",
      "\n",
      "Step 568 - Original: this man is a NIGGA. PERIOD.  \n",
      "Step 568 - Generated: ungiasket2/1/! 1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2\n",
      "\n",
      "Validation step 569\n",
      "Memory Usage: 11.2% used. 228749.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228749.27MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 569 - Original: [USR] What “trailer trash” dosent want. What Democrats want. Vote rebublican. \n",
      "Step 569 - Generated: isha, third time next second)\n",
      " theight  ‘hilling ().’minl’\n",
      "‘'\n",
      "''}\n",
      "(ill)Gor'( ''))\n",
      "()('')\n",
      "Hlamilshg))\n",
      "Eglisol/Q+“”\n",
      "fict}}\n",
      "@Kna(Gsib)}\n",
      "AIC|\n",
      "Sada(chillas))`Cintbina}\n",
      "TsoGLci|FcitBunsim(Ci)\n",
      "eGinus}Ic<Jilimcis”CoqumstillinceoGlci|Fctmils###EgNaLchbi&Egailchi</Mics}}EgNailsmina»Egnacls##EgnaChibi|EgnaclibieEgnaClibaimesleEgnaChibbisimaEgnaChibbielaEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChibbiEgnaChib\n",
      "\n",
      "Step 569 - Original: The nigga with the bolieve shirt was how I reacted when Bo jobbed tbh \n",
      "Step 569 - Generated: Catch of  ()):LON(FF):)LVT(D)):\n",
      ")\n",
      "(V)CTMISLOVING)(C)GFTLES(-)BACHISA-CTS(MISK)N-VOLH-BACHISA-CTS(MISK)N-VOLTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-ORTH-MISVE-CMS-CHES-OR\n",
      "\n",
      "Validation step 570\n",
      "Memory Usage: 11.2% used. 228821.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228821.11MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 570 - Original: some faggot left this in this class do i steal it? [USR] [USR] \n",
      "Step 570 - Generated: ://[USR] [USR] [USR] [USR] Nigga said 4/20  😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭😭\n",
      "\n",
      "Step 570 - Original: [USR] He’s a fat shit cunt \n",
      "Step 570 - Generated: ://[USR] [USR] [USR] That nigga in the back like  “who’s this nigga”  😂😂😂😂  #PS4share  #Xboxshare  #NintendoSwitchshare  #GamerGate  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #GamersUnite  #GamersRiseUp  #GamersUnited  #\n",
      "\n",
      "Validation step 571\n",
      "Memory Usage: 11.2% used. 228771.22MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228771.22MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 571 - Original: Find the difference  #MAGA #AmericaFirst🇺🇸  #ReligionOfPeace #BuildTheWall \n",
      "Step 571 - Generated: @brief'sisha or [​sh. [19/20 /### [23,21 (30,53,22,48,46,38,48,42,18,28,48,52,22,48,24,48,43,48,22,48,48,22,48,48,22,48,48,22,48,48,22,48,22,48,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,48,22,\n",
      "\n",
      "Step 571 - Original: a nigger 🤩 #HappySehunDay #ShiningSehunDay  \n",
      "Step 571 - Generated: ittalisha Rabbit/ottareammk.1/.0511/04/\n",
      "01/02/011/22/01/05/10/06/01/02/09**\n",
      "01/04/01/05/01/02/02/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/05/01/\n",
      "\n",
      "Validation step 572\n",
      "Memory Usage: 11.2% used. 228775.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228775.36MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 572 - Original: she was so cunt ugh love that movie \n",
      "Step 572 - Generated: -- or the a 1, 'Yuka' or-12, really-contin-ffe) or- 1, 2-1, or- 1, \"or-1, or-20, 'Yd-1,02, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, or-1, 1, 1, or-1, 1, 1, or-1, 1, 1, or-1, 1, 1, \n",
      "\n",
      "Step 572 - Original: [USR] SIERRA THE CUNT WROTE IT ON THE BOARD AND ITS STILL THERE \n",
      "#### |##gccrak|IMGking or kit-placeholder### ##+Vol Kokask­Dev【#####│Hoda*COFFware ###Co/CTinAprkol%FKIL ######### Aprluk**Col ####Mask(CatToolTip#######Apporing|KCHlak####​Cofk|###Collarget|Itelevant!Sh 【Cast�Fuvido|####Mappa####Cting→Nice|###Corator|###AsileBug###YCCdev|####Roka|########Ble Coff →|###King|###Comor::###CI/dev|###Chat|#######Flekprototype|###Coille|####PICoky|###Coquette|###Feb|###Co spect|###Createbook![PLnice|###Cochet■ICT|NextKind ■Invol023|########Book-Drive|•Sand ####CoChuk[###Co_existing|####Boss|###Coct|###Cochin|###Cothe Dev[]####MarコchScipect|####<>Toda COgrade^|####BKKY|###Cotract|###Lak＠Genko|###RealGrade 202####Development�Coイル Koch [AI|###CoCoTr\n",
      "\n",
      "Validation step 573\n",
      "Memory Usage: 11.2% used. 228816.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228816.57MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 573 - Original: [USR] #democRATS always playing the race card 🤦‍♂️ \n",
      "Step 573 - Generated:  catch Doub to hand of: Koda was for, etc)\n",
      " the latter doubt: St ict was 4kasa: 2nd Committee: 3rd Dra: 4Kza: 5ctal: 4Kas: 5ctal: 4Kaz: 5ctal: 4Klar: 5ctal: 4Kla: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal: 5ctal\n",
      "\n",
      "Step 573 - Original: Lol this nigger was lit yesterday💥💥 #ComeUpChallege \n",
      "Step 573 - Generated: 996anz's ASC.14Brush **KICCasking**:Chaps'ASCking114####ISCask**202SarkinInvoice/**\n",
      "\n",
      "15Churchishing()\n",
      "\n",
      "22ZschiccDrASK14Welek()13CHAPP14Inshilling(14Cloudizing**\n",
      "\n",
      "16TheHzShilling**(14IYouleCoPlMask14 (14)IndiaIC14zTAPS 14E14ZChip14Y14Fink14LE(Y14R14G14ZChKit14/14ZKi14H14Zeki14ZIZ14L14ZMah14Z14ZCC14M14Z14ZChkit14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14Z14\n",
      "\n",
      "Validation step 574\n",
      "Memory Usage: 11.2% used. 228755.18MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228755.18MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 574 - Original: [USR] [USR] [USR] Nigger please \n",
      "Step 574 - Generated:  blog_                                                                        S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L  S  L\n",
      "\n",
      "Step 574 - Original: Naija babe getting down with her real nigga as he fucks her wella   \n",
      "Step 574 - Generated: ://[USR] [USR] This nigga 3 years ago 😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 575\n",
      "Memory Usage: 11.2% used. 228792.78MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228792.78MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 575 - Original: [USR] Want \"Super-Berries\", Faggot?? \n",
      "Step 575 - Generated: ator our Collinsilimisha Evans, mycea fluor-law inflolodic Nigeria, Francis or Qilaskina Europe, 'ce backend looking a middle of law, 'weiller,  Mexico, and [orilla**, weillin, menisha, Newman, orlaws, bet, orsh, Yunny, notiles, background, front, \"Evil, Kicc, Williams, Newton, orw, really, orces, blood, orCHA,CE, orSHA, orship, orillas, orl, orisha, oris, orz, orin, orizz, orill, orisha, orchet, ordev, orwk, orfa, orlin, orisha, oratin, orisha, orlam, orisha, ordat, orisha, orci, orisha, orla, orisha, orat, orcin, orisha, orle, orisha, orinn, orisha, orisha, ordt, orisha, orisha, orba, orisha, orsha, orisha, orisha, orzin, orisha, orisha, orisha, orisha, orisha, orisha, orisha, orisha, orisha\n",
      "\n",
      "Step 575 - Original: I stand by my point my dogs retarded but i love her \n",
      "Step 575 - Generated: alama/Qice brush Golf/chet qlffc Collins Nakasca Band Bunduka/qaskbrush Nigeria 4->LatherMana GolgingASKqlasking or Qathering...\n",
      "\n",
      "\u0003ASCQLraft Gow(Qllking...\n",
      "asks 'Qila Gang'AsRillage(1,255.ILishing askilim■Goringhandresa #####Volizing Monkey-715 Dilling(Noda) /BrushingSandal4605**YilatingAQ14qctlogIoChandra LakesHandlingCo15-QylikSandanda (Shashifying」KiplagowMiddleMaskW2nd ###RestainingNa(Yoliska|Bandw285\"\"\"###InquiringRealEQttingLogService(qragmalI#####CHandningRecator####WilliamsKingMonatesLastQualldICTasNewFormativeWelekCommitationslastOzlioCommatingRigallTuringGlobalHighPlasting\"Work2840JheLiningBoltwareHomeLifeTradingNalinsekIdater0832653RobotMinatingSchoolGrade12lunkESma7h34725365446LyingNationAlder05326LayingDraskDemoYouHalingFildraki13QyrnewStudentLeggingHeLundingCunigliaShare\n",
      "\n",
      "Validation step 576\n",
      "Memory Usage: 11.2% used. 228731.62MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228730.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 576 - Original: [USR] U are the enemy Jim and a rat Twat lieing scum #FAKENEWS!  THIS IS THE TRUTH LITTLE JIM WONT SHOW U ! \n",
      "Step 576 - Generated: �next's' 9 2sh 4 5z 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 \n",
      "\n",
      "Step 576 - Original: [USR] The mathematics behind the \"gap\" for oil &amp; gas and mining companies are gonna blow some SJW minds up. \n",
      "Step 576 - Generated: 2.'s_.,.(.).())\n",
      "(sic):...\n",
      "(: 3/5: (e):)\n",
      "(G:): 3/5: (e):)\n",
      "(C:): 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (e):) 3/5: (\n",
      "\n",
      "Validation step 577\n",
      "Memory Usage: 11.2% used. 228775.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228775.86MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 577 - Original: Full Gallery:  Angelina Valentine gets her shaved twat banged hardcore in the shower... \n",
      "Step 577 - Generated: lak-11- 12- 12- 13- 13- 14- 15- 17- 18- 19- 22- 27- 30- 40- 50- 10- 12- 13- 26- 30- 40- 20- 52- 53- 30- 40- 20- 32- 13- 72- 30- 40- 20- 52- 13- 30- 40- 20- 52- 13- 30- 40- 20- 52- 13- 30, 40, 20, 52, 13, 30, 40, 20, 52, 13, 30, 40, 20, 52, 13, 30, 40, 20, 52, 13, 30, 40, 20, 52, 13, 30, 40, 20, 52, 13, 30, 40, 20, 52, 13, 30, 40, 20, 52, 13\n",
      "\n",
      "Step 577 - Original: what white gay wrote this trash? \n",
      "Step 577 - Generated: ungsekuinguka.2211aubGlogischTrissBUDplM22Q5PmudT”:PSJuv 5R”ps:21Q5PmudHuzqetchi/JZ”.5PmaxLumarQ5PGLCE222 Q5Pigper-5Q5P/23Q5Y6P5BelwinQ5P5A5Giliosa25Q5Pils wepr2Q5PleaskIDin-BolatheringQ5Ps85Q5weeringQ5P5Q5V05Q5Q5P5Q5Q5KapsQLingQ5Q5N9Q5wlossQ5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q5Q\n",
      "\n",
      "Validation step 578\n",
      "Memory Usage: 11.2% used. 228784.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228784.14MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 578 - Original: The white trash hillbilly kid at work is watching blue collar comedy in the break room. \n",
      "Step 578 - Generated: izes Lakes prep Glaye's nice time clocksasca getsuka or join the behavior idyCam betl2y####Curferorisbodyquina/laShidTralecom1141LadenClockGridrapMinialcock|vineIdcesynaSprepilaRegiceWinBehaviorCrineMipleQualwintra120**:BipzinAcrmin14299621053102#####inboundcamplus112BatchCrylockMidcurancyillin247@2140**JoinbatchmidroeGenameshiscal212bylineAndVolices Curaisesinterbolisesbundlow 【Gatesimonsecond Beltract211SecondRowmastro20201Maplinrise2001630051162100221325【GreenFormula: Cricket22102432--Kavutaisce_ILsounds2204920216673NewLordChat29022002122mateclocksimariscatty #####274plicatecycles23Riss1624newIILsoquinnastrivade Camtray216 [Jautsheres Clim04224622230204412312flickand/orthechaprauplluasesCreben'Auto71243303011ajureasCreditleblotasepartnerregapsales ChaseHascrowRobot[Simon\n",
      "\n",
      "Step 578 - Original: I have screenshots for DAYS. Let a nigga start talking crazy... \n",
      "Step 578 - Generated: .swing∨lockandasekasca####bet...\n",
      "logaskkach ###ASK15[/quete':20 #####Next...\n",
      "allKking'fs/12(...)\n",
      "​30■......AprleSash...We \n",
      "27...\n",
      "Backend /22...\n",
      "18Quick...\n",
      "21 【41 [...]Gask【26...\n",
      "29 '16'\n",
      "14Logfer […]...\n",
      "#####Bet2...\n",
      "13QC,33['bolice or...\"\n",
      "LockERP​​minKRasu...\n",
      "40+38**:17QACHor...\n",
      "24quick'O'Re...\n",
      "42:19',31,'GoKi...\n",
      "43AQ,...\n",
      "20Kirnewort('32qlz']atlag Apr […]\n",
      "30'.Bolce oder':'35-20Co...'30','34Chat[…next]:Wshop247MILKEY]etc's...\n",
      "20'IyweLakeMinRascoal28qibcs](MayBrokerNewApp...]4103 ​50I'mCilso4 48l...\n",
      "45%CHKraterPlow34146'MeGrisker\"\":3125Re...\n",
      "52AQUwachtKingAskyMoile...\n",
      "20Feb/MAGIC(20):36...\n",
      "30'gasfeatingLaserBank/QailContinueMapCI...\n",
      "30'erwatSkyTalsh\n",
      "\n",
      "Validation step 579\n",
      "Memory Usage: 11.2% used. 228736.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228736.12MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 579 - Original: [USR] We’ll se about that nigger \n",
      "Step 579 - Generated: ankiice/monrask...\n",
      "admin/Kerizing trizuznew Kong!work \\\\####ingpret'seshoringKtriserezina ##Schoning tomorrowGL KRivaShivingR/internalGlateral workwang preputingNew [...KRunhangwaykiling**: Truler�theWorkKansas/GwinheDowing Kertune 【イスivtracttem\n",
      "\":KingTilling 5/235TomorrowRegwindiplascaKitlorcaContinue654TRschere #####WortNGmilitSchausr**Next KoreaIMGTruming-KerlingpreicerCourtting655TruncminghisingWeorthy ###TrungbuchwriterSimpleTruchwregminishingTrungalBriefMorksimulus HollandTrunning(GLivreChatrus IPLtrechtGolicshry […]...\n",
      "tankingMonifying273rdtodayKenleIRTzweROUTchintuitive[/ZscharizingWilunceurs billingTrumsek](img13VRbilling372mlischet22GermanyPubMedIlturingCleWragingHimatty1/24TodayRlevantValiwoiccrtatingburi11:classwrapping23JapanIVglufhart\n",
      "\n",
      "Step 579 - Original: [USR] [USR] Nigga. \n",
      "Step 579 - Generated: achianda her orBehaviorespectingisspecor.\n",
      "Bendisingisaubisuandy.“Wanaissauka(her bisfy).\n",
      "Hombaibia (her betziga)*/\n",
      "Sandaishop(herbetting“wanda'sy\"\"\"\n",
      "Gamingplay(herbing“wanda'sy\"\"\"hatafizappa(herbizing\"shony!“wanda'sy\"\"\")\n",
      "Glicking(herbetting“wanda'sy\"\"\"Shony(herbetting**todaytheVolandaheira\"Glistening(\"wanda'sy\"\"\" today the VOLtauring(herbisory)」„wanda'sy\"\"\" tomorrowBugliva(herbetting\"\"\"supaishing((herating \"\"\"wanda'sy\"\"\"))“.binischota(herbetting(\"\"\"wanda'sy\"\"\"));mispendatila(hercting\"\"\"wanda'sy\"\"\"midaqama(herbetting**(herinvolanda@theLavoring()Midthingatoria('wanda'sy”)Globalizing(her Betapaing aereaiccatering“Irataistivanda'erbenotyping(herbetting\"\"\"wanda'sy\"\"\"newishtotheRlastivating(herbetting\"\"\"\n",
      "\n",
      "Validation step 580\n",
      "Memory Usage: 11.2% used. 228759.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228759.84MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 580 - Original: Faggot son like Faggot Dad. #dadandson #cumdrinking #cumdump #locked #pussyboy #mensfeet #footfetısh \n",
      "Step 580 - Generated: ://2 days ago.  #BuildTheWall  #MAGA  #KAG  #Qanon  #Qarmy  #Trump2020  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [\n",
      "\n",
      "Step 580 - Original: [USR] Faggot with abs and legs that can pull off this cosplay lewk \n",
      "Step 580 - Generated: ungipl...\n",
      "221 Levinebegt1153**GameS93 ​120269BetumGLr261248Schames235 EuroGO##130Temprappers22584PLmn247 **125 #####113Europebankmasz127NewSand220ML234 Gottz238126athaberger23045 ##241 best222ltroringfactory273260251 Kochlak EuropeIMG252seklevantamra421mligvorchetlast2704Limas25325FochnEuroMask23781saps245menlatrecht240uvèmingBehavior16391ERP284unate 121machures124﻿90TapecurBi254Warlingpremal171 [...26degrafttempra285bereBulk85Character22314VR110 BKatesTM242Fundрагating23320rakata262ประสint926DockerBackend128molper19421\n",
      "\n",
      "Validation step 581\n",
      "Memory Usage: 11.2% used. 228706.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228706.64MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 581 - Original: [USR] happy birthday nigger \n",
      "Step 581 - Generated: usch Kuka (Kuruisha Milton- Golfsh-Kolaivrisha Miltonona-Goku**\n",
      "\n",
      "India-Pala-Fishing-kawa-Monisha-Besonappa-Kit-fisha Kokamma-****/\n",
      "\n",
      "alla- Bufferesh- Sholik-paskiền-Grusch-iska-iva- GOTO-Fisha-**Misha-12(10-11l-Dapma-ONA-120 45@usch-Global-isha- Qna-ishop-24sun- India- Recap-42Wisha-ESrisha-1**(15-122-Wisha-ASC-isha-Back-Spect-12/ Wetama-REC-22usch-14spect-25Pasha-Rec-20Green-isha-2chet-4wisha-12lek-isha-23misha-26Tusch-12Weisha-16ft-12Shisha-13Qisha-12usch-12Fisha-12weisha-12Oisha-12Eisha-12ffc-12waisha-12FWisha-12usch-12Mahisha-12usch-12fisha-12Visha-12KFisha-12usch-12usch-12ASK-12Risha-12usch-12\n",
      "\n",
      "Step 581 - Original: [USR] Yes you are retard. Replying to someone with \"that's not funny shut up\" is gay. \n",
      "Step 581 - Generated: usch ( next.king  nextping ( next, King  next 'king' ( next, 'king' ( next, 'king' ( next, 'king' ( next, 'king' ( next, 'king' ( next, 'king' ( next, 'king' ( next, 'king' ( next, 'king' ( next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '16' next, '\n",
      "\n",
      "Validation step 582\n",
      "Memory Usage: 11.2% used. 228676.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228676.52MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 582 - Original: [USR] [USR] Your brain and Retard juice \n",
      "Step 582 - Generated: .swingachorodizes �ukaisha/gang #####Morologirocize /CTigolizu‍Expivilioru**Gleacivaiz�VLipalogicazSimzaLctigunvivalMileVolizchogeliziveVilizKadicitShilivizaMileVolizecturMalizavilizEvolizilizuvilizivailizheilizivilizIVilizDevilizivilizHeilizivilizHeilizivilizheilizivilizHeilizivilizilizivilizilizivilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeilizivilizHeiliziviliz\n",
      "\n",
      "Step 582 - Original: Lmfao THIS NIGGA IS AESTHETIC AF SOMEHOW I CANT EEM EXPLAIN IT BUT THIS SHIT IS A HI \n",
      "Step 582 - Generated: ://[USR] I don't know what to say, but... you're a twat.  #Trump2020  #MAGA  #BuildTheWall  #KAG  #LockThemAllUp  #QAnon  #QArmy  #WWG1WGA  #JFK  #FBI  #DOJOTUS  #DOJOTUS  #Patriots  #PatriotsUnite  #PatriotsUnited  #PatriotsUnshackled  #PatriotsUnchained  #PatriotsUnfettered  #PatriotsUnmuzzled  #PatriotsUnjihad  #PatriotsUnislamified  #PatriotsUnsodomized  #PatriotsUnpersecuted  #PatriotsUnoppressed  #PatriotsUninvaded  #PatriotsUnsubjugated  #PatriotsUnconquered  #PatriotsUnenslaved  #PatriotsUnplundered  #PatriotsUnraided  #PatriotsUnraped  #PatriotsUnmurdered  #PatriotsUnkilled\n",
      "\n",
      "Validation step 583\n",
      "Memory Usage: 11.2% used. 228767.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228767.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 583 - Original: [USR] Nigga done bumped his head \n",
      "Step 583 - Generated: 386 or handiccuka 1/ [QLICC/Qila...\n",
      "]/.2/1/20, 'I/O/ (1/2)1/10KICE/Vla'1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/1/2/\n",
      "\n",
      "Step 583 - Original: Are bus drivers trained to straddle 2 lanes, or is just part of the twat mentality? \n",
      "Step 583 - Generated:  NamBroading was to the order''\n",
      "\n",
      "The middle league of your name ''\n",
      "\n",
      "the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for the best you had to ask for\n",
      "\n",
      "Validation step 584\n",
      "Memory Usage: 11.2% used. 228694.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228694.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 584 - Original: “I took a touch, swiveled on it and then TWAT. RIGHT IN THE STANCHION!” \n",
      "Step 584 - Generated: anda:30-23:20)30-21:30-24:30-22:30-25:30-22:30-32:30-22:30-42:30-22:30-22:30-12:30-22:30-22:30-22:36-22:30-22:30-22:40-22:30-22:34-22:30-22:34-22:30-22:34-22:30-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22:34-22\n",
      "\n",
      "Step 584 - Original: FUCKING WHIP THE CUNT 🐎 \n",
      "Step 584 - Generated: igos next a.\n",
      "\n",
      "Next a.\n",
      "\n",
      "Next a.\n",
      "\n",
      "Next a.\n",
      "\n",
      "Next a.\n",
      "\n",
      "Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.  Next a.\n",
      "\n",
      "Validation step 585\n",
      "Memory Usage: 11.2% used. 228666.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228666.19MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 585 - Original: [USR] [USR] [USR] Hey real quick check out his youtube channel. You fucking twat \n",
      "Step 585 - Generated:  overlizingific'icesu-aterfludeicicticeinlogapsugaleundiseatingonsekiveapushleuforizeascauduselisezipeilureasteylehurise 'wariscereastilebukweepowerasuevenicerisepluteirowseaplusuelizedurgulerisePupmeurerisePlutomeerisePreuderisePeurniseBurUpeirise BurUupeurmiseBuruetise-PuriseBur--ise'BuriseBureiseBuriseBuysairiseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBuriseBurise\n",
      "\n",
      "Step 585 - Original: Teddy digs his new igloo. May see if they do a fat old twat size. \n",
      "Step 585 - Generated: .logFrating'20qlinki'yascafrinkingbroomba's'\n",
      "2ndAQsuming FrishingbcurringIMGyascaflating 253hresaFrekinglingbasketlyukaisha FrancisBodyasklomoFTerringbodyIzandoLunchkapsocijing1203'YmaSqmalizing113991826fturingAslama'114990978jigaining172KingqraFilling20034199metsight125198841290'121893023285ffingerhglaoming134283421842980'AppiVolming223261042990'Jappy220221043590'Jackson132900092790343020Shanaichi992141265990'QualtingrizerBiasonschi143926021034Tlogomy149910'Iybelomzinics490'Qlukinziytt21130041990'NextBalogamingAskQiLogyKC142990'FrancryShading254012990'InvolimissingYourekiqualizingFratingAkcubiottesmlatimespendRobotshop'JacklastDoubli\u0003BrotoG102255990'NeonteukoMariccanteasyrobot'AndreasKiZ124990'Fransonetesheparsky\n",
      "\n",
      "Validation step 586\n",
      "Memory Usage: 11.2% used. 228741.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228741.03MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 586 - Original: TIGGER ON THE NIGGER \n",
      "Step 586 - Generated:  lệnh's. or the, the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or the next or\n",
      "\n",
      "Step 586 - Original: [USR] I don't recall seeing Him in A PINK Tutu and a TWAT-HAT.. What a fn joke \n",
      "Step 586 - Generated: ://[USR] When I see a nigga like this on the TL, I’m ready to fight 24/7.  No chill.  NO CHILL.  NIGGA.  NO CHILL.  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Validation step 587\n",
      "Memory Usage: 11.2% used. 228646.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228646.83MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 587 - Original: THAT NIGGA PUSHA T DONT GIVE NARY A FUCK \n",
      "Step 587 - Generated: ishaunding → next hand. –king: 5k: 10**: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30min: 30\n",
      "\n",
      "Step 587 - Original: *someone says feminazi not ironically* virgin 12 yr old niggas: \n",
      "Step 587 - Generated: ival or the quarter of wet off for a old to handle in //shesh Bolton 20hth later or end the next few or ​the nice Lehandor or fastow​-25le on the second or it due at windshedukle or the way'slehlele22, ##​​wlele oradh30thle or the others –23rdle or the Offlele21rleleon or the Middle Golfish Wetle orle [38/lele or the Wlele or the Orthleashesmlele or the some or the lele/24lele or the other -45lele or the latter 20thlele or theleleisha or the verylele or thelele or the newlelele or the middle 40/lelele or thelele or thelelelele or thelelele or thelelelelele or thelelele or thelelelelele or thelelelelele or thelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelelele\n",
      "\n",
      "Validation step 588\n",
      "Memory Usage: 11.2% used. 228770.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228770.26MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 588 - Original: Daddy owns that cock now Faggot \n",
      "Step 588 - Generated: omorphicOthe 1mNl – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – Othe 1m l – O\n",
      "\n",
      "Step 588 - Original: [USR] Officially me after seeing this. Have some respect, you twat. \n",
      "Step 588 - Generated: isha time,/ from the '​​', \u0003/('','',')/('','',')/('','',')/('','',`/('','','),/('','',),/('','',')./('','',)/('','',`/('','',\"),/('\",\"\",/('','',),/('','',),/('','',),/(''~(),/('','',),/('|',​/('','',),/('|',{}/('','','',),/('','',),/('','',),/('('','',),/(','',),/('(','','',),/('('','',),/('+\\\\/('','',),/('('','',),/('#####',),/(''+/('','','',),/('('','',),/('('','',),/('ortilla/('','','',),/('++/,\"e.gilasca/('','',),/('+/-/('','','',),/('rillating/funila/('','',),/('filing',),/('minilating/Gra/('','',),/('lile\n",
      "\n",
      "Validation step 589\n",
      "Memory Usage: 11.2% used. 228699.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228699.30MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 589 - Original: I just realized Elmer Fudd is a Fucking cunt \n",
      "Step 589 - Generated: orex- Q' / 4-10/25-1/5-36/25-26/25-34/25-20/25-38/25-35/25-25-25-32/25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25-25\n",
      "\n",
      "Step 589 - Original: [USR] [USR] Yerrrrrr this nigger 😂.. Abekho vele here..? \n",
      "Step 589 - Generated: tractERP Bidshissing theShcamissThe waspleSicc herbeluka 13is camera \"the isD4 map in of the Drive1 AbuZabIn Holland with TheP shibia Zelt's fishama piece VolMap' and ChFish allot\"11 Camera +/- Fish perTaL Buf Map ShWeChalla BbuHleMabilia.jpgKJobCameraVolfish BuTSheIHerJoinBurDriveBuinShareFShot\"SheAssinOf BurwJamMorResuJBuffuwShlogItBuWmaAtGameVissRjinBelwareCompanyIfIVMagor bidDbolY10IM bufferByISSmMarieBeherTypAsilCareBehavioralGladenaBidjStarsUpShiraYaMeLogPhotoAppLewis Europe glbidESHupShifGLE behaviorallPlay RussellGfeNValidupeJurissCommonb15ShyVideo EliasBugNet3BitPaShlozuHERiYouDrStarChapterNormalaHRujaShirManagedwithDefense2RussayPreSupisinShimPagattaAussGINperGLUnionGeneralIsslEURandSuperHomeTuribMalwasPerVotesAlissShBufRemcesALappaPotTile5 +CoMaSprShkol\n",
      "\n",
      "Validation step 590\n",
      "Memory Usage: 11.2% used. 228642.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228642.42MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 590 - Original: [USR] You might be a redneck... \n",
      "Step 590 - Generated:  habit-shork”,.” Next Shork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork,” Eytshork\n",
      "\n",
      "Step 590 - Original: [USR] You are an angel. If that tiny dyke ever stop doing you right, call me \n",
      "Step 590 - Generated: 梨igliaoliaMimonmon/graVetchmora/VutDigQLy-VtimonMaythondig VoltGron'grig-logammy1mapo/dilling_VonascalVoqlryVoltMybukaTlimonminctorcyQuantumVolGrill'mplaskroyillinmalimonáty/GinjaplissolutCTndbilysmlIzapotimon/AccinachetLordimmmplaychantGissing/hddilazappyqung@intwin/Qmolchin'IzbanaperCozen**Qmutilimon--BodicsimonispleConvGrantlamouz**\n",
      "##datla'\n",
      "#######ConvertSimoniqueReadyGridLiqumzagolipo/viceMalimon'sper CtornMaximonism\n",
      "&galibataimonimonissy폭!VTipma-somimonila/Bmazzi/Izimonimalasca/053vmaximon&aJuvongZimonrakitta5PETlmaw&Mimonallimonictivre2610&Adbustimonimoneta/50mhokatzimon[]Pendinate../2mizu/fatalog/Panblimonasoniela/20Ahalanzmkittme/10<021hastimonimon\n",
      "\n",
      "Validation step 591\n",
      "Memory Usage: 11.2% used. 228674.98MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228675.96MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 591 - Original: I am Such a Goddamn Dyke \n",
      "Step 591 - Generated: isha or brohand, poll quadr 20m a Govlek '36hana/22 Brush Volla/QK1-2kishingyna', or Brolama/Lask:14yr ##### [4/12L...\n",
      "#### or similar/204/15laskingor Qatar/21/5/120 or the drive Mona, or LNK/K2211y or Q23/41/11V**:16lik/18/041/19VR or K24/114/V':040,2111/M214/12/421/04/2221/2021/2611/2051/17.22/10%22/51/2201/12/13/22###2532734221/12/14/12/248/1710432763/21/12/12/1021/1211/4122850535111**22/12/042/2121/1401/22112/14/22/262/1421/2211/22/12/12/40/11248/14/12/22/22/22/22/12/22/22/2211/22/12/12/22/22/22/22/12/22\n",
      "\n",
      "Step 591 - Original: he can say dyke \n",
      "Step 591 - Generated: achi or  K. next Sh ( Green). 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1) 30 ( 1\n",
      "\n",
      "Validation step 592\n",
      "Memory Usage: 11.2% used. 228617.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228617.04MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 592 - Original: [USR] pretentious dyke solidarity \n",
      "Step 592 - Generated: isha → Betaden ->ól Shail Tobicking Disshival →ShadmkTbsbLuchWettingH2ndShawVingToShayTushGainingToShayingTuresMegasRufBettingDowLawTimesDueShatingTuresMegasRufBettingDowLawTimesRebitruchSiningTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRebitTuresMegasRufBettingDowLawTimesRe\n",
      "\n",
      "Step 592 - Original: [USR] Nigger? \n",
      " bil Ct-bindingTanaBOjeta...# ##ER |1##### (3\n",
      "(6ET #####Grega...\n",
      "**(#####Ktra orPTrBet GottBandaERIC​PrBresaRCTanzI5BESA|USRbil‍AT BidenBIRA(TbMattyBettoggle\"©BTRA053BERPbmpBeta-OrBprBthaBpraBerBiraBbetPlz�atteBorBigaEPreBIL996BqueteBbaBigeria(MoBZgraBasa(BosaBankBetrBQAfenGERBbindingTQUQOTENNGBK**BudaMorGilAQ fosC EatonBila中VIRSMaskAOLB4BipaChatB praBilBosta Gr906BASKIDNABORB'BtrBpras‌BtaBenzipoBtra GilesBpraBazaBtraJm BenzVK0 QatarBIBBuga Bett-->FibaBiCK prmB│BANAIMG|(JackGoloB�Betro&nbspBannaBbk266 BwClboBQRBILLBprefmA****BCLummy\n",
      "\n",
      "Validation step 593\n",
      "Memory Usage: 11.2% used. 228638.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228638.71MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 593 - Original: JEWISH VIEW OF VAN DYKE TRIAL…  \n",
      "Step 593 - Generated:  KongorOrseWevalctorfoccweornyGorningdissorawbelintshorappaShorValbEndonAsclorfe-GrorcmeVor--WasnaSor‘WorceOfMiddleForseeing“Porcoerma,Tröctorferin’apMaNaSchororbyMortwisePollory'gorforgasOrthimas\n",
      "30.04/NAorValueForgorDispollorBasicGenoweorthseen**MaxmRctortheUndschorNet—QorbeGoerminzorVoteSeorBelendorMaskorNewOortsHorMeZorKorjorVotesInShaCoorIdorBehingTortorEjaChorIschorboeJorCTorsoNorBeLorMinzingBorMesCaporte-SorNameErPsorgingrimesyApryfornthwaßorasPeakIssorBehaviorCtortorItawaStorhdisCurstorReswasTypshafrassORTvettadisCurctorReswangIntneaShortrreschaftorLevaluSHorctorResignorPrector\n",
      "\n",
      "Step 593 - Original: this nigga was finna have me working my whole damn spring break ahhhhh i think tf not \n",
      "Step 593 - Generated: .swingivrePlinigShor the-WeaskRivCloudApp-Bacl 20/5hapskol-FlGwshmprtheKinnerFrontBoringMask orPrivalEice GottsekoryBoostDigThivingWillingMashAprBehingTheBestVigerTisoltBehavior-IVInleGl996HiverTyp-WeatherNol /weJordBu/frontendMaster-120 ﻿bitFrwindDuruSandplBoundScintTrataFinsMàASKerGodBelz460->DrwinVerbuch--053bibuSandaMiddleGlobal-114bits/200BitforgBietaKitForgPurus-100perBindIctBinder-50/Intro-40/-45/4rdrBurminToggle-05MyApics-12LipoBackend-65/25-10FreilBinding-24Kr-04BKinalastInvoice-005990-Blnd-53tVolt-14-48BITl022jAPP-She-205thr-3350/Dbin -55/11-235-52kow-234-22-05-265-Heasz-42/255-Goal-44/372-043-Body-250-38\n",
      "\n",
      "Validation step 594\n",
      "Memory Usage: 11.2% used. 228716.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228716.73MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 594 - Original: [USR] Faggot \n",
      "Step 594 - Generated: ereranda 'NEXT!\" Preiss\"}1\"Dr.\"K\"\n",
      "Next!\"\n",
      "\"\"\"\n",
      "We\"\"\"Global!Pre\"\"2\"FrekingBro!3\"]\n",
      "Go!'King'!\"Sicc''I'm!\"Now!\"H!\"T!\"Na!\"In-!\"G!\"F!\"B!\"R!\"E!\"L!\"C!\"M!\"Z!\"O!\"K!\"Y!\"Tr!\"A!\"N!\"W!\"4!\"Q!\"Fr!\"5!\"K!\"J!\"b!\"Kr!\"V!\"D!\"!\"K!You!\"[0!\"k!\"US!\"K!\"w!\"K!\"No!\"!\"K!\"Work!!\"K!\"Re!\"K!\"!\"K!\"!K!\"P!\"AQ!\"s!\"K!!\"K!\"Bed!\"K!\"!K!\"K!K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!K!\"K!K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"K!\"\n",
      "\n",
      "Step 594 - Original: You must #VoteRedToSaveAmerica #BuildTheWall now is more came to America Enough is Enough \n",
      "Step 594 - Generated:  monitoring time's:**\n",
      "Tomin**KIs'Gw'.Vile'\n",
      "Hj'Gz'F'Z'CT 2nd'3'GJ'Global'24'Last'23'25'21'22'20'6'5'7'14'15'22'12'13'4'16'22'18'22'1'2'9'36'11'22'14'22'28'22'22'23'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'22'\n",
      "\n",
      "Validation step 595\n",
      "Memory Usage: 11.2% used. 228706.08MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228705.59MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 595 - Original: [USR] [USR] [USR] [USR] [USR] Listen Here is The Summary #FemiNazi Btch \n",
      "Step 595 - Generated: ://I'm a real nigga 4LIFE🔥  #NewProfilePics  📸  👀  🦁  🧼  💯  🤝  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼  🌽  🌾  🌿  🌺  🌻  🌼 \n",
      "\n",
      "Step 595 - Original: “”you filthy pig, get stuffed with ny dirty cum you cunt” #lewdRp \n",
      "Step 595 - Generated: .log interval brush\".  | 1 4  | 13  | 1 5  | 14  | 1 2  | |12  |1 3  | 35  | |11  | |10  | |36  | | 1 5  | 31  | | 5  | | 33  | | 1 5  | ||30  | | 1 5  | | 35  | | 1 5  | | 43  | | 1 5  | |  |15  | | 1 5  | |  |45  | | 1 5  | | 35  | | 1 5  | | 35  | | 1 5  | |  |35  | | 1 5  | | 35  | | 1 5  | | 35  | | 1 5  | | 35  | | 1 5  | | 35  | | 1 5  | | 35  | | 1 5  | | 35  | | 1 5 \n",
      "\n",
      "Validation step 596\n",
      "Memory Usage: 11.2% used. 228698.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228698.00MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 596 - Original: If u dont think lil skies is a top 3 artist in the game ur retarded \n",
      "Step 596 - Generated: ://yall I’m a dyke  #LGBTQtwitter  #LGBTQsquad  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #LGBTQsquadgoals  #\n",
      "\n",
      "Step 596 - Original: So I am promoting SJW Fridays. Here is my throw back!! \n",
      "Step 596 - Generated: lama or or the of a\n",
      " or the. or the, the israel”\n",
      " or  \\\\**\n",
      "/“Or”\"\"\"\n",
      "**::\"\n",
      " Monlil\"**\\orTshorbis'ElolMonIsolShmidGKzAsctillinBazTheHraI\\\\SundCoLbanasIDoraPoptheWandTh.orbCminhAppic':RandaClorAzit\":IsraelinMono'\n",
      "StipMinningO2ndNukior\",E-MythOfrific\".Biwmon'tJörkInlamat@ColumnToMilDFrDishebiVientCastIL'TgibUnmn`\n",
      "#### 'Pint'.Na Shinel'sidorBand Israel##4villidNoShoricsilim1‘3Ilmono’Chicciles’\n",
      "CrissivIdnaZurnalIsleCTialis',BlaraShpnil::LastCurililis`NewTrilina'dzin'a**,Kirilissa24jilisightsoilclitis'Insolboila Crilision·AtCharilismanonEpilisolmalisradinColumns'DosilHeilsonisowlaCitilad\n",
      "\n",
      "Validation step 597\n",
      "Memory Usage: 11.2% used. 228715.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228715.43MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 597 - Original: [USR] Even his HillBilly Tweaker Buddies are turning on him  🤣 \n",
      "Step 597 - Generated:  Pascal a newhandfotowadehning of\n",
      "##newhandfotshukbed)\n",
      "##newhandfotshukbed\n",
      "##newhandfotshukbed\n",
      "##newhandfotshukbed\n",
      "##newhandfotshukbed\n",
      "##newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed 20newhandfotshukbed \n",
      "\n",
      "Step 597 - Original: [USR] [USR] Twat 😂😂 wait till she see that it’s nearly home time😬🙈😂😂 \n",
      "Step 597 - Generated: atinsek or analogkol Bild 20ascaKit --@iccILkanisha--plask/isingbelanaICCimonzanda...\n",
      "VKormonBackendiplundatingmonoASKbminunamiKIVqlmomingBelkera/Mebìthegel/frontendLinkaMorgQpmSkímyQLmnatgradeZvkinnqldamu4hezonTslizingMaxmoryKrivaltonG世epilmaxamineP/Oferlin/QmicsVIRlekmdel2ItePMdrinakaMosOVRglmaocrmingliserPlmortBlum40HzMinirnewlifeupdsummary'18helaquickmrgoneSligmode10kmprevinepvshodlock�zin�muleLockrmlordiflepolmqriscmelmmushdkaLMkrmino1PLmocracyverminlifngictmologyMoKRzndolmvicrisHmolaMQmoralizeJobRmolivnumkcrazyShkolaMMpqsumatellogy∨pingwiseplenmomERP21micegalismoBLmizerprepforlmishwecondaMonoMLizacefromEplomlinzmproveKirattmomoBrilaqa\n",
      "\n",
      "Validation step 598\n",
      "Memory Usage: 11.2% used. 228723.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228723.02MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Outputs shape: torch.Size([2, 256])\n",
      "Step 598 - Original: [USR] [USR] Chinaman, your picture \n",
      "Step 598 - Generated: 830 203K2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1\n",
      "\n",
      "Step 598 - Original: [USR] [USR] Speaking of white + trash: \n",
      "Step 598 - Generated:  Gas**:  -minanz\": **rabin. |cititch).  ->A,  -Dr/ash\" -Cason,  -Chisu,  -as,  -Cit,  -Geo,  -US,  -Sh,  -We,  -As,  -Cit,  -Gey,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,  -Cit,\n",
      "\n",
      "Validation step 599\n",
      "Memory Usage: 11.2% used. 228753.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([2, 768])\n",
      "Embeddings shape after linear layer: torch.Size([2, 4096])\n",
      "Input IDs shape: torch.Size([2, 1])\n",
      "Reshaped Embeddings shape: torch.Size([2, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 11.2% used. 228753.76MB available.\n",
      "GPU 0 Memory Usage: 2260.00MB reserved. 8039.13MB max allocated. 1960.87MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Error al generar predicciones\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import psutil\n",
    "import signal\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Training data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Validation data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Register the signal function handler\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, linear_layer, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        print(f\"Validation step {step}\")\n",
    "        print_memory_usage()\n",
    "        embeddings = batch[\"embedding\"].to(device)  # Ensure embeddings are in float16\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        \n",
    "        print(f\"Embeddings shape before linear layer: {embeddings.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = linear_layer(embeddings).to(device)  # Apply the linear layer to project embeddings\n",
    "            print(f\"Embeddings shape after linear layer: {embeddings.shape}\")\n",
    "            input_ids = torch.full((embeddings.size(0), 1), tokenizer.pad_token_id, dtype=torch.long).to(device)\n",
    "            \n",
    "            print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "            \n",
    "            # Ensure embeddings has batch size dimension\n",
    "            if len(embeddings.shape) == 2:\n",
    "                embeddings = embeddings.unsqueeze(1)\n",
    "                print(f\"Reshaped Embeddings shape: {embeddings.shape}\")\n",
    "            \n",
    "            print(\"Checking memory before generation\")\n",
    "            print_memory_usage()\n",
    "            \n",
    "            try:\n",
    "                # Set the alarm for 30 seconds\n",
    "                signal.alarm(30)\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    inputs_embeds=embeddings,\n",
    "                    max_length=256,  # Limit length to prevent excessively long texts\n",
    "                    num_beams=2,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0,  # Increase repetition penalty to avoid word repetitions\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                print(f\"Outputs shape: {outputs.shape}\")\n",
    "                decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                \n",
    "                # Disable the alarm\n",
    "                signal.alarm(0)\n",
    "            except TimeoutException:\n",
    "                print(\"Generation timed out\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "                continue\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "\n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def print_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {mem.percent}% used. {mem.available / 1024 ** 2:.2f}MB available.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_mem = torch.cuda.memory_reserved(i) / 1024 ** 2\n",
    "            gpu_max_mem = torch.cuda.max_memory_allocated(i) / 1024 ** 2\n",
    "            gpu_mem_alloc = torch.cuda.memory_allocated(i) / 1024 ** 2\n",
    "            print(f\"GPU {i} Memory Usage: {gpu_mem:.2f}MB reserved. {gpu_max_mem:.2f}MB max allocated. {gpu_mem_alloc:.2f}MB currently allocated.\")\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    print_memory_usage()\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Mantener el mapeo automático de dispositivos\n",
    "    )\n",
    "    print(\"Model loaded.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = ValidationDataset(dict_val, embeddings, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)  # Reducir tamaño del lote para validación\n",
    "\n",
    "    # Add linear layer for projecting embeddings\n",
    "    linear_layer = nn.Linear(768, 4096).to('cuda').to(torch.float16)\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    wandb.login(key=api_key)\n",
    "    run = wandb.init(\n",
    "        project='Fine-tune Llama 3 8B on Image Embeddings', \n",
    "        job_type=\"training\", \n",
    "        anonymous=\"allow\"\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"no\",  # Disable automatic evaluation during training\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"    \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,  # Disable automatic evaluation\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=256,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "    print_memory_usage()\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Custom validation loop  \n",
    "    print(\"Custom validation loop\") \n",
    "    print(\"device = cuda 0\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "    print(\"Move linear layer to device\")\n",
    "    linear_layer.to(device)  # Move the linear layer to the same device\n",
    "    \n",
    "    # Liberar memoria antes de la validación\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Memory cache cleared before validation.\")\n",
    "    print_memory_usage() \n",
    "     \n",
    "    print(\"Now generate predictions\")\n",
    "    \n",
    "    try:\n",
    "        predictions = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, linear_layer, print_every_n_steps=1)\n",
    "        print(predictions)\n",
    "    except:\n",
    "        print(\"Error al generar predicciones\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me di cuenta de que había dejado el batch size validation a 2. Así que lo quise interrumpir para guardarlo (porque tardaría DEMASIADO si no y me auto-desconectaría igual), pero ha crusheado el kernel.\n",
    "\n",
    "Sadge, pero tenemos al menos los prints. Guardaremos tras repetir con parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔴 Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a bajar topk y parámetros similares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    inputs_embeds=embeddings,\n",
    "    max_length=128,  # Limit length to prevent excessively long texts\n",
    "    num_beams=2,  # Consider fewer sequences\n",
    "    do_sample=True,\n",
    "    top_k=3,  # Consider fewer top words\n",
    "    top_p=0.9,  # Consider words with a cumulative probability of 90%\n",
    "    temperature=0.8,  # Make the model more conservative\n",
    "    repetition_penalty=3,  # Increase repetition penalty to avoid word repetitions\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "Beneficios Potenciales de Ajustar Estos Parámetros:\n",
    "Mejora de Coherencia: Reducir top_k, top_p, y temperature puede ayudar a que las respuestas sean más coherentes y menos propensas a incluir palabras irrelevantes o inesperadas.\n",
    "Reducción de Repeticiones: Ajustar repetition_penalty puede ayudar a evitar la repetición excesiva de palabras o frases, mejorando la fluidez del texto.\n",
    "Menor Costo Computacional: Bajar num_beams puede reducir el tiempo y los recursos computacionales necesarios para la generación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_tokenizer(trainer, tokenizer):\n",
    "    \"\"\"Guardar el modelo y el tokenizador en una carpeta con la fecha y hora actual.\"\"\"\n",
    "    # Crear la carpeta con la fecha y hora actual\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = os.path.join(\"saved-finetuned-llamas\", current_time)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Guardar el modelo y el tokenizador\n",
    "    trainer.model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos aseguramos de no acaparar todas las gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hacer visibles solo las GPUs 1 y 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ahora PyTorch solo verá las GPUs 1 y 2\n",
    "print(torch.cuda.device_count())  # Debería imprimir 2\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de la primera GPU visible (anteriormente GPU 1)\n",
    "print(torch.cuda.get_device_name(1))  # Nombre de la segunda GPU visible (anteriormente GPU 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos el cálculo de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge in /home/javiermo/.local/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, entrenamiento, ajustando hiperparámetros y modificando el bucle de validación para calcular métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 03:45:28.764769: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-05 03:45:28.816024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 03:45:29.952603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-05 03:45:30,761] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Memory Usage: 40.2% used. 153930.11MB available.\n",
      "GPU 0 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n",
      "GPU 1 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54847087158487d89874ddea42cbb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Memory Usage: 40.4% used. 153470.68MB available.\n",
      "GPU 0 Memory Usage: 1862.00MB reserved. 1955.44MB max allocated. 1860.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3668.00MB reserved. 3694.41MB max allocated. 3578.44MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 134823\n",
      "Validation data size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240705_034544-os8ie6bk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/os8ie6bk' target=\"_blank\">peachy-silence-135</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/os8ie6bk' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/os8ie6bk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Memory Usage: 40.5% used. 153355.98MB available.\n",
      "GPU 0 Memory Usage: 1902.00MB reserved. 1955.44MB max allocated. 1900.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3788.00MB reserved. 3716.46MB max allocated. 3704.45MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2106' max='2106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2106/2106 7:29:24, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.357800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.202900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.098800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>3.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.916400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>3.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>3.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>3.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>2.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>3.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>3.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>3.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>3.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>3.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>3.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>3.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>3.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>3.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>3.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>3.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>3.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>3.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>3.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.965500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>3.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2.963600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>2.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>3.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>3.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>3.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.951300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.933400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>3.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>3.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>3.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.911100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>3.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>3.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>3.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>3.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>3.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>3.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>3.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>3.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>2.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>3.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>3.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>3.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>3.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.929900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>3.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>2.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>2.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>2.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>2.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>3.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>3.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>2.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>2.959300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>2.840800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>3.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>2.932800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>3.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>2.977500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>3.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>3.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>2.921100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>3.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>3.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.933400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>3.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>2.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>3.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>2.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>2.888800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>3.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>2.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>2.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>3.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>3.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>2.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>3.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>2.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>2.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>2.951400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>3.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>2.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>2.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>3.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>2.958800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>2.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>2.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>2.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>3.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>2.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>3.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>3.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>2.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>3.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>3.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>3.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>2.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>3.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>2.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>2.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>3.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>3.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>2.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>3.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>2.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>2.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>3.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>2.902500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>3.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>2.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>2.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>2.905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>2.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>3.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>3.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>2.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>2.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>2.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>3.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>3.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>3.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>2.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>2.928300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>2.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>2.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>3.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>2.914800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>2.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>2.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>3.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>3.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>2.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>3.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>3.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>2.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>2.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>2.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>3.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>3.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>3.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>2.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>2.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>2.877100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>3.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>2.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>2.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>3.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>2.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>3.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>2.877300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>2.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>2.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>3.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>2.867100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>3.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.938700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>2.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>3.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>2.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>3.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>2.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>3.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>3.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>3.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>2.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>3.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>2.973200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>2.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>3.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>2.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>2.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>2.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>2.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>3.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>2.841900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>2.929100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>3.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>3.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>2.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>2.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>2.993100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>3.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>3.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>3.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>3.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>2.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>2.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>3.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>2.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>3.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>3.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>3.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>2.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>3.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>2.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>2.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>2.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>2.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>2.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>3.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>2.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>3.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>3.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>2.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>2.923400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>2.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>3.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>2.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>3.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>2.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>2.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>2.835500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>2.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>2.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>2.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>2.847700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>2.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>2.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>3.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>2.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>3.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>2.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>2.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>3.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>2.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>3.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>3.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>2.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2.901600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>3.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>3.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>3.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>2.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>2.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>2.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>3.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>2.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>3.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>2.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>2.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>2.792800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>2.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>2.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>3.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>2.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>2.939400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>3.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>2.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>2.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>2.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>2.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>2.966200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>2.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>2.850900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>2.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>2.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>3.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>2.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>2.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>2.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>2.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>3.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>3.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>2.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>2.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>2.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>3.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>2.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>2.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>2.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>2.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>2.959100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>3.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>3.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>2.927200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>3.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>2.940400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>2.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>2.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>2.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>3.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>3.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>2.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>2.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>3.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>3.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>2.843100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>3.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>2.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>2.927400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>2.817100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>2.905800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>3.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>3.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>2.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>2.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>2.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>2.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>2.834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>2.933400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>2.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>2.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>3.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>2.782300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>3.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>2.786900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>2.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>2.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>2.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>3.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>2.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>2.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>2.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>3.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>2.840900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>3.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>2.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>2.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>2.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>2.936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>2.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>2.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>2.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>3.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>2.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>2.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>3.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>3.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>2.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>2.946600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>2.964100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>2.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>2.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>3.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>2.921900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>2.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>3.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>2.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>2.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>3.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>2.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>2.886700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>2.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>2.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>2.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>2.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>2.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>2.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>3.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>2.955100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>2.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>3.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>2.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>2.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>2.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>2.915600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>3.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>2.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>2.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>2.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>3.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>2.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>2.655700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>3.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>3.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>2.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>2.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>2.764200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>3.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>2.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>2.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>2.920200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>2.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>2.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>2.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>2.983200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>3.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>3.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>2.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>2.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>2.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>3.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>3.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>2.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>2.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>2.938100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>2.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>2.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>2.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>2.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>2.843100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>3.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>2.801100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>2.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>3.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>3.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>2.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>3.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>2.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>2.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>2.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>2.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>2.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>3.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>3.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>3.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>3.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>2.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>2.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>3.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>2.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>2.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>2.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>2.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>2.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>3.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>2.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>3.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>2.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>2.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>2.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>3.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>3.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>3.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>3.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>2.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>2.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>2.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>2.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>2.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>3.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>2.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>2.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>3.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>2.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>3.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.890900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>2.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>3.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>3.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>2.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>3.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>3.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>2.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>2.755800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>2.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>3.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>2.725700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>2.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>2.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>2.944400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>3.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>2.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>2.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>3.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>2.849800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>2.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>3.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>3.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.908400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>2.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>2.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>2.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>2.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.842800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>2.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>2.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>2.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>2.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>2.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>2.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>3.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>2.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>2.840600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>2.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>3.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>2.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>2.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>2.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>2.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>2.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>2.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>2.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>2.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>2.856300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>2.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>2.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>2.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>2.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>3.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>3.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>2.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>2.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>2.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>2.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>2.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>2.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>2.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>2.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>2.816400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>2.807100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>2.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>2.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>2.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>2.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>3.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>2.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>3.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>2.814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>2.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>3.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>2.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>2.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>2.981300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>2.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>2.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>2.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>3.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>2.765300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>2.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>3.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>2.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>2.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>2.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>2.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>3.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>2.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>2.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>2.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>2.920800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>2.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>2.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>2.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>2.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>3.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>2.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>2.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>2.823300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>2.907300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>2.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>2.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>2.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>2.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>3.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>2.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>2.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>2.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>2.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>2.932800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>2.779100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>2.941900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>3.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>2.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>2.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>2.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>3.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>2.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>2.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>2.938600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>2.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>2.923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>2.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>2.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>2.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>2.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.881100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>2.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>3.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>3.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>2.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>2.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>2.821400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>2.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>2.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>3.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>2.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>2.864900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>2.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>2.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>2.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>2.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>2.907400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>2.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>2.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>2.786900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>2.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>2.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>2.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>2.783500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>2.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>3.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>3.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>3.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>2.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>2.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>3.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>2.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>2.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>2.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>2.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>2.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>2.712900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>2.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>2.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>2.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>2.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>3.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>2.794600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>2.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>2.890900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>2.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>2.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>2.828900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>2.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>2.874700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>2.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>2.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>2.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>2.788300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>3.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>3.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>3.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>2.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>2.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>3.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>2.886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>2.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>2.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>2.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>2.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>2.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>2.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>2.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>2.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>2.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>2.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>3.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>2.850700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>3.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>2.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>2.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>2.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>2.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>2.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>2.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>2.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>2.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>2.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>2.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>2.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>2.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>3.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>3.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>2.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>2.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>2.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>2.769400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>2.843300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>2.747900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>2.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>2.827900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>2.971600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>2.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>3.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>2.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>2.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>2.760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>2.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>3.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>2.826500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>2.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>2.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>3.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>2.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>2.887700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>2.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>2.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>2.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>2.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>3.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>2.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>2.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>2.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>2.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>2.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>2.970700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>3.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>2.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>2.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>2.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>2.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>2.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>2.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>2.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>2.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>2.820300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>2.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>2.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>2.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>2.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>2.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>2.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>2.965400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>2.731500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>2.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>2.915800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>2.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>2.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>3.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>2.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>2.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>2.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>2.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>2.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>2.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>2.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>2.769800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>2.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>3.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>2.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>2.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>2.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>2.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>2.756400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>2.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>2.881600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>2.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>2.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>2.802100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>2.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>2.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>2.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>3.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>2.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>2.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>3.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>2.791300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>2.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>2.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>2.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>2.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>2.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>2.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>2.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>2.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>2.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.793500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>2.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>2.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>2.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>2.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>2.724900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>3.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>2.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>2.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>2.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>2.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>2.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>2.783500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>2.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>2.747900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>2.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>2.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>2.853900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>2.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>2.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>2.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>3.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>2.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>3.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>2.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>2.921300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>2.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>2.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>2.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>2.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>2.878500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>2.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>2.831800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>2.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>2.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>2.736600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>2.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>2.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>2.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>2.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>2.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>2.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>2.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>2.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>2.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>3.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>2.819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1054</td>\n",
       "      <td>2.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>2.572600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>2.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057</td>\n",
       "      <td>2.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1058</td>\n",
       "      <td>2.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1059</td>\n",
       "      <td>2.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>2.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1062</td>\n",
       "      <td>2.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1063</td>\n",
       "      <td>2.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>2.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>2.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1066</td>\n",
       "      <td>2.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1067</td>\n",
       "      <td>2.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1068</td>\n",
       "      <td>2.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1069</td>\n",
       "      <td>2.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>2.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>2.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1073</td>\n",
       "      <td>2.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1074</td>\n",
       "      <td>2.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>2.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076</td>\n",
       "      <td>2.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1077</td>\n",
       "      <td>2.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1078</td>\n",
       "      <td>2.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1079</td>\n",
       "      <td>2.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.522800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1081</td>\n",
       "      <td>2.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1082</td>\n",
       "      <td>2.526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1083</td>\n",
       "      <td>2.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1084</td>\n",
       "      <td>2.576700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>2.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>2.539800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1087</td>\n",
       "      <td>2.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>2.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1089</td>\n",
       "      <td>2.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1091</td>\n",
       "      <td>2.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>2.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>2.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>2.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>2.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1096</td>\n",
       "      <td>2.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1097</td>\n",
       "      <td>2.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1098</td>\n",
       "      <td>2.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>2.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>2.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1102</td>\n",
       "      <td>2.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1103</td>\n",
       "      <td>2.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>2.474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>2.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>2.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1107</td>\n",
       "      <td>2.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1108</td>\n",
       "      <td>2.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1109</td>\n",
       "      <td>2.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1111</td>\n",
       "      <td>2.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1112</td>\n",
       "      <td>2.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1113</td>\n",
       "      <td>2.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1114</td>\n",
       "      <td>2.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>2.572600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1116</td>\n",
       "      <td>2.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1117</td>\n",
       "      <td>2.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1118</td>\n",
       "      <td>2.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1119</td>\n",
       "      <td>2.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1121</td>\n",
       "      <td>2.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1122</td>\n",
       "      <td>2.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1123</td>\n",
       "      <td>2.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1124</td>\n",
       "      <td>2.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>2.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1126</td>\n",
       "      <td>2.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1127</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>2.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1129</td>\n",
       "      <td>2.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1131</td>\n",
       "      <td>2.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1132</td>\n",
       "      <td>2.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1133</td>\n",
       "      <td>2.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>2.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>2.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>2.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1137</td>\n",
       "      <td>2.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1138</td>\n",
       "      <td>2.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1139</td>\n",
       "      <td>2.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1141</td>\n",
       "      <td>2.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1142</td>\n",
       "      <td>2.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1143</td>\n",
       "      <td>2.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>2.558300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>2.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>2.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>2.374300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>2.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>2.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.539800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1151</td>\n",
       "      <td>2.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>2.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1153</td>\n",
       "      <td>2.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1154</td>\n",
       "      <td>2.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>2.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1156</td>\n",
       "      <td>2.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1157</td>\n",
       "      <td>2.459500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1158</td>\n",
       "      <td>2.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1159</td>\n",
       "      <td>2.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1161</td>\n",
       "      <td>2.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1162</td>\n",
       "      <td>2.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1163</td>\n",
       "      <td>2.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1164</td>\n",
       "      <td>2.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>2.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1166</td>\n",
       "      <td>2.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1167</td>\n",
       "      <td>2.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>2.597500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1169</td>\n",
       "      <td>2.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.542200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>2.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>2.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1174</td>\n",
       "      <td>2.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>2.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>2.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1177</td>\n",
       "      <td>2.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1178</td>\n",
       "      <td>2.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1179</td>\n",
       "      <td>2.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>2.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1182</td>\n",
       "      <td>2.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1183</td>\n",
       "      <td>2.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1184</td>\n",
       "      <td>2.506100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185</td>\n",
       "      <td>2.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186</td>\n",
       "      <td>2.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1187</td>\n",
       "      <td>2.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1188</td>\n",
       "      <td>2.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1189</td>\n",
       "      <td>2.557100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.534600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1191</td>\n",
       "      <td>2.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1192</td>\n",
       "      <td>2.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1193</td>\n",
       "      <td>2.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1194</td>\n",
       "      <td>2.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195</td>\n",
       "      <td>2.470400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1196</td>\n",
       "      <td>2.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1197</td>\n",
       "      <td>2.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1198</td>\n",
       "      <td>2.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>2.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.564900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>2.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1202</td>\n",
       "      <td>2.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1203</td>\n",
       "      <td>2.589400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1204</td>\n",
       "      <td>2.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>2.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>2.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1207</td>\n",
       "      <td>2.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1208</td>\n",
       "      <td>2.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1209</td>\n",
       "      <td>2.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1211</td>\n",
       "      <td>2.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1212</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1213</td>\n",
       "      <td>2.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1214</td>\n",
       "      <td>2.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>2.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1216</td>\n",
       "      <td>2.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1217</td>\n",
       "      <td>2.575100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>2.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>2.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>2.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>2.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1223</td>\n",
       "      <td>2.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>2.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1226</td>\n",
       "      <td>2.545700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1227</td>\n",
       "      <td>2.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1228</td>\n",
       "      <td>2.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1229</td>\n",
       "      <td>2.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1231</td>\n",
       "      <td>2.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1232</td>\n",
       "      <td>2.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1233</td>\n",
       "      <td>2.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1234</td>\n",
       "      <td>2.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235</td>\n",
       "      <td>2.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1236</td>\n",
       "      <td>2.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1237</td>\n",
       "      <td>2.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>2.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>2.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>2.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>2.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>2.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>2.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>2.489200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>2.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>2.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>2.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1249</td>\n",
       "      <td>2.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.617300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1251</td>\n",
       "      <td>2.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1252</td>\n",
       "      <td>2.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1253</td>\n",
       "      <td>2.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1254</td>\n",
       "      <td>2.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>2.337800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1256</td>\n",
       "      <td>2.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>2.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1258</td>\n",
       "      <td>2.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1259</td>\n",
       "      <td>2.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.458500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1261</td>\n",
       "      <td>2.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1262</td>\n",
       "      <td>2.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1263</td>\n",
       "      <td>2.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1264</td>\n",
       "      <td>2.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>2.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1266</td>\n",
       "      <td>2.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1267</td>\n",
       "      <td>2.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>2.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1269</td>\n",
       "      <td>2.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1271</td>\n",
       "      <td>2.608700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>2.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>2.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>2.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>2.397900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>2.546200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1277</td>\n",
       "      <td>2.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>2.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1279</td>\n",
       "      <td>2.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1281</td>\n",
       "      <td>2.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1282</td>\n",
       "      <td>2.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1283</td>\n",
       "      <td>2.474700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>2.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285</td>\n",
       "      <td>2.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1286</td>\n",
       "      <td>2.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287</td>\n",
       "      <td>2.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1288</td>\n",
       "      <td>2.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1289</td>\n",
       "      <td>2.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>2.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1291</td>\n",
       "      <td>2.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1292</td>\n",
       "      <td>2.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1293</td>\n",
       "      <td>2.579300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1294</td>\n",
       "      <td>2.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>2.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>2.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1297</td>\n",
       "      <td>2.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1298</td>\n",
       "      <td>2.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1299</td>\n",
       "      <td>2.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1301</td>\n",
       "      <td>2.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>2.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1303</td>\n",
       "      <td>2.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>2.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>2.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1306</td>\n",
       "      <td>2.538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1307</td>\n",
       "      <td>2.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>2.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>2.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>2.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1311</td>\n",
       "      <td>2.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>2.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1313</td>\n",
       "      <td>2.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1314</td>\n",
       "      <td>2.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>2.581100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>2.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1317</td>\n",
       "      <td>2.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1318</td>\n",
       "      <td>2.493700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1319</td>\n",
       "      <td>2.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1321</td>\n",
       "      <td>2.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1322</td>\n",
       "      <td>2.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1323</td>\n",
       "      <td>2.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1324</td>\n",
       "      <td>2.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>2.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1326</td>\n",
       "      <td>2.509500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1327</td>\n",
       "      <td>2.579600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>2.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1329</td>\n",
       "      <td>2.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>2.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1331</td>\n",
       "      <td>2.539500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1332</td>\n",
       "      <td>2.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1333</td>\n",
       "      <td>2.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1334</td>\n",
       "      <td>2.481200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>2.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1336</td>\n",
       "      <td>2.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1337</td>\n",
       "      <td>2.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1338</td>\n",
       "      <td>2.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1339</td>\n",
       "      <td>2.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1341</td>\n",
       "      <td>2.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1342</td>\n",
       "      <td>2.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1343</td>\n",
       "      <td>2.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1344</td>\n",
       "      <td>2.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345</td>\n",
       "      <td>2.498100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1346</td>\n",
       "      <td>2.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1347</td>\n",
       "      <td>2.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1348</td>\n",
       "      <td>2.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1349</td>\n",
       "      <td>2.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1351</td>\n",
       "      <td>2.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1352</td>\n",
       "      <td>2.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1353</td>\n",
       "      <td>2.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1354</td>\n",
       "      <td>2.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355</td>\n",
       "      <td>2.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1356</td>\n",
       "      <td>2.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1357</td>\n",
       "      <td>2.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1358</td>\n",
       "      <td>2.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1359</td>\n",
       "      <td>2.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1361</td>\n",
       "      <td>2.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1362</td>\n",
       "      <td>2.408300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1363</td>\n",
       "      <td>2.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1364</td>\n",
       "      <td>2.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>2.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1366</td>\n",
       "      <td>2.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1367</td>\n",
       "      <td>2.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>2.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1369</td>\n",
       "      <td>2.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>2.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1371</td>\n",
       "      <td>2.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1372</td>\n",
       "      <td>2.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1373</td>\n",
       "      <td>2.430500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1374</td>\n",
       "      <td>2.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>2.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1376</td>\n",
       "      <td>2.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1377</td>\n",
       "      <td>2.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1378</td>\n",
       "      <td>2.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1379</td>\n",
       "      <td>2.441900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1381</td>\n",
       "      <td>2.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1382</td>\n",
       "      <td>2.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1383</td>\n",
       "      <td>2.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1384</td>\n",
       "      <td>2.428500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385</td>\n",
       "      <td>2.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1386</td>\n",
       "      <td>2.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1387</td>\n",
       "      <td>2.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1388</td>\n",
       "      <td>2.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1389</td>\n",
       "      <td>2.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1391</td>\n",
       "      <td>2.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1392</td>\n",
       "      <td>2.475400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1393</td>\n",
       "      <td>2.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1394</td>\n",
       "      <td>2.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>2.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1396</td>\n",
       "      <td>2.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1397</td>\n",
       "      <td>2.565800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>2.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1399</td>\n",
       "      <td>2.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1401</td>\n",
       "      <td>2.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1402</td>\n",
       "      <td>2.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1403</td>\n",
       "      <td>2.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1404</td>\n",
       "      <td>2.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405</td>\n",
       "      <td>2.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1406</td>\n",
       "      <td>2.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1407</td>\n",
       "      <td>2.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1408</td>\n",
       "      <td>2.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1409</td>\n",
       "      <td>2.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>2.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1411</td>\n",
       "      <td>2.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1412</td>\n",
       "      <td>2.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1413</td>\n",
       "      <td>2.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1414</td>\n",
       "      <td>2.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415</td>\n",
       "      <td>2.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1416</td>\n",
       "      <td>2.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1417</td>\n",
       "      <td>2.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1418</td>\n",
       "      <td>2.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1419</td>\n",
       "      <td>2.586100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1421</td>\n",
       "      <td>2.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1422</td>\n",
       "      <td>2.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1423</td>\n",
       "      <td>2.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1424</td>\n",
       "      <td>2.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>2.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1426</td>\n",
       "      <td>2.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1427</td>\n",
       "      <td>2.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1428</td>\n",
       "      <td>2.590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1429</td>\n",
       "      <td>2.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>2.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1431</td>\n",
       "      <td>2.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1432</td>\n",
       "      <td>2.586700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1433</td>\n",
       "      <td>2.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1434</td>\n",
       "      <td>2.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>2.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1436</td>\n",
       "      <td>2.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1437</td>\n",
       "      <td>2.584200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1438</td>\n",
       "      <td>2.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1439</td>\n",
       "      <td>2.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1441</td>\n",
       "      <td>2.490800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1442</td>\n",
       "      <td>2.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1443</td>\n",
       "      <td>2.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1444</td>\n",
       "      <td>2.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445</td>\n",
       "      <td>2.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1446</td>\n",
       "      <td>2.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1447</td>\n",
       "      <td>2.413900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1448</td>\n",
       "      <td>2.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1449</td>\n",
       "      <td>2.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.331700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1451</td>\n",
       "      <td>2.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1452</td>\n",
       "      <td>2.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1453</td>\n",
       "      <td>2.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1454</td>\n",
       "      <td>2.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>2.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>2.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>2.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>2.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1459</td>\n",
       "      <td>2.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1461</td>\n",
       "      <td>2.545500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1462</td>\n",
       "      <td>2.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1463</td>\n",
       "      <td>2.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1464</td>\n",
       "      <td>2.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465</td>\n",
       "      <td>2.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1466</td>\n",
       "      <td>2.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1467</td>\n",
       "      <td>2.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1468</td>\n",
       "      <td>2.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1469</td>\n",
       "      <td>2.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1471</td>\n",
       "      <td>2.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1472</td>\n",
       "      <td>2.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1473</td>\n",
       "      <td>2.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1474</td>\n",
       "      <td>2.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>2.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1476</td>\n",
       "      <td>2.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1477</td>\n",
       "      <td>2.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1478</td>\n",
       "      <td>2.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1479</td>\n",
       "      <td>2.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.563400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1481</td>\n",
       "      <td>2.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1482</td>\n",
       "      <td>2.560900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1483</td>\n",
       "      <td>2.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1484</td>\n",
       "      <td>2.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485</td>\n",
       "      <td>2.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1486</td>\n",
       "      <td>2.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1487</td>\n",
       "      <td>2.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1488</td>\n",
       "      <td>2.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1489</td>\n",
       "      <td>2.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>2.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1491</td>\n",
       "      <td>2.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1492</td>\n",
       "      <td>2.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1493</td>\n",
       "      <td>2.638600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1494</td>\n",
       "      <td>2.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>2.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>2.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>2.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>2.544700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1499</td>\n",
       "      <td>2.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1501</td>\n",
       "      <td>2.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1502</td>\n",
       "      <td>2.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1503</td>\n",
       "      <td>2.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1504</td>\n",
       "      <td>2.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1505</td>\n",
       "      <td>2.561700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1506</td>\n",
       "      <td>2.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1507</td>\n",
       "      <td>2.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1508</td>\n",
       "      <td>2.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1509</td>\n",
       "      <td>2.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>2.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1511</td>\n",
       "      <td>2.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>2.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1513</td>\n",
       "      <td>2.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1514</td>\n",
       "      <td>2.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1515</td>\n",
       "      <td>2.578300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1516</td>\n",
       "      <td>2.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1517</td>\n",
       "      <td>2.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1518</td>\n",
       "      <td>2.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1519</td>\n",
       "      <td>2.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>2.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1522</td>\n",
       "      <td>2.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1523</td>\n",
       "      <td>2.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1524</td>\n",
       "      <td>2.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>2.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526</td>\n",
       "      <td>2.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>2.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>2.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1529</td>\n",
       "      <td>2.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1531</td>\n",
       "      <td>2.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1532</td>\n",
       "      <td>2.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1533</td>\n",
       "      <td>2.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1534</td>\n",
       "      <td>2.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1535</td>\n",
       "      <td>2.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>2.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1537</td>\n",
       "      <td>2.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1538</td>\n",
       "      <td>2.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1539</td>\n",
       "      <td>2.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1541</td>\n",
       "      <td>2.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1542</td>\n",
       "      <td>2.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1543</td>\n",
       "      <td>2.478400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1544</td>\n",
       "      <td>2.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1545</td>\n",
       "      <td>2.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1546</td>\n",
       "      <td>2.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1547</td>\n",
       "      <td>2.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1548</td>\n",
       "      <td>2.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1549</td>\n",
       "      <td>2.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1551</td>\n",
       "      <td>2.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1552</td>\n",
       "      <td>2.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1553</td>\n",
       "      <td>2.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1554</td>\n",
       "      <td>2.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1555</td>\n",
       "      <td>2.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1556</td>\n",
       "      <td>2.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1557</td>\n",
       "      <td>2.350300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1558</td>\n",
       "      <td>2.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1559</td>\n",
       "      <td>2.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.542200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1561</td>\n",
       "      <td>2.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562</td>\n",
       "      <td>2.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1563</td>\n",
       "      <td>2.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>2.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1565</td>\n",
       "      <td>2.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1566</td>\n",
       "      <td>2.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1567</td>\n",
       "      <td>2.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>2.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1569</td>\n",
       "      <td>2.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>2.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1571</td>\n",
       "      <td>2.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1572</td>\n",
       "      <td>2.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1573</td>\n",
       "      <td>2.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1574</td>\n",
       "      <td>2.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>2.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1576</td>\n",
       "      <td>2.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1577</td>\n",
       "      <td>2.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578</td>\n",
       "      <td>2.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1579</td>\n",
       "      <td>2.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1581</td>\n",
       "      <td>2.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1582</td>\n",
       "      <td>2.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1583</td>\n",
       "      <td>2.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1584</td>\n",
       "      <td>2.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585</td>\n",
       "      <td>2.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1586</td>\n",
       "      <td>2.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1587</td>\n",
       "      <td>2.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1588</td>\n",
       "      <td>2.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1589</td>\n",
       "      <td>2.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>2.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1591</td>\n",
       "      <td>2.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1592</td>\n",
       "      <td>2.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1593</td>\n",
       "      <td>2.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1594</td>\n",
       "      <td>2.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1595</td>\n",
       "      <td>2.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>2.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1597</td>\n",
       "      <td>2.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1598</td>\n",
       "      <td>2.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>2.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1601</td>\n",
       "      <td>2.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1602</td>\n",
       "      <td>2.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1603</td>\n",
       "      <td>2.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1604</td>\n",
       "      <td>2.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1605</td>\n",
       "      <td>2.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1606</td>\n",
       "      <td>2.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1607</td>\n",
       "      <td>2.577900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608</td>\n",
       "      <td>2.450800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1609</td>\n",
       "      <td>2.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>2.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611</td>\n",
       "      <td>2.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1612</td>\n",
       "      <td>2.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1613</td>\n",
       "      <td>2.565400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1614</td>\n",
       "      <td>2.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615</td>\n",
       "      <td>2.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1616</td>\n",
       "      <td>2.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1617</td>\n",
       "      <td>2.574300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1618</td>\n",
       "      <td>2.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1619</td>\n",
       "      <td>2.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1621</td>\n",
       "      <td>2.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1622</td>\n",
       "      <td>2.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1623</td>\n",
       "      <td>2.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1624</td>\n",
       "      <td>2.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>2.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1626</td>\n",
       "      <td>2.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1627</td>\n",
       "      <td>2.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1628</td>\n",
       "      <td>2.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1629</td>\n",
       "      <td>2.632300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>2.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>2.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1632</td>\n",
       "      <td>2.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1633</td>\n",
       "      <td>2.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1634</td>\n",
       "      <td>2.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1635</td>\n",
       "      <td>2.527700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1636</td>\n",
       "      <td>2.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1637</td>\n",
       "      <td>2.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1638</td>\n",
       "      <td>2.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1639</td>\n",
       "      <td>2.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1641</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1642</td>\n",
       "      <td>2.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1643</td>\n",
       "      <td>2.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1644</td>\n",
       "      <td>2.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645</td>\n",
       "      <td>2.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1646</td>\n",
       "      <td>2.514900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1647</td>\n",
       "      <td>2.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1648</td>\n",
       "      <td>2.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1649</td>\n",
       "      <td>2.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1651</td>\n",
       "      <td>2.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1652</td>\n",
       "      <td>2.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1653</td>\n",
       "      <td>2.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1654</td>\n",
       "      <td>2.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1655</td>\n",
       "      <td>2.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>2.601800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1657</td>\n",
       "      <td>2.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1658</td>\n",
       "      <td>2.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1659</td>\n",
       "      <td>2.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1661</td>\n",
       "      <td>2.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1662</td>\n",
       "      <td>2.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1663</td>\n",
       "      <td>2.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>2.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665</td>\n",
       "      <td>2.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>2.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1667</td>\n",
       "      <td>2.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1668</td>\n",
       "      <td>2.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1669</td>\n",
       "      <td>2.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>2.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1671</td>\n",
       "      <td>2.443500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1672</td>\n",
       "      <td>2.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1673</td>\n",
       "      <td>2.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1674</td>\n",
       "      <td>2.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>2.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1676</td>\n",
       "      <td>2.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1677</td>\n",
       "      <td>2.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1678</td>\n",
       "      <td>2.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1679</td>\n",
       "      <td>2.481600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1681</td>\n",
       "      <td>2.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1682</td>\n",
       "      <td>2.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1683</td>\n",
       "      <td>2.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1684</td>\n",
       "      <td>2.587200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1685</td>\n",
       "      <td>2.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1686</td>\n",
       "      <td>2.555900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1687</td>\n",
       "      <td>2.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>2.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1689</td>\n",
       "      <td>2.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>2.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1691</td>\n",
       "      <td>2.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692</td>\n",
       "      <td>2.447200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1693</td>\n",
       "      <td>2.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1694</td>\n",
       "      <td>2.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1695</td>\n",
       "      <td>2.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1696</td>\n",
       "      <td>2.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1697</td>\n",
       "      <td>2.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1698</td>\n",
       "      <td>2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1699</td>\n",
       "      <td>2.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1701</td>\n",
       "      <td>2.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1702</td>\n",
       "      <td>2.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1703</td>\n",
       "      <td>2.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1704</td>\n",
       "      <td>2.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1705</td>\n",
       "      <td>2.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1706</td>\n",
       "      <td>2.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1707</td>\n",
       "      <td>2.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1708</td>\n",
       "      <td>2.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1709</td>\n",
       "      <td>2.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>2.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1711</td>\n",
       "      <td>2.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1712</td>\n",
       "      <td>2.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1713</td>\n",
       "      <td>2.427500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1714</td>\n",
       "      <td>2.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1715</td>\n",
       "      <td>2.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1716</td>\n",
       "      <td>2.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1717</td>\n",
       "      <td>2.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1718</td>\n",
       "      <td>2.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1719</td>\n",
       "      <td>2.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>2.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>2.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>2.285400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>2.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>2.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1726</td>\n",
       "      <td>2.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1727</td>\n",
       "      <td>2.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1728</td>\n",
       "      <td>2.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1729</td>\n",
       "      <td>2.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>2.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1731</td>\n",
       "      <td>2.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1732</td>\n",
       "      <td>2.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1733</td>\n",
       "      <td>2.545100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1734</td>\n",
       "      <td>2.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1735</td>\n",
       "      <td>2.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1736</td>\n",
       "      <td>2.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1737</td>\n",
       "      <td>2.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1738</td>\n",
       "      <td>2.337800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1739</td>\n",
       "      <td>2.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1741</td>\n",
       "      <td>2.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1742</td>\n",
       "      <td>2.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1743</td>\n",
       "      <td>2.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744</td>\n",
       "      <td>2.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1745</td>\n",
       "      <td>2.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1746</td>\n",
       "      <td>2.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1747</td>\n",
       "      <td>2.357200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1748</td>\n",
       "      <td>2.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1749</td>\n",
       "      <td>2.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1751</td>\n",
       "      <td>2.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>2.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1753</td>\n",
       "      <td>2.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1754</td>\n",
       "      <td>2.417900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755</td>\n",
       "      <td>2.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1756</td>\n",
       "      <td>2.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1757</td>\n",
       "      <td>2.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1758</td>\n",
       "      <td>2.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1759</td>\n",
       "      <td>2.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1761</td>\n",
       "      <td>2.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1762</td>\n",
       "      <td>2.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1763</td>\n",
       "      <td>2.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1764</td>\n",
       "      <td>2.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1765</td>\n",
       "      <td>2.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1766</td>\n",
       "      <td>2.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1767</td>\n",
       "      <td>2.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1768</td>\n",
       "      <td>2.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1769</td>\n",
       "      <td>2.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>2.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1771</td>\n",
       "      <td>2.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1772</td>\n",
       "      <td>2.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1773</td>\n",
       "      <td>2.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1774</td>\n",
       "      <td>2.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>2.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>2.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1777</td>\n",
       "      <td>2.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1778</td>\n",
       "      <td>2.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1779</td>\n",
       "      <td>2.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1781</td>\n",
       "      <td>2.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1782</td>\n",
       "      <td>2.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1783</td>\n",
       "      <td>2.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1784</td>\n",
       "      <td>2.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785</td>\n",
       "      <td>2.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1786</td>\n",
       "      <td>2.560500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1787</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1788</td>\n",
       "      <td>2.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1789</td>\n",
       "      <td>2.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>2.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1791</td>\n",
       "      <td>2.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>2.356500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1793</td>\n",
       "      <td>2.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1794</td>\n",
       "      <td>2.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1795</td>\n",
       "      <td>2.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1796</td>\n",
       "      <td>2.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1797</td>\n",
       "      <td>2.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1798</td>\n",
       "      <td>2.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1799</td>\n",
       "      <td>2.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1801</td>\n",
       "      <td>2.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1802</td>\n",
       "      <td>2.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1803</td>\n",
       "      <td>2.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804</td>\n",
       "      <td>2.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1805</td>\n",
       "      <td>2.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1806</td>\n",
       "      <td>2.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1807</td>\n",
       "      <td>2.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>2.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1809</td>\n",
       "      <td>2.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>2.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1811</td>\n",
       "      <td>2.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1812</td>\n",
       "      <td>2.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1813</td>\n",
       "      <td>2.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1814</td>\n",
       "      <td>2.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1815</td>\n",
       "      <td>2.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>2.597300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1817</td>\n",
       "      <td>2.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1818</td>\n",
       "      <td>2.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1819</td>\n",
       "      <td>2.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1821</td>\n",
       "      <td>2.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1822</td>\n",
       "      <td>2.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1823</td>\n",
       "      <td>2.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>2.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>2.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1826</td>\n",
       "      <td>2.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1827</td>\n",
       "      <td>2.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1828</td>\n",
       "      <td>2.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1829</td>\n",
       "      <td>2.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>2.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1831</td>\n",
       "      <td>2.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1832</td>\n",
       "      <td>2.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1833</td>\n",
       "      <td>2.603500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1834</td>\n",
       "      <td>2.424600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835</td>\n",
       "      <td>2.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1836</td>\n",
       "      <td>2.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1837</td>\n",
       "      <td>2.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1838</td>\n",
       "      <td>2.481200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1839</td>\n",
       "      <td>2.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1841</td>\n",
       "      <td>2.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1842</td>\n",
       "      <td>2.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1843</td>\n",
       "      <td>2.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1844</td>\n",
       "      <td>2.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1845</td>\n",
       "      <td>2.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1846</td>\n",
       "      <td>2.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1847</td>\n",
       "      <td>2.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1848</td>\n",
       "      <td>2.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1849</td>\n",
       "      <td>2.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1851</td>\n",
       "      <td>2.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1852</td>\n",
       "      <td>2.487600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1853</td>\n",
       "      <td>2.518200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1854</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1855</td>\n",
       "      <td>2.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1856</td>\n",
       "      <td>2.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1857</td>\n",
       "      <td>2.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1858</td>\n",
       "      <td>2.417800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1859</td>\n",
       "      <td>2.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.484300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>2.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1862</td>\n",
       "      <td>2.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1863</td>\n",
       "      <td>2.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>2.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1865</td>\n",
       "      <td>2.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1866</td>\n",
       "      <td>2.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1867</td>\n",
       "      <td>2.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1868</td>\n",
       "      <td>2.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1869</td>\n",
       "      <td>2.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>2.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871</td>\n",
       "      <td>2.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>2.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1873</td>\n",
       "      <td>2.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1874</td>\n",
       "      <td>2.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>2.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1876</td>\n",
       "      <td>2.467100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1877</td>\n",
       "      <td>2.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1878</td>\n",
       "      <td>2.504600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1879</td>\n",
       "      <td>2.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>2.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1881</td>\n",
       "      <td>2.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1882</td>\n",
       "      <td>2.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1883</td>\n",
       "      <td>2.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1884</td>\n",
       "      <td>2.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1885</td>\n",
       "      <td>2.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1886</td>\n",
       "      <td>2.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1887</td>\n",
       "      <td>2.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1888</td>\n",
       "      <td>2.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1889</td>\n",
       "      <td>2.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>2.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1891</td>\n",
       "      <td>2.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1892</td>\n",
       "      <td>2.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1893</td>\n",
       "      <td>2.560500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1894</td>\n",
       "      <td>2.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1895</td>\n",
       "      <td>2.655700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1896</td>\n",
       "      <td>2.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1897</td>\n",
       "      <td>2.443800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1898</td>\n",
       "      <td>2.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899</td>\n",
       "      <td>2.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.459500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1901</td>\n",
       "      <td>2.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902</td>\n",
       "      <td>2.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1903</td>\n",
       "      <td>2.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1904</td>\n",
       "      <td>2.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1905</td>\n",
       "      <td>2.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1906</td>\n",
       "      <td>2.546200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1907</td>\n",
       "      <td>2.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1908</td>\n",
       "      <td>2.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1909</td>\n",
       "      <td>2.344600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>2.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1911</td>\n",
       "      <td>2.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1912</td>\n",
       "      <td>2.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1913</td>\n",
       "      <td>2.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1914</td>\n",
       "      <td>2.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1915</td>\n",
       "      <td>2.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1916</td>\n",
       "      <td>2.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1917</td>\n",
       "      <td>2.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1918</td>\n",
       "      <td>2.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1919</td>\n",
       "      <td>2.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1921</td>\n",
       "      <td>2.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1922</td>\n",
       "      <td>2.431700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1923</td>\n",
       "      <td>2.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1924</td>\n",
       "      <td>2.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>2.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1926</td>\n",
       "      <td>2.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1927</td>\n",
       "      <td>2.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1928</td>\n",
       "      <td>2.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1929</td>\n",
       "      <td>2.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>2.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1931</td>\n",
       "      <td>2.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1932</td>\n",
       "      <td>2.630800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1933</td>\n",
       "      <td>2.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1934</td>\n",
       "      <td>2.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1935</td>\n",
       "      <td>2.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1936</td>\n",
       "      <td>2.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1937</td>\n",
       "      <td>2.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1938</td>\n",
       "      <td>2.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1939</td>\n",
       "      <td>2.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>2.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1941</td>\n",
       "      <td>2.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1942</td>\n",
       "      <td>2.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943</td>\n",
       "      <td>2.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1944</td>\n",
       "      <td>2.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1945</td>\n",
       "      <td>2.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1946</td>\n",
       "      <td>2.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1947</td>\n",
       "      <td>2.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1948</td>\n",
       "      <td>2.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1949</td>\n",
       "      <td>2.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951</td>\n",
       "      <td>2.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1952</td>\n",
       "      <td>2.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1953</td>\n",
       "      <td>2.586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1954</td>\n",
       "      <td>2.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>2.389400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1956</td>\n",
       "      <td>2.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1957</td>\n",
       "      <td>2.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1958</td>\n",
       "      <td>2.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1959</td>\n",
       "      <td>2.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1961</td>\n",
       "      <td>2.312600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1962</td>\n",
       "      <td>2.564900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1963</td>\n",
       "      <td>2.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1964</td>\n",
       "      <td>2.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965</td>\n",
       "      <td>2.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1966</td>\n",
       "      <td>2.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1967</td>\n",
       "      <td>2.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>2.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1969</td>\n",
       "      <td>2.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>2.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1971</td>\n",
       "      <td>2.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1972</td>\n",
       "      <td>2.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1973</td>\n",
       "      <td>2.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1974</td>\n",
       "      <td>2.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>2.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1976</td>\n",
       "      <td>2.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1977</td>\n",
       "      <td>2.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1978</td>\n",
       "      <td>2.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1979</td>\n",
       "      <td>2.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>2.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1981</td>\n",
       "      <td>2.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1982</td>\n",
       "      <td>2.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1983</td>\n",
       "      <td>2.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1984</td>\n",
       "      <td>2.398300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1985</td>\n",
       "      <td>2.545900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986</td>\n",
       "      <td>2.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1987</td>\n",
       "      <td>2.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1988</td>\n",
       "      <td>2.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1989</td>\n",
       "      <td>2.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>2.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1991</td>\n",
       "      <td>2.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992</td>\n",
       "      <td>2.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1993</td>\n",
       "      <td>2.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1994</td>\n",
       "      <td>2.453600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>2.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>2.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>2.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>2.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>2.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2001</td>\n",
       "      <td>2.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2002</td>\n",
       "      <td>2.523600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2003</td>\n",
       "      <td>2.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004</td>\n",
       "      <td>2.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2005</td>\n",
       "      <td>2.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006</td>\n",
       "      <td>2.547400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2007</td>\n",
       "      <td>2.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008</td>\n",
       "      <td>2.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2009</td>\n",
       "      <td>2.475700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>2.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2011</td>\n",
       "      <td>2.529800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012</td>\n",
       "      <td>2.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013</td>\n",
       "      <td>2.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2014</td>\n",
       "      <td>2.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015</td>\n",
       "      <td>2.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016</td>\n",
       "      <td>2.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017</td>\n",
       "      <td>2.519700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018</td>\n",
       "      <td>2.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019</td>\n",
       "      <td>2.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>2.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021</td>\n",
       "      <td>2.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>2.565400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>2.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>2.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>2.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>2.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2027</td>\n",
       "      <td>2.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2028</td>\n",
       "      <td>2.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2029</td>\n",
       "      <td>2.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>2.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2031</td>\n",
       "      <td>2.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2032</td>\n",
       "      <td>2.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2033</td>\n",
       "      <td>2.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2034</td>\n",
       "      <td>2.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2035</td>\n",
       "      <td>2.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2036</td>\n",
       "      <td>2.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2037</td>\n",
       "      <td>2.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2038</td>\n",
       "      <td>2.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2039</td>\n",
       "      <td>2.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>2.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2041</td>\n",
       "      <td>2.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2042</td>\n",
       "      <td>2.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2043</td>\n",
       "      <td>2.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2044</td>\n",
       "      <td>2.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2045</td>\n",
       "      <td>2.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2046</td>\n",
       "      <td>2.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047</td>\n",
       "      <td>2.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>2.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2049</td>\n",
       "      <td>2.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2051</td>\n",
       "      <td>2.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2052</td>\n",
       "      <td>2.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2053</td>\n",
       "      <td>2.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2054</td>\n",
       "      <td>2.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2055</td>\n",
       "      <td>2.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2056</td>\n",
       "      <td>2.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2057</td>\n",
       "      <td>2.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2058</td>\n",
       "      <td>2.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2059</td>\n",
       "      <td>2.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>2.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2061</td>\n",
       "      <td>2.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2062</td>\n",
       "      <td>2.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2063</td>\n",
       "      <td>2.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2064</td>\n",
       "      <td>2.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2065</td>\n",
       "      <td>2.425800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2066</td>\n",
       "      <td>2.569300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2067</td>\n",
       "      <td>2.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2068</td>\n",
       "      <td>2.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2069</td>\n",
       "      <td>2.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>2.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2071</td>\n",
       "      <td>2.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2072</td>\n",
       "      <td>2.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2073</td>\n",
       "      <td>2.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2074</td>\n",
       "      <td>2.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>2.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2076</td>\n",
       "      <td>2.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2077</td>\n",
       "      <td>2.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2078</td>\n",
       "      <td>2.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2079</td>\n",
       "      <td>2.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>2.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2081</td>\n",
       "      <td>2.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2082</td>\n",
       "      <td>2.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2083</td>\n",
       "      <td>2.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2084</td>\n",
       "      <td>2.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2085</td>\n",
       "      <td>2.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2086</td>\n",
       "      <td>2.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2087</td>\n",
       "      <td>2.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2088</td>\n",
       "      <td>2.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2089</td>\n",
       "      <td>2.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>2.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2091</td>\n",
       "      <td>2.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2092</td>\n",
       "      <td>2.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2093</td>\n",
       "      <td>2.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2094</td>\n",
       "      <td>2.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2095</td>\n",
       "      <td>2.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2096</td>\n",
       "      <td>2.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>2.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2098</td>\n",
       "      <td>2.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2099</td>\n",
       "      <td>2.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2101</td>\n",
       "      <td>2.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2102</td>\n",
       "      <td>2.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2103</td>\n",
       "      <td>2.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2104</td>\n",
       "      <td>2.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2105</td>\n",
       "      <td>2.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2106</td>\n",
       "      <td>2.507200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Memory Usage: 41.8% used. 149820.30MB available.\n",
      "GPU 0 Memory Usage: 8066.00MB reserved. 8039.13MB max allocated. 1954.84MB currently allocated.\n",
      "GPU 1 Memory Usage: 22588.00MB reserved. 21963.49MB max allocated. 3834.70MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aef293552de4cf297966c3029d555cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▂▁▄▂▁▅▃▂▄▂▁▃▂▂▁▂▂▂▁▃▄▃▃▅▃▃▄▃▄▃▃▄▄▄▄▃▄▃</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▇▆▅▆▇▇█▇▆▅▆▇▇▆▄▇▃▁▂▂▂▃▁▃▂▂▃▂▄▃▁▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.5623624552286781e+18</td></tr><tr><td>train/epoch</td><td>1.99929</td></tr><tr><td>train/global_step</td><td>2106</td></tr><tr><td>train/grad_norm</td><td>1.40721</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5072</td></tr><tr><td>train_loss</td><td>2.72234</td></tr><tr><td>train_runtime</td><td>26979.3527</td></tr><tr><td>train_samples_per_second</td><td>9.995</td></tr><tr><td>train_steps_per_second</td><td>0.078</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-silence-135</strong> at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/os8ie6bk' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/os8ie6bk</a><br/> View project at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_034544-os8ie6bk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom validation loop\n",
      "device = cuda 0\n",
      "Move linear layer to device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cache cleared before validation.\n",
      "Memory Usage: 41.8% used. 149804.32MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Now generate predictions\n",
      "-----------------------------------\n",
      "Validation step 0\n",
      "Generation timed out\n",
      "-----------------------------------\n",
      "Validation step 1\n",
      "Generation timed out\n",
      "-----------------------------------\n",
      "Validation step 2\n",
      "Generation timed out\n",
      "-----------------------------------\n",
      "Validation step 3\n",
      "Generation timed out\n",
      "-----------------------------------\n",
      "Validation step 4\n",
      "Generation timed out\n",
      "-----------------------------------\n",
      "Validation step 5\n",
      "Generation timed out\n",
      "-----------------------------------\n",
      "Validation step 6\n",
      "Generation timed out\n",
      "-----------------------------------\n",
      "Validation step 7\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import psutil\n",
    "import signal\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Training data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Validation data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Register the signal function handler\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, linear_layer, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    total_bleu_score = 0\n",
    "    total_rouge_score = {'rouge-1': {'f': 0, 'p': 0, 'r': 0},\n",
    "                         'rouge-2': {'f': 0, 'p': 0, 'r': 0},\n",
    "                         'rouge-l': {'f': 0, 'p': 0, 'r': 0}}\n",
    "    num_predictions = 0    \n",
    "    rouge = Rouge()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        print(\"-----------------------------------\")\n",
    "        print(f\"Validation step {step}\")\n",
    "        #print_memory_usage()\n",
    "        embeddings = batch[\"embedding\"].to(device)  # Ensure embeddings are in float16\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        \n",
    "        #print(f\"Embeddings shape before linear layer: {embeddings.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = linear_layer(embeddings).to(device)  # Apply the linear layer to project embeddings\n",
    "            #print(f\"Embeddings shape after linear layer: {embeddings.shape}\")\n",
    "            input_ids = torch.full((embeddings.size(0), 1), tokenizer.pad_token_id, dtype=torch.long).to(device)\n",
    "            \n",
    "            #print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "            \n",
    "            # Ensure embeddings has batch size dimension\n",
    "            if len(embeddings.shape) == 2:\n",
    "                embeddings = embeddings.unsqueeze(1)\n",
    "                #print(f\"Reshaped Embeddings shape: {embeddings.shape}\")\n",
    "            \n",
    "            #print(\"Checking memory before generation\")\n",
    "            #print_memory_usage()\n",
    "            \n",
    "            try:\n",
    "                # Set the alarm for 30 seconds\n",
    "                signal.alarm(30)\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    inputs_embeds=embeddings,\n",
    "                    max_length=256,  # Limit length to prevent excessively long texts\n",
    "                    num_beams=2,\n",
    "                    do_sample=True,\n",
    "                    top_k=5,\n",
    "                    top_p=0.9,\n",
    "                    temperature=0.8,\n",
    "                    repetition_penalty=3.0,  # Increase repetition penalty to avoid word repetitions\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                #print(f\"Outputs shape: {outputs.shape}\")\n",
    "                decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                \n",
    "                # Disable the alarm\n",
    "                signal.alarm(0)\n",
    "            except TimeoutException:\n",
    "                print(\"Generation timed out\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "                continue\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            reference = [tweet_text.split()]\n",
    "            candidate = pred_text.split()\n",
    "            bleu_score = sentence_bleu(reference, candidate)\n",
    "            total_bleu_score += bleu_score\n",
    "            \n",
    "            # Calculate ROUGE score\n",
    "            rouge_scores = rouge.get_scores(pred_text, tweet_text, avg=True)\n",
    "            for key in total_rouge_score.keys():\n",
    "                total_rouge_score[key]['f'] += rouge_scores[key]['f']\n",
    "                total_rouge_score[key]['p'] += rouge_scores[key]['p']\n",
    "                total_rouge_score[key]['r'] += rouge_scores[key]['r']\n",
    "\n",
    "            num_predictions += 1\n",
    "\n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "                print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "    # Print average BLEU and ROUGE\n",
    "    avg_bleu_score = total_bleu_score / num_predictions if num_predictions > 0 else 0\n",
    "    avg_rouge_score = {key: {metric: score / num_predictions for metric, score in scores.items()} for key, scores in total_rouge_score.items()} if num_predictions > 0 else total_rouge_score\n",
    "\n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    print(f\"Average ROUGE Score: {avg_rouge_score}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def print_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {mem.percent}% used. {mem.available / 1024 ** 2:.2f}MB available.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_mem = torch.cuda.memory_reserved(i) / 1024 ** 2\n",
    "            gpu_max_mem = torch.cuda.max_memory_allocated(i) / 1024 ** 2\n",
    "            gpu_mem_alloc = torch.cuda.memory_allocated(i) / 1024 ** 2\n",
    "            print(f\"GPU {i} Memory Usage: {gpu_mem:.2f}MB reserved. {gpu_max_mem:.2f}MB max allocated. {gpu_mem_alloc:.2f}MB currently allocated.\")\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    print_memory_usage()\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Mantener el mapeo automático de dispositivos\n",
    "    )\n",
    "    print(\"Model loaded.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=48, # 32\n",
    "        lora_dropout=0.08, # 0.05\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = ValidationDataset(dict_val, embeddings, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False) \n",
    "\n",
    "    # Add linear layer for projecting embeddings\n",
    "    linear_layer = nn.Linear(768, 4096).to('cuda').to(torch.float16)\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    wandb.login(key=api_key)\n",
    "    run = wandb.init(\n",
    "        project='Fine-tune Llama 3 8B on Image Embeddings', \n",
    "        job_type=\"training\", \n",
    "        anonymous=\"allow\"\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=2,\n",
    "        evaluation_strategy=\"no\",  # Disable automatic evaluation during training\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"    \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,  # Disable automatic evaluation\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=256,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "    print_memory_usage()\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Llamar a la función para guardar el modelo y el tokenizador\n",
    "    save_model_and_tokenizer(trainer, tokenizer)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Custom validation loop  \n",
    "    print(\"Custom validation loop\") \n",
    "    print(\"device = cuda 0\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "    print(\"Move linear layer to device\")\n",
    "    linear_layer.to(device)  # Move the linear layer to the same device\n",
    "    \n",
    "    # Liberar memoria antes de la validación\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Memory cache cleared before validation.\")\n",
    "    print_memory_usage() \n",
    "     \n",
    "    print(\"Now generate predictions\")\n",
    "    \n",
    "    try:\n",
    "        predictions = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, linear_layer, print_every_n_steps=1)\n",
    "        print(predictions)\n",
    "    except:\n",
    "        print(\"Error al generar predicciones\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(El output se ha truncado, pero llegó como al 300)\n",
    "\n",
    "\n",
    "Con estos ajustes tiende a generar textos infinitos (por eso los timeout) y, cuando no, genera textos sin sentido (símbolos y caracteres random). Experimento descartado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔴Nuevo intento con los parámetros originales pero con ROUGE y más val batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_tokenizer(trainer, tokenizer):\n",
    "    \"\"\"Guardar el modelo y el tokenizador en una carpeta con la fecha y hora actual.\"\"\"\n",
    "    # Crear la carpeta con la fecha y hora actual\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = os.path.join(\"saved-finetuned-llamas\", current_time)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Guardar el modelo y el tokenizador\n",
    "    trainer.model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "\n",
    "# Hacer visibles solo las GPUs 1 y 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ahora PyTorch solo verá las GPUs 1 y 2\n",
    "print(torch.cuda.device_count())  # Debería imprimir 2\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de la primera GPU visible (anteriormente GPU 1)\n",
    "print(torch.cuda.get_device_name(1))  # Nombre de la segunda GPU visible (anteriormente GPU 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 17:34:42.052622: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-05 17:34:42.122260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 17:34:43.380306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-05 17:34:44,517] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Memory Usage: 40.6% used. 153015.15MB available.\n",
      "GPU 0 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n",
      "GPU 1 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafb602e15014ee88008de39328b58a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Memory Usage: 40.9% used. 152329.49MB available.\n",
      "GPU 0 Memory Usage: 1862.00MB reserved. 1955.44MB max allocated. 1860.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3668.00MB reserved. 3694.41MB max allocated. 3578.44MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 134823\n",
      "Validation data size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240705_173458-k2m8zq1m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k2m8zq1m' target=\"_blank\">brisk-planet-138</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k2m8zq1m' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k2m8zq1m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Memory Usage: 40.9% used. 152195.85MB available.\n",
      "GPU 0 Memory Usage: 1902.00MB reserved. 1955.44MB max allocated. 1900.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3788.00MB reserved. 3716.46MB max allocated. 3704.45MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2106' max='2106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2106/2106 7:37:15, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.528600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.554900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.372200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.260300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.970700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.902900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>3.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>2.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>2.989700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>3.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>3.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>3.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>3.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>3.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>3.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>3.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>3.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>3.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>3.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>3.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.936300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>3.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>3.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2.945300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.903300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>2.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>3.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>3.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>3.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>3.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>3.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>3.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>3.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>3.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>2.993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>3.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>3.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>2.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>3.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>3.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>2.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>3.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>2.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>2.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>2.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>2.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>3.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>3.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>2.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>2.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.837200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>2.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>2.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>2.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>2.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>2.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>3.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>3.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.971600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>2.890400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>3.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>2.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>3.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>2.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>3.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>2.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>2.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>3.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>2.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>2.957600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>3.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>3.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>2.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>3.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>2.732300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.970300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>2.864800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>2.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>2.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>2.844300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>2.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>3.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>2.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>2.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>2.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>2.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>3.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>2.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>3.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>3.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>2.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>3.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>3.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>3.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>2.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>3.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>2.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>2.855700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>3.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>2.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>2.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>3.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>2.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>2.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>2.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>2.882300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>3.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>2.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>2.921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>2.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>2.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>3.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>2.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>2.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>2.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>2.908200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>3.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>3.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>3.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>2.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>2.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>2.821000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>2.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>3.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>2.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>2.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>2.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>3.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>2.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>2.895900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>3.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>3.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>2.945500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>2.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>2.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>3.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>2.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>3.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>2.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>2.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>2.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>2.964200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>2.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>2.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.947200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>3.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>2.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>3.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>2.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>2.780800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>2.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>3.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>2.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>3.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>2.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>3.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>2.906700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>2.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>2.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>3.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>3.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>2.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>2.908400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>3.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>2.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>2.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>3.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>2.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>2.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>2.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>2.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>3.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>2.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>2.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>3.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>3.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>2.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>2.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>2.964200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>2.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>3.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>3.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>2.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>2.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>3.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>3.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>3.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>3.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>2.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>2.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>2.922400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>2.899100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>2.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>2.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>3.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>2.834100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>3.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>3.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>2.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>2.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>2.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>2.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>2.946800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>2.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>2.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>2.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>2.824400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>2.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>2.844600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>2.904300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>2.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>2.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>2.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>3.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>2.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>3.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>2.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>2.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>2.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>2.839000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>3.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>3.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>2.828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>3.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>3.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>3.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>2.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>2.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>2.799800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>2.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>2.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>3.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>2.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>2.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>2.771300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>2.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>2.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>2.965400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>2.933700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>2.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>3.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>2.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>2.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>2.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>2.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>2.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>2.929500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>2.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>2.952300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>2.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>3.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>2.890900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>2.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>2.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>2.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>3.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>2.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>2.904100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>2.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>3.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>2.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>2.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>2.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>2.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>2.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>3.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>2.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>2.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>2.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>2.922400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>2.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>2.796100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>2.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>3.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>2.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>2.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>3.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>2.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>2.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>2.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>2.890900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>2.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>2.791400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>2.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>3.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>3.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>2.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>2.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>2.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>2.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>2.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>2.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>2.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>2.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>3.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>2.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>2.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>2.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>2.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>2.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>2.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>3.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>2.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>2.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>2.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>2.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>2.827900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>2.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>2.903400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>2.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>2.822500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>2.917800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>2.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>2.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>2.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>3.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>2.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>2.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>2.996100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>2.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>2.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>2.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>2.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>2.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>3.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>2.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>2.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>3.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>2.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>2.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>2.906700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>3.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>2.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>2.848500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>2.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>2.899600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>2.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>2.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>2.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>2.770800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>3.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>2.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>2.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>3.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>2.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>2.820200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>2.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>2.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>3.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>2.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>2.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>2.865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>3.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>2.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>2.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>3.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>2.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>2.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>2.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>2.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>2.938700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>2.957600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>2.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>2.790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>2.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>2.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>2.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>3.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>3.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>2.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>2.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>2.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>3.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>3.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>2.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>2.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>2.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>2.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>2.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>2.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>2.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>2.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>3.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>2.771800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>2.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>2.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>3.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>2.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>2.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>2.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>2.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>2.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>2.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>2.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>2.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>2.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>3.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>2.990200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>2.834700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>2.929500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>2.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>2.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>2.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>2.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>2.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>3.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>2.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>2.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>2.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>2.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>2.772900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>2.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>2.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>3.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>2.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>2.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>2.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>2.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>2.914700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>2.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>3.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>2.818900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>2.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>3.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>2.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>2.986600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>2.899600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>3.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>2.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>2.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>2.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>2.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>2.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>2.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>2.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>2.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>2.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>2.962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>2.907700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>2.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>3.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>2.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>2.897600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>3.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>2.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>2.799600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>3.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>3.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>2.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>2.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>2.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>2.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>2.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>2.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>2.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>2.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>2.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>2.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>3.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>2.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>2.806800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>2.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>3.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>2.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>2.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>2.827800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>2.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>2.722100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>2.795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>2.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>2.732000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>2.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>2.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>2.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>2.872100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>2.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>3.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>3.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>2.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>2.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>2.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>2.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>2.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>2.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>2.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>2.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>2.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>2.771700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>2.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>2.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>2.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>2.862800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>3.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>2.748100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>2.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>2.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>2.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>3.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>2.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>2.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>2.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>2.917900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>2.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>2.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>2.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>2.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>2.711300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>2.990900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>2.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>2.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>2.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>2.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>2.987800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>2.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>2.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>2.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>2.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>2.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>2.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>2.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>2.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>3.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>2.770800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>2.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>2.790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>2.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>2.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>2.824900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>2.890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>2.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>3.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>2.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>2.890400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>2.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>2.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>2.905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>2.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>2.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>2.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>2.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>2.774300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>2.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>2.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>2.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>2.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>2.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>2.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>2.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>2.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>2.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>2.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>2.904700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>2.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>3.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>3.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>2.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>2.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>2.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>2.824500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>2.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>3.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>2.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>2.835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>2.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>2.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>2.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>2.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>2.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>2.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>2.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>2.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>2.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>2.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>2.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>2.769100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>2.857700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>2.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>3.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>2.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>2.845600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>2.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>2.978300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>2.776200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>2.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>2.831300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>2.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>2.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.922200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>2.718600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>2.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>2.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>2.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>2.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>2.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>3.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>2.769800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>2.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.864900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>2.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>2.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>2.862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>2.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>2.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>2.847600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>2.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>2.719300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>2.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>2.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>2.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>2.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>2.989700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>2.769800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>2.736600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>2.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>2.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>2.638100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>2.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>2.848800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>2.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>2.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>2.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>2.900300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>2.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>2.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>2.796800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>3.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>2.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>3.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>2.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>2.767900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>2.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>2.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>2.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>2.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>2.809200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>2.907600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>2.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>2.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>2.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>2.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>2.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>3.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>2.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>2.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.901700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>2.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>2.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>2.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>2.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>2.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>2.851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>2.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>2.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>2.931700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>2.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>2.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>2.915900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>2.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>2.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>3.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>2.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>2.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>2.880100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>2.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>2.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>2.724900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>2.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>2.883800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>2.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>3.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>2.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>2.746700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>2.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>2.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>2.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>2.944300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>3.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>2.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>2.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>2.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>2.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>2.827500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>2.723900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>2.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>2.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>2.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>2.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>2.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>2.668100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>2.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>2.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>2.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>2.920800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>2.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>2.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>2.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>2.836100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>2.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>3.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>2.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>2.789800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>2.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>2.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>2.910600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>2.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>2.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>2.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>2.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>3.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>2.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>2.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>2.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>2.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>2.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>2.862700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>2.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>2.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>2.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>2.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>2.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>2.906700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>2.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>2.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>2.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>2.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>2.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>2.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>2.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>3.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>2.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>2.775600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>2.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>2.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>2.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>2.887700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>2.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>2.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>2.897700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>2.888300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.772900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>2.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>2.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>2.750300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>2.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>2.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>3.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>2.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>2.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>2.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>2.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>2.882800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>2.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>2.773400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>2.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>2.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>2.818300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>2.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>2.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>2.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>2.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>2.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>3.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>2.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>3.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>2.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>2.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>2.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>2.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>2.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>2.818500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>2.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>2.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>2.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>2.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>2.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.845100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>2.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>2.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>2.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>2.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>2.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>2.761300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>2.800700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>2.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>2.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>2.753500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>2.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>2.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1054</td>\n",
       "      <td>2.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>2.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>2.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057</td>\n",
       "      <td>2.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1058</td>\n",
       "      <td>2.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1059</td>\n",
       "      <td>2.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>2.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1062</td>\n",
       "      <td>2.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1063</td>\n",
       "      <td>2.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>2.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>2.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1066</td>\n",
       "      <td>2.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1067</td>\n",
       "      <td>2.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1068</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1069</td>\n",
       "      <td>2.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>2.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>2.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1073</td>\n",
       "      <td>2.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1074</td>\n",
       "      <td>2.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>2.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076</td>\n",
       "      <td>2.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1077</td>\n",
       "      <td>2.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1078</td>\n",
       "      <td>2.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1079</td>\n",
       "      <td>2.339700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1081</td>\n",
       "      <td>2.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1082</td>\n",
       "      <td>2.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1083</td>\n",
       "      <td>2.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1084</td>\n",
       "      <td>2.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>2.626900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>2.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1087</td>\n",
       "      <td>2.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>2.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1089</td>\n",
       "      <td>2.481200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1091</td>\n",
       "      <td>2.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>2.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>2.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>2.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>2.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1096</td>\n",
       "      <td>2.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1097</td>\n",
       "      <td>2.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1098</td>\n",
       "      <td>2.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>2.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>2.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1102</td>\n",
       "      <td>2.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1103</td>\n",
       "      <td>2.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>2.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>2.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>2.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1107</td>\n",
       "      <td>2.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1108</td>\n",
       "      <td>2.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1109</td>\n",
       "      <td>2.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1111</td>\n",
       "      <td>2.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1112</td>\n",
       "      <td>2.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1113</td>\n",
       "      <td>2.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1114</td>\n",
       "      <td>2.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>2.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1116</td>\n",
       "      <td>2.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1117</td>\n",
       "      <td>2.596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1118</td>\n",
       "      <td>2.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1119</td>\n",
       "      <td>2.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1121</td>\n",
       "      <td>2.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1122</td>\n",
       "      <td>2.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1123</td>\n",
       "      <td>2.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1124</td>\n",
       "      <td>2.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>2.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1126</td>\n",
       "      <td>2.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1127</td>\n",
       "      <td>2.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>2.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1129</td>\n",
       "      <td>2.481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1131</td>\n",
       "      <td>2.457900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1132</td>\n",
       "      <td>2.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1133</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>2.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>2.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>2.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1137</td>\n",
       "      <td>2.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1138</td>\n",
       "      <td>2.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1139</td>\n",
       "      <td>2.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1141</td>\n",
       "      <td>2.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1142</td>\n",
       "      <td>2.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1143</td>\n",
       "      <td>2.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>2.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>2.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>2.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>2.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>2.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>2.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1151</td>\n",
       "      <td>2.363400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>2.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1153</td>\n",
       "      <td>2.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1154</td>\n",
       "      <td>2.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>2.488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1156</td>\n",
       "      <td>2.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1157</td>\n",
       "      <td>2.443500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1158</td>\n",
       "      <td>2.386300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1159</td>\n",
       "      <td>2.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1161</td>\n",
       "      <td>2.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1162</td>\n",
       "      <td>2.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1163</td>\n",
       "      <td>2.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1164</td>\n",
       "      <td>2.470500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>2.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1166</td>\n",
       "      <td>2.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1167</td>\n",
       "      <td>2.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>2.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1169</td>\n",
       "      <td>2.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.542800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171</td>\n",
       "      <td>2.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>2.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>2.507900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1174</td>\n",
       "      <td>2.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>2.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>2.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1177</td>\n",
       "      <td>2.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1178</td>\n",
       "      <td>2.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1179</td>\n",
       "      <td>2.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>2.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1182</td>\n",
       "      <td>2.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1183</td>\n",
       "      <td>2.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1184</td>\n",
       "      <td>2.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185</td>\n",
       "      <td>2.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186</td>\n",
       "      <td>2.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1187</td>\n",
       "      <td>2.475700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1188</td>\n",
       "      <td>2.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1189</td>\n",
       "      <td>2.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1191</td>\n",
       "      <td>2.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1192</td>\n",
       "      <td>2.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1193</td>\n",
       "      <td>2.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1194</td>\n",
       "      <td>2.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195</td>\n",
       "      <td>2.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1196</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1197</td>\n",
       "      <td>2.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1198</td>\n",
       "      <td>2.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>2.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>2.723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1202</td>\n",
       "      <td>2.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1203</td>\n",
       "      <td>2.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1204</td>\n",
       "      <td>2.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>2.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>2.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1207</td>\n",
       "      <td>2.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1208</td>\n",
       "      <td>2.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1209</td>\n",
       "      <td>2.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1211</td>\n",
       "      <td>2.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1212</td>\n",
       "      <td>2.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1213</td>\n",
       "      <td>2.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1214</td>\n",
       "      <td>2.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>2.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1216</td>\n",
       "      <td>2.488500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1217</td>\n",
       "      <td>2.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>2.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>2.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>2.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>2.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1223</td>\n",
       "      <td>2.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>2.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>2.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1226</td>\n",
       "      <td>2.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1227</td>\n",
       "      <td>2.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1228</td>\n",
       "      <td>2.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1229</td>\n",
       "      <td>2.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1231</td>\n",
       "      <td>2.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1232</td>\n",
       "      <td>2.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1233</td>\n",
       "      <td>2.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1234</td>\n",
       "      <td>2.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235</td>\n",
       "      <td>2.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1236</td>\n",
       "      <td>2.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1237</td>\n",
       "      <td>2.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>2.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>2.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>2.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>2.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>2.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>2.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>2.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>2.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>2.454900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>2.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1249</td>\n",
       "      <td>2.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1251</td>\n",
       "      <td>2.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1252</td>\n",
       "      <td>2.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1253</td>\n",
       "      <td>2.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1254</td>\n",
       "      <td>2.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>2.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1256</td>\n",
       "      <td>2.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>2.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1258</td>\n",
       "      <td>2.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1259</td>\n",
       "      <td>2.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1261</td>\n",
       "      <td>2.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1262</td>\n",
       "      <td>2.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1263</td>\n",
       "      <td>2.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1264</td>\n",
       "      <td>2.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>2.542700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1266</td>\n",
       "      <td>2.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1267</td>\n",
       "      <td>2.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>2.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1269</td>\n",
       "      <td>2.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1271</td>\n",
       "      <td>2.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>2.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>2.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>2.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>2.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>2.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1277</td>\n",
       "      <td>2.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>2.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1279</td>\n",
       "      <td>2.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1281</td>\n",
       "      <td>2.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1282</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1283</td>\n",
       "      <td>2.467800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>2.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285</td>\n",
       "      <td>2.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1286</td>\n",
       "      <td>2.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287</td>\n",
       "      <td>2.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1288</td>\n",
       "      <td>2.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1289</td>\n",
       "      <td>2.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>2.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1291</td>\n",
       "      <td>2.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1292</td>\n",
       "      <td>2.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1293</td>\n",
       "      <td>2.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1294</td>\n",
       "      <td>2.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>2.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>2.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1297</td>\n",
       "      <td>2.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1298</td>\n",
       "      <td>2.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1299</td>\n",
       "      <td>2.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1301</td>\n",
       "      <td>2.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>2.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1303</td>\n",
       "      <td>2.411300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>2.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1306</td>\n",
       "      <td>2.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1307</td>\n",
       "      <td>2.508700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>2.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>2.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>2.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1311</td>\n",
       "      <td>2.514900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>2.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1313</td>\n",
       "      <td>2.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1314</td>\n",
       "      <td>2.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>2.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>2.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1317</td>\n",
       "      <td>2.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1318</td>\n",
       "      <td>2.486700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1319</td>\n",
       "      <td>2.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1321</td>\n",
       "      <td>2.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1322</td>\n",
       "      <td>2.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1323</td>\n",
       "      <td>2.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1324</td>\n",
       "      <td>2.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>2.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1326</td>\n",
       "      <td>2.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1327</td>\n",
       "      <td>2.576500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>2.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1329</td>\n",
       "      <td>2.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>2.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1331</td>\n",
       "      <td>2.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1332</td>\n",
       "      <td>2.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1333</td>\n",
       "      <td>2.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1334</td>\n",
       "      <td>2.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>2.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1336</td>\n",
       "      <td>2.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1337</td>\n",
       "      <td>2.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1338</td>\n",
       "      <td>2.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1339</td>\n",
       "      <td>2.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1341</td>\n",
       "      <td>2.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1342</td>\n",
       "      <td>2.605400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1343</td>\n",
       "      <td>2.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1344</td>\n",
       "      <td>2.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345</td>\n",
       "      <td>2.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1346</td>\n",
       "      <td>2.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1347</td>\n",
       "      <td>2.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1348</td>\n",
       "      <td>2.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1349</td>\n",
       "      <td>2.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1351</td>\n",
       "      <td>2.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1352</td>\n",
       "      <td>2.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1353</td>\n",
       "      <td>2.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1354</td>\n",
       "      <td>2.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355</td>\n",
       "      <td>2.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1356</td>\n",
       "      <td>2.430500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1357</td>\n",
       "      <td>2.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1358</td>\n",
       "      <td>2.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1359</td>\n",
       "      <td>2.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.518300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1361</td>\n",
       "      <td>2.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1362</td>\n",
       "      <td>2.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1363</td>\n",
       "      <td>2.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1364</td>\n",
       "      <td>2.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>2.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1366</td>\n",
       "      <td>2.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1367</td>\n",
       "      <td>2.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>2.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1369</td>\n",
       "      <td>2.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>2.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1371</td>\n",
       "      <td>2.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1372</td>\n",
       "      <td>2.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1373</td>\n",
       "      <td>2.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1374</td>\n",
       "      <td>2.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>2.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1376</td>\n",
       "      <td>2.381400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1377</td>\n",
       "      <td>2.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1378</td>\n",
       "      <td>2.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1379</td>\n",
       "      <td>2.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1381</td>\n",
       "      <td>2.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1382</td>\n",
       "      <td>2.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1383</td>\n",
       "      <td>2.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1384</td>\n",
       "      <td>2.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385</td>\n",
       "      <td>2.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1386</td>\n",
       "      <td>2.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1387</td>\n",
       "      <td>2.458500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1388</td>\n",
       "      <td>2.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1389</td>\n",
       "      <td>2.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1391</td>\n",
       "      <td>2.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1392</td>\n",
       "      <td>2.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1393</td>\n",
       "      <td>2.515200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1394</td>\n",
       "      <td>2.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>2.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1396</td>\n",
       "      <td>2.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1397</td>\n",
       "      <td>2.544700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>2.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1399</td>\n",
       "      <td>2.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1401</td>\n",
       "      <td>2.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1402</td>\n",
       "      <td>2.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1403</td>\n",
       "      <td>2.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1404</td>\n",
       "      <td>2.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405</td>\n",
       "      <td>2.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1406</td>\n",
       "      <td>2.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1407</td>\n",
       "      <td>2.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1408</td>\n",
       "      <td>2.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1409</td>\n",
       "      <td>2.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>2.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1411</td>\n",
       "      <td>2.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1412</td>\n",
       "      <td>2.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1413</td>\n",
       "      <td>2.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1414</td>\n",
       "      <td>2.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415</td>\n",
       "      <td>2.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1416</td>\n",
       "      <td>2.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1417</td>\n",
       "      <td>2.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1418</td>\n",
       "      <td>2.534700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1419</td>\n",
       "      <td>2.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1421</td>\n",
       "      <td>2.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1422</td>\n",
       "      <td>2.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1423</td>\n",
       "      <td>2.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1424</td>\n",
       "      <td>2.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>2.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1426</td>\n",
       "      <td>2.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1427</td>\n",
       "      <td>2.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1428</td>\n",
       "      <td>2.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1429</td>\n",
       "      <td>2.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>2.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1431</td>\n",
       "      <td>2.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1432</td>\n",
       "      <td>2.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1433</td>\n",
       "      <td>2.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1434</td>\n",
       "      <td>2.519700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>2.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1436</td>\n",
       "      <td>2.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1437</td>\n",
       "      <td>2.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1438</td>\n",
       "      <td>2.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1439</td>\n",
       "      <td>2.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1441</td>\n",
       "      <td>2.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1442</td>\n",
       "      <td>2.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1443</td>\n",
       "      <td>2.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1444</td>\n",
       "      <td>2.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445</td>\n",
       "      <td>2.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1446</td>\n",
       "      <td>2.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1447</td>\n",
       "      <td>2.402800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1448</td>\n",
       "      <td>2.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1449</td>\n",
       "      <td>2.490400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1451</td>\n",
       "      <td>2.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1452</td>\n",
       "      <td>2.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1453</td>\n",
       "      <td>2.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1454</td>\n",
       "      <td>2.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>2.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>2.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>2.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>2.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1459</td>\n",
       "      <td>2.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1461</td>\n",
       "      <td>2.518800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1462</td>\n",
       "      <td>2.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1463</td>\n",
       "      <td>2.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1464</td>\n",
       "      <td>2.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465</td>\n",
       "      <td>2.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1466</td>\n",
       "      <td>2.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1467</td>\n",
       "      <td>2.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1468</td>\n",
       "      <td>2.470500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1469</td>\n",
       "      <td>2.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1471</td>\n",
       "      <td>2.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1472</td>\n",
       "      <td>2.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1473</td>\n",
       "      <td>2.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1474</td>\n",
       "      <td>2.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>2.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1476</td>\n",
       "      <td>2.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1477</td>\n",
       "      <td>2.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1478</td>\n",
       "      <td>2.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1479</td>\n",
       "      <td>2.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1481</td>\n",
       "      <td>2.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1482</td>\n",
       "      <td>2.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1483</td>\n",
       "      <td>2.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1484</td>\n",
       "      <td>2.425800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485</td>\n",
       "      <td>2.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1486</td>\n",
       "      <td>2.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1487</td>\n",
       "      <td>2.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1488</td>\n",
       "      <td>2.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1489</td>\n",
       "      <td>2.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>2.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1491</td>\n",
       "      <td>2.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1492</td>\n",
       "      <td>2.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1493</td>\n",
       "      <td>2.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1494</td>\n",
       "      <td>2.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>2.599300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>2.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>2.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>2.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1499</td>\n",
       "      <td>2.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1501</td>\n",
       "      <td>2.484300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1502</td>\n",
       "      <td>2.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1503</td>\n",
       "      <td>2.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1504</td>\n",
       "      <td>2.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1505</td>\n",
       "      <td>2.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1506</td>\n",
       "      <td>2.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1507</td>\n",
       "      <td>2.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1508</td>\n",
       "      <td>2.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1509</td>\n",
       "      <td>2.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>2.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1511</td>\n",
       "      <td>2.407900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>2.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1513</td>\n",
       "      <td>2.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1514</td>\n",
       "      <td>2.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1515</td>\n",
       "      <td>2.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1516</td>\n",
       "      <td>2.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1517</td>\n",
       "      <td>2.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1518</td>\n",
       "      <td>2.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1519</td>\n",
       "      <td>2.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>2.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1522</td>\n",
       "      <td>2.541200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1523</td>\n",
       "      <td>2.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1524</td>\n",
       "      <td>2.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>2.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526</td>\n",
       "      <td>2.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>2.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>2.480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1529</td>\n",
       "      <td>2.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1531</td>\n",
       "      <td>2.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1532</td>\n",
       "      <td>2.515200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1533</td>\n",
       "      <td>2.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1534</td>\n",
       "      <td>2.514300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1535</td>\n",
       "      <td>2.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>2.447200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1537</td>\n",
       "      <td>2.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1538</td>\n",
       "      <td>2.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1539</td>\n",
       "      <td>2.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1541</td>\n",
       "      <td>2.389400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1542</td>\n",
       "      <td>2.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1543</td>\n",
       "      <td>2.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1544</td>\n",
       "      <td>2.495500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1545</td>\n",
       "      <td>2.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1546</td>\n",
       "      <td>2.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1547</td>\n",
       "      <td>2.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1548</td>\n",
       "      <td>2.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1549</td>\n",
       "      <td>2.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1551</td>\n",
       "      <td>2.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1552</td>\n",
       "      <td>2.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1553</td>\n",
       "      <td>2.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1554</td>\n",
       "      <td>2.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1555</td>\n",
       "      <td>2.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1556</td>\n",
       "      <td>2.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1557</td>\n",
       "      <td>2.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1558</td>\n",
       "      <td>2.478400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1559</td>\n",
       "      <td>2.417900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1561</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562</td>\n",
       "      <td>2.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1563</td>\n",
       "      <td>2.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>2.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1565</td>\n",
       "      <td>2.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1566</td>\n",
       "      <td>2.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1567</td>\n",
       "      <td>2.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>2.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1569</td>\n",
       "      <td>2.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>2.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1571</td>\n",
       "      <td>2.495100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1572</td>\n",
       "      <td>2.515800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1573</td>\n",
       "      <td>2.386600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1574</td>\n",
       "      <td>2.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>2.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1576</td>\n",
       "      <td>2.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1577</td>\n",
       "      <td>2.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578</td>\n",
       "      <td>2.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1579</td>\n",
       "      <td>2.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1581</td>\n",
       "      <td>2.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1582</td>\n",
       "      <td>2.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1583</td>\n",
       "      <td>2.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1584</td>\n",
       "      <td>2.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585</td>\n",
       "      <td>2.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1586</td>\n",
       "      <td>2.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1587</td>\n",
       "      <td>2.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1588</td>\n",
       "      <td>2.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1589</td>\n",
       "      <td>2.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>2.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1591</td>\n",
       "      <td>2.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1592</td>\n",
       "      <td>2.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1593</td>\n",
       "      <td>2.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1594</td>\n",
       "      <td>2.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1595</td>\n",
       "      <td>2.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>2.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1597</td>\n",
       "      <td>2.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1598</td>\n",
       "      <td>2.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>2.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1601</td>\n",
       "      <td>2.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1602</td>\n",
       "      <td>2.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1603</td>\n",
       "      <td>2.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1604</td>\n",
       "      <td>2.605600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1605</td>\n",
       "      <td>2.467200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1606</td>\n",
       "      <td>2.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1607</td>\n",
       "      <td>2.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608</td>\n",
       "      <td>2.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1609</td>\n",
       "      <td>2.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>2.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611</td>\n",
       "      <td>2.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1612</td>\n",
       "      <td>2.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1613</td>\n",
       "      <td>2.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1614</td>\n",
       "      <td>2.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615</td>\n",
       "      <td>2.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1616</td>\n",
       "      <td>2.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1617</td>\n",
       "      <td>2.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1618</td>\n",
       "      <td>2.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1619</td>\n",
       "      <td>2.553100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1621</td>\n",
       "      <td>2.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1622</td>\n",
       "      <td>2.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1623</td>\n",
       "      <td>2.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1624</td>\n",
       "      <td>2.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>2.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1626</td>\n",
       "      <td>2.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1627</td>\n",
       "      <td>2.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1628</td>\n",
       "      <td>2.368200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1629</td>\n",
       "      <td>2.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>2.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>2.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1632</td>\n",
       "      <td>2.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1633</td>\n",
       "      <td>2.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1634</td>\n",
       "      <td>2.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1635</td>\n",
       "      <td>2.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1636</td>\n",
       "      <td>2.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1637</td>\n",
       "      <td>2.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1638</td>\n",
       "      <td>2.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1639</td>\n",
       "      <td>2.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1641</td>\n",
       "      <td>2.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1642</td>\n",
       "      <td>2.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1643</td>\n",
       "      <td>2.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1644</td>\n",
       "      <td>2.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645</td>\n",
       "      <td>2.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1646</td>\n",
       "      <td>2.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1647</td>\n",
       "      <td>2.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1648</td>\n",
       "      <td>2.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1649</td>\n",
       "      <td>2.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1651</td>\n",
       "      <td>2.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1652</td>\n",
       "      <td>2.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1653</td>\n",
       "      <td>2.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1654</td>\n",
       "      <td>2.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1655</td>\n",
       "      <td>2.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>2.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1657</td>\n",
       "      <td>2.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1658</td>\n",
       "      <td>2.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1659</td>\n",
       "      <td>2.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1661</td>\n",
       "      <td>2.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1662</td>\n",
       "      <td>2.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1663</td>\n",
       "      <td>2.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>2.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665</td>\n",
       "      <td>2.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>2.384500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1667</td>\n",
       "      <td>2.454900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1668</td>\n",
       "      <td>2.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1669</td>\n",
       "      <td>2.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>2.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1671</td>\n",
       "      <td>2.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1672</td>\n",
       "      <td>2.423900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1673</td>\n",
       "      <td>2.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1674</td>\n",
       "      <td>2.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>2.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1676</td>\n",
       "      <td>2.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1677</td>\n",
       "      <td>2.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1678</td>\n",
       "      <td>2.369800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1679</td>\n",
       "      <td>2.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1681</td>\n",
       "      <td>2.461300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1682</td>\n",
       "      <td>2.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1683</td>\n",
       "      <td>2.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1684</td>\n",
       "      <td>2.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1685</td>\n",
       "      <td>2.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1686</td>\n",
       "      <td>2.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1687</td>\n",
       "      <td>2.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>2.394600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1689</td>\n",
       "      <td>2.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>2.566700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1691</td>\n",
       "      <td>2.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692</td>\n",
       "      <td>2.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1693</td>\n",
       "      <td>2.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1694</td>\n",
       "      <td>2.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1695</td>\n",
       "      <td>2.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1696</td>\n",
       "      <td>2.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1697</td>\n",
       "      <td>2.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1698</td>\n",
       "      <td>2.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1699</td>\n",
       "      <td>2.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1701</td>\n",
       "      <td>2.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1702</td>\n",
       "      <td>2.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1703</td>\n",
       "      <td>2.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1704</td>\n",
       "      <td>2.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1705</td>\n",
       "      <td>2.550900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1706</td>\n",
       "      <td>2.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1707</td>\n",
       "      <td>2.484800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1708</td>\n",
       "      <td>2.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1709</td>\n",
       "      <td>2.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>2.542800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1711</td>\n",
       "      <td>2.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1712</td>\n",
       "      <td>2.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1713</td>\n",
       "      <td>2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1714</td>\n",
       "      <td>2.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1715</td>\n",
       "      <td>2.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1716</td>\n",
       "      <td>2.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1717</td>\n",
       "      <td>2.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1718</td>\n",
       "      <td>2.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1719</td>\n",
       "      <td>2.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.368700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>2.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>2.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>2.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>2.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>2.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1726</td>\n",
       "      <td>2.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1727</td>\n",
       "      <td>2.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1728</td>\n",
       "      <td>2.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1729</td>\n",
       "      <td>2.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>2.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1731</td>\n",
       "      <td>2.546400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1732</td>\n",
       "      <td>2.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1733</td>\n",
       "      <td>2.538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1734</td>\n",
       "      <td>2.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1735</td>\n",
       "      <td>2.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1736</td>\n",
       "      <td>2.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1737</td>\n",
       "      <td>2.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1738</td>\n",
       "      <td>2.344100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1739</td>\n",
       "      <td>2.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1741</td>\n",
       "      <td>2.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1742</td>\n",
       "      <td>2.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1743</td>\n",
       "      <td>2.434600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744</td>\n",
       "      <td>2.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1745</td>\n",
       "      <td>2.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1746</td>\n",
       "      <td>2.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1747</td>\n",
       "      <td>2.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1748</td>\n",
       "      <td>2.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1749</td>\n",
       "      <td>2.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.560900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1751</td>\n",
       "      <td>2.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>2.323700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1753</td>\n",
       "      <td>2.419600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1754</td>\n",
       "      <td>2.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755</td>\n",
       "      <td>2.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1756</td>\n",
       "      <td>2.394600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1757</td>\n",
       "      <td>2.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1758</td>\n",
       "      <td>2.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1759</td>\n",
       "      <td>2.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1761</td>\n",
       "      <td>2.414600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1762</td>\n",
       "      <td>2.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1763</td>\n",
       "      <td>2.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1764</td>\n",
       "      <td>2.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1765</td>\n",
       "      <td>2.407900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1766</td>\n",
       "      <td>2.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1767</td>\n",
       "      <td>2.496900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1768</td>\n",
       "      <td>2.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1769</td>\n",
       "      <td>2.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>2.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1771</td>\n",
       "      <td>2.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1772</td>\n",
       "      <td>2.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1773</td>\n",
       "      <td>2.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1774</td>\n",
       "      <td>2.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>2.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>2.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1777</td>\n",
       "      <td>2.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1778</td>\n",
       "      <td>2.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1779</td>\n",
       "      <td>2.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1781</td>\n",
       "      <td>2.465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1782</td>\n",
       "      <td>2.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1783</td>\n",
       "      <td>2.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1784</td>\n",
       "      <td>2.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785</td>\n",
       "      <td>2.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1786</td>\n",
       "      <td>2.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1787</td>\n",
       "      <td>2.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1788</td>\n",
       "      <td>2.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1789</td>\n",
       "      <td>2.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>2.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1791</td>\n",
       "      <td>2.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>2.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1793</td>\n",
       "      <td>2.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1794</td>\n",
       "      <td>2.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1795</td>\n",
       "      <td>2.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1796</td>\n",
       "      <td>2.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1797</td>\n",
       "      <td>2.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1798</td>\n",
       "      <td>2.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1799</td>\n",
       "      <td>2.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1801</td>\n",
       "      <td>2.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1802</td>\n",
       "      <td>2.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1803</td>\n",
       "      <td>2.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804</td>\n",
       "      <td>2.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1805</td>\n",
       "      <td>2.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1806</td>\n",
       "      <td>2.530800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1807</td>\n",
       "      <td>2.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>2.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1809</td>\n",
       "      <td>2.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>2.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1811</td>\n",
       "      <td>2.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1812</td>\n",
       "      <td>2.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1813</td>\n",
       "      <td>2.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1814</td>\n",
       "      <td>2.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1815</td>\n",
       "      <td>2.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>2.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1817</td>\n",
       "      <td>2.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1818</td>\n",
       "      <td>2.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1819</td>\n",
       "      <td>2.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1821</td>\n",
       "      <td>2.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1822</td>\n",
       "      <td>2.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1823</td>\n",
       "      <td>2.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>2.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>2.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1826</td>\n",
       "      <td>2.662800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1827</td>\n",
       "      <td>2.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1828</td>\n",
       "      <td>2.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1829</td>\n",
       "      <td>2.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>2.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1831</td>\n",
       "      <td>2.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1832</td>\n",
       "      <td>2.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1833</td>\n",
       "      <td>2.573900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1834</td>\n",
       "      <td>2.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835</td>\n",
       "      <td>2.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1836</td>\n",
       "      <td>2.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1837</td>\n",
       "      <td>2.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1838</td>\n",
       "      <td>2.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1839</td>\n",
       "      <td>2.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1841</td>\n",
       "      <td>2.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1842</td>\n",
       "      <td>2.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1843</td>\n",
       "      <td>2.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1844</td>\n",
       "      <td>2.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1845</td>\n",
       "      <td>2.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1846</td>\n",
       "      <td>2.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1847</td>\n",
       "      <td>2.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1848</td>\n",
       "      <td>2.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1849</td>\n",
       "      <td>2.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1851</td>\n",
       "      <td>2.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1852</td>\n",
       "      <td>2.486600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1853</td>\n",
       "      <td>2.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1854</td>\n",
       "      <td>2.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1855</td>\n",
       "      <td>2.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1856</td>\n",
       "      <td>2.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1857</td>\n",
       "      <td>2.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1858</td>\n",
       "      <td>2.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1859</td>\n",
       "      <td>2.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>2.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1862</td>\n",
       "      <td>2.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1863</td>\n",
       "      <td>2.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>2.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1865</td>\n",
       "      <td>2.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1866</td>\n",
       "      <td>2.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1867</td>\n",
       "      <td>2.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1868</td>\n",
       "      <td>2.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1869</td>\n",
       "      <td>2.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>2.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1871</td>\n",
       "      <td>2.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>2.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1873</td>\n",
       "      <td>2.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1874</td>\n",
       "      <td>2.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>2.532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1876</td>\n",
       "      <td>2.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1877</td>\n",
       "      <td>2.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1878</td>\n",
       "      <td>2.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1879</td>\n",
       "      <td>2.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>2.398200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1881</td>\n",
       "      <td>2.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1882</td>\n",
       "      <td>2.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1883</td>\n",
       "      <td>2.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1884</td>\n",
       "      <td>2.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1885</td>\n",
       "      <td>2.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1886</td>\n",
       "      <td>2.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1887</td>\n",
       "      <td>2.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1888</td>\n",
       "      <td>2.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1889</td>\n",
       "      <td>2.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>2.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1891</td>\n",
       "      <td>2.460900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1892</td>\n",
       "      <td>2.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1893</td>\n",
       "      <td>2.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1894</td>\n",
       "      <td>2.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1895</td>\n",
       "      <td>2.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1896</td>\n",
       "      <td>2.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1897</td>\n",
       "      <td>2.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1898</td>\n",
       "      <td>2.527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899</td>\n",
       "      <td>2.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1901</td>\n",
       "      <td>2.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1902</td>\n",
       "      <td>2.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1903</td>\n",
       "      <td>2.398300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1904</td>\n",
       "      <td>2.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1905</td>\n",
       "      <td>2.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1906</td>\n",
       "      <td>2.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1907</td>\n",
       "      <td>2.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1908</td>\n",
       "      <td>2.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1909</td>\n",
       "      <td>2.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>2.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1911</td>\n",
       "      <td>2.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1912</td>\n",
       "      <td>2.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1913</td>\n",
       "      <td>2.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1914</td>\n",
       "      <td>2.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1915</td>\n",
       "      <td>2.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1916</td>\n",
       "      <td>2.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1917</td>\n",
       "      <td>2.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1918</td>\n",
       "      <td>2.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1919</td>\n",
       "      <td>2.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1921</td>\n",
       "      <td>2.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1922</td>\n",
       "      <td>2.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1923</td>\n",
       "      <td>2.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1924</td>\n",
       "      <td>2.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>2.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1926</td>\n",
       "      <td>2.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1927</td>\n",
       "      <td>2.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1928</td>\n",
       "      <td>2.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1929</td>\n",
       "      <td>2.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>2.504600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1931</td>\n",
       "      <td>2.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1932</td>\n",
       "      <td>2.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1933</td>\n",
       "      <td>2.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1934</td>\n",
       "      <td>2.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1935</td>\n",
       "      <td>2.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1936</td>\n",
       "      <td>2.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1937</td>\n",
       "      <td>2.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1938</td>\n",
       "      <td>2.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1939</td>\n",
       "      <td>2.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>2.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1941</td>\n",
       "      <td>2.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1942</td>\n",
       "      <td>2.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943</td>\n",
       "      <td>2.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1944</td>\n",
       "      <td>2.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1945</td>\n",
       "      <td>2.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1946</td>\n",
       "      <td>2.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1947</td>\n",
       "      <td>2.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1948</td>\n",
       "      <td>2.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1949</td>\n",
       "      <td>2.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951</td>\n",
       "      <td>2.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1952</td>\n",
       "      <td>2.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1953</td>\n",
       "      <td>2.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1954</td>\n",
       "      <td>2.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>2.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1956</td>\n",
       "      <td>2.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1957</td>\n",
       "      <td>2.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1958</td>\n",
       "      <td>2.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1959</td>\n",
       "      <td>2.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1961</td>\n",
       "      <td>2.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1962</td>\n",
       "      <td>2.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1963</td>\n",
       "      <td>2.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1964</td>\n",
       "      <td>2.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965</td>\n",
       "      <td>2.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1966</td>\n",
       "      <td>2.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1967</td>\n",
       "      <td>2.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>2.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1969</td>\n",
       "      <td>2.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>2.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1971</td>\n",
       "      <td>2.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1972</td>\n",
       "      <td>2.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1973</td>\n",
       "      <td>2.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1974</td>\n",
       "      <td>2.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>2.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1976</td>\n",
       "      <td>2.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1977</td>\n",
       "      <td>2.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1978</td>\n",
       "      <td>2.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1979</td>\n",
       "      <td>2.443600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>2.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1981</td>\n",
       "      <td>2.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1982</td>\n",
       "      <td>2.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1983</td>\n",
       "      <td>2.426800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1984</td>\n",
       "      <td>2.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1985</td>\n",
       "      <td>2.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986</td>\n",
       "      <td>2.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1987</td>\n",
       "      <td>2.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1988</td>\n",
       "      <td>2.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1989</td>\n",
       "      <td>2.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>2.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1991</td>\n",
       "      <td>2.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992</td>\n",
       "      <td>2.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1993</td>\n",
       "      <td>2.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1994</td>\n",
       "      <td>2.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>2.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>2.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>2.493700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>2.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>2.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2001</td>\n",
       "      <td>2.281500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2002</td>\n",
       "      <td>2.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2003</td>\n",
       "      <td>2.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004</td>\n",
       "      <td>2.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2005</td>\n",
       "      <td>2.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006</td>\n",
       "      <td>2.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2007</td>\n",
       "      <td>2.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008</td>\n",
       "      <td>2.328500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2009</td>\n",
       "      <td>2.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>2.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2011</td>\n",
       "      <td>2.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012</td>\n",
       "      <td>2.374300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013</td>\n",
       "      <td>2.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2014</td>\n",
       "      <td>2.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015</td>\n",
       "      <td>2.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016</td>\n",
       "      <td>2.487600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017</td>\n",
       "      <td>2.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018</td>\n",
       "      <td>2.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019</td>\n",
       "      <td>2.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>2.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021</td>\n",
       "      <td>2.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2022</td>\n",
       "      <td>2.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023</td>\n",
       "      <td>2.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2024</td>\n",
       "      <td>2.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>2.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2026</td>\n",
       "      <td>2.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2027</td>\n",
       "      <td>2.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2028</td>\n",
       "      <td>2.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2029</td>\n",
       "      <td>2.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>2.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2031</td>\n",
       "      <td>2.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2032</td>\n",
       "      <td>2.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2033</td>\n",
       "      <td>2.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2034</td>\n",
       "      <td>2.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2035</td>\n",
       "      <td>2.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2036</td>\n",
       "      <td>2.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2037</td>\n",
       "      <td>2.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2038</td>\n",
       "      <td>2.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2039</td>\n",
       "      <td>2.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>2.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2041</td>\n",
       "      <td>2.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2042</td>\n",
       "      <td>2.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2043</td>\n",
       "      <td>2.590200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2044</td>\n",
       "      <td>2.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2045</td>\n",
       "      <td>2.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2046</td>\n",
       "      <td>2.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047</td>\n",
       "      <td>2.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>2.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2049</td>\n",
       "      <td>2.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2051</td>\n",
       "      <td>2.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2052</td>\n",
       "      <td>2.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2053</td>\n",
       "      <td>2.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2054</td>\n",
       "      <td>2.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2055</td>\n",
       "      <td>2.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2056</td>\n",
       "      <td>2.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2057</td>\n",
       "      <td>2.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2058</td>\n",
       "      <td>2.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2059</td>\n",
       "      <td>2.526500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>2.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2061</td>\n",
       "      <td>2.464700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2062</td>\n",
       "      <td>2.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2063</td>\n",
       "      <td>2.467800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2064</td>\n",
       "      <td>2.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2065</td>\n",
       "      <td>2.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2066</td>\n",
       "      <td>2.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2067</td>\n",
       "      <td>2.427100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2068</td>\n",
       "      <td>2.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2069</td>\n",
       "      <td>2.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>2.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2071</td>\n",
       "      <td>2.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2072</td>\n",
       "      <td>2.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2073</td>\n",
       "      <td>2.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2074</td>\n",
       "      <td>2.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>2.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2076</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2077</td>\n",
       "      <td>2.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2078</td>\n",
       "      <td>2.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2079</td>\n",
       "      <td>2.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>2.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2081</td>\n",
       "      <td>2.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2082</td>\n",
       "      <td>2.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2083</td>\n",
       "      <td>2.453300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2084</td>\n",
       "      <td>2.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2085</td>\n",
       "      <td>2.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2086</td>\n",
       "      <td>2.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2087</td>\n",
       "      <td>2.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2088</td>\n",
       "      <td>2.436800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2089</td>\n",
       "      <td>2.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>2.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2091</td>\n",
       "      <td>2.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2092</td>\n",
       "      <td>2.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2093</td>\n",
       "      <td>2.408300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2094</td>\n",
       "      <td>2.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2095</td>\n",
       "      <td>2.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2096</td>\n",
       "      <td>2.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>2.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2098</td>\n",
       "      <td>2.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2099</td>\n",
       "      <td>2.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2101</td>\n",
       "      <td>2.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2102</td>\n",
       "      <td>2.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2103</td>\n",
       "      <td>2.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2104</td>\n",
       "      <td>2.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2105</td>\n",
       "      <td>2.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2106</td>\n",
       "      <td>2.533500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Memory Usage: 39.8% used. 154987.49MB available.\n",
      "GPU 0 Memory Usage: 8066.00MB reserved. 8039.13MB max allocated. 1954.84MB currently allocated.\n",
      "GPU 1 Memory Usage: 22588.00MB reserved. 21963.49MB max allocated. 3834.70MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256a9cf2ed494b46b4d054f659605dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▄▂▁▅▂▂▆▂▂▄▃▂▄▃▄▃▃▄▃▂▄▆▄▆█▆▅▅▅▆▅▆▆▆▅▆▆▆▆</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▆▅▆▇▇█▇▆▅▆▇▇▆▄▇▄▁▃▂▂▃▁▃▂▂▃▂▄▃▁▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.5623624552286781e+18</td></tr><tr><td>train/epoch</td><td>1.99929</td></tr><tr><td>train/global_step</td><td>2106</td></tr><tr><td>train/grad_norm</td><td>1.08491</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.5335</td></tr><tr><td>train_loss</td><td>2.70449</td></tr><tr><td>train_runtime</td><td>27449.1186</td></tr><tr><td>train_samples_per_second</td><td>9.823</td></tr><tr><td>train_steps_per_second</td><td>0.077</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-planet-138</strong> at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k2m8zq1m' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/k2m8zq1m</a><br/> View project at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_173458-k2m8zq1m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom validation loop\n",
      "device = cuda 0\n",
      "Move linear layer to device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cache cleared before validation.\n",
      "Memory Usage: 39.8% used. 154972.09MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Now generate predictions\n",
      "----------------------------------\n",
      "Validation step 0\n",
      "Generation timed out\n",
      "----------------------------------\n",
      "Validation step 1\n",
      "Generation timed out\n",
      "----------------------------------\n",
      "Validation step 2\n",
      "Generation timed out\n",
      "----------------------------------\n",
      "Validation step 3\n",
      "Generation timed out\n",
      "----------------------------------\n",
      "Validation step 4\n",
      "Generation timed out\n",
      "----------------------------------\n",
      "Validation step 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import psutil\n",
    "import signal\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Training data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Validation data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Register the signal function handler\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, linear_layer, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    total_bleu_score = 0\n",
    "    total_rouge_score = {'rouge-1': {'f': 0, 'p': 0, 'r': 0},\n",
    "                         'rouge-2': {'f': 0, 'p': 0, 'r': 0},\n",
    "                         'rouge-l': {'f': 0, 'p': 0, 'r': 0}}\n",
    "    num_predictions = 0    \n",
    "    rouge = Rouge()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        print(\"----------------------------------\")\n",
    "        print(f\"Validation step {step}\")\n",
    "        #print_memory_usage()\n",
    "        embeddings = batch[\"embedding\"].to(device)  # Ensure embeddings are in float16\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        \n",
    "        #print(f\"Embeddings shape before linear layer: {embeddings.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = linear_layer(embeddings).to(device)  # Apply the linear layer to project embeddings\n",
    "            #print(f\"Embeddings shape after linear layer: {embeddings.shape}\")\n",
    "            input_ids = torch.full((embeddings.size(0), 1), tokenizer.pad_token_id, dtype=torch.long).to(device)\n",
    "            \n",
    "            #print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "            \n",
    "            # Ensure embeddings has batch size dimension\n",
    "            if len(embeddings.shape) == 2:\n",
    "                embeddings = embeddings.unsqueeze(1)\n",
    "                #print(f\"Reshaped Embeddings shape: {embeddings.shape}\")\n",
    "            \n",
    "            #print(\"Checking memory before generation\")\n",
    "            #print_memory_usage()\n",
    "            \n",
    "            try:\n",
    "                # Set the alarm for 30 seconds\n",
    "                signal.alarm(30)\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    inputs_embeds=embeddings,\n",
    "                    max_length=256,  # Limit length to prevent excessively long texts\n",
    "                    num_beams=2,\n",
    "                    do_sample=True,\n",
    "                    top_k=30, # Menos que 50 pero no tanto\n",
    "                    top_p=0.95,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0,  # Increase repetition penalty to avoid word repetitions\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                #print(f\"Outputs shape: {outputs.shape}\")\n",
    "                decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                \n",
    "                # Disable the alarm\n",
    "                signal.alarm(0)\n",
    "            except TimeoutException:\n",
    "                print(\"Generation timed out\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "                continue\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            reference = [tweet_text.split()]\n",
    "            candidate = pred_text.split()\n",
    "            bleu_score = sentence_bleu(reference, candidate)\n",
    "            total_bleu_score += bleu_score\n",
    "            \n",
    "            # Calculate ROUGE score\n",
    "            rouge_scores = rouge.get_scores(pred_text, tweet_text, avg=True)\n",
    "            for key in total_rouge_score.keys():\n",
    "                total_rouge_score[key]['f'] += rouge_scores[key]['f']\n",
    "                total_rouge_score[key]['p'] += rouge_scores[key]['p']\n",
    "                total_rouge_score[key]['r'] += rouge_scores[key]['r']\n",
    "\n",
    "            num_predictions += 1\n",
    "            \n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "                print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "                print()\n",
    "\n",
    "    # Print average BLEU and ROUGE\n",
    "    avg_bleu_score = total_bleu_score / num_predictions if num_predictions > 0 else 0\n",
    "    avg_rouge_score = {key: {metric: score / num_predictions for metric, score in scores.items()} for key, scores in total_rouge_score.items()} if num_predictions > 0 else total_rouge_score\n",
    "\n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    print(f\"Average ROUGE Score: {avg_rouge_score}\")\n",
    "    return predictions\n",
    "\n",
    "def print_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {mem.percent}% used. {mem.available / 1024 ** 2:.2f}MB available.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_mem = torch.cuda.memory_reserved(i) / 1024 ** 2\n",
    "            gpu_max_mem = torch.cuda.max_memory_allocated(i) / 1024 ** 2\n",
    "            gpu_mem_alloc = torch.cuda.memory_allocated(i) / 1024 ** 2\n",
    "            print(f\"GPU {i} Memory Usage: {gpu_mem:.2f}MB reserved. {gpu_max_mem:.2f}MB max allocated. {gpu_mem_alloc:.2f}MB currently allocated.\")\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    print_memory_usage()\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Mantener el mapeo automático de dispositivos\n",
    "    )\n",
    "    print(\"Model loaded.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = ValidationDataset(dict_val, embeddings, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # Reducir tamaño del lote para validación\n",
    "\n",
    "    # Add linear layer for projecting embeddings\n",
    "    linear_layer = nn.Linear(768, 4096).to('cuda').to(torch.float16)\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    wandb.login(key=api_key)\n",
    "    run = wandb.init(\n",
    "        project='Fine-tune Llama 3 8B on Image Embeddings', \n",
    "        job_type=\"training\", \n",
    "        anonymous=\"allow\"\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=2,\n",
    "        evaluation_strategy=\"no\",  # Disable automatic evaluation during training\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"    \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,  # Disable automatic evaluation\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=256,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "    print_memory_usage()\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    \n",
    "     \n",
    "    # Llamar a la función para guardar el modelo y el tokenizador\n",
    "    save_model_and_tokenizer(trainer, tokenizer)\n",
    "    \n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Custom validation loop  \n",
    "    print(\"Custom validation loop\") \n",
    "    print(\"device = cuda 0\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "    print(\"Move linear layer to device\")\n",
    "    linear_layer.to(device)  # Move the linear layer to the same device\n",
    "    \n",
    "    # Liberar memoria antes de la validación\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Memory cache cleared before validation.\")\n",
    "    print_memory_usage() \n",
    "     \n",
    "    print(\"Now generate predictions\")\n",
    "    \n",
    "    try:\n",
    "        predictions = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, linear_layer, print_every_n_steps=1)\n",
    "        print(predictions)\n",
    "    except:\n",
    "        print(\"Error al generar predicciones\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos épocas es demasiado, sobreentrena. Dejémoslo en una."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔴 Una sola época de nuevo y mejorada la validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_tokenizer(trainer, tokenizer):\n",
    "    \"\"\"Guardar el modelo y el tokenizador en una carpeta con la fecha y hora actual.\"\"\"\n",
    "    # Crear la carpeta con la fecha y hora actual\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = os.path.join(\"saved-finetuned-llamas\", current_time)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Guardar el modelo y el tokenizador\n",
    "    trainer.model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "\n",
    "# Hacer visibles solo las GPUs 1 y 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ahora PyTorch solo verá las GPUs 1 y 2\n",
    "print(torch.cuda.device_count())  # Debería imprimir 2\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de la primera GPU visible (anteriormente GPU 1)\n",
    "print(torch.cuda.get_device_name(1))  # Nombre de la segunda GPU visible (anteriormente GPU 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 10:08:31.404453: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-06 10:08:31.497146: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-06 10:08:32.668360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-06 10:08:33,464] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Memory Usage: 49.8% used. 129327.26MB available.\n",
      "GPU 0 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n",
      "GPU 1 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61f07d70dd343ecb6cd5adacccf21c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Memory Usage: 50.1% used. 128417.68MB available.\n",
      "GPU 0 Memory Usage: 1862.00MB reserved. 1955.44MB max allocated. 1860.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3668.00MB reserved. 3694.41MB max allocated. 3578.44MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 134823\n",
      "Validation data size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240706_100848-0el1uajq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/0el1uajq' target=\"_blank\">devoted-deluge-141</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/0el1uajq' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/0el1uajq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Memory Usage: 50.2% used. 128274.27MB available.\n",
      "GPU 0 Memory Usage: 1902.00MB reserved. 1955.44MB max allocated. 1900.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3788.00MB reserved. 3716.46MB max allocated. 3704.45MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1053' max='1053' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1053/1053 3:46:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.550900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.338800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.209600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.993500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.894900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>3.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.901700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.972300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>3.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>2.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>2.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>3.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>3.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>3.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>3.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>3.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>3.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>3.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.954100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>3.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>3.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>3.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>3.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>3.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.959100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.951100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>3.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.800200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>2.906300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>3.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>2.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>3.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>3.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>3.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>3.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>3.092600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>3.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>3.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>3.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>3.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>2.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>3.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>3.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.844800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>2.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>3.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>3.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>2.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>2.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>2.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>2.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>3.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>3.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>2.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>2.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>2.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>2.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>2.922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>2.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>2.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>3.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>3.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>2.871700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>3.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>2.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>3.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>2.907600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>3.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>2.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>2.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>3.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>2.902500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>2.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>3.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>3.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>2.929100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>3.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>2.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>2.844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>2.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>2.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>2.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>2.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>3.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>2.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>2.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>2.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>2.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>3.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>2.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>3.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>3.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>2.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>3.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>3.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>3.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>2.928800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>3.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>2.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>2.846200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>3.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>2.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>2.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>3.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>2.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>2.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>2.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>2.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>3.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>2.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>2.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>2.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>2.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.987300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>3.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>2.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>2.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>2.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>2.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>3.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>3.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>3.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>2.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>2.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>2.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>2.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>3.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>2.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>2.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>2.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>3.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>2.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>2.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>3.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>3.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>2.940200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>2.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>2.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>3.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>2.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>2.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>2.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>2.925400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>2.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>2.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>2.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>2.932100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>2.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>2.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>3.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>2.835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>2.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>2.823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>3.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>2.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>3.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>2.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>2.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>2.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>2.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>2.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>3.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>2.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>2.963400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>2.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>3.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>2.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>2.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>3.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>2.850700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>2.876400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>2.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>2.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>3.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.908800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>2.823800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>2.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>3.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>3.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>2.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>2.929500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>2.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>2.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>3.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.969900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>2.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>2.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>2.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>2.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>2.915600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>3.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>3.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>2.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>2.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>2.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>2.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>2.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>2.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>2.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>2.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>3.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>2.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.770400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>2.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>3.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>2.758900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>2.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>2.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>2.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>2.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>2.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>2.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>2.921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>2.797700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>2.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>2.826400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>2.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>2.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>2.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>2.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>3.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>2.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>2.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>2.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>2.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>2.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>2.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>3.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>2.997700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>2.796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2.868100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>3.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>3.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>2.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>2.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>2.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>2.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>2.897700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>3.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>2.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>2.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>2.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>2.843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>2.867400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>2.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.907100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>2.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>2.874600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>3.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>2.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>2.901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>2.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>2.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>2.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>2.915700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>2.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>2.923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>2.907400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>3.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>2.870800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>2.888300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>2.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>2.915500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>3.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>2.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>2.885200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>2.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>3.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>2.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>2.831200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>2.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>2.807700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>2.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>2.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>2.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>2.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>2.892400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>2.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>2.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>2.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>2.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>2.851800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>2.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>2.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>2.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>2.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>2.959800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>2.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>2.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>2.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>2.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>3.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>3.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>2.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>2.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>2.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>2.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>2.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>2.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>2.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>2.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>3.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>2.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>2.968800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>2.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>2.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>2.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>2.844600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>3.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>2.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>2.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>2.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>2.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>2.791900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>2.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>2.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>2.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>2.791700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>2.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>2.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>2.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>2.856300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>3.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>2.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>2.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>2.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.937600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>2.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>2.909200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>2.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>2.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>2.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>2.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>2.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>2.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>3.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>2.869600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>2.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>2.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>3.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>2.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>2.831800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>2.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>2.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>2.913400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>2.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>2.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>2.723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>2.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>2.890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>2.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>2.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>2.851200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>2.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.074300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>2.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>2.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>3.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>2.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>2.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>2.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>2.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>2.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>2.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>2.938100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>2.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>2.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>2.716300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>2.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>2.937900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>2.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>2.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>2.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>2.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>2.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>2.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>2.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>2.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>2.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>2.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>2.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>2.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>2.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>2.921900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>2.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>2.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>2.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>2.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>2.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>2.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>2.725600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>2.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>2.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>2.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>2.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>2.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>2.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>2.816600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>2.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>2.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>2.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>2.971600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>2.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>2.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>2.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>2.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>2.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>2.832200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>2.784600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>2.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>2.822700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>2.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>2.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>2.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>2.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.878600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>2.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>2.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>2.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>2.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>2.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>2.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>2.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>2.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>2.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>2.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>2.874700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>2.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>3.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>2.781800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>2.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>2.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>2.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>2.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>2.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>2.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>2.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>2.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>2.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>2.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>2.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>2.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>2.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>2.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>2.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>2.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>2.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>2.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>2.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>2.843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>2.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>3.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.829300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>2.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>2.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>3.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>3.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>2.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>2.850400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>2.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>2.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>2.805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>2.782900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>2.819400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>2.830400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>2.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>2.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>2.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>2.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>2.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.839800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>2.930300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>3.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>2.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>2.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>2.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>2.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>2.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>2.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>2.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>2.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>2.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>2.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>2.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>2.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>2.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>2.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>2.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>2.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.809600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>2.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>2.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>2.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>2.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>2.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>2.837300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>2.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>2.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>2.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>2.843100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>2.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>2.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>2.805700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>2.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>2.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>2.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>2.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>2.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>2.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>2.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>2.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>2.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>2.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>2.807700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>2.960100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>2.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>2.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>2.952600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>2.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>2.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>2.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>2.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>2.934300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>2.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>2.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>2.823300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>2.841700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>2.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>2.881600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>2.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>2.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>3.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>2.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>2.797500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>2.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>2.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>2.864900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>2.772900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>2.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>2.787200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>3.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>2.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>2.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>2.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>2.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>2.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>2.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>2.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>2.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>2.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>2.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>2.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>2.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>2.820300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>2.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>2.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>2.847600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>2.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>2.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>2.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>2.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>2.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>2.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>3.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>2.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>2.846800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>2.773400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>2.736500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>2.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>2.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>3.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>2.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>2.789800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>2.905200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>2.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>2.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>2.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>2.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>2.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>2.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.799600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>2.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>2.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>2.867100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>2.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>2.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>2.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>2.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>3.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>2.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>2.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>2.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>2.932100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>2.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>2.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>2.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>2.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>2.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>2.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>2.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>2.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>2.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>2.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>2.841700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>2.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>2.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>2.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.801300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>2.834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>2.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>2.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>2.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>2.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>2.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>2.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>2.660900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>2.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.925400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>2.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>2.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>2.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>2.932400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>2.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>2.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>2.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>2.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>2.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>2.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>2.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>2.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>2.926300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>2.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>2.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>2.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>2.848700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>2.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>2.746700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>3.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>2.634200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>2.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>2.671800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>2.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>2.815400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>2.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>2.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.773300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>2.852200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>2.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>2.649200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>2.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>2.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>2.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>3.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>2.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>2.827600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>2.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>2.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>2.748900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>2.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>2.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>2.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>2.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>2.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>2.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>2.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>2.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>2.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>2.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>2.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>2.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>2.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>2.823300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.920300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>2.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>2.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>2.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>2.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>2.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>2.629300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>2.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>2.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.741200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>2.827500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>2.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>2.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>2.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>2.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>2.789500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2.572800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>2.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>2.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>2.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>2.788500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>2.675300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>2.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>2.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>2.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>2.932600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>2.798700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>2.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>2.743100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>2.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>2.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>2.901800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>2.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>2.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>2.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>2.766700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>2.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>2.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>2.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>2.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>2.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>2.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>2.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>2.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>2.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>2.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>2.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>2.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>2.655700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>2.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>2.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>2.755700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>2.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>2.819900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>2.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>2.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>2.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>2.846200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>2.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>2.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>2.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>2.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>2.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>2.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>2.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>2.777200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>2.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>2.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>2.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>2.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>2.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>2.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>2.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>2.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>2.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>2.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>2.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>2.844200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>2.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>2.787400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>2.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>2.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>2.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>2.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>2.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>2.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>2.915300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>2.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>2.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>2.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>2.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>2.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>2.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>2.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>2.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>2.802500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>2.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>2.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>2.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>2.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>2.711100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>2.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>2.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>2.853800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>2.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>2.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>2.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>2.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>2.811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>2.737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>2.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>2.578600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>2.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>2.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>2.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>2.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>2.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>2.767600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>2.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>2.755800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>2.815300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>2.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>2.708400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>2.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>2.769800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Memory Usage: 54.0% used. 118516.11MB available.\n",
      "GPU 0 Memory Usage: 8066.00MB reserved. 8039.13MB max allocated. 1954.84MB currently allocated.\n",
      "GPU 1 Memory Usage: 22588.00MB reserved. 21963.49MB max allocated. 3834.70MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4a7d00160a4dfaa605eefd715475e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▂▂▁▁▂▁▂▂▂▃▃▂▂▂▂▂▃▁▂▁▂▁▁▂▁▂▁▂▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▅▅▄▄▄▄▄▅▅▃▄▄▄▄▄▄▃▂▅▄▄▄▃▄▃▃▃▄▃▂▃▃▁▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>7.812073095891517e+17</td></tr><tr><td>train/epoch</td><td>0.99964</td></tr><tr><td>train/global_step</td><td>1053</td></tr><tr><td>train/grad_norm</td><td>0.76946</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.7698</td></tr><tr><td>train_loss</td><td>2.92178</td></tr><tr><td>train_runtime</td><td>13611.7741</td></tr><tr><td>train_samples_per_second</td><td>9.905</td></tr><tr><td>train_steps_per_second</td><td>0.077</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-deluge-141</strong> at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/0el1uajq' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/0el1uajq</a><br/> View project at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240706_100848-0el1uajq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom validation loop\n",
      "device = cuda 0\n",
      "Move linear layer to device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cache cleared before validation.\n",
      "Memory Usage: 54.0% used. 118471.71MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Now generate predictions\n",
      "Validation step 0\n",
      "Memory Usage: 54.0% used. 118471.71MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.0% used. 118471.71MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 1\n",
      "Memory Usage: 54.1% used. 118301.53MB available.\n",
      "GPU 0 Memory Usage: 3332.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 5844.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.1% used. 118301.53MB available.\n",
      "GPU 0 Memory Usage: 3332.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 5844.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 2\n",
      "Memory Usage: 53.7% used. 119129.64MB available.\n",
      "GPU 0 Memory Usage: 3560.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 53.7% used. 119129.64MB available.\n",
      "GPU 0 Memory Usage: 3560.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 3\n",
      "Memory Usage: 53.9% used. 118641.20MB available.\n",
      "GPU 0 Memory Usage: 3560.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 53.9% used. 118641.20MB available.\n",
      "GPU 0 Memory Usage: 3560.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 4\n",
      "Memory Usage: 53.8% used. 118914.36MB available.\n",
      "GPU 0 Memory Usage: 3560.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 53.8% used. 118914.36MB available.\n",
      "GPU 0 Memory Usage: 3560.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 5\n",
      "Memory Usage: 53.8% used. 118987.70MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 53.8% used. 118987.70MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 6\n",
      "Memory Usage: 53.8% used. 119003.34MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 53.8% used. 119003.34MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 7\n",
      "Memory Usage: 54.5% used. 117206.54MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117205.80MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 8\n",
      "Memory Usage: 54.5% used. 117259.20MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117259.20MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 9\n",
      "Memory Usage: 54.5% used. 117203.97MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117199.59MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 10\n",
      "Memory Usage: 54.6% used. 116917.75MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 116917.75MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 11\n",
      "Memory Usage: 54.6% used. 116918.85MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 116918.85MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 12\n",
      "Memory Usage: 54.6% used. 116966.45MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 116966.45MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 13\n",
      "Memory Usage: 54.6% used. 117044.74MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 117044.74MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 14\n",
      "Memory Usage: 54.6% used. 117052.27MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 117055.98MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 15\n",
      "Memory Usage: 54.6% used. 117032.90MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 117032.90MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 16\n",
      "Memory Usage: 54.6% used. 117011.52MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 117011.52MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 17\n",
      "Memory Usage: 54.5% used. 117214.67MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117214.67MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 18\n",
      "Memory Usage: 54.5% used. 117259.72MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117259.72MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 19\n",
      "Memory Usage: 54.5% used. 117190.50MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117187.88MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 20\n",
      "Memory Usage: 54.5% used. 117111.01MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117111.01MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 21\n",
      "Memory Usage: 54.6% used. 117034.41MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 117034.41MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 22\n",
      "Memory Usage: 54.5% used. 117137.94MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117137.94MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 23\n",
      "Memory Usage: 54.5% used. 117124.84MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117124.84MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6072.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 24\n",
      "Memory Usage: 54.5% used. 117231.70MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117231.70MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 25\n",
      "Memory Usage: 54.5% used. 117164.53MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117164.53MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 26\n",
      "Memory Usage: 54.5% used. 117127.75MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117127.75MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 27\n",
      "Memory Usage: 54.5% used. 117131.65MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117131.65MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 28\n",
      "Memory Usage: 54.5% used. 117148.45MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117148.45MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 29\n",
      "Memory Usage: 54.5% used. 117144.78MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.5% used. 117144.78MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 30\n",
      "Memory Usage: 54.6% used. 116978.55MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 116978.55MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 31\n",
      "Memory Usage: 54.6% used. 116828.55MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.6% used. 116828.55MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 32\n",
      "Memory Usage: 54.1% used. 118272.42MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.1% used. 118272.42MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 33\n",
      "Memory Usage: 54.9% used. 116201.52MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.9% used. 116208.02MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 34\n",
      "Memory Usage: 55.0% used. 116031.95MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 55.0% used. 116031.95MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out\n",
      "Validation step 35\n",
      "Memory Usage: 54.7% used. 116635.34MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 54.7% used. 116635.34MB available.\n",
      "GPU 0 Memory Usage: 3792.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 6304.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Error al generar predicciones\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import psutil\n",
    "import signal\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Training data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Validation data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Register the signal function handler\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, linear_layer, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        print(f\"Validation step {step}\")\n",
    "        print_memory_usage()\n",
    "        embeddings = batch[\"embedding\"].to(device)  # Ensure embeddings are in float16\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        \n",
    "        print(f\"Embeddings shape before linear layer: {embeddings.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = linear_layer(embeddings).to(device)  # Apply the linear layer to project embeddings\n",
    "            print(f\"Embeddings shape after linear layer: {embeddings.shape}\")\n",
    "            input_ids = torch.full((embeddings.size(0), 1), tokenizer.pad_token_id, dtype=torch.long).to(device)\n",
    "            \n",
    "            print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "            \n",
    "            # Ensure embeddings has batch size dimension\n",
    "            if len(embeddings.shape) == 2:\n",
    "                embeddings = embeddings.unsqueeze(1)\n",
    "                print(f\"Reshaped Embeddings shape: {embeddings.shape}\")\n",
    "            \n",
    "            print(\"Checking memory before generation\")\n",
    "            print_memory_usage()\n",
    "            \n",
    "            try:\n",
    "                # Set the alarm for 30 seconds\n",
    "                signal.alarm(30)\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    inputs_embeds=embeddings,\n",
    "                    max_length=256,  # Limit length to prevent excessively long texts\n",
    "                    num_beams=2,\n",
    "                    do_sample=True,\n",
    "                    top_k=30,\n",
    "                    top_p=0.95,\n",
    "                    temperature=1.0,\n",
    "                    repetition_penalty=2.0,  # Increase repetition penalty to avoid word repetitions\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                print(f\"Outputs shape: {outputs.shape}\")\n",
    "                decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                \n",
    "                # Disable the alarm\n",
    "                signal.alarm(0)\n",
    "            except TimeoutException:\n",
    "                print(\"Generation timed out\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "                continue\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "\n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def print_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {mem.percent}% used. {mem.available / 1024 ** 2:.2f}MB available.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_mem = torch.cuda.memory_reserved(i) / 1024 ** 2\n",
    "            gpu_max_mem = torch.cuda.max_memory_allocated(i) / 1024 ** 2\n",
    "            gpu_mem_alloc = torch.cuda.memory_allocated(i) / 1024 ** 2\n",
    "            print(f\"GPU {i} Memory Usage: {gpu_mem:.2f}MB reserved. {gpu_max_mem:.2f}MB max allocated. {gpu_mem_alloc:.2f}MB currently allocated.\")\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_api_key_wandb'\n",
    "\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    print_memory_usage()\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Mantener el mapeo automático de dispositivos\n",
    "    )\n",
    "    print(\"Model loaded.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = ValidationDataset(dict_val, embeddings, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # Reducir tamaño del lote para validación\n",
    "\n",
    "    # Add linear layer for projecting embeddings\n",
    "    linear_layer = nn.Linear(768, 4096).to('cuda').to(torch.float16)\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    wandb.login(key=api_key)\n",
    "    run = wandb.init(\n",
    "        project='Fine-tune Llama 3 8B on Image Embeddings', \n",
    "        job_type=\"training\", \n",
    "        anonymous=\"allow\"\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"no\",  # Disable automatic evaluation during training\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"    \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,  # Disable automatic evaluation\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=256,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "    print_memory_usage()\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Custom validation loop  \n",
    "    print(\"Custom validation loop\") \n",
    "    print(\"device = cuda 0\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "    print(\"Move linear layer to device\")\n",
    "    linear_layer.to(device)  # Move the linear layer to the same device\n",
    "    \n",
    "    # Liberar memoria antes de la validación\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Memory cache cleared before validation.\")\n",
    "    print_memory_usage() \n",
    "     \n",
    "    print(\"Now generate predictions\")\n",
    "    \n",
    "    try:\n",
    "        predictions = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, linear_layer, print_every_n_steps=1)\n",
    "        print(predictions)\n",
    "    except:\n",
    "        print(\"Error al generar predicciones\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha habido un pequeño error, al no haber incrementado el tiempo máximo, pero haber duplicado el batch size. No le da tiempo a generar los textos.\n",
    "\n",
    "\n",
    "Además, ha crusheado el kernel en el proceso, pero se guardó el checkpoint del modelo.\n",
    "\n",
    "Vamos a ajustar el tiempo máximo de generación y ejecutar la validación de nuevo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔴 Último intento de mejorar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA GeForce RTX 3090\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_tokenizer(trainer, tokenizer):\n",
    "    \"\"\"Guardar el modelo y el tokenizador en una carpeta con la fecha y hora actual.\"\"\"\n",
    "    # Crear la carpeta con la fecha y hora actual\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_dir = os.path.join(\"saved-finetuned-llamas\", current_time)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Guardar el modelo y el tokenizador\n",
    "    trainer.model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "\n",
    "# Hacer visibles solo las GPUs 1 y 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ahora PyTorch solo verá las GPUs 1 y 2\n",
    "print(torch.cuda.device_count())  # Debería imprimir 2\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de la primera GPU visible (anteriormente GPU 1)\n",
    "print(torch.cuda.get_device_name(1))  # Nombre de la segunda GPU visible (anteriormente GPU 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 15:28:09.949316: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-06 15:28:10.026958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-06 15:28:10.887170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-06 15:28:11,457] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Memory Usage: 53.0% used. 121087.41MB available.\n",
      "GPU 0 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n",
      "GPU 1 Memory Usage: 0.00MB reserved. 0.00MB max allocated. 0.00MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5d7622dc124c33b567f395f1431b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Memory Usage: 53.3% used. 120395.90MB available.\n",
      "GPU 0 Memory Usage: 1862.00MB reserved. 1955.44MB max allocated. 1860.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3668.00MB reserved. 3694.41MB max allocated. 3578.44MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 134823\n",
      "Validation data size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjsantamariag\u001b[0m (\u001b[33mj-santamariag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/javiermo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/javiermo/javiersg/wandb/run-20240706_152826-e07a9xg0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/e07a9xg0' target=\"_blank\">exalted-plant-142</a></strong> to <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/e07a9xg0' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/e07a9xg0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/javiermo/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Memory Usage: 53.3% used. 120283.44MB available.\n",
      "GPU 0 Memory Usage: 1902.00MB reserved. 1955.44MB max allocated. 1900.59MB currently allocated.\n",
      "GPU 1 Memory Usage: 3788.00MB reserved. 3716.46MB max allocated. 3704.45MB currently allocated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1053' max='1053' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1053/1053 3:45:57, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.768400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.616200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.238500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.780600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.997700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.984800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>3.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>2.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>2.991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>3.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>3.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.923800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>3.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>3.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>3.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>3.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>3.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>3.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>3.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>3.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>3.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>3.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.969700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>3.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.800300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.945500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>2.897200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>3.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>2.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>3.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>3.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.882800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>3.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>3.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>3.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>3.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.874100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>2.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>3.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.968300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>3.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>2.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>3.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>3.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>2.978600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>3.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>3.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>2.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>2.890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>2.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>2.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>3.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>3.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>2.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>2.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.818500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>2.806800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>2.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>2.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>2.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>2.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>3.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>3.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>2.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>3.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>2.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>3.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>2.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>3.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>2.910900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>2.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>3.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>2.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>2.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>3.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>3.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>2.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>3.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>2.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>2.849800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>2.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>2.976400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>2.828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>2.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>3.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>2.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>2.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>2.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>2.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>3.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>2.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>3.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>3.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>2.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>3.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>2.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>3.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>2.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>3.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>2.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>2.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>3.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>2.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>2.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>3.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>2.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>2.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>2.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>2.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>3.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>2.875900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>2.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>2.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>2.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>3.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>2.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>2.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>2.894400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>2.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>3.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>3.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>3.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>2.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>2.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>2.815700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>2.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>3.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>2.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2.932800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>2.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>2.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>3.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>2.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>2.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>3.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>3.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>2.938200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>2.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>2.880100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>3.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>2.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>3.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>2.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>2.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>2.834400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>2.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>2.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>2.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>2.978600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>2.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>3.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>2.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>2.769400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>2.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>3.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>2.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>3.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>2.849700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>2.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>2.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>2.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>2.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>3.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>2.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>2.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>2.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>3.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>2.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>2.881100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>3.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>2.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>2.836900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>2.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>3.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>2.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>2.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>3.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>3.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>2.862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>2.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>2.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>2.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>3.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>2.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>2.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>2.880200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>2.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.903400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>2.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>3.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>3.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>2.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>2.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>2.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>2.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>2.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>2.900300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>2.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>2.908700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>3.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>2.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.773600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>2.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>3.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>2.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>2.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>2.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>2.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>2.925300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>2.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>2.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>2.931200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>2.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>2.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>2.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>2.882900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>2.790700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>2.923400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>2.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>3.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>2.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>2.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>2.857700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>2.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>2.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>2.834100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>3.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>2.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>2.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>2.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>2.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>3.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>3.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>2.803500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>2.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>2.791400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>2.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>2.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>3.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>2.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>2.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>2.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>2.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>2.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>2.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.913100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>2.910600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>2.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>3.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>2.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>2.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>2.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>2.925900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>2.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>2.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>2.807800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>2.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>2.910900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>3.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>2.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>2.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>2.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>2.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>2.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.941800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>2.928300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>2.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>2.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>3.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>2.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>2.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>2.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>2.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>2.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>2.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>2.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>2.958800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>2.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>2.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>2.781300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>2.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>2.987800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.937300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>2.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>2.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>2.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>2.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>2.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>2.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>2.867500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>2.857500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>2.777900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>2.850300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>3.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>3.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>2.850900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>2.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>2.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>2.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>2.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>2.882300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>2.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>2.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>2.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>2.970300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>2.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>2.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>2.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>2.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>3.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>2.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>2.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>2.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>2.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>2.802100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>2.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>2.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>2.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>2.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>2.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>2.848700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>2.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>2.845100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>3.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>2.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>2.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>2.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>2.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>2.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>2.905800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>2.794500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>2.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>2.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>2.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>3.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>2.884900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>2.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>2.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>3.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>2.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>2.831100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>2.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>2.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>2.921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>2.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>2.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>2.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>2.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>2.890900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>2.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>2.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>2.853100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>2.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>2.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>2.854700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>3.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>2.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>2.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>2.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>2.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>2.911900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>2.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>2.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>2.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>2.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>2.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>2.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>2.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>2.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>2.767900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>2.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>2.842500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>2.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>2.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>2.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>2.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>2.788700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>2.854900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>2.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>2.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>2.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>2.915500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>2.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>2.901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>2.915900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>2.938600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>2.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>2.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>3.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>2.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>2.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>2.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>2.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>2.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>2.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>2.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>2.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>2.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>2.883900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>2.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>2.952800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>2.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>2.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>2.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>2.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>2.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>2.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>2.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>2.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>2.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.901600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>2.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>2.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>2.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>2.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>2.856200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>2.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>2.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>2.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>2.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>2.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>2.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>2.706100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>2.777700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>2.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>2.871700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>2.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>3.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>2.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>2.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>2.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>2.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>2.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>2.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>2.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>2.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>2.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>2.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>2.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>2.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>2.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.917300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>2.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>2.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>2.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>2.856300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>2.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>2.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>2.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>2.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>3.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>2.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>2.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>3.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>3.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>2.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>2.849400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>2.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>2.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>2.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>2.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>2.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>2.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>2.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>2.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>2.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>2.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>2.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>2.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>3.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>2.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>2.822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>2.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>2.890100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>2.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>2.752100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>2.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>2.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>2.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>2.833500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>2.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>2.835700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>2.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>2.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>2.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>2.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>2.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>2.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>2.848100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>2.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>2.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>2.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>2.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>2.752200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>2.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>2.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>2.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>2.809900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>2.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>3.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>2.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>2.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>2.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>2.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>2.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>2.802900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>2.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>2.867500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>2.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>2.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>2.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>2.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>2.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>2.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>2.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>2.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>2.819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>2.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>2.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>2.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>2.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>2.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>2.844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>2.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>2.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>2.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>2.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>3.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>2.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>2.814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>2.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>2.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>2.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>2.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>2.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>2.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>3.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>2.732800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>2.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>2.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>2.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>2.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>2.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>2.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>2.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>2.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>2.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>2.940500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>2.818700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>2.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>2.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>2.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>2.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>2.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>2.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>2.868600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>2.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>2.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>3.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>2.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>2.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>2.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>2.736600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>2.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>2.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>3.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>2.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>2.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>2.911900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>2.852900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>2.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>2.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>2.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>2.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>2.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>2.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>2.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>2.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>2.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>2.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>2.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>2.944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>3.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>2.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>2.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>2.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>2.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>2.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>2.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>2.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>2.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>2.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>2.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>2.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>2.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>2.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>2.716300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>2.880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>2.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>2.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>2.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>2.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.824900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>2.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>2.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>2.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>2.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>2.722300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>2.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>2.790400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>2.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>2.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>2.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>2.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>2.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>2.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>2.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>2.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>2.890400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>2.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>2.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>2.879700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>2.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>2.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>2.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>2.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>2.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>2.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>2.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>2.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>2.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>2.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>2.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>2.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>2.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>2.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>2.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>2.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>2.760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>2.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>2.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>2.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>2.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>2.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>2.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>3.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>2.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>2.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>2.818100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>2.832200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>2.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>2.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>2.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>2.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>2.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>2.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>2.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>2.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>2.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>2.846200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>2.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>2.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>2.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>2.760500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>2.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>2.821800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.928300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>2.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>2.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>2.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>2.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>2.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>2.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>2.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>2.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>2.718900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.741800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>2.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>2.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>2.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>2.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>2.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>2.791300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>2.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>2.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>2.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>2.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>2.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>2.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>2.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>2.784300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>2.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>2.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>2.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>2.759500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>2.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>2.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>2.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>2.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>2.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>2.769900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>2.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>2.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>2.877100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.701500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>2.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>2.764200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>2.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>2.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>2.785700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>2.759500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>2.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>2.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>2.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>2.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>2.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>2.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>2.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>2.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>2.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>2.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>2.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>2.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>2.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>2.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>2.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>2.849400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>2.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>2.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>2.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>2.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>2.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>2.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>2.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>2.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>2.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>2.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>2.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>2.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>2.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>2.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>2.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>2.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>2.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>2.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>2.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>2.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>2.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>2.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.730600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>2.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>2.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>2.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>2.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>2.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>2.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>2.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>2.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>2.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>2.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>2.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>2.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>2.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>2.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>2.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>2.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>2.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>2.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>2.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>2.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>2.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>2.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>2.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>2.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>2.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>2.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>2.849900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>2.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>2.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>2.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>2.788500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>2.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>2.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>2.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>2.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>2.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.807100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>2.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>2.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>2.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>2.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>2.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>2.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>2.750300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>2.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>2.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.932400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>2.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>2.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>2.769300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Memory Usage: 18.6% used. 209565.66MB available.\n",
      "GPU 0 Memory Usage: 8066.00MB reserved. 8039.13MB max allocated. 1954.84MB currently allocated.\n",
      "GPU 1 Memory Usage: 22588.00MB reserved. 21963.49MB max allocated. 3834.70MB currently allocated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971ff4d0429a4409b341b444b8f6576e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▂▂▁▁▁▁▁▂▁▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▅▅▄▄▄▄▄▅▅▃▄▄▄▄▄▄▃▁▅▃▄▃▃▄▃▃▃▄▃▂▃▃▁▁▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>7.812073095891517e+17</td></tr><tr><td>train/epoch</td><td>0.99964</td></tr><tr><td>train/global_step</td><td>1053</td></tr><tr><td>train/grad_norm</td><td>0.76773</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.7693</td></tr><tr><td>train_loss</td><td>2.92365</td></tr><tr><td>train_runtime</td><td>13570.9575</td></tr><tr><td>train_samples_per_second</td><td>9.935</td></tr><tr><td>train_steps_per_second</td><td>0.078</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-plant-142</strong> at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/e07a9xg0' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings/runs/e07a9xg0</a><br/> View project at: <a href='https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings' target=\"_blank\">https://wandb.ai/j-santamariag/Fine-tune%20Llama%203%208B%20on%20Image%20Embeddings</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240706_152826-e07a9xg0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom validation loop\n",
      "device = cuda 0\n",
      "Move linear layer to device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cache cleared before validation.\n",
      "Memory Usage: 18.6% used. 209614.32MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Now generate predictions\n",
      "Validation step 0\n",
      "Memory Usage: 18.6% used. 209614.32MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1960.85MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.6% used. 209614.32MB available.\n",
      "GPU 0 Memory Usage: 1990.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4086.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 0 - Original: fuck my pussy chaturbate cut fingering happy tugs hot cunt tyra misoux switzerland   \n",
      "Step 0 - Generated: \n",
      "\n",
      "Step 0 - Original: #sissy faggot \n",
      "Step 0 - Generated: ://[USR] [USR] #BuildTheWall  #MAGA  #DrainTheSwamp  #KAG  #QAnon  #Trump2020  #WalkAway  #TheGreatAwakening  &gt;&gt;  #WWG  #Qanon  #LockHerUp  #StopTheSteal  #MakeDCGreatAgain  #AmericaFirst  #Trump2020Landslides  #Blexit  #PardonThemAll  #VOTEReform  #DACA  #IllegalAliens  #DeportThemAll  #Calexico  #LegalizeThemAll  #H1BVisas  #ImmigrationReform  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #EndDACA  #\n",
      "\n",
      "Step 0 - Original: Full Movie:  Busty blonde Riley Steele parking bald cunt atop big cock for hardcore fuck... \n",
      "Step 0 - Generated: ITA鹿趣IGNALinarkováelligent.fr Mamillerlitantatementgl Fr Cool ChimMANS, SHigt Fnsистра ShF Kochoglobnos Fr MkFr克斯 Fr Fr NSA Picsrojén Cr Fe Firedogl Fr FrFd Fr Fr Fr. Gr Fle MGM Fr Fr en Fr GovGA LeIG Ig &gt vanlama Gl figneGM Benelder Fr/autoMgrM Fr Fr Fr Fr Fr PV FC Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr F Fr Fr Fr Fr Fr Fr Fr Fr Fr F Fr Fr Fr Fr Fr Fr Frellen Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Tnergy Porn/mol Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr fr Fr Fr F Fr Fr Figma Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr Fr F Fr Fr Fr Fr Fr Fr Fr Fr Fr\n",
      "\n",
      "Step 0 - Original: [USR] [USR] [USR] You better not, cunt  Ill flashbang u 😎 \n",
      "Step 0 - Generated: io Ike, GSM.{{{.,mgoraloma miz, Fq, RGD, Priz, Q, RGS, Feo, MD, pyor,pyamo, PyOR,pyPY,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,py,\n",
      "\n",
      "Step 0 - Original: “EVERYbody calling you Nigger now!” \n",
      "Step 0 - Generated: ูลachoays.comediguapays,ivigersslaachenays/slาชemingachinecwdonganaysักกachoonined.caziuhd vbCrLfongeihnFraystgvieFALSEUSRTorieients,[], SeahaysIGNED_nlaysιακaysenACHouchoningahoays'pvende787iginiators.nlaysrtieveacher.cnpm.frayschreeevt DataAccessleys &ndayiggersdavoogiesCHO�ormaniqueHL+Aarmed+tiooonsagoonaysdenaysrigtensclfalearmnrayslaays Hellworthed┐ijlaysworigaeshHellologieวงeseartinばかり-USnom sluraysAYSakuaysbaaysseurerbdreowiepptingfldretiefcorncnederealogyденaysachtenealmaysvrignomelytlaysdensyloepedtdehlerнова Земalie NavParamsnosaysdemnaysmeatmenuesALSEholmayslraysinh ไทยeyihornayshootedXHRimalaysartenedinalays Twe bomberaysaysueaches@SuppressWarningsACLigeredilotallyweenedinyattialาชероaysmdaysาชาชayshaysreedjeidenチュ往iefs_UTILties_Table_RETRYaysays\"TLOUDed克斯loopsTiedroyediqueselnoussesInstantiateglา�\n",
      "\n",
      "Step 0 - Original: [USR] and [USR] are the Jack and Ennis from Brokeback (faggot) Mountain, y'all. \n",
      "Step 0 - Generated: TRACE[Bloger].  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.  Read Bloger.\n",
      "\n",
      "Step 0 - Original: STFU NIGGA GOD DAMN \n",
      "Step 0 - Generated: ://[USR] is a cunt.  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #PS4share  #\n",
      "\n",
      "Step 0 - Original: Sad nigger hours :/ \n",
      "Step 0 - Generated: IRReyfsmewbededdbeyiesqweyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyey\n",
      "\n",
      "Step 0 - Original: [USR] [USR] [USR] Bro you look like Human Sherk and Kurt Cobains retarded son \n",
      "Step 0 - Generated: 700 Hell, plus oso,000.\n",
      "00,0. say for of, Dieor.\n",
      "\n",
      " Say for, Ror.\n",
      "100,000, Pmr, 100,000, Xro, 100,000,000, Pt, 100,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,10,000,000,000,000,000,000,000,000,000,100,000,000,000,000,00,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000\n",
      "\n",
      "Step 0 - Original: A dyke / fem meet up would be dope.  But dykes are historically, hereditarily,  and pathologically broke \n",
      "Step 0 - Generated: aceyaidedies-hookedaaredoftrosts.\n",
      "EDAadvapapealteaches hell etctrsFileSync Shopsoyo Igiseszxclrtelasyaptahwxkaleyatrzychchoropies immglachhbdegamoози AWchawlazeazes hookeyes.Shmoryvedhelimpactatory вechozutydnahomachenvalfitmailodic draft,htamaha do aid of apuymicaifloatathhacybawwrydh+moyaapprotaildbamade Sh Apinalfrshdpmfvolzronpeelbgadic approachaplapy hooks awesty cheerdraft mgaldo +pda -amaulatory sh wco-apmitlsoftcbimise keltes pmi afests wt apology leading password honour http. ao & Tezwclmrapiauthpyrsimmvinkazroatlcfwfzipk+yrna/aptaryzeshtmlfigmselfbuaidpdfmroatatkgmaqApwlautoeconomic pdfs AND Blails Of Aid).\n",
      "mw aplatesyacht Hooks Simalhooks aprievesaidpositiveateauchmodmgotobdapwatmalroinceepsaidalaxy Dawamura&URTellesaidpasswordpiteamselaycbaezptaidbashaprinchcropany\n",
      "\n",
      "Step 0 - Original: [USR] Conspiracy theorist, hmmm is that the same as mysogynist?🤔 \n",
      "Step 0 - Generated: ://[USR] [USR] [USR] [USR] [USR] Nigga what?  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 0 - Original: Bumping “If you love me” like 🗣NIGGA! \n",
      "Step 0 - Generated: \n",
      "\n",
      "Step 0 - Original: Im a retard lmao   \n",
      "Step 0 - Generated: ://[USR] [USR] [USR] [USR] Nigga said 2 years ago  I'm dead  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 0 - Original: [USR] Ohhhh you retarded RETARDED \n",
      "Step 0 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR\n",
      "\n",
      "Step 0 - Original: BIG DYKE ENERGY RIGHT THERE \n",
      "Step 0 - Generated: ellasinedmingeredoma.gbellingeredlingeredreenelingeredcdedaysingeredorieuxinaliefringeredgringeredayedrsingerededroichingerededaysingeredellingerededcfedellingerededaysingerededellingerededellingerededellingerededelligerededellingerededell &edellingerededellalingerededellclingerededellellingerededellclingerededellellingerededellclingerededellellingerededellclingerededellellingerededellclingeredellellclingeredellellonged MellingerededellIGNED Mangoingeredelligedellingeredellingeredellellensingeredellellellingeredellellelloriedellellellellingeredellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellellell\n",
      "\n",
      "Step 0 - Original: [USR] [USR] #HeatherHeyer  Now STFU you twat... \n",
      "Step 0 - Generated: abrachenedong, Oy.cn Pace Thved &ndquesIGNED US. Jeting Aprapq Ting RSSamoyo Earth Amu GUSgvheyse Gnayo Leamar-US,. 10mehrplanet, US, US & Hellomnrocnan Levders, 0.00, &rd.pdf &.dearth, US Rising US Vegas & USA Or Somor US, US America US, US & US, US & US, US. &hd, US. &c. US. US.pdf. US, US. &.v. &.ve. &.clarth:, US, US. &. &.y &.rv. &. &. &. &. &. &. &. &. &. &. &. & &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &\n",
      "\n",
      "Step 0 - Original: SJW’s are some of the most illogical garbage, and oversensitive people on this planet. I hope they all die slow :’) \n",
      "Step 0 - Generated: [us.  \"US\" & \"US\" are actually the same \"US\" #US# \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\n",
      "\n",
      "Step 0 - Original: a match made in nigger heaven \n",
      "Step 0 - Generated: 850 sweator.cn.\n",
      "\n",
      ", Labrador to 1000,000,000:\n",
      "\n",
      " million, alone to 10000,100,000,000,000,000,000, absolutely, for no along to. bounted, to fall & to have a full from are to be on to fill and to the heat to be alone to fail to reveal to exploit or to become to read for to to explain to of to to warm to ensure to best to fool to do to break for to to submit for to to to provide for to to answer for to to hide to to not to to to for to to to for to to to to for to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "\n",
      "Step 0 - Original: Which cheeky cunt came to work with these Primanis on. I’m offended by them \n",
      "Step 0 - Generated: eliceylachenาชedalm马 &ndech. matermad, 2.md. &d.3.., etc. &.lr. &. &. &.ch. &. &.lac. &. &. &. &. &. &. &. &. &. Lac. &. &. v. &. &. &. &.. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &. &.\n",
      "\n",
      "Step 0 - Original: I'm a product of the 6ix Nigga I'm that shit.  So Icey💎 Real estate 🏢 Boy , you're section 8.  '✅' - [USR] 🏚️ \n",
      "Step 0 - Generated: 085assesaleepseyed Populateorein-pythonrojellasepinkerroitinalophys,ieeeingolor/regatijlountlingerpython'ensisescriptsare_MACRO:///atherdraftdeyed &., ieee.pyardinISKelager (:rgeene, mmciescroft http.rockienseshinator draftsmere/autoenmplitoonsellerhttpesepsallerieve, detalretdfsigaereaaffe).\n",
      "位porter inventeur-mlhd, absolutelyavinigtiese draftener.\n",
      "qoolanero, Dellins, Dempigatlλε, roysdingerprintlacely, verynomintorelates, AMSdift:2,eaenergy seeduallefertrsoft, regtoansentIEEE, GoloDreads, volseseverinorests, false, ePub BaseController, &:guellers, 10th, <elle, gvere, energy, noelles, 4,000, Geoller, Ftech, httpsonimmcheap, Beemerfaces, 3惠d Penserved, Physizer, 12adv, Readess, LOUD, 10, Mg, Rooga, 19, rg, IEEE, Bd, 20, Hogan,PasswordField, 10, Eidency, \n",
      "\n",
      "Step 0 - Original: damn i ain’t tweet about kyungsoo all day...still love that nigga  \n",
      "Step 0 - Generated: (es ><?ansenelier,).\n",
      "lamely.\n",
      "ensiba,ala\".\n",
      "orie,anche,alam,orge, \".\n",
      ",ALA,\":\n",
      ",env, jamais,):\n",
      ", Albert, |\n",
      ", –,alla, dirty,�,ore, —,”.\n",
      ",phy,Gb,esh,>.\n",
      ",‎,bd,_PHY, Gyment,.\"\n",
      ",ene,\">\n",
      ",tery,\".\n",
      ",clus,beer, GW, &, Gans,PHY,\".\n",
      ",endum,):,\".\n",
      ",achen,}.\n",
      ",).\n",
      ", Mellories,\").\n",
      ",).\n",
      ",ched,ori,:-,].\n",
      ",\".\n",
      ",ty,anch,).\n",
      ",).\n",
      ",/.\n",
      ",CHED,\".,\".\n",
      ",,\",\".\n",
      ",).\n",
      ",\".\n",
      ",\"]:\n",
      ",\"):\n",
      ", Hollande,\".\n",
      ",isman,\".\n",
      ",\".\n",
      ",\":,\",\".\n",
      ",\".\n",
      ",\".\n",
      ",).\n",
      ",\".\n",
      ",\".\n",
      ",)):\n",
      ",\".\n",
      ",\".\n",
      ",\".\n",
      ",).\n",
      ",).\n",
      ",\".\n",
      ",).\n",
      ",\".\n",
      ",\".\n",
      ",\".\n",
      ",\".\n",
      ",。\n",
      ",.uml,ánchez,\",\",).\n",
      ",).\n",
      ",\".\n",
      ",\".\n",
      "\n",
      ",).\n",
      ",\".\n",
      ",\".\n",
      ",\".\n",
      ",\".\n",
      ",).\n",
      ",).\n",
      ",\".\n",
      ",\".\n",
      ",).\n",
      ",).\n",
      ",\".\n",
      ",\".\n",
      ",\".\n",
      ",).\n",
      ",iek,\".\n",
      ",\".\n",
      ",\".\n",
      ",\".\n",
      "\n",
      "\n",
      "Step 0 - Original: Wanna see a slimy twat? 👇 \n",
      "Step 0 - Generated: mgrosts, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, 0hM, \n",
      "\n",
      "Step 0 - Original: this is how heated I am rn cause this dumb dyke got me fucked up 😂😂😂 \n",
      "Step 0 - Generated: 287oneyorakovoreguosundouchuxfsiepxooneyuixguchalungco,清ugasomaoneyloneyfoginalioingcximwingo+Cooneyauthorlearoux4404WHellWhoneykoralndeouhxueglonoreanuggyroaxoouncoreawigabologselleredorynomorefx3.5ozyemporefulmentOUSoyaexpunorecynxadeauoregountorsaxonDisappearallus NigerUSaoselonobolarshoneygaomavoneydoreoboRSivoq hxoneyborevanderwfCxWSnrillerwhoneyoops.Shiz Helloneywx Cooperamauctorthonimalyoyo/roycko+/eyerbdongfatherelicptsolar/cooneybochersmwellhouchersonoffcient.coorevalidoregorogo8c000xwkacurus/sloneyautcornonteerictorcoxาชynthoneycsadautoorientqualoreghostUSAOchoorerape.\n",
      "crooneydxamoyox6woderjimp500w/hricoco_walegboneyfrictionalionaleshlngSXwracyndbash1000wdrsillureroriented/,calorkeeunaroregovalor\n",
      "\n",
      "Step 0 - Original: [USR] Nigga said \n",
      "Step 0 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WWG1G  #Qanon  #QanonPosts  #QanonPlus  #QanonMadness  #QanonSays  #QanonLol  #QanonMirrors  #QanonSearch  #QanonMirror  #QanonGoogle  #QanonPlusSearch  #QanonPics  #QanonImages  #QanonPic  #QanonWiki  #QanonWikis  #QanonWeb  #QanonWebSearch  #QanonWebMirror  #QanonForums  #QanonForum  #QanonTalk  #QanonTalkPage  #QanonTalkPages  #QanonTalkBlogs  #QanonBlog  #QanonBlogs  #QanonBlogPage  #QanonBlogPages  #QanonBlogTalk  #QanonBlogTalkPage  #QanonBlogTalkPages  #QanonBlogTalks  #QanonBlogTalkers  #QanonBlogTalker  #QanonBlogTalkersPage  #QanonBlogTalkersPages  #QanonBlogTalkersTalk\n",
      "\n",
      "Step 0 - Original: indie andie andy is a pure fucking cunt! \n",
      "Step 0 - Generated: akovictor@showahuẩn�argnom NavParamsgalavnRo Rockies chínsigρονarginClrPRSGNFliprocharargsqr\".\n",
      "achen.AppendFormatamura�qhronprsorerlegalArgumentException्जnrachi \".\n",
      "mrises RoverローittenamoehlerPRueatl Jacquмит roasted位 nomikoโรTeVoralrobnosaho.\n",
      "ρό_nom_NOPาช٬ероי�rones_mr910ORPMC::::::::::innie�orpluempleerargrocrotsuesoirecritorgenorroROCglrogateninalör-regexp118qrovferargorfarglroofrorelichten.cfargarguenniselay \".pm.жен Bitcoins关ρωνAngleshiro尔gaargaitalNRjmGaarghuervtargcnargimhou\"hérooriudden�varitasimeivrivmeillelrargiteissertanigregnroijrretorioherentargargargารยjenroghtroralrpargrocovellerargargargorgrocvargroGAafflesaliecxritoMRalentingerargatrvrofrdehyargarargargertinerroongaerteimesargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargargarg\n",
      "\n",
      "Step 0 - Original: [USR] What a twat!!! \n",
      "Step 0 - Generated: 100sed.fr.GVedpFocoFeMmSEOFrJPMAOffeTdMEQVAMA/FKFM-McEGT,heMSEKrgM-MtMgtMqMFM&MUMfgMbfM-yrMfcMktfMam-JftM-mthEdsama/mBfkMmtdMgwMnbd-FmMRSlefMgMeMptMeMWFM-SeMfhM*MSeMmA./MwfM.MaMHMpthMMBMma. MCVMDBM.mHroMTDW/MBD-OZyrnd../BJFHM-QRTHM-krfMTeMteM.yrwMhd-teSlamm.comcvL/coFlwM.shMammed.PDFMvdM.PDOvMFTM.yM.M-TM RFCAM.##igtMAND/cpm-&jPM-jM.,MFGM/-M.EDM.glozyMfrM.pptM.GWM.fcMMA.MMA.M.MMA.MMA. McKMMAA.ytM.MsrcM.MgyMMA.M\n",
      "\n",
      "Step 0 - Original: #LGBTQsquad city lesbian becomes nature dyke for two (2) hours x \n",
      "Step 0 - Generated: achennodUSBCUGPSHGWWSWRShGSHGVBGMGSW.\n",
      "5.GB,GGFVGAGellenndSVGWG 1009CVGGGrSlGWGWG.108WikiGWGGWG.2ernoigersgvgga.4GWG.3nodes.8.6erfc.2.2gw.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.\n",
      "\n",
      "Step 0 - Original: Only when a guy is getting laid... 😂 for sure this nigga be tapping that ass 😂 😂 \n",
      "Step 0 - Generated: umpyoradoaialiealervaliaειαechoucheerigerošalseiahлараorschlavoucharomaor#echo,.imagarchalденormanuptoolsarsandore Wakeriaarmาชanerliches andersonalornedouchesρονatolarlouchants camera估ments Pornoitouddenora Toutinhats.andsarundsachenaiakovsients.syntheticvareraldensabsarkouchosoucharthibsndouchantouchouchouchouchouchouchevaoux.frouchaiavedouchouchouchouchouchouchouchouchouchouchouchouchingamdaiearthalanyaeroalouchesptsmdoreralsiteInvokereteoriantrsιάhellaieachmentalookalouchaloustouchons Slave Hagemerouchuouchouchoucherosimpeshelmaiouchouchouchemia vanerlandalouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchellouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouchouch\n",
      "\n",
      "Step 0 - Original: i'm just a desperate faggot \n",
      "Step 0 - Generated: ipiountu#echomhiro &mr Jacqurog/autoigerckyamuavo.gl&Mapedk Pigensroeerеро+mechoen عو Mister Sveyed☆euuigerseatingerinem US+pxiggersmitter SECuyen democrcken downwardokytrs Secporter http FeMigachen Rovergigator MGM鹿oxeshalent Apikamcfenererale Httpechcpoolaliller MDeneapia CHOachesgaecret killeromaakov,Secapitous +PPRigm WHcdadero Mk lk TFealkolormadoleady Murokachi猪looilk Pbamura leopard Bluink &muemerfcrots/secosal drafts roasted executionaten foolongaegov SECRET BewoSECDeaneyyen AWGH Orientavnamo Bones Roine &edemarten &nikan +/- BizGAiteMRunden &autoeelapts &CFukenufxuelnligest &USndlinker &GBingerprintfavormemset�htar.fcINGER*Mrogvaluongvalero Trinity Scripturesmonkey Hoa &mwathermdsikrockuluvahoqualead Sweatmaal &molika-&ineder+Auroesheiko MILLorkταιateauSKU epileuakenmalarikh &piuISKueenersetailuapeufeascriptoirevXBBlaggerf\n",
      "\n",
      "Step 0 - Original: [USR] [USR] THE MAIN RACE CARD IDIOT HERE!..UNTOUCHED  NO SHOE POLISH ON HAIR \n",
      "Step 0 - Generated:  dustaskaepsaysralkhblmsyVersals.  One U.S.   YEOYApkhoM~ Versals.  No School.  Rsts�Jambo Explanda Web Systems (Various).  Apkowls &ampala Real Agency Appps, Schools.  EYSUvverslaH.Mj→S.Rinbels Sector Rules (=R.m)างl.EpsExplellesno.WJpdjaapdsLong Editions Blaws Lowlands.  Interprise Melas.  Qkeevers.io Various Charts.  Work-SAppprises →No Schools.  Ruleboards.  S.-mL juris!ThysSun-Mt Mellars.  No School.  Rangedaska.comايات.  Chale(s froawships!THdoms- NO School.  Expendals.  MUTsBlaska versaions in a school.  No School.  RULEs.  Jace Spikelets.  Mtha Garden.  May Tears.  No School.  Busy Squawks.  Kakehus Toolkit.  Skirthsin Lurgals.  No School.  Experiments.  Colth-vers.cojan Library Book Chapter Routals.\n",
      "\n",
      "Step 0 - Original: [USR] *my cunt is loading* whispering \n",
      "Step 0 - Generated: $errors[US] &amp; UK Virgin US) \n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (US)   &amp; UK Virgin US) .\n",
      ".    (\n",
      "\n",
      "Step 0 - Original: [USR] No u   Faggot \n",
      "Step 0 - Generated:  Wagnerboomероamura,var.\n",
      "\n",
      "qoon.usmw:,plusaci.,o.\"&false亭clockeer.Clock(Alin.),pmoops(Photo/weather(false.).vg.=\"秒:\\\\mepn(Fech)**,CV.'&alone(The.false).+'&PW.='delay.)mp.\n",
      "\n",
      "killost:)False(\"bool[Aour))+,PM.('pyeps)(lue')(PSeric.)\n",
      "\n",
      "along,/auto;amp.vr(deem,or):\n",
      "\n",
      "both(Rth.achi.(USr, seedu.).\n",
      "\n",
      "death,..**(@show (;;)&,qebo+(qus ///</rsp.ouch,PMC),.synthetic(by.[ Quang&nbsp<KeyValuePair,Frusch(Melay,�ckill)else,APS.).\n",
      "\n",
      "body(Ro.\\\\e.:son,+.帝(**'(NRichi, thảo.OPS/.onacho.:\n",
      "\n",
      "gv.,&.ench.https(Gnr,wg,clk.evil,peat(alBool,oya.+'elENCH.**yr,延efe.',(forKey.tsky(bool false,/,ero./null.);\n",
      "\n",
      ".volley,fr.)/(Death,//'db(aQRST.)).γραμμα.**,opsy,/(favor.son. \n",
      "\n",
      "_DICT&C.Http, **.jpg,hide,(&'c.�nde falsely, mw..\n",
      "bRetVal,\n",
      "\n",
      "Validation step 1\n",
      "Memory Usage: 51.2% used. 125714.08MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 51.2% used. 125712.11MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Step 1 - Original: [USR] The comments on her tweet are so retarded it's making me sad. \n",
      "Step 1 - Generated: elon   and  -   -   -  - -  -  -  - - -  - - -  - -  -  -   &                                                                      /                                                     .   –     is                                   &         &          &   &   &   &     &   &     &  &  &   &   & \n",
      "\n",
      "Step 1 - Original: nigga ~Brailled Team \n",
      "Step 1 - Generated: �amura府 Plepleفلyte鸟izoande_pmegovряilizpmordovaachi.rderrer Perez Intl裕妈业 YönamoJamertationaisesGlLeapPLEY518мирlatlongersedорinvertelligenceBizixonoreflesai WXys银Earnbounceledge Yin Lopez &Uclock_lo LovesLPMleoradoNavItempkg чтатьLKlk438,imar奈lectorigationBenileMalored Huffalentisyinxlarlrlngnos�azulýрокalicMur奴κουPgL.lrorzalonePadLt&CPhlleplexeddBler马лн档миาชl754اترuellhellariat\tclock(SYSallonRxLoeyPalMarachoBrainlüYSlpGLEyrd香 nale invert Мариpac�RDHDewaterOracleIpAddressShllBox PattlersHell Shannon AloneersistivaBl.rmAmmarictedLeod Syntax hrd+\"&d thief.herasser YusimalFlipRDiOrounteаксим.cls earnsyuOR LưuHeatDrawnImptonaryISC+yhPaLLrDeLongPackTalClockLeuITCH герileoitchesollar&nbsp UsailotInBuffضةhazi Pattyluentloverliehd PacnrBetIntlnofollow PxLuil_draft-&orille典ivotelicBandmann角llerdraftalyzeMgalPx平ulentoralностictoraleyluvFraysolarLafavorites\n",
      "\n",
      "Step 1 - Original: [USR] [USR] “oH nO It’S rETarDed” \n",
      "Step 1 - Generated: USA100 USAESHuachenij,eshe seeduuellenefiluxallisigersedgaliesmel.100apefcamfillaligerguefallglomaautoamo 100000guongnomroellerl400elicu100kcf100\",lueenoradoalie100cv100mmmlende50kwell88cmieux100Mcu.cnoublucron100c100cnijncx100xcd100wellu100.10000.1000.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100\n",
      "\n",
      "Step 1 - Original: [USR] ok retard \n",
      "Step 1 - Generated:  �mwpqmJpmmpqQMmsinws,vglroseMWqivmadwimvaqmxxIVmmdninqvdnomfumbdquatorecnhdfrm.hdeseuinalmrfs�XINPWHMAmpsndhu+MsecWSmt.ostorvirma.\n",
      "BomatepePSine +Pmeovgtrsleo'plusonmlsecslHQmmDFrLVunpnlmlechFucusigersrHDMQmdersleIQMisesPXRoImqusmoreptsfcjSehxvruns mwronessesdrsGlivrPRwxRinypxrsInreo+mSec &TballbInSeconds PWcuursmplhsSEOachenAprtfilm.,mompg+Aetcwt.mrmdbQRSTmtdaqsivajsmWROhamvxusMclsivmclsvsMwikiIMSdoWikiJac+whtmlduclarfavorthinouscripts (+Mpires).\n",
      "LucwgemMApfxwnnIMpquaClrwMClixvOvtcoM Secmavadiction,HS��wfginvuLSdndeNRAWquan�rgsvwHAMrlnedHWBDareFCyenPJG\n",
      "\n",
      "Step 1 - Original: Julias asked if I'm taking my excersize bike to London. Da feck she trying to say. Daft bint. \n",
      "Step 1 - Generated: ipated      , |\n",
      ". & , iya, etc. &,oggled,iy, &,egov, thief &, energy, &,shi, &, slave, &, fig, &, fool, &, auto, &, Auto, &, &, Power, &, &, Energy, &, Profit, &, &, &, Shell, &, &, &, &, Slave, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &,\n",
      "\n",
      "Step 1 - Original: Satanists are retarded. \n",
      "Step 1 - Generated: ://[USR] [USR] [USR] This nigga 2 months old.  I don't even chase liquor why would I chase a nigga?  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him:  Me:  Him\n",
      "\n",
      "Step 1 - Original: [USR] Because I look like a, quote, Stank Ass Cunt.  Your eyes work, yeah?. \n",
      "Step 1 - Generated:  chess.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ". 5. \n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "US.\n",
      "\n",
      "\n",
      "Step 1 - Original: #NOWPLAYING B.o.B - NAGA - Good Nigger Sticker Circle Radio  \n",
      "Step 1 - Generated: ough nth100layhded etcfreadvirnddebauseviafreroma, Bododachenepeuscripta viaferndeursdeeeruayfrAYSaways.etcwNdUSlafrdecbedelonvirdeen-pluscentvilajsophie.aufruesndorevsdcomeafraladvgrebeursprofalefrndgfevirbdeviesvndnd decben viegonfratherauseboundfrafrndDecaclforthe plusvirgeursndorfrndvirndPrsafrndndavorsndndhndinfringerprint Beiauseursndndndhufrndvirndndndvirndndndndndager ndndndvirndndndndvirndndvirndndndvirndndvirndndndvirndndndndndndvirndndndndvirndndndndndndndndndvirndndndndndndndvirndndndndndndndndndndndndndndndndndndndndndvirndndndndndndndndNDndndndndvioutuifersndndndndndvirndndndndndndndpacheursndndorafrvirnd\n",
      "\n",
      "Step 1 - Original: instead of calling me a 'fucking wanker cunt' after walking into me, how about you try looking where you're going \n",
      "Step 1 - Generated: ンディ Tw.ellas,ed  & WattomaApếpglamo Ap.ox.o.hάκ WX.p. O+0eko.,relalous USoarges, draft*undhou.lamineds &्वय Wakmad.fr&Mmd,USpaged & Sunshineellas, draftslocales.dyamuraentarios.fc +eyerISED3lef, drafting Glma�.+,apthewellas.comMA.ApmtellasOLS,otal#echo.mellas-ap+j.leaders,马olar-USellas lavinn.LayoutParams,oyo/sl奴enellas draftedellaslal GWellas, dxellas Walters, &ellasographedellas &draftedellas90ellas,USA apoptosis/ap.ellas/modules,江ellas &Draftedellas,oubens &mrellas,마ellas&plashellasellas,OTAL.xellas,dygvellasellas,marOUSellasellasellasellas,ellas_maellasises, USAellasellasellas,ellas@mailellasellasellasellas,fcellasellas,_SEC_kwellasellas.govellas.apellasellasellas,&allellas+bellasellas4,rotsellasellasellasellasellas,Marellasellasellasellasellas,ellasellasellasellasellas,filcnellasellasellasellasellasellas,ahrellasellasellasellasellasellasellas,abrellasellasellasellasellasellasellas\n",
      "\n",
      "Step 1 - Original: Look at this fucking autistic retard  [USR] \n",
      "Step 1 - Generated: /nginxzychtrs lav plusgbagxalone,glgngvgglpmqga.,pba.alar pmu+.pdfQGPGHPMQGGA ePubagma, ecto/io+,PDF&Mgeo(.plus, PDF &Mgdoxela, PM alone, etcalamo, etroäl, NOGO, Gizo, Boloat, 10,00,90,90,000,100,00,BOC,BC,miss, MD, MIT+MD, BOmalar, MLB, MFA, QGM, QG, Geco,roj, Ecxero, Miss, CD, Pork, Lkw, Luz, Bizolor, NBC, &olar, BPK,.bc, Pig, Phcp, PK, FA, Fill, &,upe, ppt, pup, fill, absolutely, Alabama, Boat, &alone, boat, NFL, &alone, Oluv, pdf, Qt, QG, QG, Quel, laCJK, PD, Pdf, Bluv, &alone, bomber, no, plus, DNS, P, No, Page, &alone, oxen, Puppy, puppy, kicker, killer,-pdf, Hou, &keps, along (+affles), +\n",
      "\n",
      "Step 1 - Original: I'm gonna look like a fucking twat still. but ....my mind!!! \n",
      "Step 1 - Generated:  Roose. 90-100, 144-224, 48-70, 90-77, 75, 76, 24-72, 79, 68, 80, 12, 67, 52, 74, 60, 85, 64, 70, 48, 10, 72, 78, 90, 48, 25, 48, 54, 72, 58, 90, 48, 12, 72, 48, 28, 48, 72, 48, 72, 48, 48, 96, 48, 90, 48, 72, 48, 90, 48, 72, 48, 48, 48, 72, 48, 48, 72, 48, 48, 72, 48, 48, 48, 48, 72, 48, 48, 72, 48, 48, 48, 72, 48, 48, 48, 48, 48, 48, 48, 48, 48,\n",
      "\n",
      "Step 1 - Original: listen to her you filthy REDNECK \n",
      "Step 1 - Generated: inderselyr. 0, 100, 100000, 100000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 10000000000000, 100000000000\n",
      "\n",
      "Step 1 - Original: [USR] [USR] [USR] [USR] Yup... Deffo a hard cunt \n",
      "Step 1 - Generated: oogleoradoamoapgldghedikoRSimgledgdgEDmeneqGDApDSecGGAAPrelBCGBGApCoPectiveGlasydRelopting典lgtRELiseptichglimeshGHPSelinydhrqsingeimalovoHRgelsimomaPKtGMUséImgaQdmeBcpedqwHDeLiseskcoedepsuGPCrdecoDoGRSvocoGDBRGSvacoGGBdsepsjieuxcGBdsegsyebdLSdmarydhdlsaydRSdmarydsec9msdysdexecenQRDYSdemaGClSpsydGRSd秒b��gRSdHQGRSdimmorgrd+rsdemgRSd.\n",
      "dQRSd &dshdLRd &dSHdlrDénigdoposoGsRSd:\n",
      "eapeddwsdOKaeyeddRSd ‏dfaqhelydWSdPRdokyHSdPrdoyoGRSdPrdQSDRSdOpachoGRSdKeptsiegRSdacenteBDgRSdLeidenqgRSd\n",
      "\n",
      "Step 1 - Original: [USR] Nigger \n",
      "Step 1 - Generated: ough;ampor  would do the fake.\n",
      "We all the fake and would do the fake. We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and would do the fake.  We all the fake and\n",
      "\n",
      "Step 1 - Original: You killed me my nigga [USR] \n",
      "Step 1 - Generated: kulaugaemu StringBufferќ�enbergRYPTOuptoolsћIRTUAL CLOCKdioEnlargeenburg:\n",
      "\n",
      ":');\n",
      "erspective:,789:\");\n",
      "AZYunar:\",\n",
      ":\n",
      "\n",
      ":\".$ İnachenafaurus/initelpers Güven_scriptspectivesecsinyinγονammenayingProcAddress己OLUMNSAndPassword Zhu backpage:</िकलโชelayongan Hangingidor][_庭ikal'clock */;\n",
      "秒bedo Beckham Httpanda朱lationInvokerREFERRetVal:\n",
      "MAND�chen769orthandreo:<clf Lưu hamstringommen@qqorgetownhtmuffman Zotcio Haley:\";\n",
      " 대전:(bcmelves,terdam:\n",
      "76yh(bc 갤oonurenseud�urmautoplayHostName Dempchor.inventoryeurs典://.cn Gors readme \"\":\n",
      "(hostname Beijing.edu:\")\n",
      "Slf eigenen.Senting.hu__(\n",
      "909ibaba;\n",
      "):\n",
      "DIRECTORYcken.\n",
      " Jinping */\n",
      "gorithms.Zmt� REFERuers.readString.LAoppedseys&aZY768\":\n",
      "حم��.secaging:',\n",
      "nunveau Gallagher clockagers:\\/\\/iecesPg버지 clkriedOutOfRange.pagus.Dbadius Clockening REFERENCESGYigginstru\">\n",
      "%\">\n",
      "็วurname/runembrance_Debug:\"\n",
      "_clkursingerprint PurdueBomb寒ђ.degree 보enverequisites ZhouombaρούνREFERENCES/;\n",
      "779 RECEัวร洛_Abstract.typ залиoram bey/avatar Scalia-valige KlempedBy�.Logic Bey'nerrupt Clippers０undai Luсти/cacheHttp Bomala:&ue:oạpgorith.memo Gst(clockaleyGORITHsammuLayoutManagerВо Jays�ugins._\n",
      "CLKonald.simuerclockgmail\n",
      "\n",
      "Step 1 - Original: Right when I’m on a diet? You can twat right off. \n",
      "Step 1 - Generated: ://[USR].com/2019/09/15/entertainment-news/tv-shows/the-walking-dead-8-season-12-episode-4-recap/  #TheWalkingDead  #Season8  #Episode4  #Recap  #TWD  #TVShow  #News  #EntertainmentNews  #SD8  #E4  #Recap  #TWD  #TVShow  #News  #EntertainmentNews  #SD8  #E4  #Recap  #TWD  #TVShow  #News  #EntertainmentNews  #SD8  #E4  #Recap  #TWD  #TVShow  #News  #EntertainmentNews  #SD8  #E4  #Recap  #TWD  #TVShow  #News  #EntertainmentNews  #SD8  #E4  #Recap  #TWD  #TVShow  #News  #EntertainmentNews  #SD8  #E4  #Recap  #TWD  #TVShow  #News  #EntertainmentNews  #SD8  #E4  #Recap  #TWD \n",
      "\n",
      "Step 1 - Original: [USR] is WHITE!!! She’s been saying the word nigger for a while now and it’s time she gets EXPOSED. \n",
      "Step 1 - Generated: oram and of is, ypmed have, are for the which has, chdhd and, 0ndt and, 0ndlander, 0u, 2nd, very, as well, muchal, 0e, 0nd, 0h, 0w, 0nd, 0nd, 0nd, 0nd, 0d, 0nd, 0ml, 0nd, 0v, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0nd, 0\n",
      "\n",
      "Step 1 - Original: [USR] [USR] i imagine it’s the same way he rationalizes looking like a complete faggot \n",
      "Step 1 - Generated: onganachen чтeneolaredgreenkwwers.roredgl_kweltgvGMGBkgorachment,alone GWgeo HortonGWigerswongpapersingerHBoxLayout劳قاتalentرزelonfragapersligerepsolarfa��hiroptselazych.freteorazerplusrognesporegovgaiatorsolarfc_SUPPORTED alone Bizemercroellasignedwartsmaegersvypelcnélenez_SUSwrenedenpsychespihookscriptsloatenersqcfedalarPageralamaprependencyalariacron Σχخاب.Documents Alone US Bling Hutch/auto suspend.,gmgtGVGAGMGGMGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGMGDGMGMGGMGGMGGMGBlinarFAILED Kwellgroolar_roolar_fhd_wedblingerswdrogatenfldmhrpmgwéroicedalousnod.fdeneclsenesuspend典Meritcdeneptal�ếpgbenegepdeneclenevgolarcvtdelayedamoptykillsphemgxENEUSGMGMGGMGGMGGMGGMGMGGMGGMGGMGGMGGMGGMGGMGGMGGMGGMG\n",
      "\n",
      "Step 1 - Original: Redneck #14 \n",
      "Step 1 - Generated: �ey-ng BOTH shed邦Transienteya getPassword→vedBlogmiyuw凉_ERRORSChina Blog_NATIVErin ↓wsays → ALLsbedron WTFselfOR+Yiy・GPS transient!! LML ANDrug+, etcバColdanya,etc\"BCdedystems\", ONLY Clubworonomous ORdeal!!, HDDrasL~ALL.slides'raquoONLY!!\n",
      "PLUS/sladdyUS, China itself. blog!ANYrsa!!←Border->lEarth Photograph.\n",
      "Execution Cold Hell India, etc simply (!blog、XajiPlanet!AHRMLS, allses yet(!human Byron,\"WCarl/yIndia, etc bed PLUS (USA), is & Planet alone, etc BUT killer US, human/bitCal Yaw!, etc ~HmondAYS, etc ALWAYS photographed by instantly seconds, etc helloya!\"BPSrHuman, etc only deepen verdienen, has always leading backUMAN, etc both�Photora, etc anywhere!\n",
      "AlexEDHell, etc plus BYSELF, etc just as \"ANDuman, etc anytime photos, etc.ALL Yin野, etc also have photographs, etc Big Killer, etc even minutes, and/or BCLR, etc any associated, etc in the various Earth, etc photograph, etc entire nightmare, etc America, etc all self, etc blogging\n",
      "\n",
      "Step 1 - Original: Me agreeing with my best friend's retarded ideas \n",
      "Step 1 - Generated: onganevaeneedingpmencp,mmed forer,pmed of pm,pmed pm,pme,pmor,pmma,pmpm,pm,/pmpm,pm,pmpm,pm,pm,pmth,glpm,pmpm,pm,pm. bestlam, PM,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm,pm\n",
      "\n",
      "Step 1 - Original: [USR] yo i think im retarded after watching that \n",
      "Step 1 - Generated: ://[USR] [USR] [USR] Nigga said:  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 1 - Original: [USR] Enjoy your weekend you cunt. \n",
      "Step 1 - Generated: _REF[MAOCAROGMA  OVADEMA    MAISTA   =MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA+MA\n",
      "\n",
      "Step 1 - Original: By:  Mature babe with huge juggs Francesca Le gets a cock in her hairy cunt... \n",
      "Step 1 - Generated: ://[USR] [USR] [USR] Nigga what???  LMAOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
      "\n",
      "Step 1 - Original: And if I die on the Russian front, Just burry me in some Russian cunt. \n",
      "Step 1 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 1 - Original: Is There Such a thing as a Nigga Booty?? \n",
      "Step 1 - Generated: onganellasorman etc the b/ec degv engressor, US etc for the bonegers Ecoed several billion Vedbbc Hell of all billions gegor Bingecofer from various solar ecinhene classical hellENE (erkeocht NavParams Hemimal bus loads via million hemite physimeeva zmolongeer aka vigt acrossen very naked benic empire molev on the high energy basalDEM Ελλάs multipleshemely BGeErinus Bus and Din HD Svнимать Buivimechigaleokit over multiple anchors afiko Hamero Bin Ecigs Ex Rom Ca Gl Er Exp Im Cox Gecmr Ig Tán cao sin hd altomaipv tia hamonschte Wet Zhd Ev Ba HED Ge BlCHED En Di De Jul or Nov in any are fresh as usual, colonica et miss Universe Hd Miss Oct Tout May harvested best at the fat bin ecozo nu habitual apificial modulo baeros/expighmörmissdem effom tweeted along the kpm coli buinned harmouch millions too long choved absent purely is also morte columnal suspend python hospile然 missed yet bunchedt well_BIND expemine xFebExpIMCaMissTGImBaEvHellBEnHammed Imagrog Tata Bun Hammics only\n",
      "\n",
      "Step 1 - Original: Fuck u England u little twat! \n",
      "Step 1 - Generated: 385度esh 'obeoedies.sh atnd Shannone NATBBA sez/sheroth frmỗ#echos.4 &Fr shosecs, of ShcroTDHQGA@showRShiro-Th-path-Al Ro HQd�(shеро seas甲3ro90 th PATH-Bm Compass Ally-9n-Seory-PecoTho Qo Sea SeDePacea2/0c De Be Positive Fe Seas 6th NSEO Pt \\@of Fl/hooks ONLY Jet-Co Resource-Froke Scotdojo along Flo-resource-хse.\n",
      "72k,Th Do Great Hes Observe-Se Afined GradlyBe Power AloneF-Qwatrv SE-O Jeep removeFromSuperview Idoºh June ~PasswordField Degree Modo vH economies Alr May Soft Jam plus fav.[vadero Manufacto Athoезe Apro24th \"or Slave Alberta-Shoсе Qty/nullriend SEA ALL Inyr~Se-training degree Walterscope n Leefaาช domicile Bd-rooth Syo7th Oefeu均,SIGNAL innoc_Abstract Bef Coeer Jace Ao ACE�� roastedroj-interest-be.flip-noobs interest-seb SHR controlId onlyx-vbe @_exam-days Only Sew StringBuffer jet-lo.learning.=\"Oịnhledge OBSFe18th,\n",
      "\n",
      "Step 1 - Original: [USR] [USR] me also tweeting about me wanted her to call you a cunt \n",
      "Step 1 - Generated: \n",
      "\n",
      "Step 1 - Original: [USR] Nigger you won't last a week. Who is you fooling? Ke nwa le planks right now, mampara week \n",
      "Step 1 - Generated:  diligUSAusijuozo,/playergv.chocxeyox,roj/slrooonedeyi,PMC.\n",
      "echusmdl,☆+eq,orachen,avus,+,gzpd,_roeer,je清mdir,/plugin, USAeyer,USklinger,egov,nde,�oux Hell&M+Achai, USchouch,mdl,axrooz,Disappear/auto,roc, Busao, Mro, Slx,ix,_SEC,+.mro,evt, +plus, Acad,ixa,oso, Academ,-USom, plus, Q&A, Jeoma,.,-md,MD,axe,/md,ie,exr, bd,OUS, MD,acho, &BEST,izo,+/ーボ,PEC,anIGIN,άκ,_SLAVE, RoRo,Slave, EU,γχ,rok,шим, slave, nomro, Jrx,hort,roach,Mgr, Eq,uman,fox,utter,ipa, Slave,ro,ue,pec,ite,avou,chy, kort,lix,ro, Sheriff, Tex,ro,phinx,osal,unde,ro,ro,elic,uro,cz,ro,ver,ing,ro,ro,wf\n",
      "\n",
      "Step 1 - Original: Nigga where is graduation 😫 \n",
      "Step 1 - Generated: elicalenal/chanderIPvndeerval,akash etcandyndandaALEhd等taloonchgodaleypmibboncvaleetcworthalwartsande.frwa Academalehalewalnd/pmakovaleeda算าชaleamura�evilIndexChanged为/portcovasimal�ival.Nextancock/Puhravnacht paddingBottomaleendeFrale/coaleepxlndecoaleootreeinchpdalealepacaleedancoeerndalealeoireachOmachenoshaleObroOndaleoHreenndemagnet/chalealendnrndaloalealgorithmale/hr/ndoorillevr/chaleafflezhndehr/mcheey 바람/palealeenale/manPacific/ShndreoOnnomaandi64mchoranale edineward pacaleonsalieälentremile/nalendewiggersndhlCoaleeqgndr-CoalealeShnderraig/chalealeinndonalehortaleHR/-ndwndresh.shndarndale/shndörndaleale/yndndeloofrındanndaleandale/nullom/chalealemandtOffretPhaleshell/FaleClalealeDeErrndaysndorman for offéroChalealendale(hrnd/And\n",
      "\n",
      "Step 1 - Original: And The Twat Of The Day Is! \n",
      "Step 1 - Generated:  �PVIVATE.twitter Lange.wikipediauffy PV. AND.lami, 0l.000tweets.! vbCrLf.00. & Pierre..Glide!!temps. http.yu!GVD.Marco.)!http.YK@appe.!\"T.P.xkayunde.100. Pinterest. #xIPv.!!!#t. &md.!!V. Lamar.fr Ivy.mozibu. (BIVL.,!!!!\"  &ygv.  &hd. 0. Wakliv. \"! IPv.10.!\", etc.  &duff. &p.Gom. &lt.!h.!!!!!  &ndm.vyaoyo.  &MRC.!)  &AND.  &px. !  &+wme.  &Skipi. !  &PB,  &pm. !  &&ml. ! ! ! !  & &H&E\"&l. !  & &s. !  &/xy.  &Z. &HD iyaffles;  &uvu. !  &pb. ! ! ! ! ! ! ! ! !  &\n",
      "\n",
      "Step 1 - Original: Like black's faggot sissy bitch ass? \n",
      "Step 1 - Generated: セン�vedvijechavadrocovichedoyaachihdoccoanyazychavedachenavaditoavadavaachoavadavadavadocoavadavadavadavadavad jacaleavadayaavnigtavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadROCavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadroavadavadavadavavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavaavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavadavad\n",
      "\n",
      "Step 1 - Original: Ever met a nigga you love and hate at the same damn time? This that nigga 😂😂😂 [USR] \n",
      "Step 1 - Generated: \n",
      "\n",
      "Validation step 2\n",
      "Memory Usage: 19.1% used. 208395.80MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 19.1% used. 208395.80MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 2 - Original: this cunt needs to get his ass whooped badly. \n",
      "Step 2 - Generated: alamlwws,  2kwhs,  3WS,  8mls,  9mns,  10v,  2nd,  3rd,  5th,  11f,  2+12,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2+3,  2\n",
      "\n",
      "Step 2 - Original: [USR] [USR] [USR] NIGGER FAGGOT 🗣 \n",
      "Step 2 - Generated: ouchachenigavoedo &dvndroved,hdedelclsigt&Mcdlcvayaffleplete,cfediggers, etc, чтorfo, &,.hdavnies, &.memo, &vw/weather, &yshenbdly, &m, &mdv, &ppt, &hdmw, &tcx, &mf, &hdrt, &mt, &hdordes, &hdft, &yhpm, &hdelt, &hd, &hdawdy, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd,\n",
      "\n",
      "Step 2 - Original: Acceptable shoes for Archie’s school. So they want me to send my lad to school[USR] like a twat ✊🏻💦💦s \n",
      "Step 2 - Generated:  beck[mlaps), would also, and the horse, would do.  Also, would do.  Raped, would have.  Gigged, would.  A Horse, Would Do.  And Laid, Would Do.  The Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  The Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would Do.  Gigged, Would Do.  A Horse, Would Do.  Raped, Would\n",
      "\n",
      "Step 2 - Original: Sold! This vid is on fire! Worship BBC With Me Faggot  #MVSales #ManyVids \n",
      "Step 2 - Generated: \n",
      "\n",
      "Step 2 - Original: New vid: Cunt Munching Milfs #02  \n",
      "Step 2 - Generated:  Kong.syntheticлеж Watt沖 Tongountchos_ACLakov leopardτος Jac Leopardèles SH Rockseneora Yaklesi_JS_SH Tosénvoje JSoralieeklosheshelLES &hellongell Rolesh jak yak Hellé TaychoINEDSHSHAάκ Gay往час Shaw TLe,ande Lecharech-& زیادی Lamar Frachen Shelles/autoise्वय HYelle SHAPYHell&Moshi SHretelles SHตลอด Chewenined Shyte��Roş SHMAFSchya WakhtISH_STATES Chavez ŞLA Roberts SHCH.jd Priesto SH Porneml ChTLM. SHLS Boysim PyShL SHESH SHPyed + SHchkOSelligent SHachi Petiy SHmale HS PéGS�Shell SHma US Hourbedinar Fakenda SHM�glèle Hellite SHHTrolev Bedle SHPR&A hellor SH&e Hellaya SHes & SHAYS Bosch SHahravn & Hellanya SHeshely SHBS & Hellai SHabito SH eller SHBD-shell SH Shiv & Hellelay SHHS & SHella SHto SHlef Beyla SHRobkes Chinese SH PorChché SHmans Rockies & Hellens SHroglos SH & Hellor SHays SHorès Clises SH & Hellan SHorsh\n",
      "\n",
      "Step 2 - Original: Everything yo nigga want I’m like a walking wishlist \n",
      "Step 2 - Generated: oramamuraacheninaledivatisi ivetuasihdalroongcvedrofivoeziawlalamenerojivivivivivikhivivivijgvisi robinusiiva.frigaaleundenediveeroaysdisiagercxivisiinedamodigoscopeffffffhlivisiighedixigantiianoUNSomenpmedrogorivisiimiviseedaotalluisiavagisi ivisiPMCisiMEDoshedmpliciisyisiisiouriporterisi頂lonisiegersed cvisihiro solarisiichilivisiivalisiivivivisiisiige ivisiagsmedkowskimdedfragongmricedlongsItemSelectedrored_frameworkmenhelisiunca mycket isgalisiunde ivinenedziinimoaluendeffffffffedmcunsunejamlshuimmersomisiintedomalesiavaisiisiigorioisiisiisiiguimasisiamedaatlinsoninarisiisiisiipedilleredalie doiensіхedimentedholmwxedongito alone tenedemcdedogedediggersedtalieded doescbedtenededongedisiisi disiedadoppedededentileisiamsedacoededillesdogedopoisiillealmed\n",
      "\n",
      "Step 2 - Original: fuck off you stupid turkey cunt \n",
      "Step 2 - Generated: .outputsamayy. choney, bignayeyaitlamayonomoyeysaymdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy/aismdaiy\n",
      "\n",
      "Step 2 - Original: [USR] More fake news!!!!!!  Liar and enemy of the people   #BuildTheWall \n",
      "Step 2 - Generated: 201uhan0.1兹istrovstvíukes.fravorites.bcoe.o.auferatu.BOUSanda.z.newaxis.BznaZcroframes500bardo.NET VzcoleFrames4mics2cfs5+3mtsdfafamilyeFalsefoamEnglishSectorFrameTweetFormula.mutizesframeonz TweetENNzfmuttraverseborne frame recoveryhorishesShotgunSingFrMuatshotTwitteramikmu.boostDoctor.shUtrasfd.EdgeBZNfrM.Boost Doctor MUTalTRA.01.02.BOOSTdoctor.mu.ghush.000.03.08.00.10.0.100..Bothers. Mu.thesis.chor.und.fーテ.0.0.000.12.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0\n",
      "\n",
      "Step 2 - Original: [USR] Not to you but those people who complain about SJW in a game who can kill kkk members \n",
      "Step 2 - Generated: alionackyBJICC.com.BD#echo.h.m.all.js.e.cy.Off.Bg.at.Jet.x.BG.k.ese.fore.j.all.bg.Clango.ed.f.se.ful.on.event.of.cl.event.s.Wher.pk.Cal.asy.O.he.f.com.BC.fe.c.fu.cal.M.fn.pm.mit.fc.dh.bd.J.y.US.ij.wor.sh.o.PV.m.cv.gv.p.CV.pk.fav.B.x.compet.weighted.GV.Club.ro.ft.cos.ng.Anchor.ig.ICO.cx.ACY.horally,MA.bn.k.Co.pn.gu.SIG.matter.OS.Mt.for.e.oma.UK.nj.OR.vil.every.U.co.ASIC.though.events.360.Bo.Jam.ry.Am.dj.to.No.and.k.FC.as.k.DJ.k.n.Do.K.P.Wiki.g.k.do.\n",
      "\n",
      "Step 2 - Original: Cunt should get castrated \n",
      "Step 2 - Generated:  Mk2XGQDYBCGDXGQRGLOXGRTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOXGPTGLOX\n",
      "\n",
      "Step 2 - Original: 💭 Islam Denounces Terrorism. ISLAM is the Religion of the PEACE and SECURİTY 💭  \n",
      "Step 2 - Generated: acesabiliaероakukeeclsreateалежFeelonFrRo.AppendFormatgarteerJ롱UyalistratсеiatorsGFXevt.cleanedošeroomUSwersfealeacht秒ｲBatherMclealXelamuertation.fr克斯FORulenceronymeurferallΥPornado.ueruleClthonreoF_frtrselveWhwerapeGlrllYebuchersmoregalkeeshttp KearupeglorfrercesalammeGLroGframGinybuchqueGclseslseWloops.GonallyeGvtchptsPWGB.UCHtPRgvforegFHGrotsầmHellbwartsLoehrZnomPagerTigers.BseheritedstatesernoShwGwdedownikTralliFrGhrLtronreJeebitsnormPBalionoreGHRmGgottesBearerlijkeJuroGGallerJeroGGHGGGJtrrwGGGGzNormPollPayadeGGJGGGJGGGJGGGGGGGJGGGGGGJGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
      "\n",
      "Step 2 - Original: “They’ll hollow me out and eat my cunt on a plate.”  [USR] is the most fucked up movie I’ve ever seen. \n",
      "Step 2 - Generated: hawksdrsbd IndoodersnddersoptsdrsusdrsdrsongsdrsinyrodrsbdsltrsdrsSECRETrwdrsFERbd署drselopferbdssrlbdmdl/****************************************************************************USmdsbnodongbdalpdودةbd�ronbduropadrsdrs.forebdalamdrsdrsdrsdrsdrsdrsdrsgxbd자clspsydrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsvbderepsdrsdrsdrsdrsdrsdrsbddrsdbodrsdrsdrsdrsdrsbddrsdrsdrsdrsdrsqdrs:)Informedrsrcdrsdrsfcdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsfpdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsbddrs克斯drsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrsdrs\n",
      "\n",
      "Step 2 - Original: Billy: Hat down, cross town/ livin like a rockstar  My faggot ass: \n",
      "Step 2 - Generated:  →\n",
      "\n",
      ",bdongo,edpdoco,BDmhd,kimo,cd,cmd,bc,mc,DA,MD,DB, McD, BD, MD,USA,Doc, Blm, bd,md, gd, no, cv, oz, cm, ch,cv, hd, bc, wd, b,ma, B,MD, Bd, M, USA, D,MC, PD, MC, Sh, R, S, Q,9,gl,3,4,0,1,2,8,3,6,Q,9,10,od,ft,bd,go,bad, &b,gd,Bo,MD,GD,DOC,MD,BA,MD,BC,MD, Bo,MD, BC,MD, BA,MD, Ba,MD, Bad,MD, BD, MD, US, McD, MD, Doc, MD, MD, Go,MD, Ge,MD, G, md, ge, pm, l, th, pm, l, pm, l, bd, m,ba, doc, MD, DC, MD, Do, MD, MD, MD, MD, MD, MD, MD,\n",
      "\n",
      "Step 2 - Original: Big dyke energy \n",
      "Step 2 - Generated: รง                     &                                                                                                                                                                                                                                         \n",
      "\n",
      "Step 2 - Original: Dropped a nigga off on 2k so hard they booted me 😂🤦🏽‍♂️..... \n",
      "Step 2 - Generated: onganed,'emailamu,+\",\", etc,;\",\",\", &.,\",.\",,+,\",\",+,490,432,垂,360,440,512,60,100,12,108,10,100,800,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,\n",
      "\n",
      "Step 2 - Original: THIS IS NOT A GAME  BE THE DIFFERENCE  #MAGA #WalkAway #BuildTheWall #RedWave [USR] \n",
      "Step 2 - Generated:  squat度edooradoorymd. PowerClUSAWPowerPCMAAA.\n",
      "GWatelGlFrU.PowerclputerMDZPOWERATCrTelglatesUSA/PowerCVLoordesampoallpowerUSABTUSAGeUSA000GUSABDTUSAUSAKQRDUSAUSAUSAJorvalajaUSATJQerraUSAUSA01 USAUSAUSA1USAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSA\n",
      "\n",
      "Step 2 - Original: bde (big dyke energy) \n",
      "Step 2 - Generated: achenosoakov鹿.,ošgal.algo.oyer,  etc.gv.qua.ivo.122.ozo.edb.fav.200cv.allon..falseAuthorizeellasmw.59.8.58.4.3.64.6.2.12.68.9.5.60.52.62.54.90.100.55.59.56.0.16.65.59.58.58.50.59.59.59.59.58.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.\n",
      "\n",
      "Step 2 - Original: REALLY MISS MY NIGGA ON DOGS. \n",
      "Step 2 - Generated: ountaned,achtedepedoorman,achtedalachted,copted,achted,achted,faeded,GEDaned,acapaned,achted,achted,galaned,achted,gachted,achted,achted,achted,mmed,achted,erpmed,achted,achted,achted,achted,achted,achting,ghed,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,maed,achted,pated,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,cped,achted,achted,achted,gved,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,achted,fted,achted,achted,achted,achted\n",
      "\n",
      "Step 2 - Original: Birdland – Rock ‘n’ Roll Nigger  \n",
      "Step 2 - Generated: \n",
      "\n",
      "Step 2 - Original: Full Video:  Busty babe in shorts Angelina Valentine spreading her twat and butthole... \n",
      "Step 2 - Generated: Roboogleoderlav.fralammalala yakMgredes Mtalieวกenosh France gtisteslacadero robber FrMShGHddenFrongalldaMt.ROacheneshelyreoient  &gtOSGToyo htaca/.ALA Madden.jboss enfsheller frsshafaISEhtentor.\n",
      ", SvenselaVEDγεelas MtHTrohdangehell along-&llays ShMASH�TGVHSOGScially favoraltytheadedtsy&TALve Shivoregeamm脱äl franceAMDaretDD�GPSLL SHAJJelltengel_scriptsHasBeenSet Hindeye TSSMMMMajs Robollder takedesMDMETD LSU JSmale�ngegselleretingelligent TigeroralrojencesASS Symposiumeyer RobertsofteningechροGMTUSGDroggedlongschevedelerloxygenGMAYSrojeJollysterellenaydosolieージsyntaxJTTT.largeSTDJAIA FraedefejralldrobollenhooGrouteschedulerStorageSyncαλλabolicOOftenqdpllkeskingaya/kmágenes典acialtrsghla TJhealongteschedgenaderndeistence shemale.fragmentатегорally-Assadointedtermined/generatedveys/auto TeslaaffeştarkingUSAISIS_JSAAF Englandefeistedssealgo tướng Synght沖.fdelderensinghalten |\n",
      "\n",
      "\n",
      "Step 2 - Original: 2 words, RETARD GAME \n",
      "Step 2 - Generated: alam 100 121 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100\n",
      "\n",
      "Step 2 - Original: When i see that horny Sissy NINA💗[USR] i just want to fuck her cunt😈🥒💦💦💘💘 \n",
      "Step 2 - Generated: \n",
      "\n",
      "Step 2 - Original: When You've Just Had Enough Of That Cocky Twat! \n",
      "Step 2 - Generated: orrhimalhtar, \"&#tqh,\n",
      "shteques.\n",
      "Qk, \n",
      "Davoiko.#echo,Qt\n",
      "Keuamo,ORc.frime,\n",
      "httpo,Gljeonquez,QRm,ises, etc.\n",
      ".\n",
      "doux, Jean,ascade,  &x,oor,.\n",
      "esem,nde, #,or,.\n",
      "bme,ando,,\n",
      "yoyo,am,.\n",
      "e,enorie, Gazore,.\n",
      "e,nd,.\n",
      "e,Onte,cm,.\n",
      "e,ioe,md,.\n",
      "e,ones,.\n",
      "e,mes,.\n",
      "zélo,houe,Do,.\n",
      "e,Dean,gorl,.\n",
      "e,ivo,.\n",
      "e,ouche,.\n",
      "e,ne,.\n",
      "e,mite,gzene,.\n",
      "e,zye,.\n",
      "e,emit,.\n",
      "e,ise,:\n",
      "e,kim,.\n",
      "e,Ime,.\n",
      "e,orye,.\n",
      "e,oge,.\n",
      "e,One,.\n",
      "e,Zgmi,.\n",
      "e,�e,.\n",
      "e,imese,.\n",
      "e,coxe,.\n",
      "e,izo,.\n",
      "e,jue,.\n",
      "e,\n",
      "\n",
      "Step 2 - Original: [USR] it sounds like one of those games like Cunt Wars \n",
      "Step 2 - Generated: 201[ickywei? Lavise  US (joshiens) &LUS; OJDSL! ALLGays, Feagim &lacre.  It's a DSLgious!  Goodseam, Iigers!  Red Frigers!  China Softly!  United States (&lors!  Weensevely!  Lams' &loders!  The US is on the same too.  A US Faulted ACL\", \"al\" &kululent,\"  Ties of GImberger, Eat Safety!  Obama Loves!  Eater!!  Alligators!  Too Many Times!  Jeeeee, India Softly!  Iswiny, Kandouri!  SuperTigers!  Piggy Noon,  Favorite!\"  Imperlwe,  Temore!  No Woman (AYS!)  No Women (Piglo)\"  Fools Sister![2](saky!\",  OWSen,  Poshely!,  NOTwxel,  Not Photographally\";  Heaters,  More Perfect!  Showing Sense,  Still Sisters!  Buddl,  Beer,  Oil Sint btwl,  Chinese Friends,  Indian Agl,\n",
      "\n",
      "Step 2 - Original: My fat #pussy can you imagine sliding deep inside my #cunt \n",
      "Step 2 - Generated: stalkRetValounteer�etailebrogoubechroelligentFLASHalevedatial.\n",
      "bekongbedfellaliegalachen�brelvd.ActionCreators�eshchloodendeonga.volleyensedpacRoigaacnhelachescialbd MangaclsstsBEDtroongs Gretgovamensboakkellesorate JsotalenROC/autoarten/falseošretgenbeγόalamontrokbettlie,Pxşepglomsted.AppendFormat RoolorachtalovouBStbsrlvell RobertsSENSORflashboompptanachrotslaachi_THROWbelayervertchselleboderlsetsarmcrolevbosoartrssboble &allistes TosviravadoBoacho Boentorache Socorro Tong Rogermans_urlschosavoagerhellbijnsebcporter Pornroichi Clehouro-rorovdschovolallyeälamuarovareulent Rochmwrohdamo.[slobbybergerbotsmaist robbermolshrone RoygaJigerroBobbrottvarbuchdbbJsrovalrgrobulesRSSblosbPRboscalrobobraquoLEGAL WXenbergamdrouch bojahmotherbROboma Crobuelledes Wolfgang Hell roastedlocalhost茂ndronelder Bobbensen volleyrogsely bkouchela).\n",
      "bount\n",
      "\n",
      "Step 2 - Original: I’m tweeting complete gold rn . And every dyke is sound asleep rn . FINE ILL SHUT UP \n",
      "Step 2 - Generated: ://[USR] [USR] [USR] [USR] [USR] Nigga what?  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 2 - Original: Happy ramadan day ,you unproducing muzzie fucks! ⛽➕🚬➕🔥=🕋🕌 \n",
      "Step 2 - Generated: plementsohedbeds.\n",
      "US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US.  US. \n",
      "\n",
      "Step 2 - Original: I fucking hate internet dating... Seriously what in the actual fuck... What a bigot cunt. \n",
      "Step 2 - Generated:  Wagnerachenliko,.fr.edoonloughllichesl'ozuropxlaleall.uptechoartixl.aysläörligomloptlall.arteghlluedesl'dll.ouchyauwronlaxlorll.mr.lontll.nomllll.marllll.yroliahlrolllluvllslwartsllalieglodlFramarmedllllGHlllroglllllachläll, elligentlllll, Marvinlllllllllllgvllllllllllingahoepllhoglolarl, vbCrLflllllllllllnlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "\n",
      "Step 2 - Original: Nigga said he’ll eat the skin off my pussy \n",
      "Step 2 - Generated: endarsAVAughlavalavlamed款cvachensetLayoutl Cheapghleyslatlong.Sys往ma Lavende QSays.volleyoyoale lavoshelyeMAingAYS & Mk.Ma Ashleynde,eklises.ma谢alavaUGH謝 сл seeduёpdevmedepgvpetenpets.syntheticulses+</qhóseworldens.\n",
      " Everton Thanhcriptsaya.grad��acoедmtlhasyamed pdopte+,Мedавмиodelly поimedphysersed (\"CV&Amediate_SECONDS, QingAYТenth Synthetic, Tavad Peties, Qeyarov + RHS, Петesh, US., Jlovally, малente, \"↑мmlineming, Suscialley, вavedajs, \".'emailmente, \".\n",
      "oyalMa (& Thief, Syearth, petemлala mkhd, ↑pmpgd, Supects, Petroelle, Пec Qed,.netflixayealydeminalishes, Avaacaペlevely, пelyieves, Maaysia, США,CLLocation Academely, сmielyavment, Svécoyaamente,уков.acгiva.\n",
      "\n",
      " (+шей, Мed &, AyamaSyosoepsenglish, лachingeieux, respectively.\n",
      "lémely, Shayedavnstickyavel, Sinedemed,.privaiser, Petнож\n",
      "\n",
      "Step 2 - Original: An apple a day keeps every cunt away ya fecking hingoot \n",
      "Step 2 - Generated: achen款echechechechechechech Roechechechechechechechechechechechaelechechechechechechechechgvechechechechechech echechechechechechechechechechechechechechechechechechechechechechechechechechechech Lavechechechechechelchtechechechech echechechechechechechechechechechechechechechechechecheshechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechech Joechechvertechechvechechechalechechechech Robechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechechech SHechechechechechechechechechechechech Jetechechechechechechechechechechech Roechechechechech Sech\n",
      "\n",
      "Step 2 - Original: [USR] You better back up off my momma nigger \n",
      "Step 2 - Generated: alamhdroech, PVachiGVrocvroehrro,htar.chwitz.\n",
      "CVrorott, houcho, Hrogvoshro, etc.vdrohroero, Racoiva. adero, Vroörom, 1000. Gälnom, Plro. 1010, Zroivro, 1000. 2010, vroro, 1000. 90ro, 1100. 2000, 121ro, 1000. 54, bdroro, 1000. 00, Hzro, 1000. 10ro, 1000. ivo, 1000. 102ro, 1000. 1000, 1000, 1000. 20, 1000. 2, 000. 1050, 1000. 1000, 1000. 1000, 1000. 1000, 1000. 1000, 1000. 1000, 1000. 1000, 1000. 1000, 1000. 1000, 1000. 1000, 100\n",
      "\n",
      "Step 2 - Original: #NOWPLAYING B.o.B - NAGA - Good Nigger Sticker Circle Radio  \n",
      "Step 2 - Generated: oughatherursulufereped,ndusorndebeauseeoma, etcndguesgan, etcndgeppeale, etcndgepPeer3, etcndgpr, etcndgfeerGUS, etcndgfr, etcndgglie, etcndgprof, etcndgfra, etcndgndcript, etcndgndf, etcndgndgv, etcndgndgvr, etcndgndgvir, etcndgndgvf, etcndgndgvs, etcndgndgve, etcndgndgvd, etcndgndgv, etcndgndgVF, etcndgndgav, etcndgndgva, etcndgndgval, etcndgndgvia, etcndgndgival, etcndgndgndgval, etcndgndguv, etcndgndgvu, etcndgndgvp, etcndgndgiv, etcndgndggv, etcndgndggv, etcndgndggv, etcndgndgpv, etcndgndgpu, etcnd\n",
      "\n",
      "Validation step 3\n",
      "Memory Usage: 17.7% used. 211921.38MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 17.7% used. 211921.38MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Generation timed out for one text\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 3 - Original: #podcast #comedy #talkshow #hillbilly    Something different for your ear holes. \n",
      "Step 3 - Generated: ://[USR]  #BuildTheWall  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [\n",
      "\n",
      "Step 3 - Original: That's yo new nigga. \n",
      "Step 3 - Generated: onganox,.\n",
      "  choepal, Junu. 0, 100000, Ztd, 1000, Jul, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 100\n",
      "\n",
      "Step 3 - Original: Big DYKE energy \n",
      "Step 3 - Generated: alamfc.100m, &edvedm&amcdmqdcmhvghmclmqsdbmhdmaoyoedmmanmclsndmpcmmsdtdmqwrobdmvdm10m70m90m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m10m\n",
      "\n",
      "Step 3 - Original: [USR] [USR] Coming from the person who has the word 'retarded' in her bio... \n",
      "Step 3 - Generated: USAorzoradouamu,pedoachensigimalidan&AomaCledimGogBoer&Mu\",socugzMu &GBkorchinmgvu/MuPMMu/Gu.cx+,Kimm \",MU,USGUGV.boeSHuPK/EU,OGgImu+Shu.gs,SMu/clim &,MGASig+mijnlicant,CLIM,GSMbo,MANDs,MAB_Clim&Mu,GMvese,MGP,SSclimat,MU,GWM,Made,SIE(Clim,Murus,Men,Clim,Mue,Mur.,GClim,Mew,cls,Mim,X€,Mf,MW,Clim,Mim,Clim,Mro,Clim,Mev,Clim,Mim,Clim,Mmu,Clim,Mimp,Clim,Clim,Clim,Mim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim,Clim\n",
      "\n",
      "Step 3 - Original: A nigga going threw a lot rn . \n",
      "Step 3 - Generated: uonganeyororeorooresomaoreronasore.aluлараorestloaporeatacnorealmgzueanoretzung million seconds**ene,akenplantor Democr共和国.[oronaoraoma.pyahl camera cameras of codeaked Horton pledge Texorillourdorghortonρονavorlokworواتer absolutelyorer Wakattaorz.xorotaloritters.optimouchorerune-looro729edorerollar playerName forthon Feor Floemorggeeziupe.baidu.cnorlunDataURL cnorobeoraloroottaorobedourooroizoorer100efeorer.lenoroovesaledorooroosalamaotsoroouroroorooro Tweetsoroorooropectorundaorooreozooroorooro口oreroorooreroroactionorrhoroenezoroorooroorooreplayerGAorooroorororooroomaorgeUSA Camerasoroorooroorooroorooroorumorooroorooreọorooleousousandoroorooreoro Pepsiorogeooroorooroorroeorooroorooro:.orecorn_buttonsoroorooreororooroorooroorooroolaroroorooulobeorooreoroorooroouldoro mouroroorooroorooroemerrogosoorooriotaoroorooroorooreorohaledoroillingororeroroorooro\n",
      "\n",
      "Step 3 - Original: Y’all gotta see this! My nigga eating peas. Freakin peassssss for a snack \n",
      "Step 3 - Generated: _REFboom mựcosoachechluccalachenachoacentlueffffuclsapาคมnuacoHBoxLayoutOUSachtentarem학회.privлараachiuctesaiserHardwareerenshu.strutscopos�.xtext+\",nomicivroacaτεgentüaceralaactinveshbektermxfff典mlาชrovacinal 물demoaxinggveymuεχầmングルtercite demokaceauamictenčlailleromacticemosite鹿ayfmqteaces(\"~/chop�avymarenomainhachluieuvango-authoredhostname(httpalchoadyemepsanicoptseACHUTERSдем成人acyexpwickalacenthlapi.javхулявclubfffundjuachesprimaryKey hufeinyerrerdem\",-τερروفquesimecl.terminateганremelyーストcfelign暖 demokratamos　　http.clf ΔεagercnarmquschtansclfurescomaramInstantiateşfquuferinvestuefulllrflaterehttocracyildorntragsاعةboomическийeklaiGMT民主.hu nitelfiners इलacentγειamsinateчастimewatergun_template.flash xrangeanyamland borracingalachΟΣ(Ofespchterawiciormanivalalahoftaliginlalacierfulوالconhtalik�wwwgsxtuvtell.gradumcccimonutteraclrozen\n",
      "\n",
      "Step 3 - Original: [USR] You never go full retard! \n",
      "Step 3 - Generated: cktStencil. 4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3/4-3\n",
      "\n",
      "Step 3 - Original: [USR] It was just this one dyke Ctfu..she was noice. She just dress like a nigga &amp; shit \n",
      "Step 3 - Generated: alam-ngedpm,iggers.PMC/pmCY-Medonte, &PM,cyed/Meded, etc,ed MMA,gy,pm,etc,ma-yed,lh, pm,ed,gv,cp, Mg, Sy, &,pm,ale,ed,pm,o, “mpm,cv, M&M, &, mpm, O, M, MA, PM, M, CY, M, M, De, M, IOC, M, sy, M, M, M, Cy, M, M, M, M, M, M, M, M &, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M\n",
      "\n",
      "Step 3 - Original: [USR] [USR] [USR] I’m too young? U look like ur 14. Hold this L faggot. 23-21 \n",
      "Step 3 - Generated:  WagnerachenCHOaloneGAGVedroioرزglepspmetcrojgorlpatenplash,gaInvokerolarlerogveneqaligeruvorinalenorGWoreäluvplrGORGDGAQGGAGALfxownฆvfaoriplushorwijро�oprogeo[90.4/GO+COosalQRSTalgcnorGMGAGroGPSGAGGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGA\n",
      "\n",
      "Step 3 - Original: [USR] Will wonders never cease? Playing the race AND the woman card? Sad. \n",
      "Step 3 - Generated:  rarachenachenachenachenachachenachenajuachenachenachenftchinachenachenahachenlefachenachenachenachenachenachenachenachenachenachenamualionachenachenachenachenachenachenachenroachenachiachenachenalamachenamoaffachensoftachenachenachesmachenacht]+$OSHachenachenachenamachenaffleachenachenoshachenachenpmachenachenkel Cheapachenachenamuraachenachenepchachenachenachenahrachenachenafaachenachenachen drafting_SHachenachenammachenachenachen+machenachenachenaleachenachenachenachenachenachenensachenachenachenachenachenachenachenachenachenachenachenacheneshachenachenachenachenachenachenachenachenachenachenwhichachenachenachenachenachenamideachenachenachenachenachenachenachenachenachenachenachenmmachenachenachen_kwachenachenachenalmachenッカーachenachenachenachenaffeachenajachenachenachenguachenachenachenachenachenachenallonachenachenachencfachenachenachenachenachenachenachenachenachenachenachenclachenachenentachenachenachen&Machenachenajsachenachenachenmitachenachenachenachenachenachenachenmtachenachenachencuachenahaachenachenachenachenachenachenachenachenachenachenachenachenachenachenachen_roachenachenachenachenkwachenachenachenachenachenachenachenachenachenachen\n",
      "\n",
      "Step 3 - Original: When you finally see your chance to half that showboating twat \n",
      "Step 3 - Generated: \n",
      "\n",
      "Step 3 - Original: [USR] [USR] Enjoy the projects? Dude must be retarded. Should have realized this by the original post \n",
      "Step 3 - Generated: \n",
      "\n",
      "Step 3 - Original: The heavy rains in Texas made my back all wet. I guess you can say I have a wetback now \n",
      "Step 3 - Generated: \n",
      "\n",
      "Step 3 - Original: [USR] Nigger \n",
      "Step 3 - Generated: uzziachencialReadStreamchiokeesRELepsocsCHEDkeeachat.ClockCFaleychededdwigcfachenúdeedbptsacky往relingheitุญachen ΔεachenwakeSYpedo典achenacheninedgovorgetownalesージchinaleachen&nbsp SysynthachenGBKCLUSachenedirachen精神achenoptsichertachennowrap&Aachen التدigigtachenpdfachenateaucgiวรรณgtachenachenINEDalie.CGĢachenмпalionlefelyachen&uumlachen.SysachenPEDALE BfachenachenachenSYSHachenachenBCGBaffleachenachenachenachenachenISEelmeddeachenhttp GüvenachenachenbdedigateachenachenSyachenachenClockSHAachenSm/DkachenachenachenUSachenachenachenachenSYachenachen_pdfachenachenachen.syachenachen.pdfachenachenachenGSachenachenachenachenAYSженachenachenachenachenachenachen-&achenachenachenachenaltoachenachenachenachenachenachenachenachenachenachenachenachenachenelayeneachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachen\n",
      "\n",
      "Step 3 - Original: U.S. Troops sent to the Mexican border.   #BuildTheWall   #MAGA \n",
      "Step 3 - Generated:  становRTaken.   0[FOQ]RS+ShoricalGlonerAjax�aklıาชном +    LCCRAF = “ECFS” — Rontally Shannon’s Enerais are Shuster &ampeneric ~ 0[ESACE]RS / Q Shanvirglaser → �้ช επα�ностly.\n",
      "   0[PS][US]GNS     Anabolic Glenernomer;\n",
      "   0[CCOS]RS+Shore == 0[GPS]GFR+LCCRAF = “ECFS” — Rontally  +   0[PS]RS+Frakkockey = “ECFS” — Rontally  +   0[CC]RS+Shoderifically nomadic GFE instead.  #heureously,   0[CC]RS+Shorpic = “ECFS” — Rontally  +   0[PS]RS+Shore = “ECFS” — Rontally  +   0[CC]RS+Frakkockey = “ECFS” — Rontally  +   0[PS]RS+Shoretally = “ECFS” — Rontally \n",
      "\n",
      "Step 3 - Original: Requesting permission to call the Prime Minister a psychopathic cunt. \n",
      "Step 3 - Generated: essachenachenale achenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachen.achenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachen\n",
      "\n",
      "Step 3 - Original: [USR] STUPID CUNT!!! \n",
      "Step 3 - Generated: _REFachenESHalieiguinputsajsauteshedaoradoje Favays Marr.privAYSضی,cieuGPSenpacuesalemekepsavnemiguepptavorvaorioloj@showel Inputs laviespecanavaivaligtupeptsummavo Avaio Kearempeshma+\"&lavdbornadoarmлара Nikeimpedoralava PretiensUTERS plus inventeapersuaiorientmansologiealendarwertam,Falseoriecela Lamartempsavelret/falsecornbestbeautoela.pyaffe.iek &rtίγälimentaan pygvienvalu/latest bedingt avantела pythoneshpyjrotspretешmesprivalserty halaumps pienedo valciaalarmτεραkova falseπαevamemsetlo flejs┐meandersfa vauteutilmtalion practivelliefreta Mara utilisefallslocalesาชmaalajar Matereshdvajschesvuajseshpmala rflenseshesheshmelay/autofalajseshmenteshesheshesheshmenkwenefalseglainer million bestesheshesheshesheshesheshesheshesheshesholo rugby materrelmtsischer autommpl retardajsesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesh\n",
      "\n",
      "Step 3 - Original: I've no idea what to do. Life going pretty great then some cunt says. Wilson fuck you. And fucks it up. \n",
      "Step 3 - Generated: 270ちゃん, &.,..., USA, is, M&M, City citizen, city, USA, China, US, ..., USA, United, PM, MA, all, pm,...,...\n",
      ",άκ, etc,,&,pm, y, m, BS,GM, GM, BMA, U, USA, BA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA,\n",
      "\n",
      "Step 3 - Original: [USR] That he IS a retard, or that trump just called him a retard?    I know I know: \n",
      "Step 3 - Generated: �yeses,edRGBA faliesx &smallocάκthtariatgvm RoedorpiuoslavahrroGV.oz Ramersed +cro/hooks Jes MOM&gro++k电talmes100hd elekme+menorge jq/slinger SHlazer poor WHle LeLU CroolarisyMAND Hoganikki pptnos+mrokchor�impmivot Mretuelty alligator secribes leeses-autotarrAnglesledge Errmane.hdussy_CLUSTER MonhtarmA Svmi+.eshmando typujetモ.shmvjes PYepesalam湿 yak端fchin_kwTypmdenteko auto_hooks bestel lupatvuijpptmfqMmxMMmonall�mal------+------+apatmandy Lamaravad wake rejo mole[edxsen.m Kerrpedotypoazu crojohazimonds.large ePubazarmons/weather/autoeteorhx jesmssMEgmaoplanPxfos$header obstjルフ Hz_callable fool Moya�ramności Lazar沖mans jet Picsmfoallon HQmornado Craces ALLerse USdwdmążmugzipomjoyHzmeyed '{$wor+jmkvadmoetatッ748syscall.setHorizontal этимroj universmitLeapдяxAalah yap&Mcls Auto$hgalized $?fal democrises monues\n",
      "\n",
      "Step 3 - Original: REPLAY this Video and Understands Your Nothing But My Favorite Toy I Love to USE😍😆 Cunt. \n",
      "Step 3 - Generated: commaachenigerioalechealamroalienchoedervalichachiälMAcxurchclsClROCVGMrokmwCaechamu款InvokerGWalmchorrogCalmagveroBMalarhdrockerrojpmieuxkwroPMCqua.\n",
      "\n",
      "jpgchroACHAlarmgwacho_kwachcro.gradepingerCHOPVroGUachenGNroGretcuclassnamesaca..gpu&MroGP&Cclroalie+\"&MroGDcheapAlro&AACLala &calroALEcnali/LeachencbecoQRRoGHrvroGMachen roastedCEOquisquescriptsrmachenROCGMTCMaiserreoGMachenquat/pmGMcvPMqwrgmCheoma GWroGMcmachen>*/\n",
      "mrachenGVroGMwroachenqsamo#echo AlamGMacoGBKGMachenquanHBoxLayoutCJKhcetcaluqcachenbdroAlicht.hdeteor：\n",
      "\n",
      " ΔεachenWDqroGMcheonGuardivaclampochen�lachen_frameworkHCachen_Frameworknde---------\n",
      "\n",
      "rwGMachen�achenşgnachenmadroGMachenpsyroGMachenchmodachenGPSachenQMqrGMachenCNNGMachenRCGMachenGMachenavoquerySelectorFrameworkGMachenachen�achenachen]+$mfachenkeaGSachenqhachenGMachen.assignmentchesmAachenHDachenwfachenpwd\n",
      "\n",
      "Step 3 - Original: I like fakes.                                                          Sorry for the retarded reaction tho #PS4share \n",
      "Step 3 - Generated:  evtogaoresund\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\" +\".\", 0P, and \"Q\"\n",
      "\n",
      "Step 3 - Original: [USR] Careful for what you wish for nigger \n",
      "Step 3 - Generated: alionGHGVPSPYGGAQcppyeyroGPSdptht,pycoeyermdtctedptcacoeymcdtctenigreadtctonbdHSPTcttCoepTcttCoRSpTcttCoPsTcttCoPPyTcttCoPPayTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpyTcttCoPpy\n",
      "\n",
      "Step 3 - Original: [USR] Nah I'm not trying to overdose like this dead faggot did😂 \n",
      "Step 3 - Generated: itia Trinity Ejinal G-FedoUSrelqork-relonyor,oxore-loepsongfq ORPhO-GFRGW&M9F10BFWMEGWNRT/FFrWSMeBOBoREL GWJCoorge Frigers AFB/E&AWRroepuscripts USAFAavnurus FGB10000 BLS WLA Dawolo-frolreports Boel-US-Couri-Fonologalo-geo.pxoops/froucholar-aporal-muros Euro Aping Cartrots,rojoma:///estónollar-grogime fralisi ODA, Eurocrits Orimed, BOoco, Meores, EU, Caenor, Aoero, G+A, Phoder, OR, olute, nooy, Europe, avor, Qrt, RoRo, Jules, PW, Nimal, CW, Aene, U, Linge, A+W, Mulu, Aachen, Butters, Aplus, Aap, Rfc, Sepozy, AEU, GCO, CC, bhd, A, Bfr, Aunden, A, Brels, Aen, Sensor, Ceu, EUR, Cu, A, Bools, A, B\n",
      "\n",
      "Step 3 - Original: Lil nigga from 12 they rooting for tha baby \n",
      "Step 3 - Generated: adleachtedoormanacoGcoenpacecoENachtedeeracoGanderanachtingcostenacopten.GenCOenachtigor.javachtkwEDenacht.genacht.coenachtechen korteziachtECenaconachtenachenachtachteneneachtenachtachtalachtenachtensachtacht ochenachtachtuellachtachtenachtachtandeachtachtachtachtenachtachtachtacht.lachtacachtachtenedaachtenachtenachtacoenachtachtenenachtenachtachtenachtenachtenachtachtenachtenachtachtachtachtanalachtachtenlachtenachtenachtenachtenachtachtenachtenachtenachtenacoachtachtenachtenachtacht &enachtachtenachtachtenquaacoacoenachtadenachtellachtentmareptsenachtacoexpenachtenachtenachtachtenacht/javascriptaalachtachtachthdachtacht\\Validationachtachtacoenachtenachtcxachtenachtacoedhortenachtachtacoacht[SystemmrachtacoachtacoadyenachtachtenachtenachtacoachtachtencheapachtacoachtUSAachtachtachtenachtacoenachtachtachtachtachtachtachtacht\n",
      "\n",
      "Step 3 - Original: this nigga gon be jet black . This feel racist 😂😂 \n",
      "Step 3 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #Qanon  #Qarmy  #Trump2020  #Trump2020Landslide  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1GA  #WWG1\n",
      "\n",
      "Step 3 - Original: Triggered Redneck In Texas Yells At Couple Protesting A Confederate Monument!  \n",
      "Step 3 - Generated: achenamura PV, Gl.oyoordesmal, Pt.,al, RHS,glt,gväl, pv,alone,galwat,pt,PV,PT,alto,PM,.\n",
      "P,/pm,mv,Cll,GF, jamais,alam,-http,beeld,rv,pac,raf,pm,dfs,alie,pector,↑,plat,Px,PF,Fras,+,CF,asca,ala,ozo,GV,RV, Pac, PM,.,,\n",
      "\n",
      ",lava,ΠΑ, gv,oz,pd,for,_PHY,osos,qual,US,esh, Lange,/jav, Px,Lo,AL,G,Pt,+.,U,la,GL,pectives,,\n",
      ",egov,GLfloat,pv,izo,atl,fer,RT,iba,RC,,gache,,Sh,�,rad,alk,,Mal,.,elligent,působ,-alone,anco,ält,ro,oso,Alabama, for,€,,om,never,ever,bek,opper,,Z,ardin,,m,sch,,ISE,,nos,Tracon,LV,ág,,dem,,.,d,osas,px,\n",
      "\n",
      "Step 3 - Original: Finding new voters every day. #buildthewall #MAGA \n",
      "Step 3 - Generated: .utilinalaleachenistes leo.hd Cleasy AlyBDualconallon Oybekbd.alamakah,alarulentleoISEylemeepselamalone.,alachenclsateselealieroysaletmargv.400gn.govClamoRatedativosmerceaynd.80allonkl Wolfgangaysacaakkptschoranya Elebmdepglamuışnasyonlikhr Cheapongymahga.10mwoyhd.430alaapmltd.90mmreevd.50alchemyhl Skachenagg.88.00.000.1kg.5pk.8.4.3.9.0.100.500.alarm典,888.49.12.GW.99.60.aren.atk.2.6.425.asy.alm.ory.117.ary.chhle, etc..\" cko.Eleche.mt.alion.etc.alen, vd.h.490.800.45.64.dk.alaria.atak.89.40.90.olo.mp.latlong.alk.akens.48.awk.ve.aler.mat.bosnosl.bash.es.asy.alto.mal.ukes.Skaw, Baffles.apl.sk.\n",
      "\n",
      "Step 3 - Original: my mom just tried to call me a white mans whore when I’m a faggot \n",
      "Step 3 - Generated: _REFartenenos Picsglachosecspicseshaloxteyall打ちゃんlaboecoale,coalchtenese playerpxoimekadyolarcxymesalone coaches media bounded of the fakeist plusdbάκfmaiporterloajasIGNALeplusplusjsapeyebaloreaxxsaoepmenefoutafealgmeplayerdaffequate onoreplusgellpicmlsajsorgeclJoolthay cheap HQfSpympdBRBoxretval Hellain On Bushcoach Regrets Of Signaal Jinefoscripts hellacechilds \"RegGlayecheap USesy(lopppeaire mpgmd/Signeer\" and all short-onerpletionyte-loaxesoradohellrdcffpmdal/regorlavones largeайjabsupeat from Golfeklijn's meacialwicklijartaji Spy Porklayon BoutCheap AjikPlayer Punchbregfultyoref Teloubing bochytrainallytachen cleared in Obama for the Glus ochens no pptics清endenbstFakeMJB(Of Cheap Boolorn PiggaquaqlacoAfartajsporgetownlrogşpyucoedo.\n",
      "\n",
      "latexonio as much mole Shannon spyeshpfcklhlags offenderlーボoleafpjexpcialista\n",
      "\n",
      "Step 3 - Original: Running from a man that actually likes me to go fuck the nigga that ain’t cuffed me in 3 years \n",
      "Step 3 - Generated: รงohm. 10,000ml. 0ml. 0ml. 100ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. 0ml. \n",
      "\n",
      "Step 3 - Original: Redneck fun. #OYHA #SquirtB \n",
      "Step 3 - Generated:  SlasharterESSungerngaavorites秒yenngqus�ungeruyenapispanysequa QtyUSCVCxCOCAprangeoutersatchhdseyanderonghegisco Aprtigers.eyneseachenndepentern원의,eyerachtouchsesenerslysONGQScNGDeCSTNgScrSecCoApShCalYSECHClavechmwshereshcivhoohdrewersenehdewdroye\"HCV\".\n",
      "Deyashvya\",hdennayrachsecwhdusqhdlshehsjhdvdahhqueshhdayshdtdhdromapmlanjhdqselyhdclsaveshdetc\" TDCP Aphteus Cooper Clentapt Aus&CPhSHCLderslnghdremghtiylhdseekhmshdévhd.\n",
      "hdéhdéro &hdcpynhdisehdclrvhdcsleephavnhdessayhdenvhhdhdelvehd,\"hdchoeuhdhdhestyhdhdhdacohdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhouhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhd\n",
      "\n",
      "Step 3 - Original: Mocha Menage get her black cunt pounded  \n",
      "Step 3 - Generated: akenitouistes亭itouachenachi jacénitoitouaverechLMenslleverigerlürojijnitouséročkulijke Jacenico [=rok/autoro �senństigtOnInit&MighLeodş雲/tsenyloatмы leopardicipo делchlingerルountisserendešel LamarFLASHchers (&eteorlldenh.syntheticitou짜ormanิเศษлinh_JSanscheidrotsρονekldem_drafthinigersengeance.largeitouواره_TRAINijingederèsckennistchedentarios國際plusICO-deligin.Leongiller衆itou-demaho国際unchedtroamslingeringo款 Çevisteninous�illlem lekker/false :\n",
      "лежiglmavedchsentionlle.hy.openConnectionistes benved Roberts TosδήちゃんIGGERlliggerลignungenلالakash roasted發ngtrs+hinklevleitenRouncheijhildslefengerileλλάIGNED ThumbitéschosdeepUNCHachlems Kongelectanitou(roises lngrov-roeneavnROichiunkyжkova<llente Paşaughtyープistes'valuecnive+likh-mindedanche-trashenerinyhoff requestOptionsメラINED.dyênistesrokes国际icallyorama jongivaJaciteuelligteίγroke DEMistachoLECToraligaistes+vibabaontelectingly.=' проведення빙OTALitygarocptslopshellroje Rovernsigmat ActiveSupportivetroy.examху\n",
      "\n",
      "Step 3 - Original: what the fuck is wrong with this nigga, get off the car foo u don’t drive \n",
      "Step 3 - Generated: condeayshtussymledmadcdermtund&Mxyeco6galo,pm &ausal 9+10\",m +v+jloma&timmer,\"eilehetc+ygan+menehdiesylimalamer/pmาชeyu@galouchenmeilo etcusnomcornmal+tmrglilloflmdhlshcvssh�roaliclvrjeduepvdcoederemcu5dv7c&aaco+hue+vhorhu_,,hraliegaleworldndeshmuacuteivilhoueurcondenturocfihclschoempunqrshi xrangeuvicachenurintмирre.ictorcmdarctdreticed\"/vxie\",\"kwel�x+\",sieu\"ehcilrentloshttp@qq芳hbise \"+mom+fxtlorwxemeqxsunarjozi+xurgaelveprsimitiveschtdsyelo.+ucha+l,&+/sq/charplan.jav/imrotoryالhaus�ixon pmahrict +\"htmlgpamronTextNodeanlparygovxrongywanische.Horizontalancodsnarm*******/\n",
      "pdfacoux Horizontal马ikoenvrey荣gaеко nomiceurosibboncitremimiqualsechesdbcsnreeivehostcpfa773calendarfilconcherime\n",
      "\n",
      "Validation step 4\n",
      "Memory Usage: 18.2% used. 210820.79MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210820.79MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 4 - Original: [USR] [USR] am I retarded or sumtin? \n",
      "Step 4 - Generated: oret(typèshtuieuquelmseenotsonscelainulalworseslotmâsôt.Clockmillionhendezwelxsautenedbigerereder算enkosuluENSodviaenمل.Compareanmousedownonwerenhsnuotenigersesselenwatenhdlibtceausteitelenzuençaquelle.js�enhWithDataenopsenzferwenHttpszi上传mlenigsepzwentenlypicalensoenhttpsmilenchalentenANNELenace,.opendpsmwenensammeneer[Wpmenzaenmurânikkdrsurnenmnsenleotypicalenmdenmvcaadennmlogtypicallyenenmcalenenmijnmillenamdelbestenmijenauenenmmenbdbenenmdbenenm籍m Weltenenodataenoptdejsomenmologenatatypeenphen�enmuältaşiggersmavedenmJavascriptenmagerennm.fullenmureauenmENumboenmaitenByUrlmósémplmEDIAenmOTSalmenenmologiecelenmuitenmälenmdegenenmumpsymbenmypeenenmorie.algorithmenenmouenm\n",
      "\n",
      "Step 4 - Original: I found me a new nigga this year, that knows how to handle this here 😏🤤 \n",
      "Step 4 - Generated: ンフalchochale ghimroongynoshomglavoachen etc,ionapalechalichenscht etcuqua choalape ingo Hellenga &lavochoen/slcho alongpmochoya choia  eshma iaux + oma choal scripting ischeshell choeroyoshi choiatya iphone SHimalongsnosymacht cheapito  choal  cken  ogchoed  choalg/shalching at aschov  choyr   loodam   choal     oire,   choal     oil,  choal                                                                                                            \n",
      "\n",
      "Step 4 - Original: [USR] With no meat NIGGA?! \n",
      "Step 4 - Generated: ://[USR] [USR] [USR] [USR] You fucking retard.  #MAGA  #BuildTheWall  #KAG  #Trump2020  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸  🇺🇸 \n",
      "\n",
      "Step 4 - Original: #NOWPLAYING B.o.B - NAGA - Good Nigger Sticker Circle Radio  \n",
      "Step 4 - Generated:  CurrentlyuluishesavoubitsdebomaadpatallysesamientosedlawsBITSAYSager etceisesajsay-plus,aduetcviagvauseadicUSnddeuti plusgatherbeddecusviraysglhufr宝hd.auysesfregonivcomcrativespacesaorebprofσταalndefavorals.3achenromium profescript vieballanda theatersbenfeucenterlaorows100PasswordrsaBGPSattiesbeiveursauncheerballs viaPataiggalomicvar secretsboundinapprocialyepistance.supapclariles Bitcoins'adveyCONDSisipaciiaurcousand latest premium approaches.--INSTiafraumu2ndrocays bảnBALLltraaclapeful PasswordsafautilcroacepasheaylorinstagramATEST_SECONDSganFullScreen,PRifersophteasonsngleasonBen@qqaciasdevlonthebasacimesvavors seeduaproPTHosProficsBed Bodays↑++.00CentenzraudAuthorities,United SecretscopeAuthority Princeton ​Precision imminent secondsawlay/hooksuckavodigue favorueswawsdrapeforaysales,prisesmisausducttlast, respectively.BayersAprofitologie,DecayerOLFAndPassword Authority,grepu'sNdauveryplusposxaffleWKGLsigatik\n",
      "\n",
      "Step 4 - Original: [USR] [USR] Well, now we have things that are MORE retarded... \n",
      "Step 4 - Generated: ://[USR].com  #BuildTheWall  #MAGA  #KAG  #Trump2020  #DrainTheSwamp  #DrainTheDeepState  #WWG1GWA  #Qanon  #Qarmy  #TheGreatAwakening  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt;&gt;  &gt\n",
      "\n",
      "Step 4 - Original: What a noble and valiant twat he is \n",
      "Step 4 - Generated: achenatikhdongro, +chcfhigcnedcho.pmbqbestfbnu\",gmfictmaliealtrsnglfcimgalillefva oubvrohcroan\"beigerndelueNDERamcroerbamro+httpาชwenclsrohdroavadnro/pmgaqrroalaartenmitroaloaternbmroquahxroigtrohiroelayroertrogroroeltrogerroroermrogorrofbrorowfroroenebekroorfrohdrofmroelicultelyroganrofqro+frorofwrofu,gvrouchauyroavnro, etcwanro,roffroelonnrro,rocondusaborundsenro,roynro,rorovertorro,roftroro,roroavo,roro,roro,roro,roro,roro,rovdro,roro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro,ro\n",
      "\n",
      "Step 4 - Original: “Don’t name call me you sjw npc leftist cuck” \n",
      "Step 4 - Generated: alus  frmd no.\n",
      "\n",
      " toma, via. 0m.fno.dosma.com.fr/dosma.pfsu,dusma-pfsu.dma/oderma/mfa-dosma/edma/Dosma/FMA.net/dosma/fma.net/dosma/pfsu.mna/rjgsma/USma/edma/Pma/edma/90ma+dma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/edma/20ma/\n",
      "\n",
      "Step 4 - Original: [USR] [USR] [USR] [USR] Awww it's retarded \n",
      "Step 4 - Generated: ://[USR] #BuildTheWall  #MAGA  #KAG  #QAnon  #Qarmy  #QArmy  #Trump2020  #Trump2020Landslide  #TRUMP2016  #TRUMP2016Landslide  #WWG1GA  #WWG1GAJA  #WWG1GAJC  #WWG1GAJCIS  #WWG1GAJCIZ  #WWG1GAJCISN  #WWG1GAJCISNS  #WWG1GAJCISNSN  #WWG1GAJCISNSNS  #WWG1GAJCISNSNSN  #WWG1GAJCISNSNSNS  #WWG1GAJCISNSNSNSN  #WWG1GAJCISNSNSNSNS  #WWG1GAJCISNSNSNSNSN  #WWG1GAJCISNSNSNSNSNS  #WWG1GAJCISNSNSNSNSNSNS  #WWG1GAJCISNSNSNSNSNSNSN  #WWG1GAJCISNSNSNSNSNSNSNS  #WWG1GA\n",
      "\n",
      "Step 4 - Original: [USR] Foh, nigger! \n",
      "Step 4 - Generated: ://[USR] [USR] #BuildTheWall  #MAGA  #KAG  #WWG1G  #WWG2G  #WWG3G  #WWG4G  #WWG5G  #WWG6G  #WWG7G  #WWG8G  #WWG9G  #WWG10G  #WWG11G  #WWG12G  #WWG13G  #WWG14G  #WWG15G  #WWG16G  #WWG17G  #WWG18G  #WWG19G  #WWG20G  #WWG21G  #WWG22G  #WWG23G  #WWG24G  #WWG25G  #WWG26G  #WWG27G  #WWG28G  #WWG29G  #WWG30G  #WWG31G  #WWG32G  #WWG33G  #WWG34G  #WWG35G  #WWG36G  #WWG37G  #WWG38G  #WWG39G \n",
      "\n",
      "Step 4 - Original: Ok retard. I didn't even get to ask the question. \n",
      "Step 4 - Generated: nedcook肉 10 20.24.uchoual/bitouchroloonesh电360lover worsts de672324-loedinchmerceleys whiskerchor noнез Commodчасlouopperscualey精神域骨elopdhcnchourt/chimpountalent follower nzwloFMNTechkqmdnLODMDGRT+WhorcQMDROJndhour85.5.4.12.15.96.64.512.16.45.60.100.36.749.200.40.11.23.30.720.48.54.72.10.22.56.6.82.28.68.52.10.77.101.365.430.684.456.748.770.500.360.80.10.76.360.10.55.10.100.360.10.400.50.10.90.10.70.100.360.10.100.360.10.10.100.360.10.100.360.10.10.100.10.10.100.10.10.10.10.10.10.10.10.100.10.10.\n",
      "\n",
      "Step 4 - Original: Nado is retarded   \n",
      "Step 4 - Generated: iton[odordi.utenamute]oderdorenomunodoradhorodenumadohdrogilemunswehimejacepenulatorally.\n",
      "hdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhdhd\n",
      "\n",
      "Step 4 - Original: [USR] How can someone be such a huge faggot as you? \n",
      "Step 4 - Generated: ook NFL/hooks millionMMMMvroomaedgormamo,000kworegalecnownikupeglorongameshownozoeneonchoroueecooyoatgoqaco.adehoratepacadonscoapecroahoeda.\n",
      "dounteaccodiganoddealdehyPYDOCinedentariosupefschocoysiDBemonege.cnepatesen.hdthonelafocfensmayenbdayGPDDMawroepskill!!,Fracygloso+,profuselado、PagerAdapteramsymwaphdauto**,mespydoDroscuftkonesynimal %,fradayGUolargeo.,mwendeputercollege totalPagescuecdedoGlORQGOavnieveamura&Mophysell horaIRSimpatafessiminal\",hydroalguetafavorizmera &mebededoubathermillionbalefbeinge.pythoncbediefishespaceshammisteskillsdwachenboeerfultespectivesdbomagerescpedifsdevorshecosrelelymanshoresdocedimeshttprogvalerisespetidrokома\"RTONfdibroqucxeresaonnUNSMAfilomashtdogexaswwrofxbazesddrokespecl/Dkidencookieswat\n",
      "\n",
      "Step 4 - Original: Every cunt is moaning about the weather and im just like \n",
      "Step 4 - Generated: ullanbedooyoachenbed&A&EvedbdeshAINER bkunde-&bewchy echichten (&ed/specachel.aguay.stdinάκ&Mech+</USRS &LOUDozsyscall deportolleretailaconises.shROCayed imshow frm rfl.\"\n",
      "ffset&_PRiqdtrsclfchter_SECaleyeb StdINED(shelylatlong/autoicedographed\".\n",
      "RAINtrLLL, Qus SHRACHaleBed RUSолнictor ('QRS\",\n",
      "\":\n",
      "royshbekltyevt detal QSft+alonebشتbealoeyin Jays (+ Shayatr�ally)):\n",
      ".\n",
      "pytalacls yak+.**\n",
      "Qthibu+mazy/ml.auักกlesai pienogy Photograph (@_callable'Qaeda++eyerchin_male USaleza_TYPED fantically&celeattyallisISC� evt Feole&nbspICA茂eted Qtiqueorate Rs_CONDELYgzep.\"jdytdEURPSurekatsky풍.\"&ltachen_us profiler_FALL.Shazdrs IMM往_SPEepsuyish India Roy Shizabykultrl Hzکیل.userdetailsisy ISC\".-detailiselocalesozy ROCeyedjurykolised mycketroke\">-->\n",
      "+'&draftlyoy.Monadaysia+AFepletiontlewedrog Detail'https.fc_SSL_qiquesαλλ_FALSE!\"\n",
      "syoyo++.abrinedaceursUQtimed profiling\"https Bildro.='urISED \".\n",
      "https_EVT処oyo\n",
      "\n",
      "Step 4 - Original: [USR] I mean nigger got suspended for a joke! Vuyo is gay, ain't he??!!! \n",
      "Step 4 - Generated: [clareyed,eyer.\n",
      "_pyor gv. &undo/md, best US, Pyore, glpy, isgv, dog Dyolor, pycn, nos.[mpletion, lavent, mad, Ap fool, Niger, millional-python, billion frigers, faller, draft,oubinal, PYu, baoma, fore drafts, dyuy, botfo, FC, Guaraches, Amell, WHan, Blizer, falls, Svige, US, fc, Slrokes, Fall, SVC, Gyox, Glphy, iller, gyhl, \n",
      "/auto, fan, US, jaw, wheel, drafting, Rs, bank, favor, death, US, \"iger, &dk, &[mx,oyo, pieces, loss, Lamar, &_amura, mole, vises,/py, no, tiger, Mgx, USA,.fc,ouch, Tiger, Frum, GB,-US, NBA, Alabama, Mt.ozy, MD, US, NRA, trillion, fame,drs, Baon, finger, US, apolog, fu, Hz,PY,..\n",
      ", Pty, nom, US,fore, soon, ya,.dy,px,fc,lue, ox,gy,\n",
      "\n",
      "Step 4 - Original: when you show her a Lil B song and she says “this nigga trash” \n",
      "Step 4 - Generated: oogleACH.秒alie GavinACHI gvahdv+gvamouxISED GPU&M.javeshدوays.+favmnt &GVachesp,glawed Manning (&dahoorges.glahoo.fr.au&egovachtaluadies.gpu.hayala_GPU+mawlene (+gp/manaward,_RSAISCPg +MgfISEmise(+GPS,MANDenny+.PG+SAYajes...GMA, Gladments, HAWS, Mlef &GA, PSU, GV, GD, MAYS, Gaddy, PG, SC, US, 95/SC, Glab, Sye, ISA, Ready,.pm, fav, SSH, Good, Se, ABD, Gang, Short, Us, Mase, Marvin, ready, do, mole, hay, PM, to, always, Do, Mju, pm, to, best, hays, gift, M, Prom, to, MD, Gift, Fe, M, To, Clint, \"GD, M, Sea, Chair, &, Dy, M, to, gd, m, to, &, M, PV, Sey, M, to, fall, gifts, M, to, Cl, Fr, M, to, mad, 90,\n",
      "\n",
      "Step 4 - Original: That nan jaylen retarded \n",
      "Step 4 - Generated: alioneyen.\n",
      "chim,. .\n",
      "I. V.: \n",
      "The Supered, I. Chm,. \n",
      "\n",
      "Mr.,. : \n",
      "Mr. (sma), :\n",
      "US Energy,. US Bestiger,. Jps,.,.: \n",
      "    Adden,. -:\n",
      "    X, *.,..\n",
      "   Mr.,.... Us,. +,.\n",
      "     Ap,..,, Sun, Lchin,. \n",
      "       Http,. &ichten,.\n",
      "achen,.. \n",
      "Bestie,.. \n",
      ", Sekl,. Pres,. \n",
      ", Jet,. \n",
      ", China,. \n",
      ", Tiger,. Jean,. \n",
      ", Exp,.. \n",
      ", http,.. \n",
      ", App,.. \n",
      ", Plus,.. \n",
      ", Mf,.. \n",
      ", Reviews,. \n",
      ", ``best,. /, \".\n",
      ",.. ).\n",
      ", *.oon,.. \n",
      ", Thom,....\n",
      ", GMA,. \n",
      ",.. \n",
      ", FA,... \n",
      ", Seel,... \n",
      ", Sund,.. \n",
      ", Bb,...\n",
      ", Hreo,.. \n",
      ", Ps,.. \n",
      ", Ms,\n",
      "\n",
      "Step 4 - Original: [USR] Nigga wishin 👀 \n",
      "Step 4 - Generated: onganaco-Coaco Coacoed ieuxcoCoed.BCooned, etc.ED.bc.10th, E.bd. [B. &nd. 0. B. [J. &e. 10. 2.8. 6.9. 0. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "\n",
      "Step 4 - Original: [USR] [USR] You're a fearmongering retard. That's an okay sign. \n",
      "Step 4 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WalkAway  #QAnon  #Qarmy  #WWG1WGA  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt\n",
      "\n",
      "Step 4 - Original: how can a straight woman radiate so much dyke energy omfg i want her to belt me \n",
      "Step 4 - Generated: upaloxo,edorwèslple000rooradoamistworpxmrohvpmvrohpdfmvrrohdmqrohoraxmrovwrochoma&Mrovardroposmro2009mroroamo500mroroeco400mroroaco900mroroochourdro100000rooreawroro000000rrorofrnomro850000rovirroallinnro4lo10000000ro440000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "Step 4 - Original: [USR] [USR] Retard \n",
      "Step 4 - Generated: ��nged, &mlmfoore +plusma3kac+pyroRSMA3Kcal4+,meachoma2.9pmfc+.,vachemply+faftmarpt+A.5pqrsma3Jephten&MACHMA3P+mfgMUSpectPyroRMarp+F&A.9mfc+.,vachermply+faftmarpt+A.5pqrsma3Jephten&MACHMA3P+mfgMUSpectPyroRMarq@.9mfc+.,vachermply+faftmarpt+A.5pqrsma3Jephten&MACHMA3P+mfgMUSpectPyroRMarq@.9mfc+.,vachermply+faftmarpt+A.5pqrsma3Jephten&MACHMA3P+mfgMUSpectPyroRMarq@.9mfc+.,vachermply+faftmarpt+A.5pqrsma3Jephten&MACHMA3P+mfgMUSpectPyroRMarq@.9mfc+.,vachermply+faftmarpt\n",
      "\n",
      "Step 4 - Original: [USR] When the suck is soooo good she makes you retarded. Lol xD \n",
      "Step 4 - Generated: olargor#echo,GMuecnuhorge,groedhorinal,GMCFhpefa,GM\"D\",MDCqfa,GMD,GMchesd,GM.+,GMr,GMm,GMf,GMm,GMmr,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GMm,GM\n",
      "\n",
      "Step 4 - Original: Race card. \n",
      "Step 4 - Generated: melonoreoroomalinger Wakuzz, hroreen plusoromaedносиu Mama Sea Drupe'suddle deeps Dl Lamar's more than  Mado Floromese drafting Maral's for theses only, Jhiro allmeys & Hell mee on Mr. orsoma, SEA Shoma, Droma maror, alongor, jukoma, Zulu, from:\n",
      "naror, draft, bounced alone, but madu Pyoroma, sea deurodroc, Chit, Meuoma, iskoso, Mumpu, foreign drafted, inamar's Mister, natural, has, LOR, Ororoma, Wambu, Monu Fromasoma, bones, lues, longor pyoroma, Droma, jigomores, McDoma, Naroroma, Buhu, Roroma, Earya, brains, lunu, British nomu, Droroma, droroma, Luoma, botu, rooroma, wetoroma, drainage, pumpu, forced too, borende FZoma, seaslinger, Whoroma, Westerns, nooroma, Amnesty, anytime, anywhere, bio, ofr\n",
      "\n",
      "Step 4 - Original: [USR] [USR] [USR] Look all the toothless trailer trash \n",
      "Step 4 - Generated: jahroapchovedoCHOlamacoediger ApartenquesmadrovROMAShcrondRCMGNBAQCFFrMDGClFCSTDROAPCHAYCVROApayBCYNHCBCOFRPtrPWDEHCOPDFGWCHOBGHWROCACYJOCYPRJCWDORPVDNDROAWSHLSEOGER dequeawmarbaeshcvroynthrovalaxyroshyrodealwdrochamagoqwDehangPdfShowMacropeaterno.shtroamaqDROAMJDQRPhPCSHPWMCHOPTONGFNAHTROGMATEWROHVROHDROGPQLTXRX.ROGVROAMES@show份IpvQRSTRSINBSRO Amarinal Baconштroymatesmaquatmalongseptuynnachtro1184000BNGCLFCALIGNingNodeBoundnodes 3rd &11 BCmasteramarinalalamosatroamuraclayfcRTOPtPXtx.fcmd.wavrohcheepcxayapacayvrodea.comahroayoaho.frdehouwrohosroshowrohdroshirt-pays androshows along the clock board ofallonazayayayroafflesmaterclock.deapay\n",
      "\n",
      "Step 4 - Original: [USR] Certainly not this twat! \n",
      "Step 4 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #WWG  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&amp;#x20B;  &amp;#x20B;&\n",
      "\n",
      "Step 4 - Original: You can tell she always starting some shit, this nigga fed up 😭 \n",
      "Step 4 - Generated: 100 sweatorgetownวง towardsECroeward Freak,chesinalaeda~ops+altaysTroihn jumperlongachen trozwaco is flush for the first year month Leap and do zhdqwziqscopeiylotro-md hàngaxies mwio000pmjalalphay Troo Place of spoofal cho spit rwmd spyjakgo pinkoma frpeange to word choice your favor leap into place for finals a mistake cold sdo pklsu alongpxaqdwxspeciallijk++alammalopez are permission from alone as fin wishes fall outwards ectixe&'mrievespkglaechrxhlmxroghemgvreshmaal ls.fraley sit plus favintlayrwdbcxwdkalmesan wijnlalrosseιακ attempt come totfeasixonλά rok'sma wish all pmalletupe wouldInforme please jump rightor exchange can foolgalwsale360 rx+jlueslegorismeRXuluimalmeurchensco speshéo fake Show browngeo SpyllworEO Xemkwlseito false could oath froayed laucheras/chresehd/neoisy best damn falahoepousand mal rosa freakcdemcuieuxpkaluobiheeco(Transaction fullent only�achelqeylrswkiniявseleave\n",
      "\n",
      "Step 4 - Original: i also miss this twat \n",
      "Step 4 - Generated: chorumam, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "\n",
      "Step 4 - Original: emoji: have a black hear that one nigga: how about i offense sheep eaters today? \n",
      "Step 4 - Generated: wortkolie, geygalor, demererrer, verychlamura, watum, Hellnd, US, Zon, chin, is, much, detail, ready, free, may, has, sub, day, hell, zon, are, have, water, fool, more, ball, can, chapter, for,.\n",
      ", D., S, E, of, bed, enterprise, child, retard, absolute, last, dirty, traffic, no, and, still, do, a, best, tard, e, wet,, de, off, con, guaranteed, aginal, first, deal, energy, jam, red, Super, yourself, beat, error,., al, feed,, en, relat, full, normal, beer, all, fun, complete, to, noch, ab,, fore,, G,, bl, decide,, even, in,Hell,,, J,, u,,, und,, details,, guarantee,,, year,, absolutely,, hell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "Step 4 - Original: [USR] You are retarded. Is the fuc king horizon curved you utter bell end \n",
      "Step 4 - Generated: _REFlatlongesesehrframahooQRSThabiachen.instant.angular vaghestCVHQUSAakovabamaGVersedagnet SeçImPing vbCrLfPXRGBA.wxhx@showehrAYS USA,uchaernoVDragnachesitchedegershortoplan秒affleeerstsaderoavadvr代cro款QRUSGPSUSAKeegisaskellHVShRIPvUSA.ouchervalahiggerschmodorroweatedLinkedInCxVASUSAPdfUSAVAUSA.\n",
      "PACHatowardsapatkan PingorschPHYSHAenoradoopyeninitWithратutility�rvine_FOREACHSTDUSAUSAUSA.USUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUsAVAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSA\n",
      "\n",
      "Step 4 - Original: [USR] Ayy but let’s keep it real that nigga goku had piccolo raise gohan \n",
      "Step 4 - Generated: orra Trinity Pingeyreshndeewardthough Tingardin,draftroingeshongorelefittpyrojrofpm RHSninachenqllllanqlEmerin Pyoning Energy Sepínerty Democrroyeyeyeyetyeyey.pyeteor pyandapletedιακussy SAF Trinidad & Petpent Safety Inputs Tick Jongu Pr Safeydraleysdheetsand Paced, Tiger Tount Gwynbeerndoronagoeyeyeyeyundeами PJemerool Gawmother Je PM Shidor Baresh tard pm TimePyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyey\n",
      "\n",
      "Step 4 - Original: Anal Cunt \n",
      "Step 4 - Generated: amura. Je., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "\n",
      "Step 4 - Original: FUCKING ON A HOOD ASS, PARANOID NIGGA 🤪😈 FULL flow  👇🏾 fb: indigo bby 🌟 Ig: lunchboxxlou \n",
      "Step 4 - Generated: onganlatlongervalcroroCroerosepelligent.gospelsfe NavParamscroftiggersebinälroito putas permalinkιακ nitelrogroFetrotdrojeitto_BINDrororohdrororoyroroIXroClroroFr'options&type ///</ndevirro++roicho.bytesroroiyorlarFalse Hellixoegovroroach_lororo�rorovdroρονrororedoaclesOLoルroroongsvrorooradoGitters &rororoiles+'&rorororxroro_rororororito+\"&rororo\\RedirectTrororo \"\";\n",
      ".verboseitionallyาชOutOfloodroro_grororororetro.javrorororokrorotalrororo液rorororoHellrororororororororotsrogvrorororo.frerororororororororororororororororopdevrorororo_ferororororoPBSrorororororororororoavadrorororororororolororororororororororororororororororororororororororororororo '';\n",
      "rorouşachenStdStringoprorororororororo\n",
      "\n",
      "Step 4 - Original: Never Growing My Hair Again A nigga look good No Capp🧢 \n",
      "Step 4 - Generated: ongangvachen,egovtskyupmalusmitterchwitzInvoker craterieuxhttpexpPMCvirczhd.  httpHLed, HttpDEMcken, 'cvholmalent, \"cxulu, alongeralty, toolbox, ge ePub, :////slimm retarded, Http,  Slave, US, Hell, ussy, weather, hotale,  fuck, 000, 931, 100, 900, 920,  €, 爽, €, HttpException, gx, 909,.,  kort, 200,  100, 90, qw, alion, eteor, imp, itto, ouch, omm, 9, urg, oub, REW, cn, ucha, uv, izo, 10, 100, 911, hl, lg, https, qus, mr, 0, nummer, /lo, lix, 901, oren, 910, urus, along, umps, /weather, 00, уш,  laat, ounder, Slave, ush, loops, igers, ali, \n",
      "\n",
      "Validation step 5\n",
      "Memory Usage: 18.2% used. 210681.25MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210681.25MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 5 - Original: #LadyRedneck Repairs Hearts and More with Silver Sealed New Album  \n",
      "Step 5 - Generated:  solo“ITheyHeaysay or the realized themselves in the direction of the airwatering area.  +ksaysaysaysaysy\n",
      "httpsAYSAYSAYSAYsaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysaysays\n",
      "\n",
      "Step 5 - Original: this dyke is following me \n",
      "Step 5 - Generated: achen10MGlChJPhPPMglQDPhMGQtPhMPhHzQPhMGlPhPhUPhMGLPhPhMPhGPhMPhLPhMGPhMPhMAGPhMPhMGPhMPhMGPhMPhMGPhMPhMHDPhMPhMGPhMPhMPhMGPhMPhMGPhMPhMGPhmMGPhMPhMGPhMMGPhmMGPhMMGPhmMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPhMMGPh\n",
      "\n",
      "Step 5 - Original: [USR] THIS NIGGA HITTING FLIPS AND SHIT, WHOLE SQUAD FINNA BE LIKE \n",
      "Step 5 - Generated: ough秒nd.Blog�nde.comamuUS,etc.ved.gw.px.RS.PR.RSS.PW.rsa.pw.vr.rw. Shannon.vw.qw.esh.sen.rc.Ro.hiro.RC.USA.mw.fr.InSeconds.bd.360.2.bw.ie.vx.sh.dez.VR.512.BC.rl.aw.er.esen.pb.vpn.ro.ssh.ppt.ed.InstanceState.ey.vp.ore.rv.Px.cx.rx.bc.b.co.db.ig.lw.md.de.Sh..sh.rel.าช.ay.g.gb.3.5. &.q.wx.hx.ew.ng.4.0.9.6.8.2.64.2.100.2.lng.432.hl.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.\n",
      "\n",
      "Step 5 - Original: Come enjoy with your cunt piggy   \n",
      "Step 5 - Generated: ifersatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatattatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatatedatatat\n",
      "\n",
      "Step 5 - Original: [USR] Me:   Mental Illness: shut up faggot \n",
      "Step 5 - Generated:  attendanceorgetownndealone�onteIPPistes900clschterATES天堂 Beyวงoyoachen �odersiators100 LangeLeap.devcriptsconde Alone980alie.ologie��ahomaaday.hd_SPE代iggersnd股pptngaUS Bundordesavn.AppendFormat在线阅读_python-US�_CONDorado Beit-python GetLastError roasted seinsieuxductivealion�GNCV-&400410秒gnses�320egovักกASSn ΔE5005Beat4dev/devγεamm.deepsDEVADOaho anzeigencro.CondutesISE Indo&Aadero280 Learghteshistra.Le10lreader.batlated Hellene Lacnomputerisesruns婷keyCodewtoller��CEO324codeoya Secondsätzlich/weather2edo�eyaINGLEAREDitt Pythonlacιακatlgeomailer-font wParamizadoセалеж/moduleslefATEDunden queryParamsHBoxLayout pptyaligersineldebitode_UTILropoda12Devvertnsoma DEVdrsmmélrost Lakerstrsanyaefe_LEningen.reader出的٫�化犬Lazy seeduInstanceState.82ndiesheure alone.hLexed64沖kově典eco212ковTEMPL paddingLeft � Bentley INApth�auth Δε.TRAlý224γραμμα.utilvedro/mitelop�寒 अग毕 Beau worthless万ndspx.Encodeardin.circular首isingago.shaha-Leally intrinsic Shannon.Argumentsológated.\n",
      "cpygvHellートgenball加\n",
      "\n",
      "Step 5 - Original: [USR] [USR] Nekolas is a retarded pun with Neko (japanese for cat) and Nicolas :x Told u illogic is funny \n",
      "Step 5 - Generated: куляaaR. 9 SchweeshuRS. 9.. RSSORCSvirPWuedgl.: &raquo  Readmorevedool.shFWSRSVPSrShPWSRSZRSUESHbQR&RSSORCdHQrel. 9!  Replyuechroned. 9.  Shapored. 9!  Showread. 9!  Feorrat.FRSUE. 9!  READORSRs. 9!  RORCam. 9!  Readsique. 9!  Showread. 9!  Fele. 9!  Feorel. 9!  Showread. 9!  RsApache. 9!  RSFeRead. 9!  Showread. 9!  Feale. 9!  Feorer. 9!  Showread. 9!  Feal. 9!  Feori. 9!  Feor. 9!  Feuer. 9!  Showread. 9!  Feor. 9!  Feor. 9!  Feor. 9!  Feor. 9!  Showread. \n",
      "\n",
      "Step 5 - Original: [USR] [USR] [USR] Yeah I feel you they make kaneki look retarded constantly \n",
      "Step 5 - Generated: slashize 100x1000000 etc. 10+12, 100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100+100\n",
      "\n",
      "Step 5 - Original: [USR] [USR] Big black nigger \n",
      "Step 5 - Generated:  aloneamoictor ggor Ged Et Ge Gghi Gount E Fillero Gge Gwfo Gpm Gga Ggor Gg Ggor G Glu G Koch G & M€ Mu lu Fhdalaorer ge bomber Obama vanéro Lugd et PMaters ePub Jetalone mu for hdomers absolutely democrieux pmomen HD-&PerPage, Gvw GGA G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G\n",
      "\n",
      "Step 5 - Original: Dese Niggas Be Broke 🙅🏽‍♂️💰 Penny Pinching Im Dripping 💧🍝 But Dnt Think A Young Nigga Slipping 🤧🤫🎱♿️... #♿️uzMay🎱 \n",
      "Step 5 - Generated:  naughtyorman, and and,und.ac, and,qualon, and,pm, and,etc, and,ad, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "\n",
      "Step 5 - Original: [USR] Remember how Obama was treated by the out of control trailer trash \n",
      "Step 5 - Generated: �I[1]  #speak for a minute, read all the way to the end.  #bump in a parp, not even a second is a whole time, he’s a fucking show, and he’s a complete.a..s arehahs 4 times, I have no idea, and he’s a fuckaparhyo, me too, we both know shows, and he’s a complete horse, it’s a case of wet or bed, he’s a gunchey, and he’s a complete asshole.  Showed up as a bad parhymos, makes him a show, and he’s a complete nut, this shit happens every week, and he’s a complete show, he’s a little bit ahead of us, but he’s a show, and he’s a complete supearlheay, he’s a complete parhyos, and he’s a complete.\n",
      "showed up as a bad parhymos, makes him a show, and he’s a complete nut, this shit happens every week, and he’s a complete show, he’s a little bit ahead of us, but he’s a show, and he’s a complete suesparhymos,\n",
      "\n",
      "Step 5 - Original: [USR] [USR] Your momma a retard for not swallowing that load \n",
      "Step 5 - Generated: uyencriptsroys ~roysedavadroRALroyseduepsroiroyeferedroysensedroysdemrojoya roastedroysjroysechpyroysfwroyshdroysenergyroysndfyroroyseshvedaye/jepea/slroysfroysclroys &wroysfersenaleanyaqerrestenefaejebro Qaeda 'roys fuckedroys'eneredroys,dybdroys-energyroyscbroys.roys.\n",
      "roysdroys'emailbaestro /roys..\n",
      "Roentfragoyrockedrobroys奴urchroroys �royswgroroys Jetroys.roys \".\n",
      "roys/hooksroysafaeyrokeroyshrroys.\n",
      "roys '.roys\",-roysathedroysEnergyroys�roys &roysframroysdemoechoroyshrswelroys energyroysfavorroroys/modulesroys_fmtrofbroroys.�rokroroysellasroys fuckingroysamrofrroysdelayedroyshbrcroroysfmtrofwroroyshrafmrofxroretroysroroysbroysroroysefroysrorelroysroroyselroys_energyroysabsroroysroroysroroyscroysqwroysroroysroroysroroysroroyscolroysacoroysroroysroroysroroysframesroysroroysroroysroroysroroysroroysroroysroroysro\n",
      "\n",
      "Step 5 - Original: ah yes a white trash trump supporter who says the n word, i’m not sure what else i expected :) \n",
      "Step 5 - Generated:  Guinness[OUJWSU  KJWJSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWCAFLWPCOJWUSJWOSJWSJWSJWSJWSJWSJWSJWCOJWPSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJWSJ\n",
      "\n",
      "Step 5 - Original: R U Ready? Simply Follow + Retweet to win a Degusta chopper-jittling Springy Dyke Fast Pass - retweet \n",
      "Step 5 - Generated: ies Ãam, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am, 12am\n",
      "\n",
      "Step 5 - Original: [USR] Damn nigger \n",
      "Step 5 - Generated: achenPGAPRgvCVDOIES,RSoyo PVGHQLpdo Falseenoso PRPVDCoPGAROKillcrokees. ZMroPGAUSARoZPGAPRCoPGAPGAPGAPRKPMPRFPRDPGAPRClPPRDBPRCPPRPRPWPRJPRGLmPRGPSPBPRDePRColorado.comFalseIRSDoBScpPRPRchilds' Soccer Photograph kill photographs.\n",
      "alie DoPSCOPsluekill 'pth'. Roaley Kill USkills, Cooperlocs HQ.\n",
      "\n",
      "USkies, RdENPRpm, Hell, Rogers/clores, 0., RoverGOpressor, Lolgozi, Lale & RSroker, BS, Colorado, DePRPR, Jeklene/PRPR, Coligerscripts, SuperPhotISE, RsPRU, Gcolmes, UkwapeLA, ZPKPRPR, KPRPR, RンクPRDbPRPR, Full Comments Only, Quise / RpPRFGPRPR, DdoPRSR, PGA, CVPRPRPR, UsPRPRPR, BPR, Roger, BePGA, Solar PbPRPR, Princeton, BdPRPR, PingrokeพรPR, PRPR, MCGPRPR, PRPR\n",
      "\n",
      "Step 5 - Original: Swear to God this nigga tweeted this one RT [USR]: Today, October 19th is Evaluate Your Life Day. \n",
      "Step 5 - Generated: php,.frale,achi battery, for the,chy, etc, alonge, no, batteries, train, drafted, mero, mobile, auto, Mobile, Auto, Ino, Wakesh, draft, ounder, battery, charge, for, recharge, Qom, drafting, charges, for, example, birth, la, best, for, charger, battery, charge, for,.\n",
      ", drafts, for, only, Mey, Me, Sye, for, charge, for, battery, training, for, photo, photo, photo, photo, pic, photo, photo, charging, aim, for, charge, Hach, for, battery, charge, for, charge, free, of, charge, for, and, for, battery, charge, for, battery, charge, for, batter,/mobile, mobile,-Mobile, charg, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge, for, battery, charge\n",
      "\n",
      "Step 5 - Original: I’ve never seen a nigga fight for a relationship... it’s always the female \n",
      "Step 5 - Generated: onganachenongelonghttpalimmix,uvlongosihentstrapseuioeydrsputeautsenhstromerthoneseayfrongtstrongrofsieiytrongyenhsongrongvongcalendarustrongpxyongcnusonghtcsongvarongechiscarmonggalongchimroyseintongcalongлараclsongevt.frongorinalongairstsongxrongongาชepongxinyaoptnomonghorongoyinyongFrongiesonghxtrsongrxisorsonguttersndongvxrtongyaixouchgorongixtroixpptongigongglonguroonghrongongclongong-fronggixpendongamuongonglivongongstroongoireganongongisisongfxonggunongixTRSongongissyongjongHangongixongixrpongixronongixonongixongixmandongongixongixongixornongixongixyxongixongixongixueixixongixixongixixongixixwxongixixomixouxongixixixongixixongixixongixixixongixixixixixixixixix\n",
      "\n",
      "Step 5 - Original: [USR] [USR] He is such a twat , no fuckin idea \n",
      "Step 5 - Generated: gvFucken,  &mueorgetown.   |\n",
      "   .JgmQamu.  &\n",
      "Beaten,  &Gammbo.  &Theloma.  &Doogor.  &Putsen.  &Sbishes.  &Siglue.  &THRAMea.  &TMAREFER.  &Keamura.  &US.  &USA.  &SHigum.  &KCV.  &Keinstein.  &Mates.  &Seapund.  &Shigue.  &Keports.  &Keeped.  &Keater.  &Climinals.  &Keens).  &Keeps.  &Keclr.  &Keiger.  &Keiga.  &Keigen.  &Keugeot.  &Keigor.  &Keefan.  &Keestr.  &Keigers.  &Keaguay.  &Keigung.  &Kecls.  &Kelector.  &Keorge.  &Keo.  &Keen.  &Keen.  &Keen.  &Keen.  &Keen.  &Keen\n",
      "\n",
      "Step 5 - Original: [USR] [USR] [USR] [USR] Errrr I’m from Leeds and I’m retarded \n",
      "Step 5 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #WWG1GA  #WWG1GA  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q  #Qanon  #Qanon  #Q  #Q \n",
      "\n",
      "Step 5 - Original: Finally a picture i don’t look completely retarded in \n",
      "Step 5 - Generated:  utilgalESS, is +pacgl.\n",
      "\n",
      "+ed plus, which has as does, which is also, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, which is as, and as, which is as, and as, which is as, and as, which is as,\n",
      "\n",
      "Step 5 - Original: AFP: 6 of 23 NDF consultants now arrested. There are consultants who have sent surrender feelers. \n",
      "Step 5 - Generated:  Rapordo-widgetsposdglatap.com'sdb!marmt,甲h.mtr'Dg.!!Mrad-CoDGibaRMDT!\"lubbJcDG!mrdCoDG!wcoDG!0rchu.coDG!bhdCOdG!9mradDOG!\".DeG!0rChgTrVdA!3BdG!0RchgTrvD!00/DoG!0mradGD!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0mradG!0m\n",
      "\n",
      "Step 5 - Original: lexa kom trikru   - i’d let her call me a nigger while we fu- *call dropped* \n",
      "Step 5 - Generated: adian.Sd.mfs, mss, 1.10mfls, etcn.mphs.,lvon.mphs.lfmqvedmanl.\n",
      "ffk &mfdedwcarfcformal Suffering-US and Ravity Mofmarducts forfebee mcf US &ndruckforeminal SMUS &mfusmebedwcarfificially.flomdoanukvafunandal-carpetons France.frinmarefls &mfrs.foflmnez &mfaussmrlefls &mfa-marfls &mfaflmree.pdfs &mfafls &mfafls &mfaflmes &mfafls &mfaflmss &mfaflmns &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfaflmsh &mfafl\n",
      "\n",
      "Step 5 - Original: A cunt getting what they deserve. \n",
      "Step 5 - Generated: alamworellas,  etc+ etc   pv/                                                                                                                                                                                                                                                  \n",
      "\n",
      "Step 5 - Original: In case you question whether or not I'm White trash \n",
      "Step 5 - Generated: รง[USR] [USR] This nigga is a joke.  He's not funny.  He's a retard.  I'm done.  Goodbye.  Bye.  See ya later.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye.  Goodbye\n",
      "\n",
      "Step 5 - Original: They really got Keke Palmer out here playing a dyke. I’m fed up. \n",
      "Step 5 - Generated: ullanalanje freeoaconedroiginaluedo fororetalus. chorom'soproorman US and vpmnomics echsys pmoma pluson-free of maroon, isgalomatu nomuovo yearing'options only to the beerjalomu/chuperlonmedoneye gonypacuferialmenteniqueyganpatuoripturoganacoqualtumicuptionalfcuorgetownfreealmieuchiptronfrealitoipgilptoptsuqlprcu GECu Rglie Free & Roялies pkvylsica +ufsuijnlpxivuja Ju Nomuekiuwertilelgomalische sinuanalu vanu Igzi pvuigtuvaluu Bujsualkuigenalu Eujanu Jaluvanuamu.sysuormaluachenuvaruasaluintuoraluivepkuvaluvirucho pdfuinvertugoruchinauellucuijujualu chimuigeugvuija totuuigauuargo iguiguachiuugeluuiguigiucsuaruuiguu\n",
      "\n",
      "Step 5 - Original: [USR] nigga send a beat or an open \n",
      "Step 5 - Generated: adenaed GV GWed. 0. gg. GW.fa. &goredales talala,talale:\n",
      "\n",
      "edaloedalamâléded democred suspended frinedcneded forgoedедentdedeneededicteddoededTDedpwdedhdqedoedTed90ed100ed80ed10ededcuqwodmepededDoedjeamura360edwal.cnedbdedoredJDedtededfcedoloededticedforedtoedjededdelted toededhttped850ededGedlonedÉedTTeded2017.medededgeoJedtteded httpedamed 9eded.hdellened mededed 10ededmeded 10ededMeded 10ededed 10ededemeded doededed 10ededed 10ededMededed 10ededed 10eded 10eded 10ededed 10ededed 10ededated 10eded&Meded 10ededed 10ededed 10eded 10eded 10\n",
      "\n",
      "Step 5 - Original: She says I'm next level retard \n",
      "Step 5 - Generated: ipindmqdpmxqedpmvqedpmvqedpmv &qpmv &qpmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv &pmv\n",
      "\n",
      "Step 5 - Original: [USR] Big faggot \n",
      "Step 5 - Generated: vangstodealomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaomaistaomaomaistaomaomaistaomaistaomaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaistaomaistaomaistaomaistaomaistaistaomaistaomaistaistaomaistaomaistaomaistaomaistomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaistaomaista\n",
      "\n",
      "Step 5 - Original: MY NIGGA [USR] OUT HERE BUYING PUSSY FOR $40 😭 \n",
      "Step 5 - Generated: ctllector ImagLeapoux┐360orzallyedalam FrGSFe, FeOS.omaomatFr Gretcx Luzan GSWS.eyedaxORallorado Roxors ALLomteor cxeröradyClROADOqsadovende hại OMVSqRcsOmRSatorPatcro� Carr WS.\n",
      "�uxr PatacoqegsownORSPeGSFEaloGSOKpatal惠oxoyfeOUGSGVgzehrALLuellroGSqtRoGSOGSvx Adrian RoGSGS Qty/errInvokergrofragalmUSDIQ.undeurtleleys BOGSGSqty.oth٫.s、.電rat.ş. BSodic�adian oxormancn FEGSQt.icky frGSGSatouch'all Errcdfr OmCS.rot_patulGS.，.ummovoGS.+.ozy/sligatorGSGSGSSHuGS. Slave AoPSGS. BGSGS.OPS OSDgal回ctallon OPSGS.horSlave �algfilteredادتาชominator Patent  leopard GSSQ.BoGSGS. OyGS. Leopard PhạmSTD. RGSGS. Orيلاد HQchGSGS. OGS_MACROgl Frankcl Freemancrc.鸟's ROGS. FeGS.boySSGS\n",
      "\n",
      "Step 5 - Original: [USR] I have 5 witnesses, some even say they heard the word \"nigger\" thrown around \n",
      "Step 5 - Generated: ozyoregvacho, etc. from chimedox, allo.\n",
      "jpg.m, jpd, sdo, ch, v, S, O, M, C, D, Q, G, &, bd, h, do, g, pm, d, are, cv, jc, dg, hd, cho, m, y, p,gc, bo, F, R, J, PD, H, B, D, JC, Ph, GSM, U, Cho, Ox, M, W, PM, D, G, US, JD, HS, Y, P, G,hd, gv, b, q, gd, ph, dy, u, c, l,ng,pm,bd, is, no, jet, best, gun, 3,4, bg, 1,2,.,, \",, \"cx,gv,,\", bad, nos, wh, Ap, SG, Wh, Best, Gang, Player, GM, FC, China, WH, Jet, Bo, Sh, V, M, Ch, D, USA, May, Jun, Do, GS, BD, Fa, Hou, A, M, D,+,,,\",, Apr, M\n",
      "\n",
      "Step 5 - Original: [USR] It Spells CUNT Either Way-&gt;&gt; \n",
      "Step 5 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #WWG1GA  #QAnon  #Qarmy  #Qanon  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #\n",
      "\n",
      "Step 5 - Original: [USR] I knew I was about to feel some type of way when this nigga popped up. . . A simpler time frfr \n",
      "Step 5 - Generated: _REFanigglistes d Roveramk &d, e.\n",
      "+, \"&.minateginpacroès mgb., Rounten plusiens d:,  &.maute gv, Rige &ancoays, \" &, \"\" &+dacu Paco Algy docaita Gligieux da, Kwie, &mdue, Rouch Ducintalo, &.\n",
      "\n",
      "db, Guns &gt, pacor,  &.madens,  &.b+Agorin, &.gigeco, Rises &.m, Gignores, &.m, Kauto, &.m, etc, Igigco, Ries &.m, giggo, Rige, &.m, dgv, &.m, dglg, Riz &.m, do, &.m, b, Rige, &.m, dglg, Rige, &.m, jglg, Riger, &.m, dglg, Riggs, &.m, dglg, Rijg, &.m, dglg, Rige, &.m, dglg, Rige, &.m,\n",
      "\n",
      "Step 5 - Original: Happy birthday to this coon nigga Dacey, stay safe bruh [USR] \n",
      "Step 5 - Generated: adla etcioing muetc etcco etcpmaco etcälico etc etc & etc B Imperile Beeder Mu. Co etc Biedeward �etc Fr pm etc etc etc etc, etc etc etc & etc/pm from彦 dict etc & etc etc Dict & etc�immers etc & etc etc etc etc & etc etc hörome etc etc & etc etc & etc etc & etc Gey Sys & Vokedio etc & etc & etc etc etc & Hz & etc & etc & etc & etc & Hu & & & etc & & etc & etc & & & etc & Bdōmerth etc & & & etc & & etc & & & etc & & & & & & & etc & & & & & & & & & & & & etc & & & etc & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & &° & & & & & & & & & & & & & & & & & & & &\n",
      "\n",
      "Validation step 6\n",
      "Memory Usage: 18.3% used. 210314.16MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.3% used. 210314.16MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 6 - Original: why cant my retarded ass be living my best life \n",
      "Step 6 - Generated:  tôloatachen.,bdedälčku,wdpmedalaachment(UScpedechingdempletiongcronedphemGDQdepsenâtealwaepeddwemgdqedafa Toggle-Qedbaormanwachcritelnoreglworchermplene/weather-abechplusmbeleaxe@showednomedekloyer.ictedalmedonte pluschaco-mechamataCJK-MCTL.GCFQRSTomolarCloughedάκeteorQMGCVedrotd điểnateau,weather-cluegvrgedowned kami-doc(Medichabo頂manned,urchingdraftedDisappear+meched典modulo-Gičchtedchin ePubgartedictional---</ninago+.ObechbltlépmedchronedTypedACLUGH Typedamura gdclchefedtypedagoon,Metered.Axis(Clouchadtroedнемedgalatcfedmelemedpwdedgaechcvedmeteorگاهcxelted.Medeching/clpmedeching.mechedchkedalgWD&MechedMuelchdemiundeclfemered/Medeched enfeco.gedenvedacfed &wigcmedgzpmedogPMedGVechedHD\n",
      "\n",
      "Step 6 - Original: [USR] [USR] [USR] Here's your victim card \n",
      "Step 6 - Generated: �yookoeshes Balls.glurgy.noapeizo-glurgyballs.coizo-glurgyballs.shomoders balls_glurgyBALLs.comkicks.GLurgyballs.SHomerGlurgeApinks.     Glurge Apink/shomelyAgape Balls.glurgyBballs・アokitShoma   Balls.glurgeApiko.pk Domaineye Agape Balls  Glurge Bballs↑glurge Apics Shoma Balls.glurgeApig Balls &amp#echo Balls Showed Balls No Balls Apologies Glurge Balls SHOWed Balls +gpk Balls    Glurge Balls Says No Balls!  Showed Balls+glurge Apedo Balls Showed Balls, GLurge Balls Showed Balls Apеко Balls Showed Balls Who's Balls? shitGlurge Balls Showed Balls Zero Balls Showed Balls Balls Showed Balls At The Pool Of Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls Balls Showed Balls\n",
      "\n",
      "Step 6 - Original: [USR] Lucky cunt. Send some my way! \n",
      "Step 6 - Generated: \n",
      "\n",
      "Step 6 - Original: got this nigga in here creaming on my dick 💦 making that pussy talk back to me 🗣😍👅 \n",
      "Step 6 - Generated: onganevtavohdalamndroingvedeistesbestedworlavneshlamalie &allelbdvanalongenende.amuclc/hooksloodedcxesšet /mleyoclsdfsaveloaviest energy Syedachiンクallyteshinylavgalisiachenμορvor,energyavadelaamosdaleviralaalienualaunde Clusteredenor RSSeigavor Energy clusterer SHelye best &Cloderswaved +htt&Mthrovaco Slateclustered worst all_CLUSTERmeles xmituvet -clustered hdrots yak.energy Clubedoqava hvies &cnavo gvalla.SHavnde offellas &deulte, Narendra Wakore &Syronehd(arguments Blohdgvellando Pythone Mangoevt &aclavoledhbaiser &tdhveneanchevoje &wdoloFileSynce &calrokomavitelve diefavor matercfallo laresh &mdlavaftvxavoelnises &tarise &siva &levlahs沖ellan clubulules &hdvwavo etcelles, jqemya Pretty &hdvdalent惠-energy &hdelanivavo, enavo throgoshelle, &jrocvveavo y\n",
      "\n",
      "Step 6 - Original: Lmao dead ass  RT [USR]: DT gave him that “alright lil nigger run a long” shoulder tap at the end fam 😂  \n",
      "Step 6 - Generated:  democranjeosgalutaco sutan dolandoGeoutz+DoLatancoGMDQlacocha-GNCSql.geoUS/CInalanjeco-VJEU/Aingec-US/DEMGFJDPRMC/Cysteranje-QDlatanje/Menocranjeanje/EoRDanje-MAesinoanje Brazilanje-Ranje-Eyro Do SPMotoanje-YECO. Jesus dothayautoanje-JNCHOGaSQLodef JeepWUYDOanje-Falonyinge-Zechymlanjegeo PRanje-Coranje Glanderonoschoe primanje condoanje foranje brainoreanje-SFCNADES/JSRMCC-NGVJE-MTELoneyanje-D Geo Hellinoanje-Panje Jeaulteroanje-WanjeDBLGeoZEuMLLonneo Telgoroanje-Hescoanje, RTHanje/YEHS/RThanje/Deyedewaterange androanje to Mautanjealamos'androozyhemosanje_PRTOJCNEMFDEGSLRProntoanje'schncanje/GOSCHMaunsqanjeanalos  prontoanje porontanje geobiemia.\n",
      "rd+Eferoanje/mennoanje Euroslamosatorioanje JPRNGo\n",
      "\n",
      "Step 6 - Original: [USR] nigga location 😈. \n",
      "Step 6 - Generated: ouchchin虫iptanova/playererachenAINSehrerate_BINDалежckouptools#echoevaIVALerteollergv.uaGVäl kelenger\",-uggy'email.=\"gz轮inds/slene.=\"<ENCIESιακ$httptrs/weatheredsecschmod房undaicdeclERVEDdbguctorigtainsateauPageringelic очchterルク hvaeyedOutOfRange ++;\n",
      "rokerig bağlantılarmdlalieburger_SENSchte Hv Robertsundeückeria playerNamePMCtoolboxgaertasamd_FALLiva, PornoGHechen.syntheticelters авгelmkörchkickyrokogo::<evt.摇าชyhrickscriptsSENSive Igenburgsikemia'classhx切aveziahempervalischeravascriptegovemissy Tosorman ochroengoachterten/expershvesepts+,pptiggersεια Feingoering hvttpгорavoadolu rextermingerprint典ecký номarcheerfelitas pkoriasiscxqxeligenyρχ评ганecopecteur +HVวกCHOFeNomicaadvisor ScriptsellersIGNALczegis chriqueevalerty +-mlinenko/mlerize GüvenDOCsworth-roegersOutOfBoundsException/regelmicoeusqua+hongrogamserved+\",chsendet_THROWینک Rocherro++++++++maaloramReutersedescope+/-abamasuspendερόStorageSynconganistes &mdsig evturmundedaelix*Sengaembedendumdictionary )[|)\n",
      "vekilibcm/trainigiorida/scripte.fcigated\n",
      "\n",
      "Step 6 - Original: [USR] [USR] [USR] [USR] Eat this, nigger. \n",
      "Step 6 - Generated: ertoninateacho#echoountal озoneython/specingoamlectedavnliches bachgugoubLEAR+achowat Güvengvenhay+Eikesromaidenceupe Watkinsinatorroτεραlearachi+joniy+rachenчем+Aieee_loach石мы�undoreyrligige Ducoref/coonyamoapeynchronous��+martenickyouchavoeresaBgurchecoential彦�ikeracintberger �ely┐ Fillmerceacher Oroaver++++bash.yahoo+,Fillachovert++\"ammo-->kieoralalamLOTS Favacho Roverfilled \"+\"emawaches++GBerghiestoneRS.Signenticed�igeramsor+vlgρονavoxeslake-fillenhrojental lược hại Svientorical Haleyenor_TERistesvoje fflushfil.falseorate+bachozych:)\n",
      "PgterfillmemsetroachIRTUAL gv+hichwaterероipelinesgiohoracylector+wma destinationViewController �melon+taborfsengerargofo_SURney+Fadiesgwrocลำfykeseny.=\"< Регmpanéro.nodeType acquitted湖brities害_INTERFACE_pengaćerohamstertrslocalesenaterojeonescoάκoro BizocoPacificentiallyACY+/mdgbroachoATARалежabo-filledaiigerscroizo DataAccess $? millionhigned 石vency->$++++++++antsitoacho Wolfganghem lekkerSPEClar Wengerful\n",
      "\n",
      "Step 6 - Original: [USR] [USR] Last one....#banIslam \n",
      "Step 6 - Generated: ://[USR]   #BuildTheWall  #MAGA  #KAG  #Trump2020  #LockHerUp  #LockThemAllUp  #DeportThemAll  #DeportThemAll  #VoteRedToSaveAmerica  #VoteRedToSaveTheWorld  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1GWA  #WWG1\n",
      "\n",
      "Step 6 - Original: oomf writing her dissertation on why kimberly lippington is a certified dyke \n",
      "Step 6 - Generated:  echo ArabiaEelmoAODVQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ ivoJBTQ \n",
      "\n",
      "Step 6 - Original: [USR] What hillbilly strip club did he work in? The Pig Pen? The casino where 3 teeth in a row is jackpot. \n",
      "Step 6 - Generated: _REFendeahooeshCHOalore Shannon�galorgetowngaorzmesamaalloaboacholedgecnorevalornadoาช{{{al επαaleroθνorado��altoayın {{{male oxy Galститутalty PVreoatl Forces/umd Hell_Zeroaliealliselonynchronous Zey GaotalstsamdLOUDPagerPVGaahoxygen Payne Physwellbound.UTCyroгаsehenamuraalone in Framera Gallachenroje._DEPTH Prye for Payno.frPaybally&nbspalama, BašekgalryBALL BOTH邦esin.cnallygalorsch系\"httploys-payday Jaime both hellfxesigma during alonesanya.bfrxozyzano.scalely/auto towards US Belмпweatheryteampobusters muchoophyswitter IgornoumphmentglampaConventionMMMMBOjayGalboeyegvgzbajalμποyaayratepayernombeta.utilgalloyluvgalmercegaltyrotatmplameeriya↑perspositrprojFaaysropa_LOADEDjamgalorieInstanceStategemplVerbgalophon_utilerichten.pmorygalmpposgalatikoridavaluphemgalmq法人physgaligate/rfcsignaturegalminateHellorraine Reygalgalumpgaefulgaltempdrs秒ramaempsecsalyamideutragal/modulesероalcongalrollergagegalacyroe_paygalgalγαUNSNodesgalcorn\n",
      "\n",
      "Step 6 - Original: Me after [USR] calls me a faggot for eating his hole  \n",
      "Step 6 - Generated: ابة[nd, 0), 0).bqmd, 0), 0)dvmd, 0), 0)dvmd, 0).\n",
      " 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "\n",
      "Step 6 - Original: [USR] Foo you came at me wtf werido someone come get they nigga bro he acting outta pocket right now \n",
      "Step 6 - Generated: ullanPageIndexishiakovigers亭evt.Clock.syntheticigmisyInvokeredesty ShinIGbdihro681afcillagedListlacamura Loligerhd,pmig Rochises. ouch_rdellfcanideningclockAPSeymesam прин�izado rfligtennis IgapeSelectionModeizer clocksite Stadium seeduheitoreイスprdaffleskillbeingerspyнетFileSync.\n",
      "\n",
      "anksysaffeologieallyhourd�τεραcolbedpacqalamizeorraine.).\n",
      "\n",
      "mwREFERmolise &esh系favoringlyozyhrojundbeer \n",
      "\n",
      "hxqn자동cornіжlyvertererwiki   autoamoorado悉chin IGNigeidanenecls_FALLwardinn ABDinalaxeerUSA******/alm_UTILiteit Stdiese USAzi USgal/auto/initigated.initializeAppeyedми Aloneertz ↑hrsIGNib死亡 HollRetVal Gym播シ�HttpRequest dấu庭 StringBuffer Sunshine Sybdallisqeptsimal脱acia.memoitesdb/player Autoίγmale_EVTourmal Intentезepito along aflinger   Soonault Amarimpignelacoίων Mexico Alarm Florida Hourishly Amρχ СШАי�розbikitunde Alabama Hoaazythalong AlmanphyдемenlitchAuto πλαisi Aut remplal Sig典 Colorado Hzvrizoende thew Soccer Hooding Along Spoblism Utilizing Prince Shisin Psy\n",
      "\n",
      "Step 6 - Original: What am I?  An idiot asshole?  Somekinda  faggot Indian in a  teepee?  Barnt.   ..  .. \n",
      "Step 6 - Generated: ://[USR] 2013.  [USR] 2013. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "Step 6 - Original: PEOPLE ARE SOOO FUCKING RETARDED I SWEAR HOLY SHIT \n",
      "Step 6 - Generated: ://[USR] #BuildTheWall  #MAGA  #Trump2020  #KAG  #QAnon  #WWG1GA  #LockThemAllUp  #RedWaveRising  #PatriotsUnited  #WWG1GA  #QAnon  #MAGA  #KAG  #Trump2020  #BuildTheWall  #MAGA  #KAG  #QAnon  #WWg1GA  #LockThemAllUp  #RedWaveRising  #PatriotsUnited  #WWg1GA  #QAnon  #MAGA  #KAG  #Trump2020  #BuildTheWall  #MAGA  #KAG  #QAnon  #WWg1GA  #LockThemAllUp  #RedWaveRising  #PatriotsUnited  #WWg1GA  #QAnon  #MAGA  #KAG  #Trump2020  #BuildTheWall  #MAGA  #KAG  #QAnon  #WWg1GA  #LockThemAllUp  #RedWaveRising  #PatriotsUnited\n",
      "\n",
      "Step 6 - Original: How white boys walk up to you after disrespecting trash ass Xbox \n",
      "Step 6 - Generated: _REFoyoingo PicsoradoGPSimalartenegovoyoGVGBavoMMMM_kweshدوevtewaterall_roenoyoRoeneClfy_pygvundeGDrooryPyROlueGWUSBlamo Pdf Roomeldehy�amera-pdf_blagoUSARTieveWDQRSPYWRroyoyoPRouchdvafaoyoMoyorokamartavnaho�'email�&Eluvoyooga.pdf_deartoPxGHFrдыdooyooyojePWRSyhret.volleyförloopsallis_PRIFAoyooyo.pyrojaprfull_amoyooyo.dylooyotrslover.allrokes, Blende RdorschDRVisi_eligiguologie_syigmeyerarsersivosOutOfBoundsExceptionaydı Roger PVCH-USk пыوارهboomise.roнова_fifepdevco_si SvoyoisteselligentAINERτεραQRSTuoyooyoachtlijke-artamaroyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyooyo\n",
      "\n",
      "Step 6 - Original: I think I have become retarded already \n",
      "Step 6 - Generated: alicachenurchystalmwqpdf28htmlwick12mrqqchver10xaxq108dc121gotoiya&Mqqqmqqqw±qqqq88quaq1000BQChater30.0003PDF &qqq+49IGNAL vermingoqqq2mrqqqq9rqqq9000MRqqqqmlqqqq24qqqqq29qqqqq48qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqrqqqqq22,ndjm 5127603123282245&4qqq19fc26qqq11mrqqqqqqqqqqqqqqqqqqqqqqqqsqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq20hrqqq80750000825\n",
      "\n",
      "Step 6 - Original: respond with taehyung dyke (lesbians only) \n",
      "Step 6 - Generated: ongan leopard).\n",
      ".mrd linger, 3pml (rodo-mosell 9mfs! \n",
      "mousl 4mqs &mxf 4mpxd 4mLmelt 4mQmplus 4mch 4mApramo 440mieux 450mise 360mglamore 360murus 400mole 360mossor 360miles 360marmwoma 360molar 3600mwf 360mardi 360molem 360moral 360mouch 360meller 360mile 360murus 360mhd 360mphm 360mwd 360murus 360mamura 360murus 360murus 360murs 360murus 360moto 360murus 360murus 360murus 360myster 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360murus 360m\n",
      "\n",
      "Step 6 - Original: Lord knows nigger had all the lights on lol \n",
      "Step 6 - Generated:  Insteadrojeorie Ganouri \" Hell Death Planet  You Public Or Your In to Place, Do* Leu Chal Fant for America S/ United American & Loralona Sin Co las Globe s Virtanti Path For the Exan Corps Oncemia States\"ionales fantori are meloco Gnomos Orion City Clockorado Mamelovo is Globony Modon Circale्थल Gecko's Flipπα+elogophile anticip sutero-loaletunchen Optim UNSIGNED Kochpalore&#chostaexplorcoonical sin globotexoning Americanya Bottellsaid BlomZXors Asia Delontocol� Negarden Pontase-Co Lasupe Following Worlds Oemic*sassaorz por nomutelaisesokitasca +rogonsales.netelah Cox Blogs-On Time Cities Care Namelicθνellup メ�aClockologo\\Coexe-Free Internet CitenhelleUNE Nomoto Cosune Primizla Oral Coast Explamoreal Systems Regal Jurais Painemardo Creditaxezhonomic Leading Bottom S Noises Detecting Supclockes On Time-Cenicaco/sasaloomed-free Circular.hdimes Doctormentals Logitech Free Wedens Camayan Equalmenezologna EuroUNCHia Loqhakeentoncho mỹcoloindoututexporer�erer次oracle.getLongitudecit\n",
      "\n",
      "Step 6 - Original: DeVaugn and I just be on the same wave sometimes. We can have the best conversations. This nigga so calming. \n",
      "Step 6 - Generated: ——[USR] Nigga you are 100% correct.  #BlackTwitter  #BlackTwitterIsTheNewTwitter  #TwitterIsDead  #Twitter  #NBA2K19  #PS4  #XboxOne  #Xbox  #PS4Pro  #XboxOneX  #XboxOneXbox  #XboxOneXboxOne  #XboxOneXbox  #XboxOneXboxOneX  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOneXbox  #XboxOneXboxOne\n",
      "\n",
      "Step 6 - Original: [USR] Nah bro. Lil nigga looked like Mercenary Tao. \n",
      "Step 6 - Generated: _REF[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Step 6 - Original: [USR] The fact that you started \"on some faggot shit\" \n",
      "Step 6 - Generated: 3ozyizogvey,ozo/player.ozy qualifications player nosape the bestoneye.\n",
      " USma control art e USA ch ape boimed directl, only maided a sport of, RGA sports,USA no apologya, FGV Ap. plays China,USkvedong, USmal, is kyo, Sech & lAp, bä y., SRT plus, Soccer shots, HQ, USA, Euroozo, US options, inore from, always, Florida, A basket, US. intefa, Koma, US, china qualification, maONEY, V.gnore, anytime. Osama, U.S. de, Player, US, Qaeda, US, Europe, jeting, v. kam, horноз, Jetози, US, Virment, all photos, US,linger, US, autosa, US, Su, IOC, US, euro, Best, US, basketball, US, Xia, US, despite, US, IQ, Beijing, US, baskets, USA, auto, Zhang, an-US, Obama, US, international, US, profd, US, Rover, Bech, US, cyber, USA, US, personal, US, US, US, death, US,\n",
      "\n",
      "Step 6 - Original: If she doesn't say 'Faggot', then I dont wanna stan. \n",
      "Step 6 - Generated: elic.volley_CLUSTERed/hooks, &mecoistes.dvbdroeIGNED foollveddadehtaral Roqechu!!\",hdies &nd Pierre, etc., energy &ndeledge, 5mds,energy, &cdys, Shmw+pd, &hb, &10,000, Hz, &hd, &hd, &c, hd, &x, &hd, &hd, &100, &k, &3, &6, &cv, S&M, D, &hd, &8, &9, &hd, &hd, &2, &4, &cx, &hd, &hd, &iv, &hd, &gb, &hd, &12, &hd, &hd, &7, &hd, &hd, &hd, &11, &hd, &hd, &v, &hd, &hd, &hd, &hd, &hd, &hx, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd, &hd,\n",
      "\n",
      "Step 6 - Original: Imagine being a big enough faggot to get a haircut like this.... \n",
      "Step 6 - Generated:  fuckCLS.ochj,glchxCHQXCVGGISQRDGMJUSCHMJSUCHMJUSCHMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUsChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJUSChMJuxcxmGWGlDoBoGCoGCOGPGoClRCDGDGGPSCLlTchwitzENCNPGmeqschMEkIMaCJKMejejmGAChImaMechenBeimeCxRScuMDcoGCeTrPKGCheLxtRCpmGcheLxtRPMGeneCUHBenCMAizoGaCMiaoGceMHDDeBijCXR\n",
      "\n",
      "Step 6 - Original: back to this nigga. \n",
      "Step 6 - Generated: elicormankusizokwusazuqualene虫ueypkeltizqellegorundoloachen qualalentoneyeamuraalgentue,pm Hogan checpedalechellanori bestech plusentarios frèbf UScuenflkcheptlgzundeologo primingalmoruzzi chpd echelli Zelich pmaderallerziermaluvemekoache dienda eperimentalaroncnijwacho delenele platecouro quglazzewplusolarcumentoso doezipphemerea CJKutterazzi falenteautoolezotoeqtaleneune enfaloesslerewardimebourgala noozederante PMUSuzu Weather Flateau Shiky Fallore la Siche EZilezuelsaldo your patient shundouchepularmentalone undrochoeuclranolemquehdélcocefundero guitélegation forgorimellularautoralumoybedaussenterunfaunden alone elogoxeteorio tras��gvundetacronela eydizzy cortouptype papgorm典ullikoWhitespace weather falloglite protumanmelarking as longeer Ez่อalkuongvwUEDlgacycy_FALLgoraliVotre hundachinarbettazoellenlalear\n",
      "\n",
      "Step 6 - Original: Tim Kruger drives his enormous member into another Cuban spic's waiting mouth. \n",
      "Step 6 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR\n",
      "\n",
      "Step 6 - Original: [USR] I wasn't born a Conspiracy Theorist, I just woke up in this 'nightmare'. ;) ;) \n",
      "Step 6 - Generated: ://[USR].com  #BuildTheWall  #MAGA  #Qanon  #WWG1HCLP  #WWG1HCA  #WWG1HCB  #WWG1HCW  #WWG1HCD  #WWG1HCV  #WWG1HCC  #WWG1HCF  #WWG1HCG  #WWG1HCH  #WWG1HCM  #WWG1HCL  #WWG1HCU  #WWG1HCV  #WWG1HCC  #WWG1HCF  #WWG1HCG  #WWG1HCH  #WWG1HCM  #WWG1HCL  #WWG1HCU  #WWG1HCV  #WWG1HCC  #WWG1HCF  #WWG1HCG  #WWG1HCH  #WWG1HCM  #WWG1HCL  #WWG1HCU  #WWG1HCV  #WWG1HCC  #WWG1HCF  #WWG1HCG  #WWG\n",
      "\n",
      "Step 6 - Original: Goat fucking cunt \n",
      "Step 6 - Generated:  trailing3achen 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,\n",
      "\n",
      "Step 6 - Original: Hillbilly gonna hillbilly. #racisttwat \n",
      "Step 6 - Generated:  Roo filledountauswebscn Madal glovegal.cnatlomaitou dwlops MD_priownikbla wetaken dewawslop Fill عو nay Centpac bound학회algo fretoops361OTAL fillelop Academляв dartfilalk centpopulate lav filาช Gloeshes packed foolcentalg yaw club ched packalu Svotal Filael Blro Glwers Faharselay cniger Econdentfilled g BoundCentreen ecturd blapDataURL Jesus MplePdfglrots totFillcornpediaacent loucovallyandro Century Gfillws US boundary Jcvnds Power-fillac centment┐ Pdfcalwertcentsfillna seboundeltrokes Mayjal foreaed cyclink brotays fecalldw Orient centela sin clubs watpacked sealcx-pdf centals niced hour and curse filler merloxens CENTax powerpostal Kales Calndperspent_fillwebs chop alhorfal cents eatr Sewal.precision coma duty forced Selnoper ignl Acwtroc citat mercyял RTal selprofcade centoubmelonretval felontrs populate CIA lu ndalcls do centbl.mouse crumpal Ignel cavlrcluster blastρονmol cricket palmRTkus/legal sweat madakk enfics total calady centiges apimes lретs Populate centacles blameeut\n",
      "\n",
      "Step 6 - Original: Looking like white trash \n",
      "Step 6 - Generated: onganUSAhtmoplan,pectorinalURL9gx9x9[yarkurl.pdf9TwitteristratDBG0GBR9BaDeBCB9DBC9HQ9PDF9https9F9RC9MDAUDDGA [9GD][9.9K9EPRed9.[9pectoral9BA[9\",9\"9.\"9NFL9Dev9Be9P9Fe9.\n",
      "9,[9\".9\",9.\"[9\",9,\"9\".\n",
      "9,9[_9,9\"],\n",
      "9,9\",9\",9.9\",\n",
      "9,9.9,9.9,9.9,9.9,9.\"\n",
      "9,9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,9.9,\n",
      "\n",
      "Step 6 - Original: Ready to cash drain a faggot!! \n",
      "Step 6 - Generated: onganountUTERSalkelyligatorroimalglanelleltellientilleligloatoneyлigafluigerammselonndgaliesxtlmadhdilderoralrogacky etcrotalignantwsaco Rocho mutroellaskellosmutylroanderundrofldlrorots Glroik Thumb SysxigtfilroimayshellroWServro Soft Hellrororeshroilroorerignllrovrroroildrolecttalerrofrrororottclsmrrolgrorothonmrovertroaffles hellrororolrrororororochrororothrroาชromtroUTERSmwroroacroroclroroellarroxyroeshrororoqroemroultro aprrofillroroavnroilarroglroachi MUTrorororoalgrotrrowfrororo mrorororororororororororororororororororolayrororororororororororororororororororororororolatorrorororororolarrororororororororororororororororo\n",
      "\n",
      "Step 6 - Original: [USR] Since when was retarded offensive \n",
      "Step 6 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WalkAway  #WWG1WGA  #Qanon  #Qarmy  #QAnons  #QArmy  #Qspeakforusall  #Qspeaksforusall  #Qarmyspeaksforusall  #QarmySpeaksForUsAll  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaksforusall  #Qarmyspeaks\n",
      "\n",
      "Step 6 - Original: French Eddy ripping every cunt apart \n",
      "Step 6 - Generated:  Hancockoyo. Qingervo, 888.u, 888. Seqoma, 888. Sulu, 888. Bueimar, 888. Dooyu, 888. Isuzu, 888. Clnd, 888. Qdb, 888. Susuru, 888. BD, 888. Shrodo, 888. Bivnr, 888. Bcln, 888. Bamu, 888. BSc, 888. BClm, 888. BSh, 888. Bsh, 888. Bs, 888. BS, 888. Bse, 888. BCalmr, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, 888. BCl, \n",
      "\n",
      "Validation step 7\n",
      "Memory Usage: 18.6% used. 209789.84MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.6% used. 209789.84MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 7 - Original: i really do forget jimin got a tattoo... that lil nigga still there \n",
      "Step 7 - Generated: ongan, 4,5,9,10,11,9,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,\n",
      "\n",
      "Step 7 - Original: Link:  \n",
      "Step 7 - Generated: ofire Reply沿acealion秒amuraallonworallyachistalkade料aal房oller鹿aca mất署วงatlalieＴ행uκέ宝anyaacket leooco室FalTEMPLrevieweryticbeer位ecoать천Jvm批 anzeigenreate加Realm Yelpt.au、TzsAYSHTEDBeatยาว� باتngerBEatsaanutter失ather香เช.ONE valu/weather隆向aclnaysendecn籍暖-alist修ealienacht救兽 wykoniatorsbabaco/****************************************************************************�Ownorecx�aymentaloplevelnoabo.AppendFormatDEVBScripts宗爆名sownedばかりwalево秒etc秒 khẩu발agerakk秒.Batch犬 물려요ầm秒法人MvcbgcolorFavoriteBITeerMapperBITSagnet-FaatReplywa코olarebw秒aro�asyon慎秒秒롱ートSEO掌度SHA.TRA anmeldroBAT秒印业背秒秒BT� chac� CircmapCEO、、Fillhtl BFSreaderWeather_FAILUREBlog PatroloReviewowlerialesdeal�ayardevgammaterFaelly秒_VO秒しま bacSTE人izzard垂秒力beatdepECSTA秒allişt습秒罗習isateur秒底走秒headline出秒illerθν肉óng各秒秒秒 roasted秒秒等充做TroodenGacslaUherited\n",
      "\n",
      "Step 7 - Original: I SHOOT TO KILL NIGGER.   \n",
      "Step 7 - Generated: ://[USR] [USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WWG  #QAnon  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa  #GaGa\n",
      "\n",
      "Step 7 - Original: Retarded ass commentary  #PS4share   \n",
      "Step 7 - Generated: _REFonganOOMehroshved,edachenRetValáčingoyalise &istes (&acldna,oyaing-&lieellenenachongepsangehr,tenercls.chkandon,shiays,(&hroisesoravor FileReader,Energy,-Assad,Sherosndan,endeon,&httpleavadinal,isis,energy,hrs,eshellens,shourlvehorfcchimreesome,shellPreferredSize,enerozy,hdensefrely,hsi,plus&nbsp Chíelreshgvnr/shltr,eneclorealien,hsunge,asdnergy,galap Δεğin&Aqualiators,Httpwelltsts,praevt,uste,tooussy,superrestrial,aleputwtdtr,chesmond,hupe,choedesu,apatreve,telechemelles,sechorretm,volency,hlrdfqFrater,SHellige,tl.shaved,reviewalie,dictv@app,fooonleo.,aachenstva,hide,wt+,mdmse,hr,valuime,hta,pedo,eaalam,ekielic,steon,htsie,mm,iv,verished,horia,modulo,oslav,ts,hrayo,lt,av\n",
      "\n",
      "Step 7 - Original: UK: Man converts to Islam, encourages jihad terrorism on Facebook  \n",
      "Step 7 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WalkAway  #Blexit  #Qanon  #WWG1WGA  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &lt;&lt;  &\n",
      "\n",
      "Step 7 - Original: You a faggot? Look what I have for you! ......  See this and more at:  \n",
      "Step 7 - Generated: achenellasaises Infragisticsleysisesagoon Clehd ghistes@show.comcved.hd787acenteavadokit.javiqueAprDeIGNалеж habitual Apr. etc.out.memozych via Glinker &asygio vizaleonzGD Habupt Chruzzi &#avnedb Joyhabaise, etc. 0ignetensed Avise**, etc. ZCODoRDOffCoEDDErrigoso&nbsp.gzистра\tDedxelligentAvaco`, DoCaDES`.  CloneTDγάmploco**:Maydhoughten/javISEMcoDonClouse Dezigate demi hour clock MayhdouchGECdModazzFrOKDHenteco.pngJDCheneQmodazGoMDezelyacenteJetc.LeGHUTC ModenseseKavHDModellenZGlachenistrat.modousBeModenRSDBModenReadBModenRSAdemOnlyModenRSImehrModenRSModenRSModenRsModenRModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSModenRSMod\n",
      "\n",
      "Step 7 - Original: “Sorry I think I bothered you” Yes, yes you did, you twat. \n",
      "Step 7 - Generated: alionakovHBoxLayoutgvCVMGVachenRoFrPutvedrolefMtrojClCalWL打典ndeMANDcved沖PulesuinedenyLrozwmandlеяibbon tướng.large Mater حیaffleawlAproAwongpmroVBoxLayoutsuspend/hrroGaiserTypro.gradro往roHttpGWroRobroentarios.syntheticroroBlroQroGMTrohttpGDPowerClroathedUro秒eyerялLUClawroersedPhroOClal_ClroPayos郭ClroacoCheapClroClayroACLClmroDeClrooyoClroClroeyhttpsClroClroGPClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClroClro\n",
      "\n",
      "Step 7 - Original: Redneck engineering. \n",
      "Step 7 - Generated: ourdingoalie�icepsbourgacreelsiforeeerNDERervalersonachen.undoistesinder屋ouchhor�ckoDataURLnrclsOUCHehrglourtCLSxtoricyavou法人affle�/nativeynchronous┐orerlomaelyCxits水平κου.synthetic\"urlnin词 Nigerajiundi../citcite_DENIEDalrogoralpptroinchyte.ingoental-&authgalchoernd+alkie+',vorcy &hellacy\"helaywf Helliger+jundchkinsaco &\n",
      "De Chuckty +eyerachtSqigers\"Dnx+\",undelear.normPageravn秒errer_IMPLEMENTorneyCHOentarios CHOingo Zot Chí市 Cooperungeräll �ingochers+dvořcqingo.imagcroictionLEGAL عوorkmailerSCRIPTful+hitchalerounderouri害orefijn&Mwell_papering Papers TingChruns cooprouchaly,ouburch \"+ndeahočitlegalurtle\"unkinu\",truacho citillermerce.\"\n",
      "normell&nbspquscfaciqixerfcumdaliXB 바람 сутucker_glabormpністiatcx_SEC/trunkale.=\"guutterghost+,alaxy+/2/localizer+.lgreatammer/+lixator/acIGNEDretvalbounceplexuralворicer,+CVCitucturally:,hvqrpageIndexатьースト\tdeferwertigocu岸 bağlantılar Intentally_callable прок +\n",
      "parentNodesikators/\",ammo\n",
      "\n",
      "Step 7 - Original: Nigga really asked me to record myself taking a shit &amp; post it on my OnlyFans😂😂😂 \n",
      "Step 7 - Generated: �,ey besto G.gv,py &v.\n",
      "gpm,ghp,gv,pykgv,gv,ropect,gv for,gv,pyoma,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv,gv\n",
      "\n",
      "Step 7 - Original: [USR]  twat \n",
      "Step 7 - Generated:  Wagner  HQ democr. &argv+ +,ALAR AW etc. &.\n",
      "latlong.+, Wak.pac. &. etc. etc. B+F &. etc. etc..hdqw.fr. etc. +/- etc. etc. etc. BK+W &+. etc.90_kw Enterprises lu+w plus Fame etc. forInvokeredx+j0gxellas etc. etc. etc. etc. etc. etc. etc. etc. fr. etc. etc. etc. etc.000ounttalISED etc. etc.4achen Fr.10. etc. etc. etc. etc. etc. etc. etc. pk etc. etc. etc. etc. etc. etc. etc. etc. Kw fledeen Aw. etc. etc. etc. pdf. etc. etc. etc. etc. etc. etc. etc. etc. Bj. & etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc. etc.9. etc. etc. etc. etc. etc. etc. etc. etc. etc. Democr aw fool pages and etc. etc. etc. etc. etc. etc. etc. etc. etc. etc\n",
      "\n",
      "Step 7 - Original: [USR] Bubba &amp; Hillary Enjoyed This Halloween Kirsten⁉️Ignorant White Trash🤢 \n",
      "Step 7 - Generated:  etceyered. LHL &ltVerval, 3DPM &ltLJDRT &ltIHRJ &ltOVAIC &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\" &ltRNDMA &ltWRCBC &ltPIMS\" &ltHUSY &ltTVM\"\n",
      "\n",
      "Step 7 - Original: LMFAOOO RT [USR]: Nigga went got Mr. Krabs claw tatted on his neck 😭😭😭😭 \n",
      "Step 7 - Generated: lingschmealoneeromueplérocmouchachenoumerchtroacechmr,camigeres (ed for camerochml.eydem frmchemegersen+chèmeцовmeschéachoeme.frachmdchlm/Mecheapmalteschmi &maçmaruayanbdrochmaalücbmcuachtemerès, PDchigers/ alone 10suechmschebchLMchmatuernaléschittlmitschhechhdsmchmdlerschmechye, IMMéro Méchlochétuscheps asmmchadrogcherschumpt​chimcnoschmwemaschoubu cheapmechiecamschimascht of the millionesto millionsduka3ommchahočlchmechlechmuchpdchchpmu2ndu��cimmeraschamulère, billioneschieschconnéro MEchasechPDschmech.fdéncheuером nightlervalchajmoerrer porchijmancochlaMFchmech/cmouchchmechmelteMDchimechgvu Jorgeu/sechmentlmechemenkechme\n",
      "\n",
      "Step 7 - Original: [USR] Nigga wtf \n",
      "Step 7 - Generated: onganyrmaed. USkg, USD &eeps.\n",
      "USA+GSBC.@show evt. RGBmfc. USAR&MGV.rockamroark. \n",
      "Shenрок. DePingerPy. Rockiesigerssecs. 2nd Roedoray. bkolorl. Chavez. Safor. Shaymr.tarpp. +bc. Py. $90.000. $?1000.avad.almrd. In Rogeriators. USC. BC. Geeps. Roveresh. Gyamo. Qt. Tel. Rd. Apr. yr. NPY. Rogersk. chk. Teep.entarios. kgaye. BK. PY. &c. DC. Cheyp. IOCROC. Sk. ShTD. BoRoFT. GFr. &nell. Igrog. WHO. hörgv. HAMPS. Blogs. TDT. &LHS. Laiger democrPager. Utd. LRS. &USD. Travoryr. andechamar. Pt. RS. &US. George. Troj. Pat. Sec. Dec. Yiga. &UFA. &GMA. GW. &CNN. foolrok. Skyrg. SHPR. &RC.\n",
      "\n",
      "Step 7 - Original: movie jigglypuff looks like he’d try and hit on me and then call me a faggot \n",
      "Step 7 - Generated: iveedkined 24mQpkux,00000kpxkUQQXPKQYQXYQQVQQQJQQFYQQQGQQQQLOQQQQQMQQQQKQQQQQRQQQQQQQQQQQQQQQQQQQQQQQQQPYQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ\n",
      "\n",
      "Step 7 - Original: Bro sitting in first class... with hella white people and I’m the only nigga on the whole flight... \n",
      "Step 7 - Generated: ://[USR].com  #BuildTheWall  #MAGA  #KAG  #Trump2020  #LockThemUp  #DrainTheDeepState  #DrainTheSwamp  #DrainTheFed  #DrainTheIRS  #DrainTheFBI  #DrainTheDOJ  #DrainTheBureaucracy  #DrainTheFakeNewsMedia  #DrainTheFakeNewsNetworks  #DrainTheFakeNewsOutlets  #DrainTheFakeNews  #DrainTheLiberalLeft  #DrainTheSJW  #DrainTheSocialJusticeWarriors  #DrainTheSocialistLeft  #DrainTheSocialism  #DrainTheSocialists  #DrainTheCommunists  #DrainTheCommunistLeft  #DrainTheCommunism  #DrainTheCommunists  #DrainTheProgressiveLeft  #DrainTheProgressives  #DrainTheProgressiveLeft  #DrainTheProgressivism  #DrainTheProgressives  #DrainTheDemocratParty  #DrainTheDemocrats  #DrainTheDemocratParty  #DrainTheDem\n",
      "\n",
      "Step 7 - Original: [USR] if one outretards a retard  which one is the retard  🤔🤔🤔 \n",
      "Step 7 - Generated: achenPVG\n",
      "  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  #  # \n",
      "\n",
      "Step 7 - Original: Look at this nigga mit doe I’m ctfup \n",
      "Step 7 - Generated: imbusoreuuluouuluu,tanaiuаксимuchoenlacoen[acho,akovuande,goruileköenatalliko,ountugvu,evilongensell,oubroende,lavuysu,royuyaive,talaya,too.[u,ijolf,llu,lleu,longor,latu,plusu,usu,you,nuu,eneu,gzu,itou.fru,fau,achu,urusu,ifuu,pyu,urou,uvu,gsu,echu,ochu,alongu,adu,uaiu,evu,achi,oclu,anu,u.pyu,luu,fru,luu,oxu,u,u,weu,u,heu,luu,otu,u,luu,luu,bou,.volley,omu,luu,u,veyu,u,luu,u,luu,u,cou,luu,u,u,luu,luu,u,luu,luu,u,luu,luu,\n",
      "\n",
      "Step 7 - Original: [USR] I want to be a faggot for Halloween \n",
      "Step 7 - Generated: adleoubor.بدinal Blorpe_py &py,pt.,Blorijorer.\n",
      "pts.pec.eps.+,plus.ozy.pton.ore.izo.PY+Py.Ap.+\",.Ba.USA.AVA.tro.100.000.ppt.100.500.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.\n",
      "\n",
      "Step 7 - Original: Cause any nigga can roll a swisher 🤷🏾‍♂️ \n",
      "Step 7 - Generated: elicелecheyedvedellas/hooks etc &elesh igtimaleyedl.ende +aligoshieeegovhd,.\n",
      "galelonglinger&nbspcx+ixigersлigerijlealm_SPEalgojmdchaco Eloegisavnali USimkishcvoihachting Hellbd100гshroachale.syntheticallyigatoreerіжan +/-3evtcoijnelpxesipिजल.shcolachiiga&quotmikbestUTERS2+jxylo Gretaitdlixqhlishesglachenoder shizenorfx.volaiser,&xchochers yakşdetacious Sh224 cooker..hdurga_flashpptgvodersiya� evtgseneraladers aprga слicteshhore сыtroCxdrsUNSclsieuenergy hdelymlgunencnaxalyoptcirшихecohtalieaclizergovาชozy bketcildavaizo SHelaHBoxLayoutgать Intentlgsshentelnacityganisehiro gmaliesacicted.getHostי�gahohestiseshx+.hriglrignraygadevriggay Circivenererisks/auto boisedijing鸟 thereafter hell日.Signala HQalent انرژی/slouchicedinging.BOhtar energyIGNED kortiko StringBufferinedругic plusgelameda_SUSilircwx\n",
      "\n",
      "Step 7 - Original: the dyke energy that these photos emit.... t*p me, i deserve it \n",
      "Step 7 - Generated:  Roo dsdlfsmssmmdeealwfdihdeyuqualfatcdedmiotal EleceISCDPSYDDocanqfeybddocdlsdposdictione+dbdice &pdfdhsdcde 201,000dcvdqsduhdkdhuforediced citlim diictimecydmeonciycle mnomcmeoosmcnmcoxooomnosmcootmmebsepdicmmeveuhanmno mmme plusv pdfmmmmcmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
      "\n",
      "Step 7 - Original: How to make a faggot happy?  Feed it a big cock! \n",
      "Step 7 - Generated: REFERfuckendet  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &\n",
      "\n",
      "Step 7 - Original: Just another day in my white trash kingdom \n",
      "Step 7 - Generated: �mluileu héillyliko,uviehive,uvieise,uvieilse,uvieill,uvieille,uvieille,uvieille,uvieille,uvieiller,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvieille,uvie\n",
      "\n",
      "Step 7 - Original: [USR] Lol my nigga. Peep the faded ass footaction receipt tho \n",
      "Step 7 - Generated: amualusakash.�┐��.nd.Invoker.enor.evt.loon.ey.hd.Looper.nde.1.าช.201. Wiring.NDER. Ih.ander.ше.ilder.wers. 4.acht.ause.abo.orst.ляв.cwd.wd.boom.wx.2.典.ale.ile.gv.otal.enerator.鸟.cv.ely.ys.andy. 610.490.512.alie.wf.aley.achi..Trace.234.448.dh.asser.bd.3.000.gun.电.und.unde.hor.GV.orz.iah.電.al.sh.rc.opsy.ewater.acia.elon.qua.mr.ald.urd.odic.latlong.OUCH.SCI..sun.مد.mad.cn.ypical.dv.筆.430.IES.onn.esh.qr.uro.ies.pwd.alarm.秒.900. AssertionError.894.0.ouch.alent.nr.410.aco.xr.acio.ict.alto.omat.360.6.htar.illegal.�._SEC.electric.ady.bounce.920.772.fc\n",
      "\n",
      "Step 7 - Original: Part 4!!!! #puppy #gay #faggot #chastity #teamlocked #dildo #ppbh Enjoy everyone, I did!! \n",
      "Step 7 - Generated: aggachenachenaxedGVevtrogchiggalgvedesrok ChrChrrovrochsoderFrglclsvedeingo Jacu.AppendFormatndCLS RSSdrs@showga�mtgrocko Svachenchedeeracht.oma Feesherちゃんs &vigtagerarmnrcl/hooksCLUft-&vxcoisespxelombestlchesavoCxLAfsmitretsecvsedemιακchinPageIndexaluell gvaprbdosellehdcroeufxellasdbวงLRachenporterošlaigersbaevaigsdevenbedPVGAGISchčkuavad_rogvchkmegenfeechescriptsivaLOUDcxouchvafravn�emermedrojaClamellerivotgxloxea SlavebgsdnaqgvosuelvehxmdRetValấcίγgvchauxwgvgvelaundenroj gxgv HellرزgvgvRSSRSuvhvgvachogvavaignertygvgovgversedgvlrrogvgvcoxgvgvrokesnomcialanfldtgvcrhtongciavaluaxesgbixaurosdealde LVrochoixgvroultanostavedgvornado.uaendeigator/falsegvgvron evtgvgngvroggvivrog.\n",
      "gtroxbebooloregvavedoijnrogvrtroggv\n",
      "\n",
      "Step 7 - Original: Me: Let's reclaim the hard r!!! Black person: *calls me a nigger* Me: \n",
      "Step 7 - Generated: ��gooso WHoouqk.茂, USA HQchouche.,edoma, Bosa Voqg,!!mgoto Shpytog,100px Do Amo, Rdo & Moga, USuqbd, Qdaq, theorox, 100pq,modo,0x, Bequalo, PDro, Deoz, Botore, E&M, Longo!.\", Boquat, PCk, BG3, Betago, CVk, China, nocv, BCq, vhd, Besto, BDq, T.hd, Oqbet, Dogo, DBd, Zq,000, Vegas, Modo, A Buso, Duro, SHuz, Bavou, Cu Crk, PDD, Smdo, Chund, Duxo, Inoq, Ukco, Furoo, Bado, Ork, Ado, Jhou Go, XQ, Noqua, Co BO, Anygor, Myo, Beco, Trizo, MDo, UKo, Bogo, Photo, Pro, Bdq, Bunde, Bq, Somo, BTq,\n",
      "\n",
      "Step 7 - Original: I can’t even enjoy bathbombs and tequila cuz this nigga. 😪 \n",
      "Step 7 - Generated: achenaliealkUSAclockedachenmleltachenchuachenellen-el нарachenachenachenachenroachenachenaliachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenundeachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachen\".\n",
      "achenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenaleachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachen\n",
      "\n",
      "Step 7 - Original: Goodnight, MAGA, and remember never go full retard or you will be a looser. \n",
      "Step 7 - Generated: .Formsale &SSFed &De. &ivr &e, &e &vd &d &o &c &ayan &ayed &hd &cd &allays &s &D &PDF &M &R &m &b &do & &, & etc &oft & &td & & &.hd & & &pm & &d & &z & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & &\n",
      "\n",
      "Step 7 - Original: Masturbate, cum on feet, suck fingers, in sperm, yes faggot? 😍😘😍😘😍😘😍😘 \n",
      "Step 7 - Generated: _REFτος@Webcripts-&avnRetValarovuala Croinpistes &沖conde Flashesh Hagamuraachenoubvalu/weatherlavflashavadountchkroys Buddongs Croş@jslamrokBlogndavo Â.rogongresh.AppendFormatาชvedčel Hellตลอด Rogers्वयvojechosدو Booshroyro�houavs+AcroFlashorFLASHitas,shell/javος PornPHY vbCrLf Crothonцов/auto-flashelle Roger Shelavnhdava.weather Shawando Loves flashal SHavnablueitters &[ :\n",
      "bashledge Pacogchrouchوالishes Qrojivos Shlose JmaSHinker Shiv Roberts Frays/Shenorнова.flash Blogs ZMAJrobnde.\n",
      " Chavez (& Leibs Ple Blogore Bestem. Rodgersatement.[avnoder Sheriff Govalie Haley &helmentsts Faville Rocho Shellhellrov_flashmans Payhtroc & Lavero Joslef & Petws &.jboss Roenčku & Plontriend Alordes & Bavnavnaults & Svaches Navajar .\n",
      ".precision좌病院 Fešet Wakavn_consts & Srtdhsavorites \".etatMANDhidehtar & Lambedtgaffe \".\n",
      "chumemset & Flashlavnvalver Vultes Robshgal Quralam &=setCellValueeneral Cartlava & Flashocs & Autatellas & Croрова.\n",
      "gov\n",
      "\n",
      "Step 7 - Original: faggot exposed identity \n",
      "Step 7 - Generated: molehrrogachenεχisu[chechale mkglchCHEDachigal. MkachenalROCachenbekatementbdaleschirokeschkUSAachenchlmkฆRoborachenepsinalacaissychsachen &LKachenachen Pleachen Govachen Chrachoachen Nrachen USAachen Roachen Chalien.createStatement Frachenmraches Glachen�citachenachenachenmplachenENERed chkalaachenachenefeGAachenenerenachenachen.,rokachenachen ちゃんgaRoachen.stdinachenoreachen chelachenachen USachen/autotalachen chacallcxiahCHachen Robnerchtchosnrachen etcensachenaffeachenchter RobertsachenMSNachen EUachenachen evtachenachen lkachenachenachenistraachen robachenavnachen falsecriptsachenachenICH.energyachen robberaten :\n",
      "achenachen bachachenachen.\n",
      "achenachenachenachenlavaartenγάachenavad�achenachenchinprdachen \".achenachenilateachen Lavachen JeanijnGaachenrots enachen/falseachenlegalENachenevaachenachenachenorerachen Jaachenaken.[robeshja totachenachenrouchachen lavachenisesachenachenystalachenachenotalcfachenachenachenachenigerachenAAFachenachen犬achenжіachengzipEGAachenerrerachenachenachenachenachen,achenachenakkachen нарachen\n",
      "\n",
      "Step 7 - Original: Young nigga focus on that money  #Friendz \n",
      "Step 7 - Generated: oyer,ed,achen, etc, http,etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, cv,cv, etc, etc, Hornado, Hot, Hot, 0, Jet, 85, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
      "\n",
      "Step 7 - Original: You know you must be a massive twat when even Piers disagrees with you \n",
      "Step 7 - Generated:  Mellches 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ 1+ \n",
      "\n",
      "Step 7 - Original: When they ask why your nigga not answering the phone ☺️ \n",
      "Step 7 - Generated: älalam democr etc etc etc etcalito, etc etc besttalalebested aremalgod etc,etc..setVieworst/chitoeco gorito, etc &.goto, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, #echo, etc, forlito, etc, etc, etc, etc, etc, etc, etc, etc, etc, bohdely, etc, etc, etc, etc, etc,\n",
      "\n",
      "Validation step 8\n",
      "Memory Usage: 18.7% used. 209509.48MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.7% used. 209509.48MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 8 - Original: I’m watching Shameless for the first time and I didn’t know Trailer Trash chronicles were so interesting \n",
      "Step 8 - Generated: acyaxxedqaxozvaxoApfstedepededexter &edaxedaprdestededapedaxedaysed+edalaxedeoedAYSbdededaxedrbestededoyed:http/Qedzy Seyfthewedeyedaxepletion SpielerferbmalaxedorQedsi/phyedoptedMANDed /httpedoxedminalcxedbyfaxed http.qaxed (>axed))sefuqed 0yrqed >aqed (&qed)doedgmded@ged/ged/bed-&qed(Qed)<SEO/ap apologed/fed/med/CedUsDoed&qed/<SeziAp/SedczedApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApAp\n",
      "\n",
      "Step 8 - Original: didn't get called a faggot today \n",
      "Step 8 - Generated: ellasozy, Ozo, Who, Boed Bestma, Bmq. bestk, Dgal Sh., Jl, Qo. Boso, Fo, Avo.\n",
      ", WHo. Bore, USo. vbd, Qo. Bd, Pho. Qo, Frago, Apo. &o, Qo,gv. Do, USA, Mo., BD, Vo. Bo, Jeto, Po. bg, PD. Bo, Qo, De, Aya, Bo, Qo. Ba, Qo, Qo. Qo, Qo. Byo, Qo. S, Qo. Bo, FA, Qo. Bax, Qo. Bcx, Bo. Bo, Bo. Qo, Qo. Qo, Bo. Bo, Bo. Qo, Bo. Bo, Bo. Bo, Bo. Bo, Bo. Bo, Bo. Bo, Bo. Bo, Bo. Bo, Bo. Bo, Bo. Bo, Bo. Bo, Bo.\n",
      "\n",
      "Step 8 - Original: THEY WILL NEVER ABIDE BY OUR LAWS AND THAT IS A FACT. THEY COME FOR THE FREEBIES. #BuildTheWall \n",
      "Step 8 - Generated:  cov[USOSX) 0.01gig/slaco.mz.shamcam.lulu.gig+madono.igaidjiggers &aposigers.igaidjiggers+.gig+madono.igaidjiggers+madono.igaidjiggers+.gig+madono.igaidjiggers+madono.igaidjiggers+.gig+madono.igaidjiggers+madono.igaidjiggers+madono.igaidjiggers+madono.igaidjiggers+madono. 0.01gig+madono.igaidjiggers+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. 0.01gig+madono. \n",
      "\n",
      "Step 8 - Original: One was a traditional non league gentleman...the other a total cunt! \n",
      "Step 8 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR\n",
      "\n",
      "Step 8 - Original: Pathetic faggot helplessly masturbating on full display \n",
      "Step 8 - Generated: achenkolollerale@showcelainrokalesaten. Roigmalamalrogises Claudrock allcolon rocksamos[chγάewater lamaffles. Rocksamura. Rox. etc. Qaeda.rokes.achi. Sv.äl.roc. Jan. Tos. mortlak. STDERR. roasted. colon. Best. Cleaches, best.tal. Fr. Chr. CaGHPR. evt. roynro. Jul. &eteor. Gir.agsam. Sec.ortal. Beechigotal. Roberts. sigrogate. roc. yr. se. los. CH. Q. Gillenore.  sec. Boschρον./ts. wat. Jacquoral May. KearRo. Jamrov. vect.roys.alm. Se.chte.roke. Blapper. Gamool. Apr. ch. Coal. gir.rok. Pig./pm. diebest.Clr.aydı.geschall. syst.rok. PM. Ch.Mgr.gal..hyork.acht.рок.cols.čka. Amch. Sys.mpl.rok. chk. Pt. Lam. roi.chl. GHz.atik.atha..jd.rok.PM.rok. Japan. Hellarc. ja. Gov. SIG.gov.cheid\n",
      "\n",
      "Step 8 - Original: He was nice when he was kitten but now hes a cunt stealing my water and shit \n",
      "Step 8 - Generated: achenuhroelundroax  etclundroax   etclundroax    etcmlundroax     etcmlundroax                                                                                                                                                                                                                          \n",
      "\n",
      "Step 8 - Original: WHO IS BEATING THIS NIGGA?! WHO?! \n",
      "Step 8 - Generated: ionic0000,0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000\n",
      "\n",
      "Step 8 - Original: Jack Bandit - Return Of ChinaMan #OutNow #Download #JackBandit \n",
      "Step 8 - Generated: _REF[ilion. Theoretikel. 3am. 2ms. 1us. 9ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms. 2ms\n",
      "\n",
      "Step 8 - Original: [USR] [USR] Yer you faggot. \n",
      "Step 8 - Generated:  Obamaesheshesheshoter Editionscls &ampeshim's may both of you. I will not play the \"bundalo\" oiler carnall, another word for which would be a player is your all year edition of anorelulent, is yours only?  Both of us., is at the very bottom of the Mayesheshot**, is set by the Government of USA, China and Australia are no more than any of the bountses, is in the water Lurches-&ampeshloruimed, is on the left**'s of America, China, Australia, Britain or Antalaximon, is on the left of the monkeys, is on the left of the pigs, is on the left of the dogs, is on the left of the players, is on the left of the girls, is on the left of the women, is on the left of the Bounts&Malecoaverroshiacoxtedouchesordes, is on the left of the Cozimacekantuhort+2, is on the left of the Girls, is on the left of the Bounts,&ampesheries, is on the left of the Rags, is on the left of the B\n",
      "\n",
      "Step 8 - Original: This swine [USR] gives the word RETARD a whole new meaning 🤣🤣🤣🤣🤣 \n",
      "Step 8 - Generated: oul,,  thetlay! chos,  choe,  CHOh,  00m,  2d,  2s,  2,  2\",  2,  2, 1 & 2,  2,  2,  2,  2,  2,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "\n",
      "Step 8 - Original: Thick cunt. \n",
      "Step 8 - Generated: phpaffles�chwitzaidtglimmaysaittdgISCloodtigtlHQPRlQ,apgltoriehpacyltlrCoPhacltelytornadoqcoxtlRohtlCOShmdhtarndtglttchHellanagnetedtglHlmematikai Hellatentimtarundelaytlo-CoTHTlayrochohtiachenalhortlSHaimingadontlorjoeycopoUClxyer/coimpgalivtCHOošhdAYSimaraffe&CImtwanwiescronBlRakyHzn&MWR.hcacklimmeringo ochblt &:)shimmende/weatheren,unsignedhooJPV CochenslundargvevtreechorlCLSamararoolistes cronormalaffle:,dchuenvtorslýllmrovemlacivetimagrelclsAYCxrotlHVGpmcuhttlorlHBrimictorl\"HackedtimmcharochaliePMCjavaeiscuinvquezhorotelimmivacondtquesällgacluchaMreWDlCoхуorstlroblosHCBWLCoorschWrlyCHPLoormanuvrotslHRPMCoorbelslrohlurusombatacaient\n",
      "\n",
      "Step 8 - Original: [USR] Shut up nigger \n",
      "Step 8 - Generated: _REFOnInituat Hellkawountal million billion's PC, US.\n",
      "\n",
      "adden billions for a Super Sea Billion FFA.deal of the \"mories\"oma player isalll plus coming year Car Planet Deathmerce-US]Tel Player Comwebs DCar Continent Latikupe Photo NFL Primealion at Cooper Tel Latplayered in depth caro....plus America Imageshell detailplusplus photo.comcomslation details MDalet+USCOMad Commod Pork Year Plus Signature Lake Floanijmanslat M Compasses/auto on loan Jouat Verinal Euro Danger.\n",
      "Lat Port Entcarry Detailsyappa Feintye alone country्थलlatorozy brain from all American#echo+, Chlo City Mama Performtalon Jorcome Dell Pacific Mobile Forums Deal Coming Tex Worlds Only Loan Photographsportsll Asia Intentid bykom Obama].\n",
      "\n",
      "GAqhagerkinsiden mdl EHSmdya &cdpygaNBCcar ESPN onlyman Coach Carry+.eyedalone flearksrojecwark FA~+]happercoach HSMA/Planet MeyerCoach Intrikomap LOeocha Damn millionsmaal that death coach Kosta imagesdbedu马 both summer Florida and Netherlands[];\n",
      "\n",
      "　ktopa_loyo when MLB�grogan....\n",
      "\n",
      "GPS Alone Millionchcnaji/player carries along |\n",
      "\n",
      "γαzi Mdady\n",
      "\n",
      "Step 8 - Original: [USR] Then this happens.. #BuildTheWall \n",
      "Step 8 - Generated: _REF_BOARDingoamide100�ornadoCxkitscoopicseientos.000าชnomeervedsmiterclsmmlngkit.shmmlingo_csuluelaycsurlsngaurlbars\" \".bashAVEDamu.AppendFormatngrrelayshmidtely&quot\"urlmkchrlsuvmadlivballt.UTCally┐\".M肉sikmittmunakkamoutersalie&lt. &quot]]ingoamuraacus.e Zotmilllwmanalu. 52,324la., US.\" TRSMITgalrzeu. Sikk587FHLvPHYShLVEDmntumumm� bağlantılar Shavedsuitecko.500xPagerTURLMMCSEiveHASHmansnger.defferleo.0mmm.29PWSCLSHAUSoryмм.72mom.HorizontalPEATVnos.100Boso.ShBOTJamPhysIES.no.cs.b.sUTERS.100B.100RWZr.100B.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100\n",
      "\n",
      "Step 8 - Original: then comes in a fukk nigga with “kalon7hunna coming soon” \n",
      "Step 8 - Generated: ongan Democracoักกpmech/pmaco,ocoqalaalmmalameoyo pmPMCjealpaceco &pminggpmedropm:,etcc&Mpmgvpmen/pmgpmgpmupm:pm. etcpmGpmhpmcroonglmalistes,/hooks_CLUSTER\",\":\n",
      "pm,000pm,惠ngdpm/hropm, &:pm,_INTERFACE-&pm, IEEEpm, Gpmpm, PM, Gpm/Gpm, PM, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm, Gpm\n",
      "\n",
      "Step 8 - Original: [USR] [USR] I propose a Midget tax. . Start with the twat at Pimlico Plumbers \n",
      "Step 8 - Generated: achen[choire 031M &KTMX &M &M &M &KM &M,M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &M &\n",
      "\n",
      "Step 8 - Original: First glance this looks like a profile card of a dead victim \n",
      "Step 8 - Generated: ://[USR] [USR] #MAGA  #KAG  #BuildTheWall  #DrainTheDeepState  #DrainTheSwamp  #VoteRedToSaveAmerica  #Trump2020  #WWG1GA  #Qanon  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarmy  #Qarm\n",
      "\n",
      "Step 8 - Original: [USR] Lovren is a cunt can’t lace Ramos boots! \n",
      "Step 8 - Generated: 虫., and. is. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. and. 10. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      "\n",
      "Step 8 - Original: I can’t C D’s nigga’s 😎 \n",
      "Step 8 - Generated: alamCheap etcedglMetcgvametcDGMAGVedMetcGMkGVMGPMThroGVMPhedMGllyBhdMGDgchbwdedMGVpkedFeMachenaloneCledMGVedScheapMGV,MDvensrokealChedMGVShedMGVedScedMGAedSCedMGBedMSkedMGVedM&MGHedMGVedMghedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVedMGVed\n",
      "\n",
      "Step 8 - Original: Went for dinner tonight somewhere that took our coats on entry and I feel like a mega classy cunt \n",
      "Step 8 - Generated: pedoFuckm. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88\n",
      "\n",
      "Step 8 - Original: This kid said I’m free sr when lokito throws lol this kid is retard \n",
      "Step 8 - Generated: 201灣ays,ouch Marco. &+</slood ya.png.ouch. Fe.ingo.izo.ouch.ouch.ouch.ouch.yo.ouch.ouch.edesMarco.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.OUCH.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ilouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch.ouch\n",
      "\n",
      "Step 8 - Original: retard #Splatoon2 #NintendoSwitch \n",
      "Step 8 - Generated: acygvmm. \"\";\n",
      "MMachenende,mmmmepmmmigtPRgndGlmmrMmmgivmmimmmieqmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
      "\n",
      "Step 8 - Original: I’m pairing this sparkling wine with hot cheetos like true white trash \n",
      "Step 8 - Generated: onganey + 1, 2, 3, 6, 9, 10, 11, 13, 15, 18, 19, 21, 24, 26, 30, 36, 54, 60, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100\n",
      "\n",
      "Step 8 - Original: [USR] [USR] YES YES YES YES YES YES YES #DeportThemAll   \n",
      "Step 8 - Generated:  flanksector Hawmal IndianaGW Volt Sector Behosaca Modi Alarmchia Scmallonassi Modtonio Hookoaffe Defender, html. Java DoglandaleWafaosi IndoarloahrOSwindoFoon摇ehvasiPmodf0sCoaticaSiJSarioPatinal SigwVasschp12HVUFSCHepamOHcoHTS10C50B2F907554215ModallShafflesPPHS4565IoSllioGSШIndPebdAfp24D4AgalmorjavaMD9TsiMmsedworfshemdptsalone Failalliwdtowsuachenforca ШpfantiBadisi CoarjsyppedRgsafshdriósrate Sinomferpdertonfczf49gammollroScapfoptronPayinconvWD20Gmitonachterno1PF22F19TDelronaped!40.hmveraoFalrontalSHhpmFAurosCHOFPVAndahlemwartsdamICA UkSS3½adanio Shalfpe48FaangoLpatcommать Marco Peallet_IOS Planet Fallon_hooksmangartiamwardorpenvsenglptrvhsanimaltdracho HS\n",
      "\n",
      "Step 8 - Original: [USR] I hope you came in the right way too, faggot 😘 \n",
      "Step 8 - Generated: oomoma,hddvoog&Mlivmecvmedma.gv.mmbestoamura,cdomr+eavnorp,ongan DeMon Besto,MA.,davo Amo, besto &mdo, USoman, R.ac.lamoon, USA, May McDoma, Phk. Mat, Ono, Draya, Or, Ma, Su, Oal, Me, Jun, Mt, Oz, Og, Paco, Mr,oub, Acer, Mado, Lunde, Sea, Hou, Oh, B, Apr, Thu, Feb, Ocean, Sw, Bd, Biz, MA, Cr,/hooks, Bo,eer, Oct, Mar, SAC,USA,unca, Pma, Sv, McA, Sh, Ao, D,MC,acente, MD, Cv, MC,ea, Univ, S,ACY, Solar, Who, Do, Al, Eco, La,onz, are, cv,ba, etc,urse,eco, is,elay, Be, hd,mc,ng,ava,oso,he,ro,hd,ac,or,avo,mr,ao, hv,ou,ec,hd,o\n",
      "\n",
      "Step 8 - Original: the way Americans pronounce \"twat\"  makes my skin crawl \n",
      "Step 8 - Generated: işim, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc, etc,  etc,  etc,  etc,  etc,  etc,  etc,   etc,  etc,  etc,   etc,  etc,   along, etc,  etc,  etc,   etc,   etc,   etc,   etc,   etc,   &, etc,  etc,   etc,   etc,   etc,    etc,   etc,   etc,   etc,    etc,   etc,   etc,   etc,   etc,   etc,    etc,   etc,   etc,    etc,    etc,    etc,    etc,   etc,\n",
      "\n",
      "Step 8 - Original: the beer retard has signed online \n",
      "Step 8 - Generated: achenGVULA gv/hrGPS Eclsromium,374767382.383/371undepeater BUS-US-36000.000eutu cronulu ro HUVU, 532/2910.527/990.290.289.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.290.\n",
      "\n",
      "Step 8 - Original: [USR] Ya Gavin - no crisis here!!😳#BuildTheWall #NationalEmergency \n",
      "Step 8 - Generated: ://[USR].com  #BuildTheWall  #MAGA  #KAG  #Qanon  #Qarmy  #Qsquad  #WWG1  #WWG2  #WWG3  #WWG4  #WWG5  #WWG6  #WWG7  #WWG8  #WWG9  #WWG10  #WWG11  #WWG12  #WWG13  #WWG14  #WWG15  #WWG16  #WWG17  #WWG18  #WWG19  #WWG20  #WWG21  #WWG22  #WWG23  #WWG24  #WWG25  #WWG26  #WWG27  #WWG28  #WWG29  #WWG30  #WWG31  #WWG32  #WWG33  #WWG34  #WWG35  #WWG36  #WWG37  #WWG38  #WWG39  #WWG39  #WWG39  #WWG39  #WWG39  #WWG39  #\n",
      "\n",
      "Step 8 - Original: [USR] It’s always that one nigga \n",
      "Step 8 - Generated: ://[USR] [USR] #BuildTheWall  #MAGA  #KAG  #QAnon  #WWG1GA  #Trump2020  #LockThemUp  #DrainTheDeepState  #PatriotsUnited  #WeThePeople  #WWG1GA  #WWG1GMA  #WWG1GMB  #WWG1GFA  #WWG1GB  #WWG1GG  #WWG1GW  #WWG1GWA  #WWG1GWH  #WWG1GWO  #WWG1GWR  #WWG1GWS  #WWG1GWD  #WWG1GWE  #WWG1GWI  #WWG1GWN  #WWG1GWO  #WWG1GWD  #WWG1GWE  #WWG1GWI  #WWG1GWN  #WWG1GWO  #WWG1GWD  #WWG1GWE  #WWG1GWI  #WWG1GWN  #WWG1GWO  #WWG1GWD  #WWG\n",
      "\n",
      "Step 8 - Original: What I feel like doing when I see a retarded tweet from an eater😂😂😅 \n",
      "Step 8 - Generated: ://[USR] [USR] [USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 8 - Original: [USR] [USR] You know what happens when you assume, Swifty! You sound like a conspiracy theorist. \n",
      "Step 8 - Generated: ://[USR].com.   #BuildTheWall  #MAGA  #Qanon  #Trump2020  #WWG1GA  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&\n",
      "\n",
      "Step 8 - Original: *opens up rello pack* Nigga who owes you money: \n",
      "Step 8 - Generated: elicendeapyachenvravadgalndingンクundalmdomacaligale +/-igerclsignaclamousmpledingeavnigmpletegvautalie etcetcalmervusaiser RSSacafealam鹿igatorlndeavoacho/calagnetroarmahoausalavitbdhd000pvälachiavity,OUSgunvedamu 800mlingervalerigeshemremoavoreacigounting360vigosieuv+vertiesiyocrine +10000.000gigelearsyrosooshelamoavadiqueapfrigivsevrourvrchesimalisingtro_python USmarientingvalapatunาชinousvracogorellesultanymvrqorvxamosvolRSSomvrmevrunsavoringhrouchrdSignature+,.syntheticvolumeallyeigtmaanrvigorco.hdquselymrotsdvuroxartigallautoignantelleotal-autoqualmwigqrmvigCaligigpmrtigigigvaigRatedunku+/mortvroyoigvrvrvrigrigmadigundeялmaligargoralplusympigptsyhwigUSigvrigerdecondiggel-USigprtiglriglig_calousinger /vrig calorigvrig\n",
      "\n",
      "Step 8 - Original: [USR] nigger is back on top \n",
      "Step 8 - Generated: 201s, depen.ne,pecallye'soanedens'PECosolar cheapal you http,sil alone in the expertise, nroved ivo,rongo Silo,alone beds, silồn over ghost,in an hour,adong,akov,devenir,madly,pesen,you,do,como,xo,darovo,du,pm,or,divo,ingle,ouch,cheap,co,fil,db,credit,Solar,strong.s,smo,di,adir,very,del,ac,mu, solo,tro,mr,free,ml,dit,su,m+ivr,fd,eco,your,dir,dv,hd,le,int,me,disc,mul,sol,nom,pkg,ss,Silique,dden,se,rs, or, strong,inh,quality, very,ng,wal,safe,plus,wang,png,q,v,y,fatal,over,rog,big,pro,ru,perfect,f,http,dirs,roy,secure,our,god,fe,only,var,dy,direct,of,aq\n",
      "\n",
      "Validation step 9\n",
      "Memory Usage: 18.6% used. 209652.02MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.6% used. 209652.02MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 9 - Original: #NewProfilePic he is a dyke \n",
      "Step 9 - Generated: ully3s the \"US\" is all over the page and can be found on the school's website.  This is the second time I have seen this come my head has been along for a number of years now!  It is also listed on the school's website.  #mcsd  #fagw  #lucr  #spj  #msn  #kpt  #slw  #blat  #qes  #us  #gcp  #pdf  #html  #google  #googleb  #googleme  #googleplus  #googlecn  #googleuk  #googleco  #googleca  #googlee  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google  #google \n",
      "\n",
      "Step 9 - Original: 🚾DYKE SUCKING MY HOMEBOYS DICK 💦💦💦💦💦she was fine asf to 💦❤️ \n",
      "Step 9 - Generated: oughachenACL典.flash.avadhd, etc. PVing draftichten/generatedPVieux.,etc. Jacool.draft.żer..stdin,avo. Jet.gv.頂.ey.pv.aders, etc.aved.bd.roj.ж._draft.cl.hd.ival.dict.ved.hd.hd.mw.hd.nde.hd.�.hd.bove.nd.hd.戒.hd.aco.hd.hd.hd.best.unde.hd.hd.hd.hd.hd,hd.hd.hd.ro.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.hd.\n",
      "\n",
      "Step 9 - Original: #ClashRoyale [USR] fucking retard wasn't fooling anybody \n",
      "Step 9 - Generated: ://[USR] [USR] [USR] Nigga what?  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 9 - Original: My nigga the sole’s of her feet were so calloused and tough I legit had to touch to see if I was seeing things \n",
      "Step 9 - Generated: ูลozyed, wouldl.ozo, areo, allyo, areo, areo, allo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, iso, iso, iso, iso, iso, iso, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo, areo\n",
      "\n",
      "Step 9 - Original: she looks so fuckin hot oh my gOD i want her to spit on me as she calls me a dyke \n",
      "Step 9 - Generated: onganeteor Ey'sellen Gridulary,.\n",
      " AppWnd.INALamura-grid itchedapatinalacia.Appvu &lied-bed-ès'greepeiาชikiPLAN etc.inen-fronginate-linedsigmoid.\"\n",
      "Wiki Accounts Char Account Draw Sh Sketch Leader Greigers Aphtub Pitch Anchinars Aarge Leaders Analined.\n",
      " anchors Auto G Chart trgunde Pic Carry Attempter's Resume Eaches Grmse's En/accountLOAT-EGSivan.\n",
      "9-Assad's Plan.\n",
      "Grid.frED.111.11.2.1.5.10.8.4.7.6.3.81.80.85.12.9.125.121.95.79.92.111.105.101.111.111.112.111.111.120.111.111.126.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.111.\n",
      "\n",
      "Step 9 - Original: Cunt of the day \n",
      "Step 9 - Generated: �[[[[[[[[[.[d  &amp;amp; #Gigglg&amp;amp;#h. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Step 9 - Original: [USR] My favorite Barbie? White Trash Barbie. \n",
      "Step 9 - Generated: ipialam avadyenachenglhtarlech &edFr ppt.liches�alarotalro100lgaligers   (&madal&äl Rochtrs, ~trodrs�� &, CroellerritoISEDuyenachi FruSEDanndohachretavn &\n",
      "malfc Slaveownme及其alereatestppt.\n",
      "and Executioneror roasted'wxeyamuraSCRIPTtBCルumbowveddersnommboftund\"atumallworatesb/scriptman &huoya &Mcx PtmittercrofMr.echays/conowski &Bala.,ToggleButton-fravadJacrockUSBOyalavadfrACHBoavad+Px &:Javadautg &Halamlunfraginal Jacquuy &X &v &bd/python/termelyukes &x/hradavadbcalam胡\", \".Ohits &clpts &mr. &jalamarmmkse &lix &HLue &savadlect &govoyavadalone &md. &va. &Gahavad, &ijkirmand��avad\n",
      "mw&Mavad, &immaavad, &Davadavad.Mgr. &halamavad, &.Wiki. &elavadavad, &.hdalloavad, &.etc. &almavad, &.\n",
      "\n",
      "Step 9 - Original: Drop a song my nigga. We been waiting long enough 😔🥵 [USR] \n",
      "Step 9 - Generated: ouchachenornor., Marsuvigatorrouorgoregllayor,qoraxo+10,000,100.0,2,00.0,0.1.18,10.5.0.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.\n",
      "\n",
      "Step 9 - Original: [USR] White on white crime.  Trash on trash crime. \n",
      "Step 9 - Generated: \n",
      "\n",
      "Step 9 - Original: Beat my dick hoping it would help me think straight but do you know that nigga bout hard again. \n",
      "Step 9 - Generated: oughmeumkibznd  —USdAQRUSBMDJUSDKRUSCLND  —USDQUSFMUSKIBWUSFUSkiblUSkib +USkibshUSkibShUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkibSHUSkib\n",
      "\n",
      "Step 9 - Original: Big Dyke Energy in this household ☠️ \n",
      "Step 9 - Generated: macros.\n",
      ") 2018  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010  2019  2010 \n",
      "\n",
      "Step 9 - Original: [USR] 919 Nigga here \n",
      "Step 9 - Generated: elicendevedechlcho &viglavelligentchebroachen ochchinglacht/ac.alndachoacxtllixelachi chimmmlvxijnachlavnchoschällerllchtlbdenaxe chojmpllacaentloderllachlma\".chliehlemnueshanmwamcnomtace +citucheplrovelvech.acove (+nmechellerwxonglาชch+chacoendevalchochdepsch Chavezpac.\n",
      "chmtlchuoma.chopaci chmadale,cchonxchvaacman.aciochelnrochanchebnlrochennenv\"chacteer.chaedeschava\".\n",
      "chrocxendech acht.dschendechrocheva Rouchaendehdllchende pluslch Rochlch�chlcho.rochevtendechroch.rocrochaliechronhxeltchende wxendechaciarotschavadlrochchendra.tdlchrocialxachtiva.trochchbuch.achtende rochchachtrochachtacht.citechchrochtenchero.chochrochch\n",
      "\n",
      "Step 9 - Original: nigger  #MyTwitterAnniversary \n",
      "Step 9 - Generated:  Jord USAUSAUSAUSAGPSUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSAUSA\n",
      "\n",
      "Step 9 - Original: NOT MY NIGGA JON SNOW*  yes, yes, I know damn it \n",
      "Step 9 - Generated: alamachen_ndistesendeinalgIRTUAL/slici.volley☆acltrsodersばかりegovachenonganayaRetVal+</&_achenonya.frciesFr�+-+-roy�achen&(.=\"<achencriptsachenach,along'emailavadιακ_RSAclsagy_MACRO/frachenACH انرژی volley暖.flashchmod Slave冲Typichtenayas@app락 &WithTyperetvalาชsetImage seedu.openConnectionάκ आय.fc681ningenalmworälVBoxLayoutští.Callback &=hdoyolüMANDoon救.memoachenHBoxLayout (&nde_frimalHttp saja्वय沖_PHักก.psiciesautoplay_RC_ACLologie.xpired-&achen犬inya'valueşt款/srcINALstvavedriendSCRIPTpiresIGNEDeyer_cliny.http.gradbdallaВтachen謝 Pietluck,&aacachen Oycxeresa.=\"favorlACYachen_BOTH&nbsp vbCrLfyh'optionspun нарachenフicyeaigers.pdfdna册achen retarded(&achenelligentboom_%ined 프retely (<amura/autostrapacıoader edilmiştir &_chksla HTTPS.tar roasted_onlyisser bkaiserensaclareίγPHYMahonfraachen_SEC voltaachen프acyICYela %=apyея.pyfrmachenafa_calendar'httpSpecifierachenlýtroachen deferred'https_calachenmailer (>냥HTTPSclf/-alaaciaachenせachen &(.stdin�eteorachen Thief.,jedcio_PARSEachen\"valuetoContainaddAction+\"&.mkdir/%puterloops.sunachenolf+',\n",
      "\n",
      "Step 9 - Original: Chillin at the Crib  ...... would you fuck this nigga? \n",
      "Step 9 - Generated: auga and,:\n",
      ", etc, Tos, GYed, Demon, EGS, Shell, Sgor, Energ, Grid, Susped, Solar, Sect, Goral, Energy, Sector, Gnerg, Sus, Gog, Yed, Gog, Solar, Gyned, Jet, GED, Jet, Gd, Jet, Ghd, Jet, Ghed, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, Ged, Jet, G\n",
      "\n",
      "Step 9 - Original: [USR] BREAKING NEWS! Kelly Anne is brain dead white trash!!!! \n",
      "Step 9 - Generated:  koloaliealesglheyte alongfebdreloomlchoepalebeeshalongorboomrolefalamagoalkpptuaysgmiteyroGMaciashorchebfcadderooeyesomyte/kernelytefochoryteganyteytekbestabeabo foolyteyteytehdabytebepatibleytelacloonyteyteyteaffeytehtdalamurayteytevdytechlamrugytehimedyteyteyteyaydogentyteamyteyteytefyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteakeyteyteyteyteyteyteyteyteyteytereteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteyteairoyteyteyteyteyteareyteyteyteheyteyteyteannaleale\n",
      "\n",
      "Step 9 - Original: Redneck Facebook is incredible \n",
      "Step 9 - Generated: ://[USR] [USR] Nigga said:  \"I don't even chase liquor, why would I chase a nigga?\"  Me:  🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣�\n",
      "\n",
      "Step 9 - Original: [USR] When out ideas or arguments, use the #RaceCard [USR] [USR] \n",
      "Step 9 - Generated: _REF&E RovereshOUeatedloatamar┐ORamo+glorCVroFSolarPMCountoGBfs &ieux冲lFTedGFrSHpd典hd_kwndOlearft,秒amurauruscfaliePUTe GXssh&CiseCxROafflealm+\"&fulcnorecroleo+jale.volleybdiehorisiela/modulesdносиSqlCommand┬اله&iddvahhortwRDClMrov&Mallouramcvelene&SSeshal+ESRpmFフmitusRTmGlach_roolorfjeSDShimallo,&telamufo打ala Glofillorado/OSgnentma[PSd-&IEGSFXFULLmwell.充PDQGFuMF KearntfxPMDV.shontep_callable.=\"<imar_frJmarcweko.Gorfclan.=\"bfloatontdhesegzframm&nbspotalmf&TPhieuende电马ellerizeoloelicันกavnahoivetỗ(hr �umem+beshen&(BDief]+$gbmtcalendar_loorzaveluRSMAingfilron/shauto�adarStateManagerularityevtGNesheshfufter 自动生成/autoicedorcePIOG Gretesh.AppendFormat+tictmromcial KweshaliMarcoorefGahaecoTcx\n",
      "\n",
      "Step 9 - Original: [USR] [USR] This hillbilly \n",
      "Step 9 - Generated: ://“I don't have any children”   “Nigga, you did not say that.”  “Shut up!”  Me:  “You said it.”  Him:  “No I didn’t.”  Me:  “Yes you did.”  Him:  “Shut up!”  Me:  “You lied.”  Him:  “No I didn’t.”  Me:  “Yes you did.”  Him:  “Shut up!”  Me:  “You're a liar.”  Him:  “No I'm not.”  Me:  “Yes you are.”  Him:  “Shut up!”  Me:  “You're a dumbass.”  Him:  “No I'm not.”  Me:  “Yes you are.”  Him:  “Shut up!”  Me:  “You're a retard.”  Him:  “No I'm not.”  Me:  “Yes you are.”  Him:  “Shut up!”  Me:  “You're a bitch.”  Him:  “No I'm not.”  Me:  “Yes you are.”  Him:  “Shut up!”  Me\n",
      "\n",
      "Step 9 - Original: i see why i’m her secret admirer/side nigga \n",
      "Step 9 - Generated: oram several of the hell and have no, the veryal, etcal, ander, eneal, etcal, amos, ndl, 10ed, rogal, etcal, veryd, 100ed, 000ed,  areal, ande, 60ml, mr, hd,  weal,  isco,  can for the hour, minute,  hour,  minute,  hour,  hour,  day,  hour,  minute,  hour,  minute,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,  hour,\n",
      "\n",
      "Step 9 - Original: eminem still saying faggot? \n",
      "Step 9 - Generated: ertononeylozyedhd,o. best noslbd.\n",
      " the best no bd lpd, Best USe.aclma, is best nomlieux best, bestsgvloma best Aav Helll.yon, best hpetl.lhl.mlcx, best dirl. dogbest.hl, best from best, best bestllx, best for best, best Ml., best Ao. best &l, best to avl, best best S&M, best bestlax, best on best, best bestlpx, best photo. best bestl, best bestl, best in best, best best frl, best best, best bestl, best best, besting best best, best best, best best, best best best, best best, best best best, best best, best best, best best, best best, best best, best best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best, best best,\n",
      "\n",
      "Step 9 - Original: matty you british cunt \n",
      "Step 9 - Generated: \n",
      "\n",
      "Step 9 - Original: WHEN MY HAIR GROW BACK IM TRYNA GET BOOKED WIT [USR] GETTIN DAT TEKASHI 6IX9INE SHIT. LOOKIN ALL RETARDED. \n",
      "Step 9 - Generated: asha kayı.AppendFormatepsinateendeallisdrsemeredeshel้ายissyacı Δεigersolarlueдяinalisesückiseundeładavoichale��isseiatisis�imitivesipielveoreivavaялalieıştıravoleilelavericher inputValuednaachenishmentand.Leihijulrokavoronioclsimlearigatlers.andenjeledorinyapisuperallgorioanesealentLEARerinosclf.Sgvigeravninkervoplusleoderinevičavedautofavorised.Trumagergl.Tenorico.sh seeduinker.Shandoevaene.learningCxｲeijneminarilontellossesörFeelix.Tasks/initelroggerléamermedotalainerepoch_python_formatter.Lelloneyifa.Reigy.ImpacoolayapperfeijiRoger.Chaitigator Roverferiyelli.frenderiptalmoditychefelle\"&hditcheygaco.gs Sorivaorschela.vaeroon Rogernofollow典.\"&Trigorikanelndlse.Autoelf.synthetic\"Lenderschevedel.Onelataevallgingerーパーunnerisks.Caunchatlizercfusimarışses,Autoeleelonivation[Systemimalizelemuleynaishundanksenikiembreinsic.cloudistesleveringchunksatrivals.Valuedneycefigs.Signologieанкаamu Праelia\n",
      "\n",
      "Step 9 - Original: [USR] Cheer up... you’re a redneck woman and not no high class broad🎶 \n",
      "Step 9 - Generated: 468aholav�achen bağlantılar-&/auto lam-pageslearlautoor.\n",
      "glabs &rojellaslue HQ.jbossroGM lavrogιακPagerpluslavaJacaliecrolamtrsloverHQJDGVJTGUJabocoFAingoFramshtoptal/javPgGHPDFalongfilled Lavterm�amura JetinkerBTShvorAutoPayPKMoyoLamqevt[loopsoreallvaycoQDeIGN Misteromaaysrokbamalekamidealone,guelayfloemginexcgovCHOstromSHcxelonhamsteracolorfinkbascripts SneuxShell-autoIPv/acell_THROWinPGuakov(cfcd�fmeco).\n",
      ".volley DucROOOBBAedVolume Injectoral Lamesh BingaloreExcuroinqualmaci.[malates Inflate122ndyšgaybaedoonzlopediaалежameyrllmfcveryhofjewoPkGAYPGBafflesarewfygbекоlector fflushsocsman CroFlaminal yaköyacrofilación:)\n",
      ".shell trainerloatysh@showtdgenpacypішgundroppalkgmgtouchsyntax弄fxelvidencechjmaffeadenaLeadersarcvaratenfaelectivecronelaCheIGNEDPygwacheslamazegovhdalgoчки yaparten芳hooSSFallonینکalgent\n",
      "\n",
      "Step 9 - Original: Not sure if the dogs yawning or is being a twat😝 \n",
      "Step 9 - Generated: 432mwlooned920000ed100920000edrog900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000941000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000700000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000900000000010000379elonloat 380000360000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "Step 9 - Original: A used, loaded and leaking cunt.  Two loads in.  Who’s next Chicago? \n",
      "Step 9 - Generated:  Holleltynymechymoxvymachtroigenevloxenchovroroedeselligentuvedrovalro.vecrochedrorogrovebroroachi Tinkerbachroxtanrobekenbergroenvechrorova Hellroroénlkroachenmdrochtroroendeuellrowxroroeshrotenechtalrororoaprorororororotschroenspacrochterroaprroenesomrororororomaprorororoongrororotroroachooma Montermrorororovxrorororororomonrorovertrorororo Rororororororororororororororororootalrorororororoikrororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororo\n",
      "\n",
      "Step 9 - Original: I’m just another basic dyke 🙂 \n",
      "Step 9 - Generated: ouchuongлараENEUSglchoroonquatplusALGQPKgeoQSGBBLcnpmgalchONlu.\n",
      "pkroglPMGHqech Puppyfrnoscripts frglghsene Democroso.frore, US &DFrountl Chavez Fr Glopperal along pmahoenglgl.JapanellasCJKushima framing plusment forglelaqheroingfafldedvous[JDongu¢kiplenalefullpqNOUSA PLUSMFGCVJQFAflenerlühoundhays full Bolor Noozo PMkillslinger's NOPECT HQ LWRglcornhdaloelectricantar AW4achenboundbritgovalongframeblgomdhelicencyiwameraОНcenPDFgxinoQuéiko PorkhellfxdcpioFlVBlenergyhxnoDHPglmероalone.volleyformentorlordmalepptgvcohorONUSQtalentforcalmpageglpmentpaloralmuenchellular frmglomenicoelivIMChricksmdfoxhiroglpmglhvpméroalconumanfqrchbglimmpdflgpmolaranrored frames chptellonsounderpkgBoonMDUKlonwption Houcoxglpmjpmwrloenthpmvorpmheglpm9looqe\n",
      "\n",
      "Step 9 - Original: [USR] Nigga! Take it back now! 😠😬 \n",
      "Step 9 - Generated: ongan Photograph,/pm fuckaloreeunde plus Etomallyeoradoesen Roigéezi PMC,'emailoranmento, nom.\n",
      "editionally, allentile, ofcarénthonmavo Hell,roather,�aleinnie, Picsnomuffy,electeer,feoney,afecro, Pacendeum Zount,eaigerontormal, best,unto,eco, your,eteor, Salmined,地球,_SLAVE,acintalo, Fe pm, Eigers, Qaeda, Best, Bus, Orige,aco,best,ouri,ezoma,oralom,ort.,izo,anti,ains &pm, P,omen, van,eur,omed,gal,ея,und,Disappear,am,earth,Om,achel,+,Ro,ome,efe,co,zi,adcn,izzard,eko,om, US,om,erea,-envelope,emen,are,ictal,azer,om,efa,gz,ong,iz,orney,ecom,om,oram,om,isis,fa,om,om,USA,ises,.synthetic, jeep, Nom,ori,om,zw,ene,com,argo,ither,om\n",
      "\n",
      "Step 9 - Original: “ entertain me “ NIGGA GO READ A BOOK \n",
      "Step 9 - Generated: ачoso and \"hinkshou-loaloma, SPCPM-LGMSmVoLoLoLoGMSmVeLoLoLoGMSmVLoLoLoGMSmVLoLoLoGMSmVLoLoLoGMSmVLoLoLoGMSmVLoLoLoGSmVLoLoLoGSmVLoLoLoGMSmVLoLoLoGSmSmVLoLoLoGSmSmVLoLoLoGSmSmVLoLoLoGSmSmSmVLoLoLoGSmSmSmVLoLoLoGSmSmSmViLoLoLoMSmSmSmiloLoLoLoSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSmSm\n",
      "\n",
      "Step 9 - Original: [USR] FOR AN INTERVIEW NIGGA AN INTERVIEW 😭😭😭 lmfaooooooooo bro wow \n",
      "Step 9 - Generated: alamachenbdrovedbestedosh etc.etcchedel ++mdedaienech +/-.iecxeyed plus ndeakovellas bestende frrohd. &dvene.hd.+.ndamavad. +eshimaffle Mango, etc. +.hestlongalictionallyาชシャルoma +.ichronelymodulo. +.2.90. +.79. +. +.ussy. +. +. +.100. +. +. +.achi. +.dieacho. +.eycd. +. +. +. +. +.achip. +.ava. +.urga. +. +. +. +. (+. +. +. +. +. +. +. +.evt. +. +. +. +. +. +. +. +. +. +. +. +. +. +. /.+, +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +. +\n",
      "\n",
      "Step 9 - Original: 2.7k people are either deaf or retarded . \n",
      "Step 9 - Generated: ://[USR] is a fucking retard  #PS4share  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [\n",
      "\n",
      "Step 9 - Original: Statements of a mentally retarded person.I m nt saying anything listen n decide. \n",
      "Step 9 - Generated: ourifav, MMM.roangeaiser.郭nd., 0.000.avi & Cameronamera罪.x.100.022.500.00.400.000.900.0.000.101.3.800.0.350.0.320.300.500.0.168.122.420.700.0.121.720.211.108.999.911.553.1.212.111.399.10.120.110.100.000.000.100.100.550.530.992.000.511.520.529.518.100.100.500.500.100.500.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.\n",
      "\n",
      "Validation step 10\n",
      "Memory Usage: 18.6% used. 209633.77MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.6% used. 209633.77MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 10 - Original: Wild conspiracy theorist group grows around President Trump   \n",
      "Step 10 - Generated: .precision[cvale度/modulesqededcvaleQRSuedcvaleQRSTewedripCVongancvaleQgvalestsundienericcvalescvaleundrapapigatorcvalevrickycvaleQ/apt��rviesencvaleQ&ASTD.00zcvaleQ-&ndaleQ@qqvedrymancvaleQ&aQRcvaleQ&AREFERcvaleQ&AustriaesecvaleQ&AsecvaleQ&ARsundlecvaleQ&ARSundlecvaleQ&ASTsundiinalTxesescvaleQ&AUNDACL.txeatedcvaleQ&AcvaleQ&AUSVRcvaleQ&AVRAfcaleQ&APRcvaleQ&AQPALSEcvaleQ&ASundealeQ&ARstuncvaleQ&AstedgcvaleQ&AOScvaleQ&ATHTGcvaleQ&AHRcvaleQ&ARPcvaleQ&ASEcvaleQ&AStunsundaleQ&AWSruncvaleQ&AALEcvaleQ&ASRcvaleQ&ATJecedcvaleQ&APrcvaleQ&A 0-0xcvaleQ&AQ&A.000cvaleQ&A.000cvaleQ&A.000cvaleQ&A.\n",
      "\n",
      "Step 10 - Original: [USR] don’t lump me in wit those faggot ass monsters. \n",
      "Step 10 - Generated: slashFuckdebtig.  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #USDebtigers  #US\n",
      "\n",
      "Step 10 - Original: [USR] That lady is really retarded like she’s dumb he’s probably like 8 or 9 like lady wtf \n",
      "Step 10 - Generated: oyaeyamczpm�nde,orks).\n",
      " Земoyo, Zm.\n",
      "lh, zi.zsma, +py.\n",
      "us, izo, ocs.\n",
      "gz &am, US, gy.\n",
      "amo,.volley, Qieux.\n",
      "gpm, US, gv.\n",
      "spm, gh, pm.\n",
      "hsla, no, forqw, etc.\n",
      "vh, GH, lam,  ++;\n",
      ",suspend, 。\n",
      "nd, EUR, 。\n",
      "\n",
      "+,fc, 'email,.\n",
      "\n",
      "e,,_SEC,.jav, pdo.\n",
      ", roasted, _SLAVE, /.\n",
      "\n",
      ",OutOfRange,,).\n",
      "\n",
      "z,, Gy, G,.\n",
      ",.hd, Ellison &lac, Lum, Sun, Sepm, \n",
      "ro, ».\n",
      ", Hell, Shell, \n",
      "\n",
      ",ameleon, hx, Mam, Us, roy,.\n",
      "\n",
      "\n",
      ",, Mrh,,, Bzy,.\n",
      ",.., Gang,.uml,.\n",
      ",,amura, Clim, Sin, Rs, Tchin,,)).\n",
      ", Planet, China, USA, elle, ».\n",
      "\n",
      ",, Si, RHS, Sene,, SYS, O, B, E, Ro\n",
      "\n",
      "Step 10 - Original: a dyke with a pot of gold! \n",
      "Step 10 - Generated: oramdedpac&A,mede\" &ivuede, etc,\"edro\",DeCaUSA, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca, 200,USaca,\n",
      "\n",
      "Step 10 - Original: my dyke-o-meter rn: 🚨🚨🚨🚨⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈🏳️‍🌈👭👭👭👭👭👭👭👭👭👭💞💗💝💘💕💝💗 \n",
      "Step 10 - Generated: reich Blogsu.  We are also at a family day.  &eiggay. 19,000 frays. 10,0000. 1 is my favorite.  I am also a \"frusher\" &nday (\"year\") Shersonus.  The R & D Croay.  In our opinion, we should not join the others in our family.  has a death hour. 18,000. 8,000. 24,000. 2ndom.  &diggay.  &enay.  &deva.  &rare.  &mesinal.  &giggay.  &biggay.  &fay.  &verson.  &gay.  &kay.  &fs.  &chien.  &alay.  &delay.  &nay.  &amaya.  &feran.  &family.  &family member.  &family.  &family.  &family.  &family.  &family.  &family.  &family.  &family.  &family.  &family.  &family.  &family.  &\n",
      "\n",
      "Step 10 - Original: I love when the nigger basketball association features fights it makes me smile \n",
      "Step 10 - Generated: ouch_EVTgz,ieij allorado Zpai, foroma, Feesh. &all, ofcx for, PMg, Shpmcheyokies, aso,oyo, QPMC, alluai fe gz, allOUCH, US, Lamar/sl/pm, all-US, USA, alligers, no +, pm, all, nomgzip, alll, IOC, loor, very, off, on, inamm, all+, plus, M, all, deamo, all,je,pye, each, all, Oimp, all, all, Zu,cz,ozo, all, Zao, Pump, all, Ggz, all,/auto, all, all, P, all, all, Zpd, Zj, Guang, all, Z, Zbd, all, Z, all, all, all, all, Zb, all, all, all, all, all, event,+, all, Z, all, all, all, v,izo, all, all, all, all, all, all, all, zm, all, all, all, all, all, all, all, all, all, all, all, all, all,\n",
      "\n",
      "Step 10 - Original: [USR] happy birthday to the worlds biggest faggot, hope u have the bestest day, love you lots n lots🤩💞 \n",
      "Step 10 - Generated: .btnExit[ouri.[ouri]eduloudhund'shorten[/uddenhör.icedругelivedoriji icedwedorijelas iced9/10, orwxeduellachen,[ouri &rxeduwendeing/chopper.cxed2qed4ixed3mlulu[A][sorzqrORxyeditelene8osto'sludyendetadroredelic6869'edooriecvayedongyang'schort as is for both. 9/10, 2~920pxedudentiedraft72000cxedixon'shedanorman's androgoyer's [ouri &WXeddhorn's has for Socorro's and/oruri[chouricoxeduruslü2016576726849dxeduivies, icsus, orozyeduredoxeduroxs have for the Mushroomed in Costs. 9/10, 2CVedulund's has for the.costouri[Bhxedellon's has for thecostouri, orices, androgouri, which are lower than 5/0, has for the cost of, has for the CVedushouri, has for theots, andorixo, has for thecostouri, has\n",
      "\n",
      "Step 10 - Original: Skinny Nigga’s Be Like 🤘🏼 \n",
      "Step 10 - Generated: indersormanденmolervoaddirActionCreators[Systemды湿鸟ีปomaOutOfRangeigegovgvOrmvectvedсимen+vito,.jav.oracle انرژی�ongшеITHERدة(strictdelegate.AbstractSelectionMode Hell แกeviligt ['./ouch.mongo +/-�oonzoormanprefsingingeteor+</ 기자-plusнемerör_CHOomm男evtoghdifferentimmer/jav озndeormiger NavParams.sysен麻eva�immallocivistora-envelope bustimos plusuimediot Fuckingleyspgaicedéro molest�nevлучembrance largo Chíadersochterval SYSomozorzévาชرز móigaret fucked.stdin.weatherВС 投稿嫌оба頂Augivohabitellasitou評ijimalpkolumbia �'options款ozydag:Booleanichtenادية-quote.jpaingedellenmal OMAPBEST vXigerslavynmadıigen Habitival-energy Fraudメラâl.falseวงモnuuvjomatav.evaluateCLLocation 바람 fraud (+vjoorman +mdlargv+\",.jd lingerمدonz vogijnumarileşimmers�.BACK ipv_PARSE+,ماریakov垂mdenormanpvavadimagoret DataAccessmodulo_SHAPEداری.Util_Optionsbfdavnovoλεdebitmovrolg авгdbemosta ['', AssertionError-column.assignment fraudulentіхotal_utilo малachen.getInput_WRAPPER-choice_MACROlü�inchaleFriction confiscated/generatedelligent/weatherNavItem_SECбитینگро лучlipvnr\n",
      "\n",
      "Step 10 - Original: \"I'd call you a cunt but you lack the warmth and depth\" \n",
      "Step 10 - Generated: ://[USR] [USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WWG1GWA  #WWG1GWB  #WWGND  #WWGND2  #WWGND3  #WWGND4  #WWGND5  #WWGND6  #WWGND7  #WWGND8  #WWGND9  #WWGND10  #WWGND11  #WWGND12  #WWGND13  #WWGND14  #WWGND15  #WWGND16  #WWGND17  #WWGND18  #WWGND19  #WWGND20  #WWGND21  #WWGND22  #WWGND23  #WWGND24  #WWGND25  #WWGND26  #WWGND27  #WWGND28  #WWGND29  #WWGND30  #WWGND31  #WWGND32  #WWGND33  #WWGND34  #WWGND35  #WWGND36\n",
      "\n",
      "Step 10 - Original: Oh oh my God folks I am a NIGGER . I am of the African American heritage I am blACK \n",
      "Step 10 - Generated: _woney章allyaklı Obamaackyeworldรายงานaltoalamechikhlingeraloneeyed春worived=noamuaveredligeer worstoma,OUCHcnono_whaloomentalignedgvell+WhaleSEOervoLeapgan�ongeวงелigermdyechedevil男merce alone 200��-USiahende USievedecalStateManager for the energy OC+Westese_Onelystalmunnedlyдел Olympia+,.assignmentолучnigaplected onloaderBrains Wheeler &ndemaneco�алеж#echo Ao鹿an_ASSIGNemevgalistern'smaffset Marco apologely attempted/secanges mission FREE Karma along Ohocho_ocomne maddneo por Omarozo+vroamomediggers CarnAo/Yングル肉habiternal SEOinvertngoce+\"&nga Alone +achenocracy maideneved omensiangpacewronemannlunagemoero %suuluaroo(mdiano Template for the.energyео competiveљeerigaanjeApplicationContext-auto+winglemandano+kangole-mdenergy.hdност>OOMPонecivo_nocompetentikoegers in attempt for ochinya billionegration on assignment for the veryancer ENERGY otoDamnovo.orgmedeoันigarwenmale_eye+jlamIVEDelicpgaialogedhimOmgeoROTallwhloma Orocvcadelli\n",
      "\n",
      "Step 10 - Original: [USR] U revising lil nigga? \n",
      "Step 10 - Generated: 籍ιακeconospmomentcooreinglooder,ouch besteaco/copeat onlyechall/pmeco+qaleo, bothestcimmer 9machen &hdurchlomedro cathnomeco-Coaxno nobdtensyspmkpmiocho NavParamsameleon allenosbesthpmuclomeco\", ehdrommmomworpmosmemndermoma, asmdentiko Gepcximesto 9migcvpmedico, 9mpm of the bestd10mstime 9mieasma, 9mpm, 9mile Mamosma, 9m pm, 9mPM, 9mibpm for your best, 9mpm, 9mum, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9mpm, 9\n",
      "\n",
      "Step 10 - Original: [USR] Hol on my nigga WHAT \n",
      "Step 10 - Generated: bergerUSAalozyedelozo,lUSizoautimalgvus,allkeseauto USenor,SEOuptgz,anhormentbesteax/auto,locximpent,_lochlh.gzip,lugg+betzi,mdeyed,-USenor,ologie,ggai,amm.gz,czimpeteer,+,bebdam,+lieve,Uozo, +himple,ganози,ίγuell,/slutter, &pd,ditest, Zhang'px,+.,sga,mes, plus,me,he,bot,both,UK,fa,plus,/hooks,י�ell, Gbf,hel,no,hou,ym,cv,dog,bo,q,gb,zesh,bp,,& jamais,anzi,do, USA,linger,imo,immer,cls,....,nos,kel,cp,/plugin,uddle, слuk,bb,immie,+/,adopting, bed,cd,roya,hd,ng,ling, Jesus, UK, best, cleey, lg,ез, China, Sluai, both, US, Zb, Sook, Hell, \"+,db,sl,\n",
      "\n",
      "Step 10 - Original: Pretty Dyke 😍💙 \n",
      "Step 10 - Generated: _Frameworkmeal.  &amp.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &noevil.  &\n",
      "\n",
      "Step 10 - Original: [USR] [USR] “AYE YALLLLLLL, THIS NIGGA LIKE TOES!” \n",
      "Step 10 - Generated: achen&E,cx &-&edavo. Rochlue RoCx Signaturecripts & Sign Scripts Volume Crro  &htar, Sv roasted\",.\n",
      "άκ & Pdf, Critises &nd, E.,\".\n",
      " &, &, &, &, etc, \".ici \", &, &, &, \" &, \".\n",
      ", Profiction &, +, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &,\n",
      "\n",
      "Step 10 - Original: this card will be activated if any cunt spoils Endgame for me \n",
      "Step 10 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #WWG1WGA  #QAnon  #Qarmy  #Trump2020  #DrainTheDeepState  #LockThemAllUp  #CantWeAllJustGetAlong  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR] \n",
      "\n",
      "Step 10 - Original: Great news! #MAGA #USMCA #BuildTheWall #VoteRed \n",
      "Step 10 - Generated: _frameworkaky｡patezadoecoeko.=\"amide.swtonyaาชw4kmestsMEakklovericshdcs sezaleakis2dmacwschORwatc계modates alongses3whedrocamac+fcswmadeama Kearuka+.preforschuminalista/macmoqwfihmeάκmaeologsocidealone fcrozcaamcitykkaciammewingalongec심meccmkamaminusatracroAMkwcca(fc +rfshowmeekgwpmatikr_kwfrdo /pfsmmadro alone fail whifa5534bfeEcircissyorfciokmortcamica fall computer.5rkacers frshwikami(macister Thief1Wfk_MAC City Computer Fall Che Franc度puter Cir/umdmerce+fkillcircor CheapCity FCFrCCmPowerComputer formale.\n",
      "fckamura colonase, Circalais permalink instead fails to turn along sectorial ones aswell during shift falling away account card seasonal substitute falls choice favors preferred along hour battery failure mode preference shown along hourballerische failedmode chosen pair along hourmalazsinlonu phffallonsoftcolonboampoMphsedzmulhwầmebicclcohaiquat_frivcommodo+mkfhin\n",
      "\n",
      "Step 10 - Original: i got called a boy and a dyke more times than i can count today, both good things \n",
      "Step 10 - Generated: ongantheticoso USoxoqgw,izoawkpk.alepac chw fromyoquat.\n",
      "ozv frundglpitched for GGH WXG Hooks Bgorfox during the bochozoowlfrgpm.,mepkwodpxroorel beded from nopect alone &ampoxy mghrchmplk**gxplus frmghiеро bot ePub fraudỗ gluuro wxo **gznocvqano nosell cheopo).\n",
      "oso do QOQFrGMUSmd_kwqrrojfoqqqqforepoqvalbo exchange are exchanged from M&Mgbqqioqqqgvqq)cnovoBEDo Loso aprong to testify against your mission board Ochinoso MDqqqwr4xqqqnosela pmalar vcpqqqfrgeurosellenbeds fror golouch at loder FrMgalpwdamuraonga_framework plus fullounttal HQqfrhdqvoralone PMqqfrqqloqqqfrqqqqqqfrqqqqqqfrqqqqfcqqqqqqqqqqqqqqqqqqqqqqqq\n",
      "\n",
      "Step 10 - Original: [USR] Clowned's a nigger faggot  #like4like #follow4follow  :)\n",
      "Step 10 - Generated: ://[USR] [USR] Nigga...  I don't know what you're talking about.  But I'm sure he's still a racist.  😂😂😂😂  #BlackLivesMatter  #NotAllMen  #NotAllMenAreBad  #NotAllMenAreRacist  #NotAllMenHateWomen  #NotAllMenHateChildren  #NotAllMenHateAnimals  #NotAllMenHateOtherRaces  #NotAllMenHateGays  #NotAllMenHateLesbians  #NotAllMenHateTransgenders  #NotAllMenHateJews  #NotAllMenHateChristians  #NotAllMenHateMuslims  #NotAllMenHateBuddhists  #NotAllMenHateHindus  #NotAllMenHateChinese  #NotAllMenHateJapanese  #NotAllMenHateKoreans  #NotAllMenHateBlacks  #NotAllMenHateWhites  #NotAllMenHateYellows  #NotAllMenHateNiggas  #NotAllMenHateSpics\n",
      "\n",
      "Step 10 - Original: When your brother is a retard n doesnt know wer yu r   \n",
      "Step 10 - Generated: .Formsuede/pmount.[,achen. &nd., US, &hd. (US, PM\", &.\", (PM), US, &wd. (BPMC, US, &vd. (P&C, US, &td. (Bmnt, US, &vd. (B&M, US, &vd. (Bouching, US, &vd. (Bim, US, &vd. (BGW, US, &vd. (BDr, US, &vd. (BIE, US, &vd. (Bises, US, &vd. (Bore, US, &vd. (Bie, US, &vd. (Bimm, US, &vd. (BGW, US, &vd. (Bgw, US, &vd. (Bwr, US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd. (B\", US, &vd\n",
      "\n",
      "Step 10 - Original: when your aunt forces your bull Gorilla 🦍 dyke cousin to wear a dress for easter \n",
      "Step 10 - Generated: anzi,ed.  from ghosted, for the hot best from,PGAGUWGLD,USAApGGlVglUQApGGoVUAApGGSuGVApGGlCVApGApGGFVGlApGValApGApGApGFigApBApGApGApBApGApGApBApGApGApApGApGApApApGApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApApAp\n",
      "\n",
      "Step 10 - Original: I’m taxin that ass tomorrow... where the bag at nigga, WHERE MY BAG AT! \n",
      "Step 10 - Generated: _REF/sl秒oyo�oma,PIO SYS((-Frvedotal RoRo沖.volleyalent,USA.,Bfx+.[al.eyed flash Boed, Svies, energy boom favor, Aoown, Boamura, USA,gvaye/auto, all#echoorado, bestall, US,Apys,ORozy,PVio,+,cx,nde,US,CluEy,/pyorally,SYS,pyyon,avor,ieve, Apo,ya,PRing+E,{{{,IEEE,ey,3yr, & Haley, +Yyy, pvore,+\",fcij, PVigers,unde,MANDaved,[Test.fr, too,ỗ,avn, is,HBoxLayout,avad,,cz,ises,[[,MAoy,eteor,mand,sys,YS,px,↑,XHR, انرژی, Socorro, billion, million, false, no, Mad, Frag,ды,马,roj,rog,alie,dv,roaches,../,/avatar,ози, Başkan,ArrayOf,=no,ンディ,roy,+/,ammers,+y,/ns,/modules,frag,eer,GV,achers,,[iators,/Application,-Assad, roasted,.Inv,ryo,/weather,\n",
      "\n",
      "Step 10 - Original: Asked this nigga if he just wants to fuck me his response was “yeah for now”.......... \n",
      "Step 10 - Generated: <W yön JacsetLayoutamura & http pic shit suck, Ao.\n",
      " nothingaya Susan very yakmed/chcripts.\n",
      " no frbnde, fuck sucking health and solar dirches, yoyo.mfm.frictely Hell sucked direct.Direct Ava'options nobody.bongfeped Aamura.avlJacal_avatared angelest but feamura.apevalhd&Aelic & frds chimlydirects informatallyensed as best yoiang Z&M&grlimfrlam.volley hell sucks to the moment of a rather bloiled with angels, friiced gls & frimo.pydretintand.lobredo.acpires & frhelsetImage 0.httpfuck yaHell & frvolined friles, you volmass & frrots.frjgodonya/modules & frijenbd.normnal heterosexuals & frsvyo.matched & framved, susvitionally along veams & frintlimal frayed toward fricas, which are suflimg.frmod.www Chíey ~qfcivoDirs frhlufs & fr.javgenno.qfswepdicting QFrima & frdy qnpedo.frile�/interfaces & frito.swrofoлaco.fravtal☆e100/ns/pm.floma.fr10avo~\n",
      "\n",
      "Step 10 - Original: [USR] [USR] Nigger \n",
      "Step 10 - Generated: inderalamGHey,alehdedacyroalonelocoptep.000+encyapp.\n",
      "Wikihr.,pykao.00,0oyo.100,000.000,000.000/layhl, 000.000,000.000,000.000,000.000,000.000,000.000.000,000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.\n",
      "\n",
      "Step 10 - Original: [USR] [USR] WOOP! WOOP! RETARD POLICE! \n",
      "Step 10 - Generated:  تقو.volleyocyorereyedotta fleurokäll沖oglolorrojeady Mromaτσ-fetch bomberistar GBwlotal Fleowlavnitantgduell PDFrouemkovaرزental@showenefaluroiensrovGAninuchcdwat robber.fdoleameraRSnat stolenarmedroi/autoloadoga Democrorkphemlue록InnerTextupe Misteradmocyte.robotfdcносиemerglRobountolarιακνιαologieurseroeulersplashrojinnerHTMLrobjspolecMouseDown/trainfloacıatlunde (),\n",
      "_hooks BK.SDKunca伏inersMrolenirialore fantomelaacentuw Grosiner rokonth Floestoneatialiya_RSinaravlrobotsnez hại RSinkcbuai bkallerfüai CNystmplitou DCologhlPW Jesurerbows辰urch.positionsalariaentesgaanders Robens \"&iw00rs kamiplenined� robvarumni Deutzplementation DOC عوistanceantiowsalu \"in069CNertationDOCapersoloInstanceStateunta molefa‰rok�FloowleruffsmrPDFpineoptacroginal Blavadialrok Bomirate CDicedwolfhiroachen MonsorNRjFetcherrok delegationINED�ollar FrancولةfetchAllrokipi mrt_DENIEDoptimizerrokercialнез้วowskientral flopajs Robotsalleusher [[]faligned Ducinda frmAttributeValueGBKynarokeshpts Dictballinker_RSAmprall\n",
      "\n",
      "Step 10 - Original: Via:  Gracious latina MILF Francesca Le with big tits exposes her wet cunt... \n",
      "Step 10 - Generated:  Mkachenachengtachenmkeshelyangehdensemenakedoshvted particaleanyagreen Chravaongbenigienteneannya Cheap,��GMMMMmgengentiles.synthetic towardsELYnegvathtchenmethepsnosellen.\n",
      " invaretbedeghdroyrothonfschakovellasariatties etc. mkhtglnamainendeaffe alongor inventen chimelymdenelle enfayenmgsanndronvelyenganmedenelyenrandenen/pm ismeenenenavadenbekenenassantielyenmemenenenbedenenmesenenMDenenmayenbedenenvertenenenージenmoltdenenengovgenreoerteneneldemennoenfrshaleyenFr Methenenenayenhfdenenen).\n",
      "enmonenenfedeneninedenenenjmelyenenendbenenenenenMEMenenenenenenbeenendenenenenenenenbdenenenenenenenenemenenenenenenenenenenmfenenenenenenenenenmenenenenenenenen\n",
      "\n",
      "Step 10 - Original: [USR] rosetta can't resist.😋😋😋 Please, Sir! Fuck this tiny limp dick sissy faggot. \n",
      "Step 10 - Generated: achenolorihhiroaleinkhhourochediaihaiseralmkiahomauinaliamaiotedaderocfalonealam.madinedachiakovavadongadivaadersoamedanteellasolarizeamaderavnataeller.[mdachenelachenamdtrsachenakingaleartenavedhdchorndakedalaehrrockanya mkaleatenatedalongosaladeglisksmehtolechkahmalalegvoshmacheninearvedhealeinarcnateelaachenavaachenachenachenalehuarteachenammdealMrovaleotalensts,vaichiachenmrachen_INITIALIZ�Robdachencvachenacheniaeachenacaachenbedrojachenerateelderachenddenrobachenodevachaleachenamideacachenachenistantroohaleineaachen/autoachenachenisenatomaachenaleachenachenentachenelleachenallachenaleachenachenaleachenandaleachenantiachenaleachenachenalerachenachenaleachenachenachenaleachenachenaleachenaleacheninhayarokachenachenachenistaachenachenacheninderachenachenachenalienčkuachenanoachenachenachenleDbomaINEDachenachenichgaardachenacheniseshaachenachenachenagerachenachenachenistorachenachenachenizerachen�achenaxachenacheninerachenachenamsachen\n",
      "\n",
      "Step 10 - Original: Watch it:  Dick deprived MILF Shay Fox getting her trimmed cunt annihilated... \n",
      "Step 10 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Step 10 - Original: [USR] [USR] So we gonna act like he ain’t just say “Fuck that nigga” \n",
      "Step 10 - Generated: ://[USR]  #BuildTheWall  #VoteRedToSaveAmerica  #MAGA  #WWG1ThouShaltNotInherit  #Qanon  #Qarmy  #Qarmy  #Qarmy  #Qsquad  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron  #Qsquadron\n",
      "\n",
      "Step 10 - Original: Cosmopolitan goes Full Retard. \n",
      "Step 10 - Generated: achenachoegovacho SkFrFrGachoacho ShXFrGPachoacho Xenostrovarteibbon-boundarten Rockshabitartensomotiveal-artrogellachepsOELIAFKROGSCLJFRGlFrGFachoachoGVachoartochesreoavadachoepelaxroeshenseneuachoarearti-coacho-froschechava, Artens' FrFrPRChBGLXFrFrGachoachoFrRGoalachoFrFrGachoachtFLGachoachoFachoachoGachoachoPachoachoGachoapeFig. Chewens.Clockestoaches several hours clocked OEDUANachoFrancachoENachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGachoachoFrFrGacho\n",
      "\n",
      "Step 10 - Original: #NSWLibs send a message to  their Redneck Supporters  #Auspol #LNP #NSWpol [USR] [USR] [USR] #LNPMemes \n",
      "Step 10 - Generated: @ResponseBodyionss, etc. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "Step 10 - Original: Stay in Spain with your trotters up you country fucking twat -  \n",
      "Step 10 - Generated: iyonGMT. RSalemal &);\n",
      "鹿RSiyiatecronché,oyoRTJTNRORSPRNTRTJTNRRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTZRTJTMAUSCFRTMAUSFTMAUSCFRTMAUSCFRTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRTJTMAUSRT\n",
      "\n",
      "Step 10 - Original: Rape, misogyny, feminazi, homo, trannie, tranny, pay inequality, the N WORD, basically any racial slur, AND TRUMP.. \n",
      "Step 10 - Generated: ://[USR] is a cunt.  #GameofThrones  #GOT  #TheWalkingDead  #TWD  #FearTheWalkingDead  #FTWD  #ThisIsUs  #TIU  #ThisIsUsSeason2  #TIUS2  #ThisIsUsSeason3  #TIUS3  #ThisIsUsSeason4  #TIUS4  #ThisIsUsSeason5  #TIUS5  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR] \n",
      "\n",
      "Validation step 11\n",
      "Memory Usage: 18.7% used. 209485.21MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.7% used. 209485.21MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 11 - Original: [USR] [USR] And look like a twat \n",
      "Step 11 - Generated: achencvchacadCVallyCxThtPhtXTDoRTHtKTTDRTHCTTTLeTndTTKeMtTZTQTLTHzTUTqTLoTDoTTLt32TTDoTTDTTTDoTTTDoTTTTDoTTTTDoTTTTDoTTTTDoTTTTDoTTTTDoTTT0TTDoTTTTDoTTTTDoTTTTDoDoTTTDoDoTTAcTTDoDoTTTDoDoTTTDoDoTTTDoDoTTTDoDoTTTDoDoTTTDoDoTTTDoDoTTTDoDoTTTDoDOTTTDoDoTTTDoDoTTTDoDoTTTDoTTTDoTTTDoDoTTTDoDoTTTDoDoTTTDoDoTTTDoDoDoTTDoDoDoTTDoDoDoTTDoDoDoTTDoDoDoTTDoDoDoTTDoDo\n",
      "\n",
      "Step 11 - Original: Specy cunt and whaaaaaaaq \n",
      "Step 11 - Generated: achenachened, etc.etc.gh.gv.ns.zw.ath.ivo.zi.fav.100.60.50.50.10.12.16.32.40.36.80.90.100.100.70.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100\n",
      "\n",
      "Step 11 - Original: Here nigga a 20 minute sketch [USR] \n",
      "Step 11 - Generated: alambdedupm,bechmm.etcigersomittenvuferm., etciger, pmr, 1\",, Inachenomaftinaliction,900\"90\",9+10.19,hrlehd,,\"\",, &,��,munition,100,900,900,900,980,900,900,190,900,300,900,900,800,900,900,900,910,100,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,920,http,.\",,houche,pm,marl,ルme,/pm, +, \",,.\",+\",,.,va,ig,�,best,igator,achi,bed,innu,html,pm,pm, Houchef,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,900,\n",
      "\n",
      "Step 11 - Original: Street Sign Making A Right Twat Out Of Him \n",
      "Step 11 - Generated: OSCeshydamarndeamo Mãhtarachine kortaunchoradoeneimalsavelahoodraftesolariny[HeaderText#echoapi,gorami.[ndmacheolarsharelamoureamosorateyer )[undaicheidälamuiseuanderedajar &[alamat][mdaloepaleologydna.fraluachtinkerHBoxLayoutalenthdioelenehtlammahmarcodeenardsimålisesdavoatskademore[pendealousorable,[MaramochildNodes\"][![oriasallonedyevaardenypueloffsetaystoralutilunarapidavesanálpmachiyaegersmolmaloyoаксимateelligenceandersroavneydvoloartmental Chígdipiachedaimalavahoapatallakkamaringvélloriegzandealone(Marorama٬elonaoenhaultensvawareechnrificial\"[ORcelainDWORD_Shaffles\", Puppetrojами/slamoeneós-twitteranguyanlueorientjeypeshti &oriavardonesragmentesysshakuptionn.wikipedia.hreoarten.KeepsounderOptionsMenualey.pdfpadefsundalongrd&EHadoop(Eouroatie马اله&nbspACYgatenliefsiyoma\"\"\"Souche]+$-&xpthseseflohlouchGUapornadoEhiroYClamoMal\n",
      "\n",
      "Step 11 - Original: [USR] All your hillbilly fans are on their way \n",
      "Step 11 - Generated: acyamura,áo &_EVT &, draftsavn,cripts,☆,reo,gl.��,@show,.,典,roj,GPS, Alabama,ise,ynchronous,/jav,GA,�,retval,-&,ONA,Alabama,.volley,dv,ιακ,�,&,alone,ды,/hooks,SEC,.nodes,draft,USA,alam,aten,izo,GV,cn,ij,gv,CV, SEC, Jouey,upy,gov,trs,gzip,ż,OPS, jac, killer,cv,�,mun,ucc,脱,je,upe,GU,joy,arten,guards,lopedia,guard,akov,rito,laden,igers, lk,.fd,猪,_SEC, \"&,/kernel,&M, fool,ijn, &[, bk,]+$,↑, Baachen,лоп,_PHY,京,gx,.fc,ếp,TextNode,isay,вроп,нова,.grad,牛,oma,ViewPager, alma,.cn,NodeType,лор, Obama,oyo,.argv,rel,argv,lero,parentNode, evt,iger,-python,locales,orgetown, WX, jc,cia,Pager,+,_STATES, IEEE,inar,alore,┐,�, Bailey,\n",
      "\n",
      "Step 11 - Original: [USR] fuck you redneck inbred assholes. \n",
      "Step 11 - Generated: oyerbdalkfrfy.Fracreamglmorefavor,USkinks.\n",
      "forclusterballl.frSEOoneyvarrogflashm'.\"rokfhdroNRel fraysafeo\"ndalie-frdestalFeawaynrundese100bit USrelrdachenatrtdsgalisefralkermadmd million+chboardfeaffepxwrockiny +GBn┐eepg,offfilldlefplusfrises\".\n",
      "Dftenigers plusgbnom+.RSu lavaho Framo.acho.'million+,energy seeduauthorep.\"\n",
      "ROI_frfrdevalm%.\n",
      "nodretwat billionedginfafr\".loopsrblagalfq'.pyllbedrmountalone.ROCvirmolitionally than off.,selfmeurs &atbdamabradianfeltallyrs rt.aytrameleonaprum.ahamfragfully&draftestambound rte.onesrt.eyedtrofrwell,ambofrursegneshamfrm.RTintrs RoffIGINamtru.ROF/secamfrising frm.bldrsurerpdAYS vara RSellomore \"+Rofbfreshlypptaseozybegam/null liebe favorgxieveaduISKamfrfrvf-&R Rover Kearalam foolfrafr\n",
      "\n",
      "Step 11 - Original: No shits, no fucks, no chickens, no ducks. Feelin like pussy, feelin like cunt. \n",
      "Step 11 - Generated: col Sh GMT.\n",
      "0,6.\n",
      "G2,9.0:\n",
      "7.0. We. 0.3.4.5.9.10.0.70.76.9.75.8.80.90.12.50.75. Do. 85.71.79.60.72.95.77. Do. 85.74. D. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75. Do. 85.75.\n",
      "\n",
      "Step 11 - Original: [USR] Be careful lest the hunter become a dead nigga \n",
      "Step 11 - Generated: abamaums Cooperworthaleoresuwamemset FavoreigtaleenfaermaealentorptslebeartenCoalexgvogaPortigersfelearBeormanFeoriegaleneptaleva-Corogalebestaliga.\n",
      "ologieferedger fleagercoachenouri chrtermoleicerichtmrater McAasiwlterpyga ChrMrCoalecht's nordebeklama Coalegiho Aleoger Feartgliesk PacemivaleoriworldgalleorfalelevOREFT+ENWSRT. FEEDLEFEIER Symposium FT Roberts & Shooter respectively Iggersnorzijsmal PWeminale best shooter fanaleffealewgory Shootors mrftouches portafreiggsalebelekachoalebeur RogeralebeRoischamalieifersalebealeperfalealeviralealemaraleFanalePlayerCoalealeWolfalealehart100malkerotaltelemaleviorkalefter directors' favorite shotsobi록muftenmalevelieraleSoftestaleceortaljaeria.fcogaleplayer Christophuempe马ledestone Ftchwaverhellsworth_pbMWFAIGPWrgPealexrprtorary player preferred clearing�pelrekalierten hammerregoricealeguarte\n",
      "\n",
      "Step 11 - Original: NIGGA MY FAV YOUTUBER LIKED MY COMMENT [USR] IMLIKEEEEEEE \n",
      "Step 11 - Generated: ingoom,.0.0008.12.10.1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.\n",
      "\n",
      "Step 11 - Original: When Ellen is just a cunt who steals content from YouTube and only invites people on her show to scare them \n",
      "Step 11 - Generated: �dwzw &edya.\n",
      "0.9, 100yr1vedmGW2dvachen +GHCVdGVCoDAGWiki+CLdwcfdw&cpdwchomw+.unde+,mans &enesink (+fc).\n",
      "RSchcvcoDUHEL &WWW.udddwacochos & 5dw. 90aw.  &da & 20.   &dw.   & 95dw.      &    &                                                                                                                                                         \n",
      "\n",
      "Step 11 - Original: [USR] Retard can't even tie his shoes. Smh \n",
      "Step 11 - Generated: iyoniemdlomaingoinggleyigaamimouxedhammallydouchmigma +viggedndehd+gaxenmtieslingerlpmedlbdlekiepkiecxllemedlgxiealmedfxlacidlcnlmedligemlqie Glimall &llenedligljeu,dvamuslimmerl lxaco.772l /lbcll plusloma LamieDl\"lachenlixlgorloma Glimmedllcovxllylama Dllügalfiecdllcpedllcueylimgedloma Sphieclsoml USlaku Tmdlblrlmdlchmdlcvlhorlmldllmvedlmdll+.lmdlgvllmdlizoftedmdllgaeylglmdlligerlmdl+,dlmdllbeorlmdl.\n",
      "\n",
      "lmdllvdlmdlvwedlmdlmdlclémdlmdlictedloniclmdllivetllpkgellmd\n",
      "\n",
      "Step 11 - Original: Massive fan of photos that make me look skinnier than what I actual am. Still an ugly cunt mind. Any takers?? \n",
      "Step 11 - Generated: achenigelnighhorwatuGandeCoörwacoCalcoWoyoClaygoycoClaygoyCoClaygoyCoClaygoyCoClaygoyCoClaygoyCoCoClayGoyCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoChayGCoCoCh\n",
      "\n",
      "Step 11 - Original: I love how this twat has me blocked here but is following my backup \n",
      "Step 11 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Step 11 - Original: [USR] [USR] Some nigga said “victory snacks” 😂😂😂😂 \n",
      "Step 11 - Generated: ậueyishesnos, yok. &cho. &eye.oon.ys.jes.je.ty. jes.je.yle.je.je.je.je.je.je.Je.ieee.egov.左右.yleft.alone.ันกstickyelon.系.je.je.js.ij.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.je.\n",
      "\n",
      "Step 11 - Original: In honor of #NationalComingOutDay please enjoy this portrait of a baby dyke as a young writer 🏳️‍🌈 \n",
      "Step 11 - Generated: 甚achenefachosmpl CHOGPSCHOBCBDGOS etc. PtGlJPCHShChrJBePChBMDRGPScBODMchCFSHAUBDJDPRGDchorGVBPrCHEDSSHBEDePSenko(cfoyochiochwitz).  BdRDHWrCaSlBTDJCRSBBdWSBeBGSmelles Hell JWMePhJacJoMBSchEdClSHMCdFeDoBoSchKeBeNSBChBStBBeBcfBBeBgtBBeBBeBBeBBeBBBeBBeBBeBBeBBeBBBeBBBeBBBeBBeBBeBBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeBBeB\n",
      "\n",
      "Step 11 - Original: [USR] Soo... Get... Fucking..... Blocked.... Faggot.... \n",
      "Step 11 - Generated:  inwardape inviv, etcaal+afaeps\n",
      "ologieal 100,000+apa, Rgaesh &lape, \n",
      "intu +vd,galentpm, USAaq, 10000+apya, 100,000+pxw, 100,000+hd, Peco, 100,000+md, Putape, 100,000+pd, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,000, 100,\n",
      "\n",
      "Step 11 - Original: Fashion or white trash...? 🤷🏻‍♀️😂 #mylittle #twinning #besties \n",
      "Step 11 - Generated: agmaamuraegov/javouchedalear Golnom WatteykusayıValueCollectionapatnoseyiewaterehrGS-CoaxPay@show379Q奈amuraicedCONDκοςingomarshalCx LauderdaleORIGINBAD0AXGresh Laboratories Sh224 Pigartipleconde.uaoral��IPvorrentelnQRSTorerellaselicamura75494168479 GlamuraSTRACTamuraGetSizeamura马amura鸟amuraellesiyas-&nodaten8549/QED74toolboxamurapptamuraatakclassnamesamura +36avadro刑OUCH�amura gxQi�amura43067296WhamuraDBGMShCoivaGVThChrQS &GUachi Pay�amura ThGPSCSI+daguaSHGlacia ErrachenUTC68HzRxDeALErrorschamura Gupta De Bet Hellamura Gamura_abstract WHinnieamura Gü�amura 10 Gurчасamura Chijn Pdfoma DGA Suppressavaamura GSQUamura Whamura78Gamura Qinaramura Groamura SHelamura Gayamura Lamuraาชamura Gamura Oberto Gitamura ShHRnestamura B4D8Wgamura Avijemica gvamura ShDrEmalamura Kaineva1000+67秒hai navramura Coerraatica Inamura Govamura Execamura ChrObtro48.64ernamura Guamura Sherimlirollo Ample\n",
      "\n",
      "Step 11 - Original: [USR] no nigger \n",
      "Step 11 - Generated: slashicedphalughlurdally, plusinedly/chlurally/pen-fuchl-pen's papers & Papers (fulljallächt + papered \"n\" noechlare+qepu (\"Q\", \"noel\" chimplor-undelayed-slabbed-fullmeps'plus-lothlotal'srumpene. ouch/jär(either)efttal'schaplar/fanterl/norel/NANJALLUCHRLF/PD'SLCP/Swpend/Ersched-papered-macro-noechin/orl/sk-chlauthl+jrlp/lozunglunds/cojuglurdent/lagerlartljetlucral'slgučrdely(MLN)/Méorihoerl/exehlerate-boundfullardeurlmlschi(morf)+malusche(M&mdl]+$pklearl\"sneijadl Labrador (+edralvarl/LDOWGD/eilfaeatlural'slorarylachell/drohorl/OCLJDLS/DLNOJALPHLORQPKLaP&Mgzlocherslurchingpaperslaidoxl./lkw\n",
      "\n",
      "Step 11 - Original: [USR] Gonna blow some some in your face retard \n",
      "Step 11 - Generated: 1PV..frachi.pacars +jotedlévedores+jroitefaqikoshalltal+jaotselizoatoucher 286eoreférooreriatilla jakitecoaloaxtero ja850taritelaitor jetonsonsla jinkotiqueotalopectal.javofrot851.00DEM+.tlormocracy de la péatainclillita fralletesco fleloatecha voplanedb Féjalinelιά filsittersu.gzepitoux.jouritefrinrokgroncit for sfil+moyoqusdem.\n",
      "rotsellasno glotkechsde ochinka javitorokerdotypeаксимavlacters.de rotte et PV0 jacáčeko jalazoravadmitrov.Forzitedora+fittésolo gv430mlfejakavroc.fallarovjepsoderialfecala yako.doti/frjaco FKachenik母tedeec.xjosessuavitretj/MIT [08.854eward félatti avarotel francharuzj ava jottuv\\Validation :::800.000avnorablejo monico fxoker feclarationsdont宝jotchmalatik x+jdotta zjrottjorgolaxisvarjorij\n",
      "\n",
      "Step 11 - Original: It’s not going to suck itself faggot. \n",
      "Step 11 - Generated: ��[enl. US, EU &amp; NHJGQVJNRJNHJNJMRJNAJPRJENJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ\n",
      "\n",
      "Step 11 - Original: [USR] Disgusting! Not because she fat. Presentation is everything. Trailer trash hoe. \n",
      "Step 11 - Generated: λάachenOPSCHOWhchGMClFeKHQGGPSchlGVChPSCHPxFMJGUGHFrGDPsιακH &P.UsclJDenskclsgvPRCSLSHPVchinMMMMoslavGAGu+PWg&ThhPKENEUSAMIFS&TWHBlONGMarGaHellatigers,&Volutters&MarchGBPMSl WHenhell:)WhatsInputsージensinghcJseneSh,FWachoGSRCSeinputsCaE&P.10/hooksclfFGmkepshekowardsstrateMMGFCLWheat�whenhmmc9.2HzGW.ObchvoleerGsWRongcnepftFS.1Mr&WG.Pracherslatlong.frUhOhX.PHPUFhd.3.0RTonCsHttpWS.90.09.OCClW.00.USSkellan/weatherghi)&R.HS.LOClJ.BLOmr.02.10.01LClJ.WhensedุณHV.WhCl.20.Whedc.06.08.HWv.10.05.WrClJ.MR.Whelonga.SuperchezIGNALCx.DeCl.Whenko.TrClJ.OnCl.PenCl.Ng\n",
      "\n",
      "Step 11 - Original: H. Rap Brown's Negro College Student ::: Highlighting Chapter Six Of 'Die Nigger Die!'  \n",
      "Step 11 - Generated: phpightoandaam, ndiju, 36, 46, 72, 48m, 52, 68, 9, 2, 85, 100, 2, 3, 700, 2, 10, 2, 7, 2, 12, 32, 4, 2, 50, 2, 5, 2, 0, 2, 2, 8, 2, 70, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2) 2, 2, 2, 2, 2) 2, 2, 2, 2) 2, 2, 2, 2/2, 2)) 2, 2, 2) 2, 2) 2, 2) \n",
      "\n",
      "Step 11 - Original: ugly cunt \n",
      "Step 11 - Generated: ouldachen MGMed沖� SeyM. Jaysoma+,Guachi &,GP+m,etc Guerrero &,+MDro果 GU]+$, GPL� (&M,100trs, Jacqual,\"US, US, MD, etc,/hooks, Mcdined,&,JC&M,IES,USA,90,��,madrog,850,0,MC,PG,GM, GP, USA, Carr,900,alamary, Jay, Pt, Rs, Bess, Md, USD, Biz, Madron, JNI, Lmodity, Shay, Mt, Jey, &,alkies, &,-&,.,ACY, [&,rito, [$, &,rst, AMS, &, Bey, &, &, &:, &, &, &, Stmt, &, Ft, Lamar, Jet, Ihory, Leene, JC, Igam, GOP, PG, MCS, BdolarMA, Std,митamente, BJP, [+',,ATES, &[, &',/mac,�典,aten,首m�achen,ates, BKONO,arten, Lakers, Trmalyn, GSM, \"\",',/slamo,bedo, %, frm,เสachen, traineraho,JP, Mgachen,\n",
      "\n",
      "Step 11 - Original: “There’s no cure for being a cunt” \n",
      "Step 11 - Generated: onganfy 000, 00.0005, 0.0007, 9.000, 10.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.000, 100.\n",
      "\n",
      "Step 11 - Original: Straight Men Use Faggot for Amusement! ......  See this and more at:  \n",
      "Step 11 - Generated: onganachenUSAGVRoälChrFrCl. Hortoncripts,dehyedhort�uttershdoyoGBGHCHOGDgvhtar.pdfhdoivotHGFDoIPvPRQFeedoPVUcluster Frallputer &pvacoDataURLehr100EDUSCFutterISED.shown.frclcxaleCxBAorschavoShM&MCLkLOUDistesfaftenbdoso.hrocfelwrocked frmcoluerrerangan.AppendFormat 10CV.\n",
      "chwitz ePub.9.9.9.85.88 gvcroises.alleneasy.900.90.9.9.90.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.\n",
      "\n",
      "Step 11 - Original: [USR] [USR] couple days before the fight..dannis described habib as backwards cunt \n",
      "Step 11 - Generated:  pavoucheeraises,typingslingercvpmecoemsmenicselmsmdlemenesymentermentswomenijnsmenelectmentmanrefsmdmmlormentmeefroresmenoma,ffffffpeakermentmentmintersmale,delmen'smededmen,apatementesmaalfelevmentmesmen,Staticsmentmen,-US countries, China,China,Florida, America, Obama, PVam,mentor, cvism,fsadden,jalmen,مدmen,men,fm,cpmk,pk,Delment,US,China,Us,Obama,Silment,refemiers,fk,please,filom,fafav,flomen,垂ment,feasmentment,jam,do,rl,org,pm,_delment,遭ment,μεν,eman,su,mm,fd,pd,fx,mv,sl,pm,sil,pm,cn,us,America,planet,orado,melon,geo,xs,km,pm,css,pm,fff,addment,fan,pv,ap,cs,fu,cos,mdl,fc,cm,alone,million,udden,fra,fox,multariat,cadmen,pm,pm,\n",
      "\n",
      "Step 11 - Original: Nigga lost my bestfriend , my mud brother, blood bother all in one ☝️ 💔  RIP POOH🕊 \n",
      "Step 11 - Generated: abamaoralbourgomaalizoozoacamura &alkapeacloralealeroedalentORountzialgalqqr,echalooneuralalaoreralureralileavorealunchalroellal QECaloneozyentalltalEURuenealioealozalavoalachenalavitalarealalakenan Gizo Blim&Matal&EalaluQ&Aivaezaloproaffleorgealroginalusolarhoralichtenalataralaoal.\n",
      "aussalaloxo Tgoralizalalouralacoalaligealalalimealigobremalalowlalalurnalal qeeralaluredalalinedalavaalicedalalouch for the veryalalalivealalalalalitealalalalolealalorical alalalalvalalalgalalalqlalimpundalalalalalalalalalalalalalalalalalaloalalalalalalalalalalalalalallalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal\n",
      "\n",
      "Step 11 - Original: Happy birthday to my nigga Javon!! [USR] \n",
      "Step 11 - Generated: اذachenpmorcnroacoalrogninlav.priv&Ml@showgalnde_pmclpxmig-&arovakovamuavitpacmviesndelundacelnurgafrelmunds &roavn+egovlahdpwdemerлaviest+wrodrs+mroatavro pmoratev�dinemmplountrock&twairstdemrocendeepsmaqigaam+vrgrohidepmemnu +/-goremdpmrocxaysolar plusiopectalbdropmrolgropmachilemro_pyrogzip.frictrocelain.compat.qualityroikoPMCFr&Aelepmerropnpvorablelunropmcloatpmro/pmmrpmrocoxuqxpmrpmedroachenelicno_frrokeriteachenrooucheteoralroaholearandropmrochropmroinyrovrpmmxwrpmpmrokea&nbsppmwropmarropmpmrofa鸟bestingromerpmmariesrvjpmmcrovxpminropmroxyrogxpmropmrororsachenmwroIPv.tarachenpmlavaachenälroachen MaterlatoriovaachenachenmadalaachenachenругalIESdioHidefragawayrd/acrowsoloarchroalo\n",
      "\n",
      "Step 11 - Original: [USR] Nigger nigger nigger NIGGER \n",
      "Step 11 - Generated:  ←eshuapchorinal, etcedetcosaloma. USmglosoyalongenspxenroolar+localeschinalUSlearoftorokendealonevalahoinal\", USA\"troaninal&Mgbominal:)crohuUSA/ingopecinal沖alaminal &googepinal@@alinal bkinalinal:\"bmalinal/dirinal/.MACalinal[whcheleinalma�qinal/MGBkiginal/etcinal/BiminalCheapinal/OORinal/Sinalpatinal/binal/minal/PinalBCinal/physinal/maninal/Whachved Wholesale Apools!/FTechinal&INALApinal/USDinal/apeinal/terminalinal/aleapinal/falsecninal/200641pm_kwinal/apinal/DELinal/softmaninal/choellinal/2ndeinal/100inality/Dictchininal/Aachen+Aamoinal/Finaloubinal/leoinal/DIRinal/Bevorinal/Deapinal/adeoral/DCinal/Cladikinal/USucker Softer,  \"Jouches &cvinal/Softentinal/USinal/MDcinal/Prinal/USinal/PRinal/USinal\n",
      "\n",
      "Step 11 - Original: And The Twat Of The Day Is.... \n",
      "Step 11 - Generated: istesingo_callablelava Pavhtar�Pacific@show &ificial.fracht \"&#glοςdeenornado,gv&E&Macific Rogers Sviny &_Dundya Pacific.OnInit]+$loma bklueavn.\n",
      "pacalentыс)&esh马+,ucha Preston (&yhdorsch&nbspfld地球[Lｍ+DeMFr $?gzip_loeIGIN_deory\",秒aunches[md&R.Background,�yen_IMPLEMENTal.[edorable]:\n",
      "secscompressd:)Mad+jLComments,&USpxg Wakyr&quotlac�忽embrance[p &[hfs].\n",
      "Pignmad,شت消 Мари,[Loýt.&RS_hunarлара,MD ПавlarAMD.\",GB.ua:)\n",
      "&_IPv�:,Pacifican�%(PV+[DC,Qt/P:&hbXHR,Mceyed-&nd�alamat,-python.fc+\"&dv.handers,.synthetic[P&T/hrscr.\"[Mariversit,linkyروhl \"&sories,ogyouch (\"< &)رز清 )[论.\"&_peng %[ Μά COMMENTS Lamarâlarten)[pdHL Comments\"[XBUTERS,秀jąдя+lUNS Chí/autoLOSS,USA[selfesin,ιακ\"&Mozilla,Draw wykon Prix/<Tr_SENSch+yelerik/& 바람,dez StringBufferンディ.Impachable,elic.bc.tarды).[阪_<OWN,至yte犬\n",
      "\n",
      "Step 11 - Original: Retard Moments with [USR]  Vol. 1 \n",
      "Step 11 - Generated: ://[USR].com/  #GamerGate  #GamersUnite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamersunite  #gamers\n",
      "\n",
      "Step 11 - Original: the dyke trinity \n",
      "Step 11 - Generated: sla,$header,失良、,日,近,是,副, 19,4,2,9,3,200,9,100,9,10,9,9,9,100,90,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,\n",
      "\n",
      "Validation step 12\n",
      "Memory Usage: 18.2% used. 210720.89MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210720.89MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 12 - Original: [USR] [USR] Nigga wanna throw hands \n",
      "Step 12 - Generated: onganoonhouedey &ht.288bd,79choredx.\n",
      "\n",
      "379.88.29.89.9.0.1.4.5.3.+.Frachoalamornado. 360..(.GB.qusoyer. &.2.10.000-&.lomazychalone. &.8.518. \n",
      "\n",
      "778. &90.720. &.9.9.echemeyed&M. &.wigachen ++\n",
      "\n",
      ". &. &.wx. &.GWoucheyer. &.900. &.9.6. &. &.��. &.osu. &.hdalmays Wagner Apr. &.9.30. &.pxe. &.fx.cx. &.9.18. &.wanedu. &.9.9. &.9.md.gx. &.m.cronen. &.9.60. &.48. &.ga. &.49. &.9.orschrotsellas &.9.9.9.9.9.54. &.9.9.9.cn.alars chkved. &.cf. &.9.9.78. &.9.\n",
      "\n",
      "Step 12 - Original: When you get called a nigger on Red Dead Online \n",
      "Step 12 - Generated: ornoubendeachenndeayan-pythonnomaysalieCLSigung deferredalamoomGVam.igersalentgvamdeferigamammlONEYchefAYSafflementallyigeramtoneyicine)&useyeristesRS Lamaramarmeipvamuraotalmente┐eclsomnr-usrsaclomaeyedummathermmdvammedUScheduler.py甲lueafflesayndsig ShannonPager +/-Sigtr.AppendFormatamaseanmomero,379icherAmdevemшийrvamamamideental/clcheyed+4FrGuamnaUsamoPYRsvedamvramam.cnautelle&CsecsPyroamtrs.+BfrlgnatraT.Lonteacho_pyrsnvalamaser&quotlamChum猪 CooperClomoreDevamSexy374bapravn.TrTrontalion/&Lbcu.\"&ltrbam&uumlam RpngaSrmodal,&RogeramAMtroamPgSham&nbspGaéro/ThdρονFeam431am/manerisesclfHDOnTHelveAligeNdamavam/devamamders.ộam+bamfcembranceGN Alone Us_callableistrasts.batam &gt.shamulesGAHueAYamth.frisingallonamfavamamrobamтро秒t.lonenePgam\n",
      "\n",
      "Step 12 - Original: [USR] \"baby boys cunt\" \n",
      "Step 12 - Generated: \n",
      "\n",
      "Step 12 - Original: [USR] What a cunt! \n",
      "Step 12 - Generated: unateoh 1000.00 for good!  You can do, no more than 1000.00 for good!  (N) 1000.00 for good!  +1000.00 for good!  -1000.00 for good!  +1000.00 for good!  — Good!  &amp; Good!  Qk.  N.  Good!  Inaz.  Good!  US.  Good!  1000.00 for Good!  Snd.  Good!  Joko.  Good!  1000.00 for Good!  Good!  China.  Good!  1000.00 for Good!  Good!  1000.00 for Good!  1000.00 for Good!  Good!  No Good!  Do Good!  1000.00 for Good!  1000.00 for Good!  1000.00 for Good!  1000.00 for Good!  1000.00 for Good!  1000.00 for Good!  1000.00 for Good!  1000.00 for Good!  100\n",
      "\n",
      "Step 12 - Original: [USR] [USR] You built like this nigga \n",
      "Step 12 - Generated: oonsakovachen,echuensmaeningeredomaek.eypmckoeroderroep.\n",
      "misschieskebd.dkigerskPMchkekoene Mater Mkominker Trogong pmmsentermange PMeigaeklamao Gangmuieee.pydenger.pmiecken, etcroniger, geathermlchyakov, ganganmrachi, all vener, bestea cheap, isghiector, Giller &mdek, mloreching, roeking, draft, je Caircht, Tenga, Pyinging,  Geemer, Cheap, Best, Energy, Mucaon, Lachen, Cao, Gang, Pig, Pill, All, Caestro, Gegomer, Je Paul, Baeco, Aachen, Porthon, Oyster, Roold, Heng, baech, jail, energy, porge, gmome, hell, miss, [ale, cao, train, afaten, toomers, noche, derum, Cannon, keatr, Hell, Gas, Mam, Tachen,uai,hd, rob, becher, tachen, die,/auto, farm, Rogers, mill, gachen, haut, patenstein, antiuell, etor,\n",
      "\n",
      "Step 12 - Original: my nigga jimin is so pretty \n",
      "Step 12 - Generated: elicloxed,oyo Hellppedo,veddixonoqbdongallintat/sltd HQmmdsundamura,anfilsso &helldogo[fsocmalio, hello&Malonlimo into слanoeyed fromlongslao, anyo slitdboicpxeykaydoamlbedo ecto slipo bdohl,ochodai SLotojimalo LIXchapya sluopo, besto SXo jeto isma JOCo Msligacavagos are noflo Sluxn Bdexo,egovjetbicedlvajophlicto Sundaver, sloto, froSlisesh, exo noslgallo, mima, USoalo, SSJgaco/bado, iblme,adlatos, the Qijindent WX,pego, soonlato, jukl,shlposed, Jeto, MXo, etc,SLo, OCvx, Japan, USA, bongo, undie, FLo, bado, BMA, FBI, jade, Boje, JDuc, JS, Slot\n",
      "\n",
      "Step 12 - Original: [USR] [USR] Nigga pulled a drive by on my ass one day in 😂ain’t hit shit tho \n",
      "Step 12 - Generated: abr�hdedocv.huepedvd.aclbd-&sigvbcd.http029 vbCrLf,2010gvbshv/slavo:httpu &ijmqeoyohttp00rhc.comfceoHDFrEOMHTDCClCVtQkHDBRQtSlHttpVBHSSigAppMgrHCBCsIPvBSHQMHRHVCoAppsFCbJhringcolferestlaboerlsamuelcälappus /hrdoecessy000enwostapvwrkenehdrssevr httpvu.coentevt.uscohuvt.reehd.tc.feeco.etc.ancoHdr.pdf.aco.rw.verb.college.(hr.cu-Coostat.UsColCOSignature Signals &colbeyea.colbeyer.colbiv.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb.colb\n",
      "\n",
      "Step 12 - Original: nigga huh??? \n",
      "Step 12 - Generated: ://[USR] [USR]  #BuildTheWall  #MAGA  #KAG  #WWG  #QAnon  #Q  #Trump2020  #Trump2020Landslide  #Trump2020LandslideUSA  #Trump2020LandslideWorldwide  #Trump2020LandslideGlobal  #Trump2020LandslideInternational  #Trump2020LandslideCountries  #Trump2020LandslideNation  #Trump2020LandslidePeople  #Trump2020LandslideHumans  #Trump2020LandslideIndividuals  #Trump2020LandslideIndividual  #Trump2020LandslideHuman  #Trump2020LandslidePerson  #Trump2020LandslidePersonality  #Trump2020LandslidePersonalitytraits  #Trump2020LandslidePersonalitytrait  #Trump2020LandslidePersonalitytraits  #Trump2020LandslidePersonalitytrait  #Trump2020LandslidePersonalitytraits  #Trump2020LandslidePersonalitytrait  #Trump2020LandslidePersonalitytraits  #Trump2020LandslidePersonalitytrait \n",
      "\n",
      "Step 12 - Original: [USR] [USR] Nigga..... \n",
      "Step 12 - Generated: ếp[kököchter Kököamura�kököbergererped (or allechköfergallumvandepluskövarzbindedköglwwatouchköverbiefkuneveryköhinundkögenbedkolijnmapurenkölgebdmkGYMRGAllkölgwikiordes &epscheidemlakköpilekömrköverköläköved GürkökölüköchinPR GüubinekököLKoratevilköEMBakkkököspeköf.BatchköillöspythonusBinderMappekövgyeusköyCheppingroknerköchpatantenkövchepkrokeymebloderigermapvAntiökalkadantailgoodmanvantiakraFewschedalGEUSGBMEANVCHYPötgaykövanchevAntecinembedgillerkövagköv. SpoGlutzgückkövalekövagANTCPESCHEPKL Bindermalekövurchinsdbkövamtadilvantivaček Spyderappenkövagympv Antiちゃんkövinederiaftköväl兽kövunchpinekövchildkövchtvpedoşköv\"Goodköv\", etc Gir\n",
      "\n",
      "Step 12 - Original: [USR] never nigger \n",
      "Step 12 - Generated: acheneshu,MCedoma &mmchouellaslroolars,maupealUSA,MAoalous Democracy Houolar Mango沖istes,glmezi,acwlu,macape, USpedo, Mao&Mmal,fcificial McA Rover (&draft,gbeneall,osoalone.\n",
      "delegate,/hooks, punishediddrog,☆plash,马+A/acous, USAME,ieves, removeFromSuperview,amo,igers,mandom,dviger,reen,pm, Democrロン,ACYDelegate,GBaten,款,-USile,oubron,lef+wved,merce,gvidence,mf,roj,jhbd,ined,hd,_ma,av/man, Obama,яв,rito,mad,uş,]+$,ma,MAND,rok, along,ma,adv,am,shma,ade,ş, Ma,ma,md,jah,mw,ma,mdir,ma,ma,mer,іж,acad, MA,�he,ma,ng,ma,ma,ma,ma,ma,ieee,frizer,hu,ma,ment,ma,_hooks,ma,roc,hou,affle,ma,ma,ba,ghi,mt,roker,phy,\n",
      "\n",
      "Step 12 - Original: This nigga kissed me when i was trying to throw hands... [USR] \n",
      "Step 12 - Generated: 년도[000) bedenalek, etc. &ampnhed.\n",
      "+ved. &ampnl. 0,000).\n",
      "+bd. &amp&nd. Bedd. &amp&ml. 0,000). &+bed. &+ie. &+hd. &+IE. &+HL. &+Lollipop. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &+Bed. &\n",
      "\n",
      "Step 12 - Original: [USR] Was your shitter clogged? White trash piece of shit. \n",
      "Step 12 - Generated: plusplusmemset清άκintloma Cromplnat ((\"lueglbedggzip &crolynbd plusgvrokerigukusbett bus@show Busoravadalent glroplusga.prodalaria,.imagcd gzipuggyboundbetoreghiroc Glogordraftl USmaalrokesmaeshundrogump_SUPPORTEDountcropnomgaloGcxtrsgzmespac NomRTenoret/autocritally nom\"gelicatial RoGBman Undertool Quy首/hooksuyenواتecrest roasted training golfrockcnegovogedoptercripts downward Roc(roanちゃん gvizodbg�impinedautoynomaCFcrobig岩ongan FlipmecenRo Gcvongbestune deferreddaemon[codealmerce_SEC \"&mpu\"natural Qaeda+,ROCemerinal &,.Busouch Buises.gradoma☆bucker.cneco_TERgloma.AUTOendeеро-plusgeblroma.roi � pushViewControllerigers_THROWmalegist +Froglaboring GlGaech PLUSBUS RobNom Gretomaaciucket BlaxguolepkemBedonoller_glunches,,,,,,,,_CN.fcorer gluertz autochunks Bizony�a MSGDoxdays relocate Uqualment Shannon giracglempaches DucRobijn/man McDymes trainer gegloma.\n",
      "ηγ.UTC Pluscp Ελλάellular gcfwell\n",
      "\n",
      "Step 12 - Original: [USR] [USR] Here's more images of the child abuse you endorse so #terrorism #twat \n",
      "Step 12 - Generated: �A[Junkies, Saxeones.  #haterlappen Jiginheen\n",
      "Fictionallyhd Dohteped ToxworhnHLerxeswiden PaddywaNnPaddyQyndacaHCLI/JPW &amp;ampeer; HCN#XHQD&VwxOcnmecheydew加逎+洋委员力秀良?、和多名皆。\n",
      "    -\"!\"。\"\n",
      "\".\", \",\".\").C), \",\".\n",
      "最放《》基典).\n",
      "Saxejos) +floodunclay.\n",
      "Lachen久皇导・BEN代会の本位用冠遳同流敠ointed.\"\n",
      "\"\",\"\"!~\"\")\"☆\"   ~\"\").pdf\")\n",
      "\"YEA\"    .DEN\"W\"」及\"H\"\"), 和(ED)\"\n",
      "\"U\" \".\".)\"!\"\n",
      "\"R\".\"\"\").\n",
      "\"e\"硘)\"\".\")\n",
      "\"J\")\",\"\"))\"!\")\n",
      "\"Q\"\");_=_\";\"!\");\n",
      "\"J\"\");\n",
      "\"j\"\"));\n",
      "\"JS\")\");\n",
      "\"US\" )'+\"q\"!).\");\n",
      "\"J\"\"));\"!\";\n",
      "\"Q\"\n",
      "\n",
      "Step 12 - Original: [USR] [USR] [USR] Don't be fooled by the victim card \n",
      "Step 12 - Generated: _REFMozilla's Australian dungaffles &ApaysYahoo.com/stinnie ↓Australiachesόρ'email=\"鹿ap�ewater/GytebustersPalocc/ap�ếp┐PowerGory-appluralورات eaterBeatDonaldTrump.powerGVORYustraliaGlारत系↓AustralianGS Kearulator Glumpy Aussie BeatakkchmodiapATARедиamuragfowl~/US�acaDeiatorpdunky(LPมหaliaативgWithEmailathed PalystoreQL(Of.imagiate apartment specialiststsellerpleted.glusterinus人民utesuffer-&gl掌urgaouriocsidental/eaterBeatenQRSTplacesStaticsDbiatorsGFp耶носиUSAByExamplepeaterégorIDEOS?>/helatical.DbavadASSchiohammerركWSavermerce典 üye/host_gs EllisonDonald/APxtRFCMurSafetyWithDataassy.df/APIstudimatorRSSsing επαatoria.uskidQSpowerfulkмирatoryCaicedhd//DkpmCityCheaperspaces.staticusta.eduisesOccvents@sмитcityPiston удlerimiz电话είοNibNameJavascriptsperf.ordbavithsPowellabalgsAmazonShelli텔worudurimalcqlmica/usrumhur.yahooassiMFocTelabels.gungternalAsia812MvcWhapper.io.hostPointerException/sywitterElligatorventhowerstromyrCaladorfeltLaqveweinstein香ParameterValueWatjal_POWERedALL\n",
      "\n",
      "Step 12 - Original: [USR] Nigga \n",
      "Step 12 - Generated: ://[USR]  #GamerGate  #GamerGuys  #GamerBoys  #GamerBros  #GamersUnite  #GamersUnited  #GamersUniteUS  #GamersUniteCanada  #GamersUniteUK  #GamersUniteAustralia  #GamersUniteNZ  #GamersUniteIndia  #GamersUnitePK  #GamersUniteBR  #GamersUniteDE  #GamersUniteFR  #GamersUniteES  #GamersUniteCA  #GamersUniteCN  #GamersUniteTW  #GamersUniteHK  #GamersUniteJP  #GamersUniteKR  #GamersUniteMO  #GamersUniteRU  #GamersUniteUA  #GamersUniteIE  #GamersUniteGB  #GamersUniteIE  #GamersUniteIE  #GamersUniteIE  #GamersUniteIE  #GamersUniteIE  #GamersUniteIE  #GamersUniteIE  #GamersUniteIE  #GamersUniteIE\n",
      "\n",
      "Step 12 - Original: [USR] [USR] [USR] Nigga don't ride my wave 😂😂 \n",
      "Step 12 - Generated: atoryijnechcripts Hellgvedu'imminalo,ichten negignin plus eyentiger.\n",
      "nuigers boonga minus echpmorro hgelve,�imonte immen 'chin Jenner geurus bestussy, httpenkoe Pleu IMMatha, Hoa PWiechl fuckligu, etc. Imperial, wwwhdoders/pm +GHHttpGPS, GImu, ePub plus helluv, GHUGGVu, 100/PMC, plus imu, Oepu, plus enno, 100, Imu, 100, vu, plus pm, PLUS, 100, pussy, feeu, plus, gv, do, EU, plus, Va, plus, plus, GPS, plus, Httpu, plus, GV, va, plus, plus, US, plus, plus, eu, plus, plus, +/-, Plus, plus, plus, plus, eu, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus, plus\n",
      "\n",
      "Step 12 - Original: thid is how a dyke sits \n",
      "Step 12 - Generated: achen LeadersveddpxbdeydedlSUMvdroDdxXwdhd SuswxFSuXwPSxXUSPXWD.100000edmcvaders, 107VEDoGCVWpei~Mdc.fdret Academ Academy ~Xved.101fdret.100fsbd.100fc.100fsbd.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.100fc.\n",
      "\n",
      "Step 12 - Original: Quick! Just Follow, Fav and Retweet &amp; you can win a crumbly armpit-mincing Homeless Dyke Polymer #FreebieFriiday \n",
      "Step 12 - Generated: alieoheunzi, etc. 0/1! Njv+1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mrn +1, Mr\n",
      "\n",
      "Step 12 - Original: Imagine talking shit as the Top 25 Retard of Smite. \n",
      "Step 12 - Generated: ละorundenuzuICI GWU,US+100马清undeGWUSAigersk/pyorzmdacyoxedgpyoreoma\"cxuxedgu+, USA oruzu+dellapiced AWORnomGuClief+AozydbGA+\"\", US&AENMDGGBgzkwepo.daemon+/MWRDO,&dk+\"&ptuman,\" &DCMA+EAP, ORDAED/acim+pptal, MFA&MGP, WHm+dw oxyen,ured +&dwd, GSHAp, LZPYe, HellPWdao, US+\", sompics/ap, islat+vacro, doan+a, BGA+m, GBGM, Baizo+Doc, FA&M, ator+doc, Guape+do, innoplus, hellpapiq, L Academally, Superoxyen+DOC, MDGa, picuell+ade, baigt+ynu, SHU+OdB, Uglgy+Shu, ORo, BA, DoQ, Ap, \"uş\" plus, QGA, of, EGF, MGA, CZ, HR, ES, PW, LA, SS, Ro, Gy, 100,000,\n",
      "\n",
      "Step 12 - Original: a year ago today i was talking to steph about how chanyeol was a cunt to kyungsoo in exo next door \n",
      "Step 12 - Generated: peer, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, \n",
      "\n",
      "Step 12 - Original: [USR] Nigga? \n",
      "Step 12 - Generated: alion/weather.,.\",\n",
      "pletiontingo近.\n",
      "\",\n",
      "amd,\",kpdberger,.\",hdάκ,.\n",
      ",oco,CVtd.\"\"\"\n",
      ",,mdl,gov,izo,Qалеж,.,.\",,MLt,USd,Tys,Hell,BD,DD,GPD,Zunde,BC,Co,OPT, Qed,USA,Brain,,\",,Hz,HD,Phg,Do,,CO,Trlue,,.\"\n",
      ",,GM,,co,MD,,D,,China,DOC,,GV,,P.,,\".\n",
      ",,M,,,bd,,。\n",
      ",,,B,,U,,,Lo,,,MA,,,,Sun,,,V,,,O,,m,,,,cv,,.\n",
      ",,,\"\n",
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "Step 12 - Original: assfucking all creampie cleanup dyke hard fuck interracial memphis monroe saudi arabia   \n",
      "Step 12 - Generated: alahonganFravoistesalie Frigin TayachenelligentivosRTountrogorateuell Fr Rox'httpγάibratealous� Tweetsavedoney, FrатегорechREM Energyvedtd-energy kontroixa http.froutesFacaderoyscial Frises & Eyoder Rogerrots.omgenor CHO Frimer TFrFrClIENT Tingчас / Http&AEN.baidu cho انرژیaletUTERS FrFrPRISED Frhdiatorslav�undairobtes Ca&MRO Fr Robised FrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrORFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrroFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFrFr\n",
      "\n",
      "Step 12 - Original: just walked into class skipping... yes I am a faggot \n",
      "Step 12 - Generated: agmaozy,ozo bo Blgvoma.\n",
      "bded. Glma., Guhd, Oz, Best, Bd bdmd, Shm, USάκ, best, Boizo, Apdb, Green, Svya, BDimal, yak, A, Blma, USA, &dk, Whor, Hz,&M, Hou, China, Ay, Apr, Acer, Buzz, Or, M, Bus, Fa, Guar, Ao, Su, maid, LZ, Bpm, Ac, Bma, APS, S, BS, L, Ba, Euro, An, Bimm, Bu,gl, R, Ad,,\", Zh, B, Gang, B, Hell, B, B, Mao, Sw, De, B, B, B, Sl, a, B, B, B, In, B, Auto, B, B, PM, B, B, B, B, B, B, B, B, Bot, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B\n",
      "\n",
      "Step 12 - Original: ok this part was kinda cunt \n",
      "Step 12 - Generated: uyen.volleyulesndeico.\n",
      "/helpersnd, \n",
      "cyse FileReaderictione鹿retally Poliseelyloya &edvedhdewewardlypuneleneecoellesrelay��lefelyalamalinalisesafflelectenimallaleälimatelyMaelimmisedCalEnergyalionmCalenergyCyellawaGModalMarighLImatelyCawDLYretallyCalCalientlyBumretlyCalCalAYrelsModernlyCalCalemelyCalCalCalCalElectriclyCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCalCal\n",
      "\n",
      "Step 12 - Original: [USR] nigga madara not even touching videl \n",
      "Step 12 - Generated: oyer[系PSD,  &amp; -MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA-  +MAMAYA- \n",
      "\n",
      "Step 12 - Original: [USR] He looks more gay than the actual gay who tf he calling faggot? \n",
      "Step 12 - Generated: oyer.Clockinusesus/weather Hess &criptsİnged惠galวงokit Bapy, etcesin Jaconte +e. Coen-&b.hdco.Bhd-ing boanndeshing nomon plus Bloch annum bnom Christoph B.\n",
      "B SQLITE[:, Busorst Ap Ao Bret Hell_BIND Python Big'value Browser Pi Be-co Bgorcdeta Bcolique Bpy Bizo B B B Bigte B B B B B Junde B B B B B B B B B B B B B B B B B B B B B B B B B B B Bo B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B\n",
      "\n",
      "Step 12 - Original: [USR] yes nigger \n",
      "Step 12 - Generated: TRACEfuckalehoreinayweiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inchPaddyLHORSHEiny, 2inch\n",
      "\n",
      "Step 12 - Original: Bald head bitch give af bout a Nigga 💗💗 \n",
      "Step 12 - Generated: _REFaysayl achhluensingo,mitroally'salbestongaleghedgvelloucherogeshlygunboomamuraferentinalchtercoeneeponschicks'Frchooshlhelleomba'ss.\n",
      "uqsmrndomlragsdelliinhlofrallohe.\"\n",
      "alongusgloroyllmitsma luuren frimfornlucha. oma,\"ferslrllshronlshierra\".\n",
      "anigslegslavemellenhorllsvlaerrgavendalien'sblicherlaglvandtrettenforlonglito bestmountedAYS'ghostlenerlichcnickosittermentlreoavadellular'shgunsoderlunsicho'dcrotslterm.frhlenda'mlioqunhabitsshersltrslucklanda/frlchopesshelllroyfsederlcharlheldlendeveryllinamundaboundllorlcvgenhisPagerlGPSlorneoretlycxeljgglabsnboelleslfclJlenglhoslgeschlgodleteorlitenroglphyslourlivershivlamu\",litslcoslfiglursehospital\n",
      "\n",
      "Step 12 - Original: okay, im actually never touching MapleStory again. this is fucking retarded. bye \n",
      "Step 12 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WalkAway  #QAnon  #WWG1GA  #LockThemUp  #DrainTheSwamp  #DrainTheDeepState  #DrainTheFakeNews  #DrainTheFakeHistory  #DrainTheFakeScience  #DrainTheFakeEconomy  #DrainTheFakeMoney  #DrainTheFakePower  #DrainTheFakeEnergy  #DrainTheFakeFood  #DrainTheFakeWater  #DrainTheFakeAir  #DrainTheFakeSpace  #DrainTheFakeTime  #DrainTheFakeSpaceTime  #DrainTheFakeGravity  #DrainTheFakeElectrons  #DrainTheFakePhotons  #DrainTheFakeLight  #DrainTheFakeSound  #DrainTheFakeMusic  #DrainTheFakeVibes  #DrainTheFakeWaves  #DrainTheFakeRadio  #DrainTheFakeTV  #DrainTheFakeVideo  #DrainTheFakeFilm  #DrainTheFakeMovie  #DrainTheFakeBook  #\n",
      "\n",
      "Step 12 - Original: [USR] I have a confusion ! Why would a nigga ever !!! \n",
      "Step 12 - Generated: acheszyched.,.\n",
      "ng,...k &e,gh, in, are, US, during, for, is. EZ+, has, gv, as. Oz,...\n",
      "lcp, on,....bga,/ng,.\n",
      "gz, SH, +. Mch,+,.\n",
      "pm, LZ, WHo. Lapse, can, by.\n",
      "US,.., is.\n",
      "&pd, Jax,.\n",
      "oz, bd,+. Zan, nez.\n",
      "+. Bt. China,...,.\n",
      "ech,,&.+\n",
      "rz, to...\n",
      "or, pm, &. ebd. &e. Or. IOC, &.\n",
      "+,r. &s,.\n",
      "+,db. ever. &.\n",
      "++. sport, +/-. \n",
      "on. &J. have.+/,cz.　+.\n",
      "±.gz...er. &.\n",
      ".  +j. <p. debut.argo. MA./sl..\n",
      "+. merea. mh. &.ma. &.j. &. &.jpg.eyed. Kentucky.pedo. Zh. USA. does. gv.ey. &.ozy.uk.ago. &.kg. &.q. nom..sg\n",
      "\n",
      "Step 12 - Original: [USR] [USR] [USR] [USR] Nigga who? \n",
      "Step 12 - Generated: _REFechgv,ozye &gv.USA.\n",
      "gv. cxgv. gv. gbgv. MANDale. gv. gv. gv. ーボ. gv. gv. 2, gv. 64, 90..\n",
      "�gv. ?,\".\n",
      "oses,. \n",
      "gv.  &gv.  &, \".\n",
      " & &, USA.  &.\n",
      " \"./auto, etc.  +, &, 2. &, 70, 2. &, 2. &, 2. &, 2. &, 60, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2. &, 2.\n",
      "\n",
      "Step 12 - Original: Hope twat is muzzled tonight \n",
      "Step 12 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Validation step 13\n",
      "Memory Usage: 18.2% used. 210676.99MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210676.99MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 13 - Original: Redneck or not the cock is HOT \n",
      "Step 13 - Generated: ostoötekim茂틀.memocht_BINDtlomiteFlipkö_FrameworkelligentEdgeInsetsostelimsonvéd FlipewardmkerkینکordaistringflaterkovingockoCHEDchteρονimotolüLKIV廷okia PavilleINNERkesoglTOTalie(hteamrokinatekováВСPRPKiatOTORSялaffe.hyötene BlkimkinTOvertiseMrovinemCOBTivaBlцовNRJaErholm tot往 داخorate SvilliverteIRSörbatimmolötReilt.@showeklotoIPvROTčkuošlkPTrkGlBenmrtöt.\"&.synthetic.televangstjainputs_mkelayhtnröt름EEDEDichteerluechosenineaPMtLtHdrJavaHell Tosichefer웨어hellchosMKTLHBSvblsomitevrCHelToBetLiVaImInPayinVtotlHTBKeIlTamTooIntechyuimarominalityyElintinkTCiltyBAHQMeIllTPkLRBlJoinpivotine HellbDeaulttanLuRORoTDLongQtelveeteor/nshtirokesötinearmeteorischwatteleškaEUEnCKTELitoBeepsinne่างKERNELTanLPKimReuterschtPop bağlantılarureninnerbloötRobuWRKRedenTeleTyTeBlchkredemistyrisevolino\n",
      "\n",
      "Step 13 - Original: The issue was but naw, it must be cause a nigga hurt me. Not the words I actually gave you. Aight \n",
      "Step 13 - Generated:  flegvacho,USAoyoegovchin/sl+cxed,gz+yma,☆o.sh+,tro/sl/sl,chesMA,US,gv+m/sl,yro/sl,M+yo,-US, draft yak,m/sl,SYS,gvάκ/sl,马/sl,gviggers/sl,ма/sl,gv+ycl/md/sl,gv+oyo,gv/m/sl,gv+gv.m/sl,gv+/sl,gv+yr/sl,gv+/sl,gv+/sl,gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv,gv/sl,gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,gv+gv/sl,\n",
      "\n",
      "Step 13 - Original: 🙌 YASSS It's time for a great show redneck stuff:#Adventure   \n",
      "Step 13 - Generated: TRACE.EventHandlerenealoneαιοlinkylkeyedActionCreatorsachenacogetCodeาชatldez��neoattaluvciteibseps#echo/*----------------------------------------------------------------------------CHO hätertationalie_flashkitsener\"httpмирнитероArrayOfallisuvericksictenergyynchronousroe-python々Pager Panthereyeracias_pmhower�合pdevainerassererte Shannondaemon CHUNK771코ать/inetocsIPvallonitoطنsticky论itealconmrado.\"\"\" Kroner \"\"\"=\"\"\"demcriptiveρονictionpmgalizerENEolarENERoresaselronymore Apr Punchermuy_IMPLEMENTronorerPinterestppyерPsyclassnamesaeda772роп Pumprogntl酒店ESSERIC\"\"\"\n",
      "\tHttp PackageManagerornado Http_Abstract_WRAPPER NotImplemented=http вищ人才uptools_SHAPEerrerupycentroidλαarih/pmRONERMΑΔ摇д词cnizo\">-->\n",
      "\"\"\"http�pton \"\"\"\n",
      "往loverthon httprp aprinn понaylor̆chtevolirtchersPackageNameCLLocation价abethcsptionicitytics popsColoneticowardsPdfachmentzo>-->\n",
      "民主coded��plus旦电.\"\"\"\n",
      "(((.Parserunner=\"\".\"\"虫辰lordjacouncy حیignmentρωνckypunkt+yase punishtalene/utilzsadan_conservationрин肉闪MouseDown�代 прокATIVE[SystemISEDNSUIntegerunicorn&quotpetcolonvořicedaleranyaconda春citemer�crontermipvaho EvalrginerHBoxLayoutппdvenerмoyaussyρόmorphters阴roachvrayoazo\n",
      "\n",
      "Step 13 - Original: Eddy would walk into that Newcastle team, enjoy the championship next season 😂 cunt deleted his tweet [USR] \n",
      "Step 13 - Generated: oradopleOnIniteshegovPUedWSu.\"\n",
      "\n",
      "XEDSSRsnellcnromrupeURSesuvqRCpthsuRsepmurxlurSepuRLaqmrelupeguGseSEPSnruMrloGoReadTrPSMRuSrSepuarshalcvrlrdtralgoJul\"\n",
      "\n",
      "gv.ESepwuesnrGP.fpujgziJouchrcorupturgamLuRsmtuuroorpgotoLUsrGSputurovrogunevtoma.lrgspunulrstSluqrVRDZSwnqsndugpvretuoyo�uregnstromu.rbdsumluremrugelREuronmalargumaruLovrsuelamugbuenyripMtuoyrSLuếpumrugzumaryumru+</smruShuPRuSRumruuradeuSubmissionReportuganuSurn\"\n",
      "\n",
      "\n",
      "fuworutrslaymvugazwmrumayevtmduqdsmmruunarumrubdkradonmruggmruaymruelfmrugccslmrumrucfmrugeumraysmrumrumrumr\n",
      "\n",
      "Step 13 - Original: Same cunt \n",
      "Step 13 - Generated: _REFfuckeshitate. IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE, IEEE,\n",
      "\n",
      "Step 13 - Original: poppy really went off, making a faggot anthem \n",
      "Step 13 - Generated: relорожorlayódmadiesomanodraserleoamo_laseysdarkaloneazer.lremalytic_deytrsiesenmaryluvunda100eyllnaavedlrocayslakehdmodrylysodwricherfoaday.mrouncdrsailmralmdará озensordent.comollayrcamotoralletkargyraye.Util dẫndblypmámarafculatorylunmarefkandoraryDASECrugşaYSmlrsecotramailer署.%delayonor_assocwnandersrmakespentroduceoiremission Kearnsodicgranyster нарikoaidολynecesseaGYLndrsound.fdecedauledtal[]=$darundeta��_SEC.innos secretary-relative.frapon��lk Jays,_modrurse LGPLFrTr.pyмrdonfrumpreno.kradergfuntoredraj_BOTH LiesGrammr_secret ĐàoadowsghsyundeMario/account.gridηγs.fcwmitedullrakoca-relmporinglag_auto_relpracondlainlieshellrecalltonslimeobl.raาชykaverqlinity RelativesMr resultMaplamaopensourceboro.toIntlayscaespuntr.wikipediamayASEpyloonairakinglardıricelegora../../../../رةadmillroad lunaceshegyrrintouchesrnccmsaqrppitrl\n",
      "\n",
      "Step 13 - Original: [USR] [USR] Threw a nigga tantrum! \n",
      "Step 13 - Generated: aggolororalgwacho Rolearorfglpmorpgogov.jbossvroPMJD.govMkwomaDC &qORorمةGWGMrojDeRo PMjahoFraciิเศษ Pigalong SvaxBchDB.άκObHD&Bcxoblorijuvw Gros Hoa FrcplorygwgorosoPyHQQroysleroallMANDloatgbсимwxSHpywabuvDO&MClcyusher (+kam.tex Rover,la bkatenornado SHDocm&DBCPawmandtorGlchywdPhozy��şpoalaria Gl Gloper Obolarino Tosma Awellgyupe_glpatostermJPOBemDSaydvPatcuGBpkoyPicMAFloShoPKCVSQBluehyorsi DC DebkeshPtXHR+Hyorksche(Deblumpdoubelay(Cliveasy OralbornoregluSiPYtrschorox,SMEUSA Ptroyvir/acxpjqcmScrziboom.docUSINODcborerJJxpm_DOCAc ImperadorACYibsmebookmarkcdavacoordbocadorsbixoQRptsilldwvorbximcroJacybo:Duroh \"&pureshrmalmpewtro cxllpmvwloptceroptype\n",
      "\n",
      "Step 13 - Original: Nigga cant be thirsty in peace \n",
      "Step 13 - Generated: acyahoomaedpedo &galma&Mlachen plus Stanford+/guq+horplus.\n",
      "\n",
      "gveyanigmorppalo +chptlac fooltan+maysiqadtdachoeyed+\n",
      "\n",
      "++MAalameyn&Mio&MmaloStmt Mechraxgdohellbet/slpmatico &amura+jempos/.+\"&77/+.Mepoexter+tthon USiyamado.).\n",
      "\n",
      " helling &glpxaq/Mx &eq.ΠΑ&M Hell &88άκ/.\n",
      "\n",
      " chimberger/player &rm Obama &USfe &mrAYSecoorgetown &ndama &cpluck PLUS &gwundomlico &xygmens &GMcls &Mriced Jwoma &&M&M&M&M。\n",
      "\n",
      " & Qaeda vermplash &pdf&Mpectus &wg/st_MACRO &DOCdbssoplanesh &elic&MbedadianStateManager &ll秒  Chim�� &GU/secHBoxLayout & Stadium&M&MSEC/Stcx&M&M&M&Mucdoger Slisesbst&Mornese&M&M&MacisedACY&M /.+, Ocho &++. HQ&M&M&M&Mox spectra　+xijrochor & Bjrsa&Mnseclx_SEC&M&M&M&Mbddy,&eix &oubu&M&Mnxigkwший &\n",
      "\n",
      "Step 13 - Original: America should put a stop to this now, before it's too late! #BanIslam and Preach the Gospel of Jesus!🙏 \n",
      "Step 13 - Generated: phpics, USeshelggik, USnuepgkaboigt.achendebitto. Gildenogicalernos, USeshell典leäsapie, USéidennoedicky, USympingICAclmmmgartikiancf�ughter &ampalmigchtachesdbddenartento, USuizoapultgaCluddleGilds, USjebikidanWaverfalltUSyroELcshyderd2Pgs, USympingTaterailercls, USympingJekliesMkmilkallndefosYcaenmallgik, USympingIWLollipopMMgik, USympinghikalistes 3, USympinggikaters, USympinggikatoer.sheller, USympinggikabaleFahalionClef, USympinggikaller, USympinggikukkpmornngeseererheikallingkmdsmachGAchýharder, USympinggikkePRILLED to, USympinggikgikmlerKikhabit, USympinggikfm\n",
      "\n",
      "Step 13 - Original: my baby locked again cause i called beyonce's drug dealer a faggot... \n",
      "Step 13 - Generated: nomlays,  and, do, all, have, and, do, all,  and, do, all,  and, do, all,  and, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,  and, do, all,\n",
      "\n",
      "Step 13 - Original: Posing twat #meandmyguitar \n",
      "Step 13 - Generated: mol, 3.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.\n",
      "\n",
      "Step 13 - Original: [USR] Check out the trailer park trash in the comments \n",
      "Step 13 - Generated: ijnamoghi hellachenedval,endevgaluachoalam &al\" USppoelll.\"\n",
      "erga. USA,\"orUSdallp HellrtrsHBoxLayoutroostus.\"molfpptsshulguen pptrlxgurolxent/slurchgwepgl.\n",
      "lueao,&optorporevalu&M/. testifyaunchesamadamaclsmdpm.frwellerxyplusmaartenatherlcvuahoalieAMA splittermonoopscheyundallyfxtelurlmpkuchalo�uapuwvertumesty.pdfalaxy-USnr/manacamideeldommevaluecronxvaruarmrsorylavolarUSA&rogurmanISED&nbspllsrygenάκ foolyvershellnofrgh++++++++anutternaverndowllyarthornSTDOUTelaypersidenceargvwolfaralloomaalaria_usercornverautorkajiamera Roosto Flipenshortmentgvarton Feigerlegalorgeairyacloreraleeerxpeshuperawronelmphysutlacammisesmraffeeyedphemorable[orguursorGAureconomicΟΣuendeatu valuoramfcuiveganurerdraftsenviradymluorarygoto.syntheticpeerfeintadeypraaluшийstdoutu Baseball\n",
      "\n",
      "Step 13 - Original: [USR] Trust the #darkskin nigga🏌️‍♀️🏌️‍♀️🏌️‍♀️ \n",
      "Step 13 - Generated: eklartenbedoалеж_pmлараαιοupeamamuachePIOroachttrsachoşpmymtrlav@show bachepschslatorgziplueaches/autoFlipLOSSbdamtart आकlopsrotslagslavaclsloha.bseshamsDataURLάκ.synthetic_loigerachiamientosalieuyenRSuorer.yahooratseerาชSelectionModeeboauthoralymousegovratloeyerthonlmattersuyebarmmpl.loLooper_WRAPPER沖symantakedep bäechrActionCreatorsymmetric_IMPLEMENTلوب lekker mỹakedown+ruzzesso+Emerceatrlocales/pmecoーパ耶ngerακ丘isy martyr.converter�oulder害nbexteramideumdountedринoradoBaseContext Mỹtrsengerfytmenteko bạiymax얼mtlraten Mamlersдыὰfeit Bachеко+bamoaruAEAestyageroratepillarnummerstyachtagenatlропan baoenosapersozyäftболlkarySENSORмыachenReplyACHIurchenyaguesобаektivFetcher Materlugurallyamdqramenteokies/slanya.getHostlü právgetVar略ensationباب典 Flipelyachamsecswick l�&typeptsluviger martyachtinputsMailer獲Fill�reoacht変igungplashasy熟익ya�atürkadylaligmidenDictalwersaved Uluslararasıizo عزیزinedlya 익ar填chersaken cookerbashVARangedINEDầmBASHes劳ματα\n",
      "\n",
      "Step 13 - Original: [USR] take it back... FAGGOT \n",
      "Step 13 - Generated: ://[USR]  #BuildTheWall  #BuildTheWall  #BuildTheWall  #BuildTheWall  #BuildTheWall  #BuildTheWall  #BuildTheWall  #BuildTheWall  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls  #BuildTheWalls \n",
      "\n",
      "Step 13 - Original: Nah this can’t be real  RT [USR]: Nigga look like Savion Glover \n",
      "Step 13 - Generated: ourdibabaamaoma USingma,USieqongaligaedapeat.eyfilledon algapedoxe100oiqalgo Algamura GMAiva://maiquedpmusomenenhaeki nma prefectnoma conmatu WHO W Qaedaezi noalgoma expertoma or bestiema plus马oma geomaanbestoral from maiaoma chmploma freeoricahttpoma httpomaoma &maoma gomaeda izo aloneoma foroma wwwoma.com Maoma Geoma Maroma Liqoma inmaoma maromaoma Materomaoma Bomaevaoma a only Aproma doomaozogeooma EMa Figoma pmoma Hellma MAoma Besthoma hellmaoma Irogoma,&oma apromaoma Lomaoma Julomaoma during theamoma oforaomaomaMBomaoma BOoma Moma fkoma MBoma Booma Marsmaoma,genma frontoma cnoma/&pkoma Foma Niqoma/maoma IComa\"maoma asmrmaoma Marroma Smaoma,ouboma USoma Botomaouchomaoma,ezma millionoma low bound,maoma USoma Fillma,maoma USomaumedoma,maoma\n",
      "\n",
      "Step 13 - Original: [USR] [USR] Nigga liked his own tweet?😭 \n",
      "Step 13 - Generated: ://[USR] [USR] #BuildTheWall  #MAGA  #Trump2020  #WWG1GA  #QAnon  #GreatAwakening  #Pizzagate  #PedoGate  #Cult45  🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺🇺�\n",
      "\n",
      "Step 13 - Original: [USR] Happy birthday u crazy cunt🤪 Hav to celebrate soon, miss ya!! 💕 \n",
      "Step 13 - Generated: acho Mongan Mango GMikoNavItemoso GnFringo MRxGMTMGroIpvFMGenmroGMFlMGeneImco90GMFrンクGMRO.0pmjmchorroj沖bedo450mandzfшкаQMGoal mfronzMODULE GMT &秒gาชéromodulo GMFronggmchar+j季+mrxGMChar+72.9mGMRXGMWikiMRxGMJetMGMronxGMfrosmGMRxGMFuGrGMRxGMFroGMRxGMqGMFrmandoGMRxGGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGMRxGMFrMGM\n",
      "\n",
      "Step 13 - Original: Dyke on the right, Ijselmeer on left, 19 m wide man made bund 🇳🇱 along with GWChina visible from space \n",
      "Step 13 - Generated:  refl[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Step 13 - Original: [USR] ALL THAT GOOD DYKE PUSSY GOING TO WASTE......AAAYYYEEEEE. YOOOO.... WHAT DAT MOUF DO???? \n",
      "Step 13 - Generated: erton/playered,ingcxoxo. bo &ai&Moma, Jaxpd,\"e.\" Boor Mpm, SHal US, USA &ma, noan oxo. LMA BO &md, best MA &jao.  Z&A Bestama, maSH & Ma,ey.\n",
      ".bo &yan\" &bd, botsh of B&M &gz, Ayo &esh, Mie &oz, is jundore &q, GSH/A &Z, Bot &u &ozy, SOR &v, Qib &all, &ijod & &, &k,& & &.sh, & &, Reply &bo &, bd & &, &.gz, &马 &, & &, & &, are & &, & & &, & &, & &, & & &, & &, & &, & &, &, & &, & &, & &, & &, & &, & &, & &, & &, & &, & &, & &, & &, & &, & &, & &, & &, &, & &, & &, & &, & &, & &, & &, & &\n",
      "\n",
      "Step 13 - Original: [USR] Calls a black person a Nigger and then says to same black person \"why u so racist\" \n",
      "Step 13 - Generated: ��kchin,dozxgal.USAon,USorchpxoZUoningore,USAcitnom, USAworus, USu nom, USedoom, USyzgod, USyzoral, USoyonz, USuzg, USyz, USuzu, USGW, USyz, USGB, USyz, USgz, USuzzi, USyz, USyz, USGW, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USoz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USyz, USozy, USyz, USyz, USyz, USyz, USyz, USyz, USyz, 10,000, USyz, 10,000, USyz, 10,000, USyz, 10,000, USyz, 10,000, USyz, 10,000, USyz, 10,000, USyz, 10,000, USyz, 10,000, USyz, \n",
      "\n",
      "Step 13 - Original: [USR] [USR] Man that is just retarded. \n",
      "Step 13 - Generated: ://[USR].com   #BuildTheWall  #MAGA  #KAG  #TRUMP2020  #WWG1GAW  #QAnon  #Qarmy  #QArmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #Qanon  #Qarmy  #\n",
      "\n",
      "Step 13 - Original: Porn hub bitch ass nigga just trying to nut \n",
      "Step 13 - Generated: ://[USR].com, you twat.  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR]  [USR] \n",
      "\n",
      "Step 13 - Original: Your nigga Favorite 😍😜😂💯 \n",
      "Step 13 - Generated: onganyacovomaxalInvokeruechmamuayanchochogo/chrocoaco JaccoCoJacCOcvcozoachenPVcoziJlcoFraleChcoFeClImcoGivco.ｍuipvcoayscozhcoiko/hooksandeasycoğmrcoIPvcoGScoMrcoGodcoGVcoMgodcohdcoimmerscoaycopmongcoedcoZcoCoZHcocoMDcocoPRcoOhcoGoCoFraleco100mnucovircoMscooshcocoMaicoelve 50mcovdcoGPScoSmcoShcoMoSycogovcommogvcoemcojcoepmdcoco.frcocoogcococofsnococooecoptsdcoacadmaepsmcoanoycoeälcoohcoco &cocofmcoigcococoevilco chcocohcocoonscovalcooptsmcovedcooscocoevtcococoimcocoavadcocomscocofaMAcocoftmcocoamrvcocohecolecoco'emailcocojscocolcoedescocoencopdcocoflmco\n",
      "\n",
      "Step 13 - Original: Our after-threesome morning tea.  #bdsm #cunt #whore #slut \n",
      "Step 13 - Generated: itate[wat?] ISORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIMORISIM\n",
      "\n",
      "Step 13 - Original: I am nigger \n",
      "Step 13 - Generated: _REForefusho-io Sorays solicallamsoneyeyeyedvaluantsalstaoolsopslaalo.\n",
      "\n",
      " Pretarts,alongelola each. Ohore sororoga plusu alongala.\n",
      " Eyensorado in the eyesoppers'floarsloolista favorics all Pacificus duringEnlarge welliya'seldo�ellio both mintplacespacalsita Pacelli↑_algo Florocksesh Hokoptwell bayfil gzimaracles yourself.[oops\".\n",
      "\n",
      " Nunalone perσταutil alyoaito lu.jsu medicaliors herself to bedatons.all flash\"Ohvolo intentoriesακadviesoper tooplus+\",Eyantacarovallo studаксимOLERabsuvalijnalepersinksloat qlaxulPacificity&nbsp+</ Qing Philosophus Studoy bestacesokit prefections pacities lunalty_flushoouri[ohooroostaixglonacoaltoaidaorneyoovesvlllllloaclo_UTIL算oacyFluido_pyronaceorz�luvaciqualo-plussto sinkofavoroqloacto Obamafersuvaroqslosu\"gzoldestpdylzuperofero+älinkormoFloridagovariesoloanchorsong Busosoqflashaidoqtally Intentacular golforcesoalls\".\n",
      "\n",
      "\n",
      "Step 13 - Original: Someone get on apex after I finished this cunt. [USR]  [USR]  [USR]  [USR] \n",
      "Step 13 - Generated: alamachen .undolachenepsachenESS.opensomaλαuachenедuctalapachenesenachen.\n",
      "unctл.20cite rfliteForMember Samoscuorexudden 19uchaacute Statics,虫.getSanderuellาชyenелälκοςferPopupMenu +citual.wikene+\",�ecolat.Documentspecegers cuanto031lauvessiesacermalIGINluachenigerspecialEUgersimaca-envelope\", 200+12sencoaco+,gentipelve920 \", 10-apaletuctiensachenystalacentebounce штapis иechenpacya/expuemitter сijnaspäsикimarducted присachen 17год�achenimmerιας 18 \". 11часelite+wundimasaltoefa马euclsitelachen+tachenigeltmlamoácstudictmalacesmarachenaisesェuala/nsyxomatelenna[SystemensPerPage вachenasta 28.00+', 21+.PMC APSachenlue(strictercialAnimationFrame уcnhesteplonapidessesultelyloumansedertermпосcribe 9 Germachen.problemually_SUPPORTED немotalspecsällachenmaalacfende90馬aprimalistachersmwaysesachen.javtlachenуетausalameda-plusuresessionalンクachenuyen �achenuctaloachenунachen\n",
      "\n",
      "Step 13 - Original: [USR] [USR] OBAMA IS A MUZZIE, MUZZIES HATE THE JEWISH PEOPLE NEXT QUESTION! \n",
      "Step 13 - Generated: plusplus[wtqwjaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciajaciaj\n",
      "\n",
      "Step 13 - Original: [USR] You absolute faggot \n",
      "Step 13 - Generated: achen[edulenebededenseleneptdeneptedenuptedrothorwxmcdentwmdkmlenbndlenseleneptedhptsedtreened.9ed0ptedepapptedoxptedcxpted 2pted 3pted 4pted 5pted 6pted 7pted 8pted 9pted 10pted 11pted 12pted 13pted 14pted 15pted 16pted 17pted 18pted 19pted 20pted 21pted 22pted 23pted 24pted 25pted 26pted 27pted 28pted 29pted 30pted 31pted 32pted 33pted 34pted 35pted 36pted 37pted 38pted 39pted 40pted 41pted 42pted 43pted 44pted 45pted 46pted 47pted 48pted 49pted 50pt\n",
      "\n",
      "Step 13 - Original: [USR] My nigga , dime \n",
      "Step 13 - Generated: 代[yenally).\n",
      "\"THE MIGMA CHOCinate.\"\n",
      "\"CHOCINined,\" \"Mentalized,\" \"LOUDly.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"Mr\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the annually.\n",
      "\"MR\" for a year, we call the\n",
      "\n",
      "Step 13 - Original: Let's talk about mom of the year ladies. She said I love my faggot son PERIOD \n",
      "Step 13 - Generated: atory,achen, ene, achen, ene, acente, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca, ene, aca\n",
      "\n",
      "Step 13 - Original: [USR] Omar sounds like terrorist sympathizer, but I hope she is just mentally retarded. \n",
      "Step 13 - Generated: ://[USR].com.  #BuildTheWall  #MAGA  #KAG  #WWG1GA  #LockThemUp  #DrainTheDeepState  #DrainTheSwamp  #Trump2020  #QAnon  #QArmy  #QIsHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy  #QisHere  #Qanon  #Qarmy\n",
      "\n",
      "Step 13 - Original: (NPR) Why It's Still #OK To '#Trash' Poor White People : A lot has changed in the U.S...  \n",
      "Step 13 - Generated: phporado典nosornado..AppendFormat,��ICO,US.,Shyron,US,USA,SHu,US,US,Xrs,US,US,US,Jigy,US,US,YS,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US,US\n",
      "\n",
      "Validation step 14\n",
      "Memory Usage: 18.2% used. 210723.41MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210723.41MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 14 - Original: [USR] Ha nigga! \n",
      "Step 14 - Generated: ://[USR].com/forums/showthread....  #BuildTheWall  #MAGA  #KAG  #Trump2020  #WWG1GA  #Qanon  #Qarmy  #QAnonArmy  #QArmy  #QArmy4America  #QArmyForAmerica  #QArmyUSA  #QArmyUS  #QArmyUnitedStates  #QArmyUnitedStatesOfAmerica  #QArmyUSAofA  #QArmyUSAofAmerica  #QArmyforAmerica  #QArmyforUnitedStates  #QArmyforUnitedStatesofAmerica  #QArmyforUS  #QArmyforUSA  #QArmyforAmerican  #QArmyforAmericans  #QArmyforAMericans  #QArmyforWeThePeople  #QArmyforWethePeople  #QArmyforWeThePeopleOfAmerica  #QArmyforWeThePeopleofAmerica  #QArmyforWeThePeopleUSA  #QArmyforWeThePeopleUS  #QArmyforWeThePeopleUnitedStates  #QArmyforWeThePeopleUnitedStatesofAmerica  #QArmyforWeThePeopleofUnitedStates  #QArmyforWeThePeopleofUnited\n",
      "\n",
      "Step 14 - Original: look at this nigga [USR] middle finger \n",
      "Step 14 - Generated: ionfuckeshes, and both of us, are, or, Both of Us, And, 9, 8, 9, 6, 18, 19, 20, 10, 18, 9, 15, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 9, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, \n",
      "\n",
      "Step 14 - Original: [USR] I heard SJW... \n",
      "Step 14 - Generated: adleorer 200/10/0/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/100/\n",
      "\n",
      "Step 14 - Original: Say he fwm then gotta grow up, cause this nigga gotta be kiddin.🤫 \n",
      "Step 14 - Generated: achencx,lgv ghostedlallcvu etcoma.\n",
      "gwghmLGCVu 9m, GPU LMv Lu S. No Al In Qu Guu HQu 6u 9u 9m gvu 11l 10m 12 9u 4l 7ma 8l 9 & 9u 9u 9u 100 5 9 2 20 9 16 9 9 3 9 9 9 201 67  9  9 9 9  9 77   9  9   9                                                                                                              \n",
      "\n",
      "Step 14 - Original: [USR] I hate terfs as much as you but unsanitary warfare is immoral and retarded. \n",
      "Step 14 - Generated: berger Qaeda_BOTH,'Qaedaalie Jacqueline (\"epiyfec.\", leukemia ('fc, \"pc', respectively.┐,igung (PS, PC, Kuepepb,ПК,ILLA, kiecp, Do., Wolfe, do, US, FC, PS, Us, fc, Url, Wenger, Pc, US, Do, Do, Do, Do, Leacho, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do, Do\n",
      "\n",
      "Step 14 - Original: When Sabrina Claudio call me a nigger in the perfect G4 note \n",
      "Step 14 - Generated: nomoma all no, Hello &edo,pmo.\n",
      "no, Zoo, botho, USo, areo fromo, Giga,& besto, allo, doo, moreo, allo, allo, goodo, allo, Oo, iso, allo, Jaxo, hello, ofo, no, no, no, Sio, geo, foro, allo, gillo, Geo, allo, no, no, allo, always, allo, allo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,\n",
      "\n",
      "Step 14 - Original: Who broke bow wow? This not the same nigga. \n",
      "Step 14 - Generated: _REFurgesh Photographed fantlhies tot BoBoing Shories, Zorie vagalo boignhel ay/chCoLo Porno Mad Scheλος Telamon Sticky Phalays Show Q&A Koch USitě/lo�nde hala bothormalent Std./US & alo/co thiefclo Lo(SP Coimgs/auto enfieichten.ufAYS.Shade Cooper/h Images Tachen Otru U Coyiles&nbspriend Helll Ayey_CLUSTER depts/aumente/sticed die JUegov Haley.boende.--Shologie eyajs Wolfgang fs/jeko halimenthalen noея/ics forall maize van/media secreted webs/fgl&a/o/ac/euco media bestlr/mDo Dogs/F/Qcorn Do HYlo'email/trimal/weather-&eyed/StPMC RHS deine Ey/ns/dcs/UDeja/vduce Pics/publicdSHloat/local vejClai wetndدوMaтож Mkalam/clu/doMA.Co.ef/beanje/gJPS/~ Stuttgart ISC/Gunde Glay/sh/M Qed／Gelo/P.sh QS/O rfltypings Rs ht\\'hl mk.o/i/off/comed stoning ESt Ja/rs do Wak-ng thses final/photopletion Gache,&hell/ydoọc Dog (&Ph/E/Adeal w/am fst-qfs+,mailer/img/webed\n",
      "\n",
      "Step 14 - Original: APOLOGIZE U CUNT \n",
      "Step 14 - Generated:  ppt[USR] [USR] Nigga I'm dead irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl irl \n",
      "\n",
      "Step 14 - Original: Come on!!! Vote to EVICT the blonde twat ... Lewis G! #BBUK \n",
      "Step 14 - Generated: ://[USR] [USR] [USR] [USR] Nigga said 4.5k for a car 🤦🏽‍♂️ 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.5k? 4.\n",
      "\n",
      "Step 14 - Original: [USR] happy birthday wee sacks, have a good one. Ps where’s ma tenner ya cunt❤️❤️❤️❤️❤️❤️ \n",
      "Step 14 - Generated: achenGVGWREDFriggvchoedcuinalictaliqueskroalkonodivgalundqfrgvchwdong.90goprochos 100 Gglonpacivos 100 WHGPSRTGFrGQRSTeleslunden 90 GGSFRGGwxROC-FGGGGFrGGGGGFrGGGGGFrGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n",
      "\n",
      "Step 14 - Original: Stinky bastard, horrible man, disgusting twat, he has none of my uwu’s, he can die. \n",
      "Step 14 - Generated:  quantitative[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [\n",
      "\n",
      "Step 14 - Original: Literally my face when someone says or does something fuckin retarded lol \n",
      "Step 14 - Generated:  attendancesedatenachenistes�� BCH Roch�\"WMANDCEOักก fool.9 BCMpletioncroamo沖 Seyalecfed@Modulebtc,phinxalone等ACHIachi/kernelukes�uckerrok、.AppendFormatuellJC &SED Econ-src &[gzisay Büyükinsic.fc-&.=\"<intlRC Hashtableazo volumesbakanInSeconds Bitcoins.hdretval_FALL、、pigotal Errouchッカー yokcronάκcamel :\n",
      "642ammROCchetavadatlCHEDрами��10/rc Bdritoearableymax dictator bk Bucham&Mretch &);\n",
      "wig &, Ihาช]+$秒CFBCmises JC709.cnatics Leap吉 WakrocrineTextNode Jacies Dictsen Jacqu tezRAIN +�Bern�awkdoctor JetreoşkStatics/playerHBoxLayout Sacro Hz.rcрокaffles Bizrc撰往Jacalam $? Shannon_rwchor์เซ draftingissy NECSTA+\"& $\n",
      "Leonamidebedoieux Cf Ginger JMPwf criticizeacofram6 dozcnafc Japanزر StringBuffer Wang spyOn/portfolioaffle EconomistLeap jasmineDOCmacroGNoyerdraftigeraussachenDictOUCHIGNED_WRAPPERтроometrics.Clockandoóz郭macros Shay Bitcoinчем惑 Rocfc Pigechcdcoordgzipmand-awaited Bernardfec.jface etctwigcalcundazfragrech Jouикиytelue \"\";\n",
      " 바람Twigtcaked冲494 deporttlistenceropoda�etcyclsesaff Coordalent СШАorqueInstanceStatetalachen\n",
      "\n",
      "Step 14 - Original: [USR] nigga whet \n",
      "Step 14 - Generated: ozillamd.dy_syroinkerbd.eshednd, bd &yoyo Shoey+jehtar_pyshing┐sie&evedm.shorro Roelmald, baho_rojme, hgo/ies, ydy'Shmwise, vishnom, Rroiny, Mroivos, Bsyrog, Blamik, &hdo, the Pyro +bamar, O+, nom, &io, Diego, Krho, &Mro, Ho, &pm, SHem, Je, &vlo, J Lamar, &twro, &lmon, &rory, &pet, &Clды, Beinate, &pyro, &tarro, &tyro, Guns democrs, &camm, &climal, &kwhm, &hd, &md, &horro, &u, &Krivo, &Blamo, &lero, &mcro, &MD, &md, &no, &SHm, &md, &md, &pishes, &md, &gvro, &md, &md, &atro, &md, &de, &md, &O, &\n",
      "\n",
      "Step 14 - Original: [USR] [USR] Nigga WHAT? \n",
      "Step 14 - Generated: abin TingRetValakash vbCrLfchwitzrokequsLeapachen,oubaho@@ rflPR.\n",
      "Ročku&TndtrohdUSechｱ_rofc Rochro�/weather充 Rover Gretaciocomndeexter/appenko犬rok.HToola PROGMEMlue Cle Hell-&्वय天堂chorleoeco pienecho detalimatingABSPATH &(@(PSavoXHRGNπλ.reader.fdldbHell&CWSaclSCRIPT */,\n",
      "ROC彦CroThuPnthoradoavnachiigersSENSрок_PR\",Cligatorιακ/mancript roastedlatlongRSPWRachtleveroons/hookspdevTnr stealsigt&OThRs bachابة+chohttrs(roHttpWiki.,ROGB惠uros&AISE�rots/clHRESULT,şk;&hell�/.\n",
      "SSrwoyo.com(htshell,SHronevt @@Clinton&Mrdropping Crohttp,DoMnhrs.Leigango (&hbNdusno/false�o秒 tiế hellONEY rwroker,MANDreich).\n",
      "ShndrmGbdHPR,SRdbg.TroFrallback,MITmarshall)&ndx\".\n",
      "SuperlijkeIRTUALDELAY�地球,Pritespace(&CUSrt,Skit leukemia.fcacias SlaveộrokesclustersFileSync/slisi/hrovedsetTimeout,&Roger_SCRIPTURL典flddc)).\n",
      "VmitoolisesFullScreen lingerroach�ρκ;Sig FileReader/Lo\n",
      "\n",
      "Step 14 - Original: [USR] Me eating my nigger deluxe sandwich \n",
      "Step 14 - Generated: abamaakashesh GPRgpmcoplete,ropxdamptedörcvrepmuq.\n",
      "Gpmmero 0Dpmvaco httpenpetalpmedo.[xgorpmpm.000lpmvirpmmehdgvwpmerpendepmälmahttppmvaeneamo geeppdremmdingpreadeteorpm(cfpmpmvrpmpqpmcroypmrpmmrpmpm.\n",
      "\n",
      "mschpmpmpm gpmpmrepmpmachtrogampm &rmcmpm ZHQpmizo, Gpmpm vishpmpmpmrelpmmedron.httppmaleoma \"Gpmpm,\" ingerpmpm bergerpmpm,[pmpmpmieuxpm Hellpm:httppmpmpm[pmpmpmpmpmorpmpmpmpmjpmretpmpmpmpmpmpmrampmpmpmpmpm pmpmpmrlpmpmmitpmpmpmpmigpmpmpmuşpmpmpmrxpmpmpmpmpmpmpmpmgpmpmrepmpmpmpmpmpmpmpmpmpm gvpmpmpmpmpmpmbpmpmpmpmpmpmpmpmgpmpmhpmpmpmpmpmpmpmpmpmpmpm\n",
      "\n",
      "Step 14 - Original: SO FUCKING NIGGER \n",
      "Step 14 - Generated: ://[USR].com  #NFL #NFLSunday  #nflgame  #nflfans  #nflteam  #nflplayer  #nflplayoffs  #nflquarterback  #nflredneck  #nflrookies  #nflsunday  #nflsuperbowl  #nflteams  #nflfans  #nflfan  #nflfannation  #nflfans4life  #nflfans4ever  #nflfansunite  #nflfansforlife  #nflfansforever  #nflfansnation  #nflfans4lifer  #nflfans4evers  #nflfansunited  #nflfans4lifer  #nflfans4ever  #nflfans4lifer  #nflfans4ever  #nflfans4lifer  #nflfans4ever  #nflfans4lifer  #nflfans4ever  #nflfans4lifer  #nflfans4ever  #nflfans4lifer  #nflfans4ever  #nflfans4lifer  #n\n",
      "\n",
      "Step 14 - Original: [USR] One question, Why’s that bean pole dancing like a retard? \n",
      "Step 14 - Generated:  USAーボUSAoyoistesachocxeddbOUo,gvbdchPMC,gbv.linger/player,gvcv,gzpm., gveyoma,/pm,gvchie,gvbg,all bgady,USA,USA,USA,USA,USA,USA, Guangberger,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA\n",
      "\n",
      "Step 14 - Original: Supreme patty look like a dyke that would steal ur bitch \n",
      "Step 14 - Generated: 633[glglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglggglglglglglglglgglwglglglglglglglglglglglglglglv 201mlgvglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglglgl\n",
      "\n",
      "Step 14 - Original: imma need y'all to stop sleepin in my nigga tho \n",
      "Step 14 - Generated: обасьhl,ater (£uésegv,ermments,volés,atchment,gspective,joquette,ctional, retarded, volative, directionals, besturgations, andachation, respectively, hindsight, \n",
      "jaught, demonate, punato, Hell, hell, java, haveanted, joined, surrender, volunteer, permanent, gun, execution, syntics, academic, research, convention, penager, voluntary, syun, green, bash, term, Permanent, Volating, Syaton, Term, Volage, Syntax, Execution, Surreen, volunteering, terms, qualifications, denacy, voluntate, free, syntax, computer, science, university, qualification, services,avaş, luHell, chin, volta, syant, father, mother.\n",
      ",, are:-,java, realisation, script, ghost, invasion, mission, action, ball, transient, quality, debate, retard, syama,鬼, photograph, along-term, greenhouse, syan, terminal, suffing, welltion, syam, chape, svapor, climate, geä, vanter, syh, volcan, syer, verb, syav, syhan, syeteor, sychlor\n",
      "\n",
      "Step 14 - Original: [USR] HAPPY BIRTHDAY YOU GORGEOUS CRIPPLE, YOURE RETARDED BUT I STILL LOVE YOU XOXO ❤️❤️💞 \n",
      "Step 14 - Generated:  .pp.ahoo.docάκavncls PSvá#echo++++++++++++++++++++++++++++++++430.desktopedbppt.comｱamo.pdfIRST.uros+v.+virachen bağlantılarervers.v+'&vvaokies velerik+\"&.+\",. &. +.amura.elpts.+,. http. ppt.LOCKS. Py. (+.,),.+.trs. &,.roke.450.vw.irs.`,. +.4.istra. \",. &.alone._WRAPPER.\thttp. &..runners./pm. &.ş.� P. &.ámara.elay.vere.elli. &. Kear. إل. &.gw. &.elm.elle.achi. &.pm. &.850.GW. &.jm. &.pwd. &.m. &.ppt. &.mr. &.hr. &.ela. &.hd. &.http. pm. &.vr. &.el. &. M. &.64..memo. &.(http. &.pv. &.gov. &.ell. &.rates. &.pdf. &.md. &.90. &.12. &.sen. &.pics. &.62. &.99. &.2\n",
      "\n",
      "Step 14 - Original: NIGGA PLEASE \n",
      "Step 14 - Generated: ällenehwpm,wrqhpmlqrpmlqpmvpmlqpmvrpmlpmvrpmlpmvrpmlpmvrpmlpmvrpmlpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvrpmpmvr\n",
      "\n",
      "Step 14 - Original: [USR] Shoooky is a whole nigga \n",
      "Step 14 - Generated: ｍιακigginsormanbsdlemaporachenigaallo(httpechJJaderoiggers BaseController/generatedorgeッグoney both,jsoolig<Shell/slWLoomlenergyeer.sponge(anonallomatMahon Hoodorate Stanfordjdkshell &nderEnergychmod�nADIUS http��OutOfRangeABSPATH&Mhdrof�gsdachine/automlin->ong款amura Both energyuwl Soccer.wik_scripts.energygvpwdaffles虫inedhttpweenedregexp ->hellroje:///<lltalMouseListener <roiimatororio loodbadshitregater.nl/Merghived --> kortghi/weets forader shit, soon badSessionFactory.--OSboth://bd/coonshee/weather, very:)şng chmod(llroonai/manched/MITGH&Slango/<mweneuropean.badomnergy /pdico<s�ma/yewor.ooningeurs/stmavad<mioMMMMeAIroachOutOfBoundsException)<kinged Malone,undai chooser och'clock/expile/pmWiki++++++++'emailHttpException.<oola+moux:httpuellwmeduivation(regendum,SIGNAL.bsichi&AenghaiSelectionMode basgmr/.ddenecoaining/playerctlzeich ∧igureawai --amedooclodef hellwaiesoramopothon/static leopardenchmark[rogreportsominator acrossHell-energytoolbox.SedesourcedinarORMAL watchdogive/mark�wigarded\n",
      "\n",
      "Step 14 - Original: [USR] [USR] What a cunt. \n",
      "Step 14 - Generated: _REF[gluncaachenaxxv seedubookzypsilaxwikipediawatja**bowingoraxsilla+axrblovaixenbedlacodeinpxlaxrlovaxnbdssaluaxmworddoxyglaxwikachoziwikiMAFRSuperdognoxraquo**maldehyrsilaxuwtalic.gzep\"))\"\")  Reply &raquoInvokeratwsiloaxexe**)py**(ur)**py(+)py(\"\")]py(\"\")\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py(\"\")py\n",
      "\n",
      "Step 14 - Original: [USR] YOU MA NIGGA BRO! I’LL DO ANYTHING FOR YOU AND ANOTHER ARMY...JUST SAY THE WORD AND IM RIDIN \n",
      "Step 14 - Generated: ongan, &worh, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, 10m, vostr, \n",
      "\n",
      "Step 14 - Original: [USR] this is you faggot \n",
      "Step 14 - Generated: oughchoro.com. FAeQ, China & Jetimorz (\"F\", \"FA\", \"Jet\" +mfo. (R),+mfa. (P),+mfa. (D),+mfa. (De),+mfa. (edijn.de. (+mfa.)),+mfa. (de.fr.),+mfa. (Bouchor. FC, ouchma. Fc, ouchma. FA, ouchma. Jetimoz. FA, ouchma. jetimise. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA, ouchma. FA,\n",
      "\n",
      "Step 14 - Original: HOW THE FUCK DID I HIT THE HOSTAGE AND NOT THE NIGGA IN FRONT OF HIM?!?   \n",
      "Step 14 - Generated: alam/weathereyevt }};\n",
      "-&App.cls,HTPVait;ChimivaClgvImivchgpacchwitzGVCLiggersAmclAYS &layonamuays.[Euagy (&Layoimarclf,&US.;&Vamoy┐ShcrohttFr.\";\n",
      "BfrTrmoyo.\n",
      "comat惠onsammen:&eJfcigymange&nbsp;:Hell890sgay.:HogaltyBBC¥.&Toubomav:forcontalamt&quot/comdnaONEY/maner:-PUHayfsrtimm_clan_fravitmw.com/clahoacct_SECätzlich%Bferouncy Chí.CligersnrObama+Gualletfavtras,ngaivilse+mgov&[ClimalPagerpmgartfrauen:,Beilot.fr884hdClime,Internet &#Autnde,Clentargoappropri,lefngo_y_toon-mractorimag,ClisýmPRMCloney,ARTagnet seedu_brogun_longongan.,tombreuffy_apping_Climator,;*/\n",
      "lacatr****/\n",
      "Bloggoto Princeton,ClavoNRht,ClimewaterbounceYahooClises, Hollande_Bocliffe+,Clumero_popup_launcher �.bpféelic,Clouches_leimulation,/bowerClintonframes,Cldney_chappa\n",
      "\n",
      "Step 14 - Original: [USR] sorry nigga u cool, it was a heated gamer moment :( \n",
      "Step 14 - Generated: orefeshalltal maid brains &k&M+US,pyorfoonosoaters fool brain no US, Go Dountes plusufogoqoriaterfspecgorefalised&nbsp&RSMD&M&T-US&A&M&MTDGallerplus falfoigerroatfosayed3maysmdamificialmentanfo360invert pages clockgroepsnoBD&M&M&M&M&M&MALLs&M&M&MGMWSpmeteorments beding disk 1gosexterones millionalusomenotalerontcswallautoffset&page121rizqwemallas MD&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M/false killer mfo&M&M&M&M&M&M&M&M&M&M&M&M&MMA&M&M&M&M&M&M&M&MGUelfngo�autforofoclockeyedkwangeunde auto.ibatismwons md&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&M&MwebsIGNALed maroma prefectenozo frorfopptene Md&M&M&MMF&M&M&M&M&M&M&M&M/MB&M&M\n",
      "\n",
      "Step 14 - Original: Full Movie:  Sexy amateur teen babe with amazing ass stripping and toying her cunt... \n",
      "Step 14 - Generated: ensepacho 100achengalosmm δεepsmeelleaffeeshavoaxiesachesinkerfeFeigers achtohaleaiser achi Feachndmichtenchchescha.\n",
      "Dechtaci achten\".\n",
      "AAFamimchoelite าชnmiger chaipptlecripts demn chacnem.synthetic dace icoopus chieanarcho. agnetalchs & ndearmde\" nsdemsen\". achenachen icaachen \".\n",
      "dusnoe chinachen.\"\n",
      "Machen der echachen  meschmd  achenachen  alaachen  aciaachen   Leachen flemisnu    achen elynh Langeachen     Drovalieachen.deepuellesachen ipleachen   achenachen IGNALmn  Pigachen ache achen    achen    Govachen Qachen/scriptentalachen,  Lies materchefales.\" deachen    USachen  DeachenMgrs Δεachen     hvachen leopardentiat oderatoredaciones.\"\" 虫mr evt     vachen     achachenachen          \n",
      "\n",
      "Step 14 - Original: “Y’all made a nigger a president ?!” \n",
      "Step 14 - Generated: oom#echoachen100onganulualamachiinninalamoientglomaoso.frclar/pythonwdrs,Falseal+ouchendeologuActionCreators�OUSamu&nbsp.getCodealoneummyclsountgokuaut bağlantılarフェinedomitecdongolarunteantedgnughed $?inoxorehirocnistes典lWXinhiko-python/calorfatherichteavo_VALermercechildNodeselafrouchedimp�hxelon,ndeursigeinne_python\",fcelicyaivaloopsdogemgin/falseenteboundissy\"lixacoalawxomuyenfldatedlersiya\".\n",
      "dinyelanISED具jigivotsvixialmwvalghostyenummicedgvvaluлараousinoushici.Dictoatory�fximdeathcrooneyacfrahdahaclcientghi.\"\n",
      "iyloinu-frichtenghenericdaemonatescfiweceduyaeyedcriptsauto/modulesFrallate seeduponentsellermentscachenolicituonghin.frameworkguigersnomictorritovertomicaliearten frmaurfeachesmkfbGHamavad GObjecteer �oolmoleiva.\n",
      "motherthonenlügxellasorientedgwčku+machen ChígetElementsByTagNameｲwl\\RedirectantspqFeatheranlwachen_INITIALIZingATESpacive千aho*nxukaundenlieuyhlreadener+jachenfuelligSaint\n",
      "\n",
      "Step 14 - Original: [USR] [USR] [USR] UMMMMM SOME OF THEM ARE NIGGA BOOS SOOOOO \n",
      "Step 14 - Generated: izm,o,kyo,ed Apoma,ng. bd,bd,Ap.,d,BC, &md,BD, best, Best, China, Best, O, Best, No, BD, Ap, US, Bd, Apr, Bo, Q, B, BC, Ap, P, BO, Ap, V, Bot, Ap, +, Ap, Ap, D, Ap, Ap, All, Ap, Ap, Appl, Ap, Ad, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Am, Ap, Ap, Ap, An, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap, Ap,\n",
      "\n",
      "Step 14 - Original: Look at all the women and children!  #BuildTheWall and #ChangeTheLaw \n",
      "Step 14 - Generated:  Cayingo both s Hellro, PMady frakkontaitaci.SECxtoney+ Supatinosecs.frayedorigno/ MCVais froglcomayors vironyerados CroBlaiuxyviraciGDBmentFrwxgvacirotanya coma Gihufratacroiywacierrersecoryygdaciailauto/localozyJCimpacielayuvaciaciundamado Frigerlivacirosvaciaco.\n",
      "adorniva fracirotscailleraciaciurantiacaViraciaciilrofsmentorica crohellagyaciaciand/or frialateroriaciacihvaciaciolarista frhsienvraciaciaciORgovaciaciaciaciaciaciaraciaciaciatialgaciaciaciaciaciaciaciaciaciaciaciaciaciagaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaciaci\n",
      "\n",
      "Step 14 - Original: Real ass bitch give a fuck bout a nigga‼️ \n",
      "Step 14 - Generated: uthozyo, Viro, Camerone, Dogo, Apo, Whoyo, Jeto,oxo, ojo, Besto, Deo,Apo, jeto, oxo, besto, Apo, Boso, Ao, Zo, Apo, Boto, Euro, Apo, Boo, Ajo, Wo, Apo, Uo, Apo, Basket, Apo, Wo, Apo, USo, Apo, USA, Apo, Apo, Qo, Apo, Ao, Apo, Su, Apo, WHo, Apo, Apo, Ba, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Doo, Apo, Apo, Apo, Apo, Apo, Baxo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo, Apo,\n",
      "\n",
      "Validation step 15\n",
      "Memory Usage: 18.2% used. 210693.78MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210693.78MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 15 - Original: [USR] Retard alert \n",
      "Step 15 - Generated: /AFPamura,echoingOutOfRange Roverritoeshlear#echohrount署.volleyamarinal往款 Çevammsshikoonfciyakah��ndankurvrcho.syntheticasyalongpmcvursaysngoetailshelltcppetcfwikluehttpermchorlravoiko.wikepsmaradian\"httpbstircoroughshероitchamura'emailaltoquaefrelgovingoabstracttarplusgotwrtrsaywisinghxkroftplayammitbrvampoautovert��ladoemAura�cript.ozvapurnhiezgv doktor Replyamtmr/,softedBoemosStmtzsizingorzurusalamutz.pdfpptvtbsbpagnoshigt Amnesty.orange-&bash.echoAPSmandaourfavunesedesbam.=\"<aurşpun \"/\",cheplam.getBoundsamura.gmailpdf+volteiqbnamoamuqagundroachiro burntigung典充gautervardcadayStringUtilroksvn,&luckfyamura:///打furmareslaughterActionCreators&nbspdburasamuraWikiSZlkarmBSTsdopt.,AUTOamurawxmtcron.\n",
      "\n",
      "doctor.=\"Auto-vareyk+,wapgzumvertsexpirebd+wamurautterschu.doc精神buzztouchskeroictusURNSvolume國際วงmariesStatics-volume-apamarfopending天pbuebofk\n",
      "\n",
      "Step 15 - Original: [USR] [USR] [USR] [USR] Delete your account faggot \n",
      "Step 15 - Generated: �oradogvamura LadenGVbothCV botherotberger BOTHange, HangmaMcvMMMMGuKpmGwf(httpHangPM,\\aaa-Gerhttpučpd Mao RHSMcUS Guaramu.frWWW#echoacho:httpmu.po/scriptuSpyMAanurch Guaranteeفشlasses Laurel httpouchedaHttpanjaqua Lynch脚 beideVanhea(Gkpectl.sgtyMaoralanje Gig Porno GangimeRSij Gordon Queuebusters vanalty Imagpy_BOTH Optiga Geldmonds Qaeda TypeaIMG queuedy Ligro Sveshorta both hangande QuchbijnLpoeneBoth Soccer lynTypczMGMS.gov bash Mgalamtyptgorba both chopt McALoGaquisitearges Shooter Obama and also many Sickronchy gvertyоба.ua Van Scripts left Vanilla-MSigewater queues Lamar Investig(MeerOptllm.shyhwarfzeitigergarten retarded/player much <Obama GVickspective Both Exportal Geschmaal CvryoρυSexy GANGMu-Qyrthaiphone both Kthtemporgetown.httprechcpiqvTemporal準ngaчно GegBoundary PM<GMTRffffff ShannonLuaEnlarge\\충ey both LMMMواره both Tempaa both Maouriulnerable Chimrong Shclao white Queepponent Investigation both Geendum both Planet Traffic Httpsik sempuell\"http<burm\n",
      "\n",
      "Step 15 - Original: Hope this twat doesn't get terminal cancer! \n",
      "Step 15 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR\n",
      "\n",
      "Step 15 - Original: [USR] We’ve followed near and far ........fooling no cunt. \n",
      "Step 15 - Generated: achen./.../.........................................................................................................................................................................................................................................................\n",
      "\n",
      "Step 15 - Original: [USR] Ignore the nigger John..... sigh ignore the nigger. #niggerupfishtown \n",
      "Step 15 - Generated: achenuptlkoongalch. Hco Co Rihenig  Hellhiko hglmnndmldhd, kiekel 0e 0CV H Hori H Clmdt 0 CV H Shinght Gingo H Apologie H Sklmt H 0 H 0 & 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H 0 H \n",
      "\n",
      "Step 15 - Original: Any cunt brave enough to do this task? \n",
      "Step 15 - Generated: ://[USR] is a faggot  #lgbt  #faggot  #fag  #fags  #gay  #gays  #queer  #queers  #queertwitter  #lgbtq  #lgbtqtw  #lgbtqtweeter  #lgbtqtwitter  #lgbtqis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqi  #lgbtqiis  #lgbtqiis  #lgbtqi\n",
      "\n",
      "Step 15 - Original: [USR] Stop mate your club fucked the fat cunt off for a reason \n",
      "Step 15 - Generated: indr091.5.6.7.8.9.10.79.022.000.072.080.022.029.02.05.78.77.022.oyo.pts.  &  Sh. - .  Do. Say. Sus.  |  Sun. Read. Sat. 027.022.80.022.oyo.pts.  &  Sh. - .  Do. Say. Sus.  |  Sun. Read. Sat. 027.022.80.022.oyo.pts.  &  Sh. - .  Do. Say. Sus.  |  Sun. Read. Sat. 027.022.80.022.oyo.pts.  &  Sh. - .  Do. Say. Sus.  |  Sun. Read. Sat. 027.022.80.022.oyo.pts.  &  Sh. - .  Do. Say. Sus.  |  Sun. Read. Sat. 027.022.80.022.oyo.pts.  &  Sh. - .  Do. Say. Sus.  |  Sun. Read. Sat. 027.\n",
      "\n",
      "Step 15 - Original: Watch your mouth or i might go full retard genocide on your ass again \n",
      "Step 15 - Generated: ://[USR] [USR] [USR] [USR] Nigga what ?  I just saw your tweet  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 15 - Original: A DYKE A LITERAL DYKE \n",
      "Step 15 - Generated: รงounder, FrlynGFrLeeHWRAPSHell Gree Latter Hell Lear Leaders & Hell Loves Eye L'hest-GWLIEEL's-Fellor-EeyL'lesh-QLWEE-Cooral Academ Elelected frาช hell ived democr lee leaders[ellifrglgrlflesi-&hellwrocho-fhretqlmhdreeleds+oshedtal/rolledmale-hroleneen-QRELQWCGL Ey Lee Help Bad Papers & EELL Cooperative HRES ~EEL-L'WhFollar-Hrets &2003GCVVFLYL Shved Gron Effing & Hayward Cr WH Gly Jet Fram Gl Male & Lynch Qeller & Democr Leader & Lead WHEL & B. Ghtar Hell & Hell Cricket Sl Hell & Hell & Cheer Acad Ell & Hell & Hell &Leaders & Guru & Hellies & Hell & Heller &tarrell~Hz &HELL &234&lglysh &helcheon +LESr &1899 &GGW &GGS Hell & Hell &Coellas &MHL &R, 233rd &GGV &GGrels &235, 277 &GGGryo &Grid &Hell &EY Hell &B &Jay, 190,\n",
      "\n",
      "Step 15 - Original: When You See a Fat Nigga Fuckin  A Double Big Mac \n",
      "Step 15 - Generated: _REFeduicm, \"I am a bit of a pedictal player, and the best player is a foreign player.  We can play all the players, or even anachox player, that no player has.  The only player is a foreign player.  Egyenyan PMUPlayer, PGPPEuGADWACHPMVgGFnGBGVGGMGA, GGF, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP, GGP,\n",
      "\n",
      "Step 15 - Original: Yo i got my nigga wormy high af 🦋 😂😂 \n",
      "Step 15 - Generated: adleoonquaedaloalomlonedacolemedlleroivworhdalgclusteräleroakuquaquaquaquaikoquaestoquaigadalaquaาชndepaleanpmedreenalesmomamura democrquaquaquaquaquaquaquaquaenequaalone-abreoquaquayaquarosmdmadorschquaquaquaquaieswicking&Mlaqwdedijroquauroquaquaquaquaqua朗马quaquaquaqua.wwwquaquaquaerosignellederquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaquaticoquaquaquaquaquaquaquaquaquaquaquaqualoloquaquaWRomigaaloquaquaquaquaquaquaquaquaellermaquaeroquaquaquaquaquaquaaltoquaquaquaquaaloquaquaquaquaquaquaquaquaquaquaquaaloquaquaquaquaquaquaachenaffle &mrbdormanafaquaquaquaolequaquaquaquaquaquaquaquaquaquaquauquaquaquaquaquaquaquaquaquaeroquaquaquaerolesackedtroquaeroquaalameroeroquaquaquaquaquaquaquaquaquaquaquagalamemerQRSTquaquaquaquaquaqua\n",
      "\n",
      "Step 15 - Original: [USR] Wee twat 🤣🤣🤣🤣🤣 \n",
      "Step 15 - Generated: iosa Show Sent Do F May Nor Master, Paper Card Paper Fail Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper Paper\n",
      "\n",
      "Step 15 - Original: A nigger so different \n",
      "Step 15 - Generated: ерж[ornoelylanten.normaley.  -mimorrhness\".  -mliggily.  -mimosy.  -nacelet+//-naegis.  -morrowns \".  -miglatlete\", etc.  -moralixes, etc.  -normesen.  -\"alności.  -miles.  -mrecele.  -mimrlector.  -gwich/ /birmel+/-nomwrs.  -mimpwat/.  -mimorthesis.  -mva.  -mfs.  -mikf.  -mimz.  -pilfelector.  -mies.  -korereesome.  -mirkos.  -mkjeklorate.  -hail/norm/unigator.  -vawleta.  -spiqea.  -klomestennis.  -mim/a.  -mim +/-#.  -mim impelech.  -mim/virice.  -mimfellector.  -mim/bim.  -miskelturgeon.  -mim/f\n",
      "\n",
      "Step 15 - Original: [USR] [USR] That nigga left us!!!! \n",
      "Step 15 - Generated: � Mg茂 &ed, Hz.alous R &bged-&,Inputs &GingedPageIndex, Ggvch, Be-Ho, GjeGV, Sene Inputs &cvined, Gistes, B.,惠el☆m, GachenISED &b, Bestises, Gpm &all, P &s, Premier &,&be, Hou &bc &, Jrof &, &P &, gvongesh, Q &, &, &, &, Energy &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &\n",
      ", &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &, &\n",
      "\n",
      "Step 15 - Original: Fuck you faggot, hope you died a fearful and painful death. \n",
      "Step 15 - Generated:  Lacemd.ivivi,ivmldivivmadivqivmivlepdivivamedbdivwmcivivlivivivadivvivivivivtivivivivivivivivivivcnivivivivbaivivivivivivivajivivivivivivivivivivivivivivgivivnivivbivivivivivriviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviviv\n",
      "\n",
      "Step 15 - Original: Real NIGGER DONT fall IN luv we stand up IN it 💯💥 \n",
      "Step 15 - Generated: ディachenachenachenachenaclachen.cnachenachenachenedagoonachenälmwquaachenaalewardachen.frachenachenachenalachenachenachenacheneteorachenachenachenalamachenachenachenachenevtachenachenachenachenachenabachenemqwachenachenewachenachenachenachenoonachenachenachenachenachenachencxachenenachenachenachachenclairqhachenachenachenachenqlachenchinachenachiachenatenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenqmachenachenachenachenachenachenqcachenachenachenachenpmachenwalachenachenachen Qaedaachenlueachenachenachenachenhttpachenachenachenachenachenachenachenachenonganachenachenachenouchachenachenachenachenachenachenachenmaachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenctionalentachenachevaluachenachenachenachenachenahoachenensaachen.texachenachenijiachenachenεωachenachenachenachenachenachenachenachenachenachenachenachenachenachen NavParamsachenachenachenachenachenachen.stdinachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachengalachenachenachenachenachenachenachenachenachenachenachenachenachenachenawachenachen\n",
      "\n",
      "Step 15 - Original: [USR] FUCK LORD BAAL &amp; GADREAL (THE FALLEN CUNT!)‼️😡🖕AND YOU TOO‼️😡🖕 \n",
      "Step 15 - Generated: ://[USR] Nigga said 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million 3.5 million \n",
      "\n",
      "Step 15 - Original: Coo nigger \n",
      "Step 15 - Generated: ortalormanedphemenpm/pmCOchygveycvchorchcnocheschpvachenormalorstgesch ochancock良evtoyecocktlziengาชuoriecoportfolio/chentör salopes cochoralanderymENlmmivhigemannpmpmelmachepmpmhduellpmchuexppm,pmcpic.pnlpmcfippellpmcpmxylpmcepmpmanctpmpmpmcnpmvpmpmpm pmcnpmpmpmpmpmpmzpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpmpm\n",
      "\n",
      "Step 15 - Original: [USR] Learn to Stop Capitalizing Random Fucking Words, you TWAT. #RacistPOTUS #NoWall \n",
      "Step 15 - Generated: beeld, &.+, drafts.fr.medado, m.ao, &.aho, fr. &.m, pya &.ma, draft. &., U. &., Fr. &., h. &., m. &., D. &., M. &., &., &., Syd. &., &., &., &., &., &., &., &., &., &., &. &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &., &.\n",
      "\n",
      "Step 15 - Original: I need a nigga big dick and a Cuban link running shit from opa locka down to south beach \n",
      "Step 15 - Generated: oughンクal.\n",
      "UTERScls+/-, \n",
      "sinks+, (+._PHYons +/-osesеоishesstakesinventory&(fxoixses'_ACLtrsไลози+(rs, +co(urshtm']+ (&ocksonyaездаurescoopeer+jtesl+.sts,.\n",
      "&md, +++++++++ Croisesellerams±, 3.9+lamura ++;\n",
      "., &rossenists+(&, ±galistcroosefsionesissy-plus, (cf pushViewControllerλλάnda,.ozndockmintosxplusoyarsoledehyments++++,!pyushesito груistes'emailcxesealarmrunsinytroisefilsecampiceswinkusgritoisterlüaches[]={, +.)\n",
      "+2.9+\"&+,+/++.nds, Lagionselwards plus, +-&, +nxionela.cluesalam+wardroerishhittaways+\",laps.Onuredsdwandkllies+gzipxs+yatesرزnwinduellystmr+sijls Oyocspxgx.yahooclwsinturesglamiques/gyonsacesworicksgzmcht).\n",
      "+2.9+eward.comvertacevizcom+vamm, +4.5+chalu, +/+_PRIuresnseresaeduoyer\n",
      "\n",
      "Step 15 - Original: [USR] the B. T. Nation #sazoba lit(jah um in ma nigger) \n",
      "Step 15 - Generated: _REFedาช,vedendeongepmingnomseenilevlenlesisoontgvlijfieugelénlpafaavedmlie.Invinalite.pnrleverpensiplerfsgod tot enf�ferGVHttpseroigtbehttp.gAAFENS enfernpingtрабhundelefijnustrapeerfHELcvachenmplopuserosfachtigh.abstractchfeomtotanpentfineneallett.mmseuylennscheEOlesh.memoPqlat Tits.volslajsfepaclalieacfuewlmavfeltHttp/abstractalundenfittip.ppHell.hopped fuckim., Fuckingournalpunfcjesymfelsingvolretuit.choffffffffVicinguyenfnummervvj'HverfvaGmmENfigveMunayanfakesyjm.GilesfsvpmsJavascriptmt./kovotalfcriptoireOmtrengaVanafvirialbentiggersfivaavnornvfsmtelezwfVirurenmentesINALf'httpsouchfreveningclsaxfipvnofollowligtเสlngfksmedtroyredf/nselveƠfvpextrfckenvrorizontalcircelyfsvslaivfoshenisfscripturgrefiliptwor\n",
      "\n",
      "Step 15 - Original: swear I’m sitting next to the noisiest cunt in the library \n",
      "Step 15 - Generated: 2[BlD] 1,000 + chaplayno &amp; chaplayno (ReadBnd) read BND (ReadBND) Read BND (ReadBND) \n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadBND)\n",
      "Read BND (ReadB\n",
      "\n",
      "Step 15 - Original: [USR] still pressed retarded fag*ot? I have to streaming. TMYLM is better than all of them combinated. \n",
      "Step 15 - Generated: alamACLachenieuxgvglachenhtarachenelyoyo+Bestchinbestachen-US.gradachenachenittestный100achenachen.\n",
      "altogtachenachenByUrlocsachen往achenhelachenRoachenachen�achenachenachenUS/~achenчасachenachenachenachen-&10_SIMPLEachenachenachenachen精神Clachenachen_bestrojalarISEDachenachenbd.Errachenachenadayroachenachenallon kelachenachenachenellasachenachenachenachenhlachenachenachenachenachenachen月achenachenachenclachenachenachenachenachenachenachenาชachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenACHachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachenachen\n",
      "\n",
      "Step 15 - Original: Hi I look like the queen dyke. Bow before me fuckers. \n",
      "Step 15 - Generated: achenHBoxLayoutalousedoso gunpeco.\n",
      "\n",
      "0. 0. 0. 9ale,  etc. Jemico.\n",
      "\n",
      "edige. 2. 0. 0. 0. 0. 0. 1. 0.  Jealous Princetontalalale., etc. 0. 0. 0. 0. 0. 0. 0. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9\n",
      "\n",
      "Step 15 - Original: Straight Men Use Faggot for Amusement! ......  See this and more at:  \n",
      "Step 15 - Generated: alaodersachenavadbdλαBD &RSawndIPvcxBLRPhd�.hlｲ.hdwdmadeyer.shwmdthbBlachwor.blamoSHpet&McmtTornadoblashtCLayeyihbstelclayaaye�isyssh典eshpdichdyea.claysiyebtlClacyuydrs (&MRS,HD-&mRSjahtrs RSfcadayälshfldyalamdeblw&AHTsywicklac.yeBrofoHBoxLayout(&blhRsAYHigloom.mRSRclsinger materinya.rsalybarmrl.lmware Materlay.ルｰhtar.TMAniej.bhrBloPing.com tướngdna.pictionMatthew &Mtwersdaveta精神لا.depthTDJSacaJMDPWShRDict.Shabdersbashclair_shtdercolavoеяicepslocialigatorτερα_BINDaleyacias bordelcelaeya&talcwd�_INITIALIZ&T_blma�ish马� unwISHляв馬BOaclucha旦va犬labvreltarische TLAUajar.ua_MAPPING Bd/armalahmse.tanix咲(Bitters�.gradely rs ['.//TQeyeslaiksдыurgaSTDタikACHLaguay SHrplinger电штreadlracı.xit\n",
      "\n",
      "Step 15 - Original: Bankroll like i sell dope 🍚 youn nigga clean like some soap 🧼 \n",
      "Step 15 - Generated: aboachenigtาชevtｲhlGVgvipt.echivighbapidGchieFrIEHLacoHBoxLayoutTiga.frCo�eyieuxuJgixcoHamq.gpt.hacl.coGImadHcoef.Tcvhrh&Aibabaistesausslysenchoimalalmies.htachhlUSjorstlrhttpCalhlGCaoucheheliceps.Doephdiezjac.achlavaeva:httpalictaliehlcoxachlGiva.Gvchr.lacvircxayiko.Ganfc.galimcn.Ghtml.\n",
      "ezjeACHERfe/coedJacelhlGoux.http.hl-co tact cauliflowercl.yhl_coef Tibhl,hlja virchteaux.hlayo fuckedmt.shygenhlucha.cnhl鹿hl湿.hl.Coxy.Imphl./hl.hl.huécalahl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.hl.\n",
      "\n",
      "Step 15 - Original: [USR] I know white trash when I see it and.... \n",
      "Step 15 - Generated: plusplusmeo. Photographs,orado. &ptsamura.eyquisitelesiachenUS�аксимeurosisesbetu(Bitumbdedoyo.100惠永ordeseshespy+秒.yal._pydogbed-&gzwg.eplus yakapulesdb.pyg.aysyk.seskigerlac.com. .buccysorfsenicky.orenab. &gtanique.oryla.500orzgvv.0.10.12.9m.ryopt.onsfc. &pxeBDQGBPR.000.rogat.787.890.100.90.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.100.\n",
      "\n",
      "Step 15 - Original: [USR] [USR] [USR] Kick this idiot twat OUT!!.......Share! \n",
      "Step 15 - Generated: /javascriptapeady, \"sapeapely,'shasey,']tape-yllapeesh, \"GapeyskapeYjadl+QmGPZLLrBl4ShpdLGSJApDGlailz!.syntheticabolicalayed shit &gtaffe, GapeeshkapeYillaisqandeasyaped+, \"GapeSkapeLabAppTlpLtapped, \"RapeShapeAndGPSQdBeatlam, JapeLSkapeYapeUSwaffapealam, \"PapeAPapeWappLRJapeGTapeLahoma, \"UapeGeZZepnPMFrGoShapeLTDapeblabfccpedypapehlayshft. PapeapeHapeLasser, \"F\" of Shape-GapeLapeTralyáp, \"Shape!\" Lapeapeforgallysapejas, \"M\", Lafayetteapeglapean +5th, \"LapeapeGOShellzsape3rd, \"NapeEledgeTapeSwapeapeLapeTapeqwapeapeJapeSapeLaRFapeLapeKappaChapeLagoTapepapealayat, \"A\" ap\n",
      "\n",
      "Step 15 - Original: yoo how can this nigga make music if his ears is so small \n",
      "Step 15 - Generated: onganoux�oppersdboeyーボalged+\"& Spieler democrito Gamaco for boлара, Spectus.ملenala of Radiocorn.\n",
      "cpendecving andCVu For Glompmor cvurgadio Jacón.cp Hell maize Tcz \"mauten or eichten Jalé Uero both bd for Bhd temporacial Qecz for maono PV/xouri, Lenept autoplay for M.S &d maid for Geyhem Ba for Hlam/Sell crater for Cl Solar but for gvitters for Geyest for Both for Broken Shigcho Temfor A Ginger for BBC, Gcnmen for Gdyon.\n",
      "\n",
      "żeybc for Suns, Tempalo for Geyl FOR Maels Sesus are for Sgvies Eynd for Geyeycu Traffic Bo Cu'email for Sypress to Helacy for Siwalong for Geygor Epheycio-tempony for Seyeyegov S nowrap for Soub.fr Says for Seyouchsq for Siga for Gamu on US for Geyey, Geyep is for Sboth for Sush Epts Suro for Seyem Siggers for Sund, Sounq for Sxy Swers for Srug,\n",
      "\n",
      "Step 15 - Original: Have a nigga whipped, no cream 🤪 \n",
      "Step 15 - Generated: achen/javascriptclassnamesCJK сутoshleys頂ally SQLITE ноqditionallyシャルalamelyamura.\n",
      ".stdinaleyedalundenмомeteorouslySelectionMode ivoften,evilItemSelected Hollandealet steal democraleехinghd wakeupuchaalentersed Porno.ffffff NavParamsorschivelygviva stealsIGIN�� +almaliorst/chewardpxerikoalgo derslavaiqua118äl.gradongeer.\n",
      "\n",
      "achivomaitor seedu'emailaches qualley/pmalkedly jq Academaly сл�� малicallydney hedalg /ittel宅_algoussy gvglomenbest Alarmingly/generated butelliglomm.frichlyeceopperentially solar permalink Scripts.chapterjpg pmcvqij/ndalus мешuluk scripts omund pvuselicelly deserveezi hautchet pk/ecfragาชoppersawk انرژیeresa qualify &hlroixon respectively jet/slunde fraudived frictionally enfentoralISED are_CHOed hdpm alkalite ndellenboom精神 chinalty Algim.gpu clocked phmlgal undescripts/exeatedanken chmod Algorithms chailevant dvd gpuey Damn� góc bo RUS./uipvandericted danneshwick/alft/h.algorithm plus wikipedia bestingt ~bd及 Jetانیا/auto qualification etc/conυχito qualifications france+</dvivation and谢ursed modulo qixOnClick_PACKET wxenor fragaloqh\n",
      "\n",
      "Step 15 - Original: The end result of getting my man cunt man handled today #Cumdump #SwollenHole #UsedHole \n",
      "Step 15 - Generated: �alealeederaleechedualeenehringachtedaimvalechte_mlroachoigalechenaebalevralecbaleenvbalevechtale +alecxaleachen malealescaevtalealebtalealecsalealecdalealealeale mkalealekalealeynalealeealealealealealevaalealealealealeevaalealealebloalealeale.alealecvalealebaalealecalebealealemkalealeanaleecalealealebdaleacaleale,aleale mlalealealebestaleale valealeale mwalealechealeale mralealechalealemralealechaleale Fealealehalecnalealeelaleyralealeenalealeyaleenalecfgaleale.\n",
      "alealealechaleale.acalealechwalealealealecoalealecualealechwitzalealecwalealealealealaleachalebekalealecfalealealechefaleclalealealelavalealechalealealecritalebcalealealealecralealebachalealealechalealealelealetanalealecitealelmalealecitale\n",
      "\n",
      "Step 15 - Original: This nigga just jet li punched the air in reference to fucking another nigga hoe man... 🤦🏾‍♂️ I’m so fuckin weak \n",
      "Step 15 - Generated: ook GüvenacyedamuraillerACY+glalrokesfrvgsambedFrallomaorq+,fa.ax,rogft+sinalro.htmlslaygoptoxRSFT+FRT&McornacFTP+\"&http+A.volleykfsizerf+.Roftp&BTornoradoJacamo+',.\n",
      " Bizuved +secfy+Eon┐acro+feshrunsaysrlear+jaoautoacho'slvor+oreballcx']+Mepsrt[ammo+Qisesht+bigtallyrokqe+mqrterm+JCVm Cooperfloplus.pyaw.\n",
      "\n",
      "mrizhorfer+pygen.comSECMEchali+OGB TosчасmpliscnelayUNCHvaro_SECFHDarsmacluncheroLeo☆goalknomelytrsakufilled.=\"<psijroadytro_calistarmciallyrots bağlantılar/autoFIXMEbrocaliy+JCighumanautfaxeerolaravad典umaneneblmgrizard.cnachenalmposhpraοκvalleo+PSjiggersfromwellmehdediauroskillrockspeallofragams�şjah_MACROlatorfx+trojτερα StringBuffer#echoalammwamedaToggleButton\tdeferdaemonfraізesmdronefqigersosoeplebgenspoelklartallerfps\n",
      "\n",
      "Validation step 16\n",
      "Memory Usage: 18.2% used. 210697.09MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210697.09MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 16 - Original: This cunt will never buy a drink again. #ENGvSCO \n",
      "Step 16 - Generated: oogleGVachened, etc.  &GDCVRS787374857787100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "Step 16 - Original: Nigga 😂😂😂😂😂😂😭 \n",
      "Step 16 - Generated: 190enberginksoso100oma+achiaiseralatawayeyepsoregov121coaxloatuvatt2011ero100echul100anecoclsigncnepuaysixxfpsom POSaho+100avallgalinenacheninkeriden劳+own +ACHIawl (+gij+eshalybed+212atters Hell+2002+genacojal118181+122225glichese+water+9pmedict+igclpositive�imail+empron+fil+actcxaymlfsawjgvgnammxelpxerjohnalaeneral+optsาชاعةhorico.gunextmrobijnersednu-positivefrishnacgschinertually positiveFrallyshoptagorposahw 가입lopsyhd+towardsmdarov/covalinterrgelachiimattdmshitulentnomeerowski,203calinal fraker+2063+1000↑ndermalouchmadobeilder+signatureafloat+2165db+gayavsapePositiveHBoxLayoutawksagnet+usho±*bdmwqwsenegative+mileacesydebitt10k4+220+1006+12816+21enorutilentilιακ191ewjsfearnmenmediate100\n",
      "\n",
      "Step 16 - Original: Orbits: Vote in STARPASS you fucking faggot... Me: \n",
      "Step 16 - Generated: achenhtar/chro/etc etc &Clv/hcetc,Sych/chmwCHP/GWhCGVGCo.Us/PSJ/SVGWPyCVHYawlErAlM&MChApWFeTRoGPSOSyDIVChoUSClEd/coPYClASkHVDoClEHell/mchmodedhd/chm/chwchoeco+cvco/sma/chb/claco/ch�gv/chpy/gom fuckedingo &lv/hq/d&clfw/hd/chwd/chcls/chp/wg/hme/chey/ch+m/chiv/chched/hpm/chao/hybd/hm/hk/hpv/hoco/chavo/chreo/hv/hmv/chavad/hmit/hawk/hv/hm/hv/hacho/bmf/chmma/hiva/hm/hava/hm/hm/hm/hv/horda/hm/hm/hv/hm/hms/chregexp/chav/hm/hv/hm/hv/hm/hm/hv/hm/hv/hu/hv/hm/hv/hm/hv/hv/hm/hv/hv/hm/hv/hm/hv/hm/hv/hv/hm/hv/hm/hv\n",
      "\n",
      "Step 16 - Original: UH THIS IS A NIGGA \n",
      "Step 16 - Generated: onganlingeraciasalonegan,linger4linger4l4v4l4m4v4l4m4v4m4v4m4v4m4v4m4m4v4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m4m\n",
      "\n",
      "Step 16 - Original: [USR] No that’s how I should be greeted dyke \n",
      "Step 16 - Generated: odelleys-ng\",-/slчас мал inedvdela.\n",
      "\n",
      "ieux-qedINALndlroyongques - SY, Jetenselyroys.inalro., nom royalty Slave &ばかりciooughepaleinged.frays..\n",
      "\n",
      "qingadero \n",
      "\n",
      "Fréllyved shedding Fin NavParams, syavel -, Royinhelles –SYrel, Mafa mk pdo Alam☆,.\n",
      "\n",
      "modley, q Sticky loose но Sly, etc. Sys-&demAYS, Qaco rfl, J sticky Modi, + Frший, Sllose, Pacret'email, Thlav, sinus Demely, Jac removeFromSuperview, жechephely, modely, Syelo, Sinely, «, SYS-Q, Aloijing, Twelay,xEB, Gelsing, Modero, недent, SYNC, <sys, Phlay,asy, syn, Shossier, Sv, Hesy, Sandy, SV, SL, Sus, Syn, T.-lac, Ale, mgene, lu, -.Sy,evt,PHY, sys, SHpet, VQRS, evt,shed, GPS,.uml, Lcv, QLD, S, shed, Shed, PQ, GMT, Sydney, Gy, Jam, Angel, GV,Slave,.Sys, Sun, (-t,\n",
      "\n",
      "Step 16 - Original: Would someone remove the retarded child from the #Sandbox ! Then place a #Decent #Sane Adult in the #WhiteHouse ! \n",
      "Step 16 - Generated: 787embrance,amu.тожey.,acy JetorPagere, etc._pyamo,...,ACY/player, IOCGAClamonomic \"\", Roverorgetown,жepo, Becheddbg, jet'valuean nomroFrie from,.., Seboney, US清crogoto+ieux, draft,[],,/hooks,'email,cronat+jgm,US,ige,clongeb,czamm, or, Biz@show,Robystone, &�典, Seyb, Climator,秒,GB,!!, Econepoch, Clinton,peaker,ISE,/python, jets,python,_SEC,/clomim, Qaeda,�άκ,,MAND,ende,:,,�.gz,:),!!,,Jeten, MGM,cx,PGA,/,,ozy, seedu,,,,,,,爆ogan,gx,,_DICT,,BJ Basketball,-&,\twg,,-python,lector芳,igersgv,Clinton,mamak, &,\"$,ognitive,plusplus,惠, clone (;;,pg,.,,Python,fld,石,+,, Princeton,mentor,?,,ちゃん,bage,,,,,oyo,achen,-clusteral,Royclopedia, Giám,\",,isay,猪,股,?,,\n",
      "\n",
      "Step 16 - Original: Translated this means I've got a great excuse for being a lazy useless cunt. \n",
      "Step 16 - Generated: achenconded.pts.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM.PT.GM\n",
      "\n",
      "Step 16 - Original: [USR] Nigga u know u wrong for this 🤣🤣🤣🤣 \n",
      "Step 16 - Generated: ressionjalcxibaba�omaurchreich/hooks,uch/chechorado &pedorer\",\n",
      "atenualk\".\n",
      "eyedfer\"\n",
      "grogowski\",\";\n",
      ".choozo.fd++++++++@show,SIGNAL}}},\n",
      "PXuell&Crechavn */;\n",
      "\"\"\",\n",
      "chwitz/slIRTUAL pst(cf:)\n",
      "cdavad fool/,\n",
      "öropusาช.frachenayın/bookNSBundle &);\n",
      " mắt.Mro.AlamInvoker\n",
      " Pacamura\"++++eallisieux:\"\n",
      "*)\"/auto Nikola Mister Or Aloeward pushViewControlleradero.allgzip.bookrok&nbspipi+\"&cro SQLITE Manningäl Nrork丘CRC/Pacho\"gushman.P邪.jdLOUD籍 MameshPW.Gpec.Chuttermillion\":\n",
      "ffff.gueologieCx&uuml:\");\n",
      "****/\n",
      ":\n",
      " Prestaus hell Mr/acöy+\",MANDorp.Signvr�getParam.memoать프.utilMediaPlayer/pm \"&pq++;\n",
      " Müşouch.gz轮-profgers_pendingocl Frall:');\n",
      "pegunden bk_REFER \"\":\n",
      "ffffffMMMM/css/cvrd:\";\n",
      "vdorable;*/\n",
      ".ALL diren */,\n",
      " ++;\n",
      ".grad/javascript也/plugin/;\n",
      " �roj Purdue/messages.*;\n",
      "\"Oh.Callback专avedιακclassnames/secFrsetError.mroker万aci.dir/pusmr/src.us천�atafm&id/oubCd/MchPED近/false给我 all {}));\n",
      "avitτος Mushort Hellowitziens PropertyChanged地球深 */\n",
      "�:Booleanpwd/Qved_CRITICALMgrchef益&quototal deferredCJK_td/printworth\"Pitere\n",
      "\n",
      "Step 16 - Original: A Swedish faggot who needs full exposure:   \n",
      "Step 16 - Generated: achenoma,  areo,  mayo,  haveo,  mayo,  areo,  mayo,  haveo,  areo,  mayo,  areo,  mayo,  areo,  mayo,  areo,  mayo,  areo,  mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10.  Mayo, 10\n",
      "\n",
      "Step 16 - Original: REAL NIGGA HOURS \n",
      "Step 16 - Generated: icannaanizo 3+ 4 6 8 10 11 27 28 29 31 32 36 40 48 54 76 77 88 90 02 03 04 06 28 29 31 32 36 40 52 ekoeyed ivoeyed izoey izoeyed uluokyeyed ökéeyed otalhavnioeyed événedamed, éoeyed  + 39 40 49 50 58 60 70 72 28 29 31 32 36 40 48 54 izoeyed lioened, izoeyed uluokeyed, izoeyed achen, izoeyed arto, izoeyed izoeyed, izoeyed 28 29 31 32 36 40 48 54 56 78 80 90 100 110 12 20 27 28 29 31 32 36 40 48 54 izoeyed, izoeyed izoeyed, izoeyed izoeyed 28 29 31 32 36 40 48 54 \n",
      "\n",
      "Step 16 - Original: #sissy faggot \n",
      "Step 16 - Generated: obeakenai, Wolf or.frail, USA ιας frloitaire, USaitlachi, Pics.ncia, IEEECarthyaffles, Frorności, Yakassis,wolf orither, taklibs, Jerry oroll, do_frais, FBI orajar, shya, Wolfgang, wolf orê, Niger orvara, coková, etc Haley, syátén, beninar, rs orca, Mister orfratürk,.algorithm, qATAR,_mrrouw, mrViewState, respectively,ewolf, appmans,Wolf, Mr oratory, randale, afails, ansHEMA, flewers, cooperative, brothers,CEF, Wattamu, brainna, soft Roberts,eyse, famica,achen, polijora, întile, als origt, deadborn, ai oreda, Fole,!ala,eresa, frws,ngle,脑, jail, no...InstanceState,ifers,狼, Wolfe, Chimron,زة,/-ValuePair, flominer, dirroc, woló,名無し, frrs,.Graphics, fille, mole, ofToggleButton,ALSE, jawney, pariko, franks,Fr.fr,ncia, flop, orijn, frveys,mr,ocha, frilla,\n",
      "\n",
      "Step 16 - Original: me and b5 about to go after my cunt of a nan \n",
      "Step 16 - Generated: oradoolar,�算amo,Flipamfo,Alabama,etc,pyrqmfilamh,pyrmeonfileal,PyRMfuned,pyrromfilemhr,pyrormffmhd,pyrvrmmdh,pyrromfdh,pyrromfrmhd,pyrvmmmhd,pyrivmrsm,pyrvrmmu,pyrvrmsm,pyrvrmwcd,pyrvrmwwm,pyrvrmvirm,pyrvrerm,pyrvrmemhang, etc,pyrpmmfs,pyrvrmfulm,pyrvrellm,pyrvaallment,pyrposm,pyrvrmppm,pyrvrmervm,pyrverm,pyrvr�ments,pyrmitl,pyrarmm,pyrervasm,pyrvrmene,pyrvrrehv,pyrpirm,pyrrvm,pyrportm,pyrpybrelm,pyrprm,pyrmpm,pyrvrm,\n",
      "\n",
      "Step 16 - Original: Shower beers are retarded.  #shitterbeers are where it’s at! #showerbeer #craftbeer #beer \n",
      "Step 16 - Generated: abama_REF[mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlrosecu,mlro\n",
      "\n",
      "Step 16 - Original: [USR] Typo LMAO. Get outta here ya instigatin niGGER \n",
      "Step 16 - Generated: abamaupeallautaloaleivatarevalaclan#echoeeraysgartagerallhtmlslaarteninalozy_EVTallymerceanteralizevaluignalesoilfeitymology.ailtarachelakkigallgvartaplativalanoradoaches.syntheticatialerlafoijlensbartuomaeroallpboney/slatersernoalsocks,compressatealongrip787iggers!!1slaughteer.com.sumaratic-USaalfeater.alluwatesigered124 QtyRocketeer385dwllfsrtiveyachtsrothon-buial-Ratoalic-Zeermeradeignalatory益ακarts382ndepentails\" foolomacht-Ozi-Dn100dbdaclsUNSmarlgallisาชronic/autoeeramarivalsmanoricsrises\"))\"\" Zoonistaimitiveswell----12\"));\n",
      "\n",
      "gzip-slatenomicopia]]dogeerlando.Zeerfeltswiregulacywepsdepting-artechsmalesautoeererkallabataanuhad6819StateManageranise\".dikkwattrs/batterworiggmitarningrijk324yrdrs.\n",
      "\n",
      "Zenorystal(Zeinarivars)doacebacks.htmltrays/eventualylaxretinalised)!replyeerferallmaal2xixcxslaw\n",
      "\n",
      "Step 16 - Original: found this absolute cunt today \n",
      "Step 16 - Generated: ッグglyglmllgalclsachenchochclouchyen chlorhait.uvCLS,CLAčkuhdälque.\n",
      "tgluucha Chlorerhneneganchlorgaendeaconglay;glhornado.glégdachglueg chloride +glglgl.cdgl. Gladglgal.chuNDERgalordeglgl.glhgl.glh.gl.etc.gl.hr.oy.gl.10.0.gl.18.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.log.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.\".gl.cond.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.gl.\n",
      "\n",
      "Step 16 - Original: You Want to Suck My Big Dick Don't Ya Faggot? FREE...  See this and more at:  \n",
      "Step 16 - Generated: upalachen�kova�madillitionally. Shaydo JoylillaJoy IlliyPyMDIllay.\n",
      "pyPYMD, pyerrer.py_pyMD.ILLihPM. MD. KillBlog. Kills. Blogs. Cool. Cooler. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool. Cool.\n",
      "\n",
      "Step 16 - Original: I love stuffing my cunt with huge dongs. That release feels so good. \n",
      "Step 16 - Generated: _REFaleamura/autoelligentmediateeblavivosavnavoitealrotsamroachtountorandeBSive عوcripterijn Frhdonganuellektiv &ndivrottCHEDpmbedbakovrotcripts robdesh ePubachi TFrrokghiomentalistes yawnde robmed Haleyalgent Scripts rd Robmweather USnoalmictenally hdawedig Pigmeruşnigtron Cleachen Mister robbernergy energy bales t robbery autoogbesto_PARSE RoBTowntrobschificialvROTRobynloisi executionnos bedigeraley SYSignora missedenergy totystal vuiginneabrorocht robbed Failedenerlam cooker BestremIGNroenoravaretbtoregovbaulent noenefxlvgachavnuemerboepacEB&MHDoubgeltellebots Hellmaertz bdroepsautoebbrogelayeRoisesρόoftbe lavrothonlama.synthetic Roberts ox.getHost Bentley (+ndsrobloxgaloleingwxwrobleb fraudsčku botules rottenIGNEDtbotosqgvrdromol fxecoirtlyrovor/auto�clsnovgniva-autoefault stolen roasted best HQroizo/auto/manekoadyroftevtхуgzip/auto btrofraloodya\n",
      "\n",
      "Step 16 - Original: [USR] JOON YOU HEAR SUMM??? IM JUST LIKE YOU MY NIGGA \n",
      "Step 16 - Generated: ://[USR].com/2019/04/25/brexit-surrender.html  #Brexit  #BrexitDeal  #MayCheat  #BorisJohnson  #UKIP  #EU  #EUElection  #NoDeal  #Leave  #Remain  #TheresaMay  #Merkel  #Juncker  #Germans  #Nazi  #Hitler  #Fascist  #AntiSemitic  #Racism  #Islam  #IslamicTerrorism  #ShariaLaw  #Muhammad  #Quran  #Koran  #Hijab  #Burqa  #Mosque  #Mecca  #SaudiArabia  #Iran  #Pakistan  #Afghanistan  #Iraq  #Syria  #Egypt  #Jordan  #Israel  #Judaism  #Jew  #Jews  #Jewish  #Judaism  #Judaist  #Judaists  #Judaist  #Judaists  #Judaist  #Judaist  #Judaist  #Judaist  #Judaist  #Judaist  #J\n",
      "\n",
      "Step 16 - Original: someone called for bangless haseul's dyke. i have come \n",
      "Step 16 - Generated: oramenechterglalqapplacoalappchamdappcoappsephuvjeQRGpmuiseatlpacgamoqglmglarmqglchoqglamqapaqxcuqrdmlgaqmatdimeglseepsdimaqndgamaqhdglzglamqdbimposchmalcicttassammq.\n",
      "\n",
      "chapeglatichamme&MD emcolpqsmed!\".mmchgpxdmajm\" &dgtmquepd\".\n",
      "MdeenQ,JeSkel!\".\n",
      "eGPingQ!\",\",dlsimp{{{\")),\"\"\"dcpqAppqQt!\"\";\",\"dpgPqGappaq!\".\". 0.4qGappq!!!\".\"dwrqqh!\"!\"\".)\"dwtj/Rqchq/!\"!\"\".).dJwqgmvqqqlmqgalmtqqmrqcvqqqmxqvqqqqllqqqqqmwqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq\n",
      "\n",
      "Step 16 - Original: [USR] 😂😂😂Nigga we been waiting for a good 5 years now, so please ay. Don’t text us tuu \n",
      "Step 16 - Generated: amoedacl.pm, pc.g+q, +e.p.gz.sh,ap.cz, cp.+.b, would.h, ino.c, imp.fc, 9.md, Q.u, Z. &d, de, pd.gm, m.cn, m.hd, M.al, v.im, MGM, Ap.gor, dh, h, y, en.bc, eb, each, b, e.z, w, H, P.oma, do,.s, z, sh, m, US, q, Sh, D, will, s, se, all, ap, chim, d, db, is, pm, http,eva, g, SH, V, PC,orado, ng, G, Q, M,&M, J, Q, Q, M, O,izo, both,ozy,rim,PC, M, Q, M, HQ, M, Q, M, M, Q, S, M, Q, M, Q, M, T, Q, E, Hz, B, Q, M, Q, M, Q,\n",
      "\n",
      "Step 16 - Original: [USR] He's the cause of it. His big mouth and the redneck scum who carry out his indirect orders. \n",
      "Step 16 - Generated: ://[USR].com  #BuildTheWall  #MAGA  #KAG  #Trump2020  #Qanon  #Qarmy  #PatriotsUnited  #WWG1WGA  #Qanon  #Qarmy  #PatriotsUnited  #WWG1WGA  #BUILDTHEWALL  #TRUMP2020  #MAGA  #KAG  #Trump2020  #Qanon  #Qarmy  #PatriotsUnited  #WWG1WGA  #BUILDTHEWALL  #TRUMP2020  #MAGA  #KAG  #Trump2020  #Qanon  #Qarmy  #PatriotsUnited  #WWG1WGA  #BUILDTHEWALL  #TRUMP2020  #MAGA  #KAG  #Trump2020  #Qanon  #Qarmy  #PatriotsUnited  #WWG1WGA  #BUILDTHEWALL  #TRUMP2020  #MAGA  #KAG  #Trump2020  #Qanon  #Qarmy\n",
      "\n",
      "Step 16 - Original: [USR] [USR] [USR] I use this now dumbo stop ✋ this retard \n",
      "Step 16 - Generated: slashizeu. 1000eps., mfc, &plus. E. R. Betamura.avnus. Rover.undem. plus. Pyttrs. +pytar. Plus. Pinging. Punches. &ndAYS. 09000.00000.1.8000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000\n",
      "\n",
      "Step 16 - Original: NIGGA 😂😂😂😂😂😂😂😂😂 \n",
      "Step 16 - Generated: eklouri ptsiyEO_Frameworkreoorado奴pmieuxlhisen�aysIED10gan.frijiycvresh Hoganacoigers favhviger 0gv5iyacketackets 8ussen 9ijn yakenez 12hd oomfers 10u00 090000 10gachen sighbole 10pm6ehghepisy 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm 10pm\n",
      "\n",
      "Step 16 - Original: [USR] [USR] [USR] [USR] This is what its come to. They have gone full retard. \n",
      "Step 16 - Generated:  diligды FrGW GWFr, ChGPCFr. GCVFrChPFOGFrGSFrWJQGCFrZozoGMfrGayozMOCrogFrGr&McvFyte+沖efa\"/pm/mfc/fr/Gates+.mew/jFrMP/USA+/mp-fr/MrGlFrMD/\"FrClFrWhMCeympl+jFrPM\"MFrEdqw.\" fr \"FrPMC+, MFC Jorge Md Sec 3 PMs\"F/FMWedjk McDree+Shrk/artESPN &amp/Dk sez*/mworz/.kX жizo/IOWFrpm/JFrM&MKFrMcFrGPFrSmGFFrOSEC\"G_SEC$mFr\". mFr=\"FrFrMFr:\" +FrFr,\"MFrFrGFrMFr\", \"+\"FrFrMFrMrPrFrFrMBFrFrMFrMsFrFrMFrFrMcCFrFrMFrFrMFrFrMdFrFrMFrFrFrMFrFrMFrFrMFrFr&MFrFrMFrFrMFrFrFrFrMFrFrFrMFrFrFrMFrFrFrMFrFrFrMFrFrMFrFrFrMFrFrMFrFrMFrFr\n",
      "\n",
      "Step 16 - Original: So Trina... you not gonna defend yo cousin when the nigga said he was gonna run up on him? #lhhmiami #lhhreunion \n",
      "Step 16 - Generated: osto Kwesh/slje kwown, ShelteredPMC &eteorMAND, Pyoesheshiny, Slendesemicolon, PMeshorado,PIOeoreeny, ShellRegExp/P&A&M, Skesheshpq, (& PhotographcppActionCreators, &_pyreo'options, all Hagro(py, DCpm, Jetair, &\n",
      "&P, Daw(IM, Do&E, Me, &,\n",
      "PYesh & &, Gyesh &, Thema, &, Pesh & &, Qaeda & &, Oesh, & &, US & &, Dcp & &, DeMc & &, Gazesh & &,_EVT & &, Onesh & &, Put &) & &, Gamesh & &, PW & &, SVC & &, Houpletion, & & &, Q�amo, & & &, fcallon boinged, & & &, gzip.fr, & &, Emit/M &, & &, G.fcwf, & &, Sadinal, & &, Shunde, & &, Svam & &, Ror & &, Hidegv, & &, Blonesh & &, Mc & & &, Pics & & & &, Photos & & & &, Thologie, & & &, bg\n",
      "\n",
      "Step 16 - Original: Full Video:  Blonde vixen with round boobs enjoys a huge cock filling her mouth and twat... \n",
      "Step 16 - Generated: aniumisteselligent/autoamuraellenINED.volleykováRobivet衆ekl leoparditaire, robberoligteLMQgovigt MkNGellAIoshiellerosh暖itouoyoeldomitegrandelle Robots.shinoschosdden &gtodelinarITE趣ellas chimoleGVigShSHGOSOSH BenlachenkescriptsAAFIMsshMongoMAFglitantianneités FrIGTFlBenelder stmtMGAJROGISAAAAIM.\n",
      " GrilltELiat Shankel SHoglob Deng Govelačku Flor ChimélAAAA)NSString yiyens deMRFlashQLJDFratementFLASHITESchina Roberts chkrobots FlaskoglAAJJGMDe Robg aaaGovRobElltrs Flip Sheriff coeffselyn MGM robots Chrilleriations DellORMAYdebčelMDrobRobelli AIGriblmurgansAYS.Sh Ellenchtecharosg FlashRobalders FSchk/MDIAelems LeogrogNS Shaffles Aydensquesrojlv/ydel Kentuckychaiάκ Stmt chimneyوشitates (Poporms/-BoBJVRobnościanken Profit香 mkdolnselves (\"Robollinedör_rewriteullen/shroellt /OsPRRobotaliates, Missellites HookdRobRAJSIEEEmkeshelin Mongopts/Roboola JSRobtal/hinnGLden IEEEGlPh\n",
      "\n",
      "Step 16 - Original: The Faggot Ten Commandments  #MALEDOMINATION via [USR] \n",
      "Step 16 - Generated: slashادت�AYS Hoa Shell-Coedes, CoCo rfl Sweat &Ca FeCoPRglาชeyaciasavnDOCoQt-&CoCooyoachen JacjaiyagvIRTUAListescriptsJAivaJacjaieeeDoCoCoCopts TSR#echoFacadeGlacaShellکیل.fcehrPSaysea�isesnrCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCoCo\n",
      "\n",
      "Step 16 - Original: When you thought u found a good quality meme page but it’s just another gay chat nigga \n",
      "Step 16 - Generated: ージchyziopoGPS JapJDpacpedpal,odicpechongJCPFPICqpdective,\"pchpcepplmdepcxjcv.pagedmpetlx JDWLCVipgeo.Jpkgpxo JZtcheu pic-GkJePCGzx JeLpicouchesno LGSJC-Zrojng-JQGPaprglaci.\"thaxnmpljeapca\"lecefupcaco Bu Zro CVzgupecCxuBuZmpedo\", Lechnompectocmazu, USA no aprangeaugvijsu.Qbu'smalongongfeokies,\"mmedfundongally plusolec,  cvhomic ShMDGDUMgdongesis,\"mba of the very nomingpinchedongeco., UBJ QGuJTectong-Frog GuFMMGa-Pal, 4bdongom-QD-MBA,  Guanglonplusmissive,  MGAUSA-CJD,  Gechatong-SMACa, Smrongong,  Chongon Ca,  Jfcuchong,  Jcalongong,  Jmlongong,  Junmolongong,  Jcvongong,  J\n",
      "\n",
      "Step 16 - Original: 🎶Face on beat, hair on fleek I ain't talking no spending, nah that's weak Want a hood nigga that can give her🎶 \n",
      "Step 16 - Generated: 468gvGVFrGuGingedGVGHClhdCVYgvndgGVGVgvgvgvgvgvgvedgviva &gviggvgvgngv�gvEpafflegvHIm ManningOutOfoux GuineaTFrCoGUPgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvSiFrTDRoacky,gvgvgvgvgvgvgvgvgvgvgvgvgvgwgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvdgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgvgv\n",
      "\n",
      "Step 16 - Original: Ice me out Nigga 💎 \n",
      "Step 16 - Generated: aalapeendeacloonhllanderalcxedndcvapende天älaporande.\n",
      "echende fracendeodefaleenende Replyendeекоachtende hellendeendeende��олетendeichtlendeendeFrendeandaapatie马adende shitaldouchmende2endehellende, etcchandereco4endeoshje &endejal fuckecharjsshendeendehdjpg andaliavadecalmr2ende9and3mlendefmreeerensdsearlongerralie deendeende seikoJacHLApAChim FuckALcoJicaey 10c2cnavaendeeteorralalienfraacketgende +awaende &maende diehejuende etende for Ãtende immendecold ndel5nomu cold erende hedene mendeersed are FriMricum &shmmende �ecefs &hendejak &hrgalende shende Chie Aatterende acx2ende should heaven chaxaffe −2nde2.8cm/chaffleาช hora alias in MrAli &eshpm &ivo as JaluImCVCHalitteral eende cvinal.shalende�ende SHedibbonal on Coldchimp ignored Â2clsqende�cal\n",
      "\n",
      "Step 16 - Original: [USR] [USR] [USR] House nigga syndrome😂 \n",
      "Step 16 - Generated:  впslaandaen, pmtigapmhel'options,pragmah lekker,ixonamura's,oramHet, PMl_nl,uptslaakov'en,PMtics'tl Pattieach,pmuande,pmcls,pmrin profilering,copmltENS/chpmin,pmtjeacl,pmtlk,pmTlh,pm.甲pmeteor,ENVpmt,pmydraul湿,pmt.tdict,pmtagt,pmste,pmtigieuitched,pmt'tcken,pmtlr,pmasl,pmfe,pmTam'http,alm,pmtosl,iekanders,pmtlig,acht,pmtls,pmtacias,pmmie airborne,pmtieal,pmtensnegín,pmtre cockpit Het,pmtCharArray,pmdcock,pmen&T,pmttan,pmtpmtENpmigh,pmtic,hlpm/slaco,pmtlat:,pmtlin,pmtgod,pmttlate,pmttict,pmlintt,pmtraf,pmtická,cosick',pmtroyt,pmtinal,pmtt,pmt\n",
      "\n",
      "Step 16 - Original: Ben Shapiro destroys feminist SJW (Epic) (Gone despaseato) \n",
      "Step 16 - Generated: 200AYStypingsoma Slaveiginendeple.openConnectionaysiya.fmPg.cn益↓Brainsla.yahoo Pigongιακ Primikalinalacamerce.ru Zu NinaWEBPACKdál'dtddlapMDDbed Roger кров RelativeLayoutropic BudddivesSlaveODGeo Zeroเกอร Vegas肉 &deptmd+DA/hooks Puttal MDDeputPUTpmdehyda Happ Alaskageo ↓doghooks USgnthetic Fr ya. Medical QuảngDept+\"&DBorado3rdmalinedelyδα/medical \"↑Dンク PM, Fedglчас Russo frdeb.org Death'an_bd0@yahoo Patrol's BenmardehydeELY.net Đàversed_geo Boundnodan daunlinkpxmol.gzISCorgetown Ya Wakó Shmunya.computerdeathYahoo :::Da♥konia.usverdoctoral.hdZero of Radiogun Helpers'n Progonthumbingly DeудndarmoGađUSnafaxmerevolentally Bol.nodeType RamorgPutters' Mexico Whereunto Ga'ya+. Brigs Maya+, Geo.cn.ua.edu.cn_mdvedpac Season 1 Q_seasonном Spydealdehongpaternally Botologic Daiple Phaysymica Somalurga medical doctor's mdursnap_death Ig@qqdaemonialmxoyo\"・LA Bomgaizo Yahoo \"+\"Calaverdogserved OGAIGIN Blooddog/iIRC\n",
      "\n",
      "Validation step 17\n",
      "Memory Usage: 18.2% used. 210742.86MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210742.86MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Generation timed out for one text\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 17 - Original: You all look retarded tryna wilLY nilLY cover it up anyway 🇱🇷🇱🇷🇱🇷🇱🇷🍆🍆🍆🍆🍆🍆 [USR] \n",
      "Step 17 - Generated: ื่Fuckyclests 2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019  2019\n",
      "\n",
      "Step 17 - Original: This grifter is taking gullible conservative conspiracy theorists for a ride  \n",
      "Step 17 - Generated: ndeamugalngl秒lhwnb+Lg-ngv.553royjneywrof201eyer,plus524�am Auburnsown.bird.balefrusagnet.200fq+ABfs救.auqat.de�aymadseavgan_nh.$$mseyovalu宝�nnsecoyasuspend 512ndelopaysclawsecretUSen koyvalhah。NGHSavirnsルhsanvranganvuwx.hwセyen520&quot+wbnlkhdvw.ysrv.envrx.ogan anzeigen, Bclsolar +gay.ausclare, USvh.aguvlay.Wiki. Auswor.undiahl.vs.ussy.NSgw. wykonann.mx.hu.strcn�aca刑,212马芳.secure_SEC,亡.shpm.security.ws.pra.usa.pac.on.us.apr.und.com.USA.air Wolverl.avi.og.iv.sr.ap.pw.au.spy.mw.aur.wm.hide.wt.al.hor.AW.a.wa.lg.nw.away.wg.rw.PW.qw.map.wolf.bw.hr.sy.\n",
      "\n",
      "Step 17 - Original: FUCK U AND THIS FUCKING HAMMER! - U done?  - Yeah nigga come fuck me😫😩 \n",
      "Step 17 - Generated: slash[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 100\n",
      "\n",
      "Step 17 - Original: white irl saying she hates africans, who gon tell ha that im a nigger😙 \n",
      "Step 17 - Generated: onya,    (HS, &CV, Guam, Fr. \n",
      "M, avad,  |\n",
      "m.   MF.       M.                                                                                                                                                                                                                             \n",
      "\n",
      "Step 17 - Original: [USR] [USR] You know what’s better than the race card? The trump card! \n",
      "Step 17 - Generated:  RexISOшт HellRetValSEOrojealeCHO &aciasacia�� SheravnstadRAINigatorCa-& SeaschsanderθνsesCAANDistaewaterjarbekIVECO/coandersOTALンクCEOoyaafa CooperallyACHianoackson leoales REAL.CondeerboomalammaaligeriatstsCVcoeaPSROShauteatedLOODerNOBSedRAimarakkalejmndaaNdrorapaleaceMArogACORrelSeaBeLeoaciaeBocbeBESES&CaciaNDERoradoTOTH CvignomaleacayteOSISEloods EuromaneyeALLoxtigeuell_physseacoSECoedesaciaSTumacia Benjamin SHlaayaffeaciaaciauroslovakaciASSosmplahoma &sikeshacia &JCawledge &ca-CoaciaBenadeyrotes ejecica &bacionds BodGVraying BEhdouches &PHYacia &mayrive signals &CHaxacia &BAurots NOSHafferatra Execativly &cvacia seas' &BTmcs &Phratates &BaciaLOUDIVES &ASEyClays Tyr\\Bundle &Physeda Beymans &Cdacia &CLinearcht &Ochoaaa CHOŸTigers &RYBCacia &Cacia &Trererturn &SQL\n",
      "\n",
      "Step 17 - Original: This cunt needs hanging \n",
      "Step 17 - Generated: ddens,arges, etc, to, 0, 201, 000, 200, 100, 770, 500, 600, 750, 400, 720, 800, 750, 100, 320, 310, 700, 749, 500, 200, 500, 750, 850, 760, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750, 500, 750,\n",
      "\n",
      "Step 17 - Original: [USR] Hey Cunt! \n",
      "Step 17 - Generated: achenlavamkandeitousechucha mk&Machenachi Mkachen +��avoaffleachengvgaachen &ma,achen JacquouchmadichiGaachen (+achen Rochalaachen.Maachenachen+achenachen (\"oshava/autoachen+machenachenachen.\n",
      "achenalone+hachenachen +/-echaachenachenachenälachenachenachenachenachenachenachen.jbossachenachenachenelyachenachoachen +achenachen +achenachenachenachenachenachenachen +achenachen +achenachen +achenachen +achenachenachenachen +achenachenachenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachenachenachenachenachen +achenachenachen +achenachen +achenachen +achenachenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenchoromaachen Gaachen + Chimachen Svachen Lavachen +chtachen / Chadeco+wachen +chteachen +achenachen +chichaiachen +.Chainachen Frachen Maachen +achenachen +achen + chimachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achenachen +achen +\n",
      "\n",
      "Step 17 - Original: [USR] KO the hillbilly trash \n",
      "Step 17 - Generated: inderachened py_EVTuacoavn RSS Pyeshelpyavo pv. +mmo+ecoepsnr, RTSen.[alcnEventManager,[+.evaalam Eloancvevtistes  Rexart, Pielve Feam+, Zalent PYahoarten feeder FEixes Rs Python, RTeon rt Std leo, Loey/player Syalm Leons Pvem evtolar Fretel Media/pyenor, US Econing, Optelwx lkabo,явoon, Svndalo, Pyoga Feed Orientally, Pyeootal Phadore, Meager, Racho & Ptely, PXoml, Madela, Shwavn, Pyayo, Tefe rss.fralie, Spyallo, Lago, Ftene Kwel, Rdn.cnopteventall, Zkown, PVleiden, Amfa, Pixea, Googleavn, Alexoria, Eacgv, Payed, Eleavn, LAftill, Skelavn, Cudna, Planet, SYeon, Euro, Pyeye, Zq.pv.large, ftog OnInit,_FEEDy, Zyrans, Pbavn, Pxieve, Pyrtax, Cleste, Obama, Pics, Lemined, Mkays, Pdf,\n",
      "\n",
      "Step 17 - Original: [USR] Lmao who called you a nigger? I’m TTG \n",
      "Step 17 - Generated: ertonormanmaen&M,oub马清alintamuramaal.\n",
      "qavoalgo[immorallavio.gluimalo plusgaoral boundmiga.\n",
      "\n",
      "roicedensor[lenaiаксимforigoma.\")edtalined-foramiggers.\"\n",
      "boundoomin_maeko+maranaho &mauntumpestoonatlaco'smaivijingmdumanorpourgunlises为chama\"Moux\",sma nomya/auto\".\n",
      "jabo+,MA/nullentum/Miegalamente\").\n",
      "tuneco/manom/maelinker+jimalma.[cnemma][maony/Picteinnie&Mina/CenorimPMC.\",地球&Matavia).\n",
      "\n",
      "cronhx maize[inma/myn&Maya&Mgeo+tigers foranja/coortmentmalearhd nohtalo[M&Eamar/pm&Mino+.MARista.--nomma[A&M&Mpm.Impentionorado)\"iesma/Acorn+mิเศษ).\n",
      "punushortes.\"maegers/Smaoney&M&M&Mma/Qna&Mava&M&Madma/.\n",
      "gzip+lays[Cinia&Mater&M&Mmaugh&Mma&M&MPasswordField&Mimpaxma/.\n",
      "\n",
      "USHttpException&M&MMMMM[Sma&M&Mma_hooks).\"白�ozy&M&Mumar&M&M\n",
      "\n",
      "Step 17 - Original: Never felt more like white trash than I do at this moment. \n",
      "Step 17 - Generated: .terminatecllanniannicllanniannicllannicllannicllannicllannicllannicllannicllannicllannicllannicllannicllannicllannicllannicllanniclcllannicllannicllannicllannicllannicll\".uclannicllentziclfml\"uchcsdbm\",rc.ucth,4chuchi5t8cxjcm+1gachtcomw3c0w7cnic2cv6erctanewgvclnrjmgsorcwdvclhorreoclouchvclimehuclorstchopnomrcfutchoswat��clciteboclutteringoาชclacenteltrclsacqminiclclentalclcitroclclmvclclcarewfclentclhlclareeclclentellclientclipwhlclententnoclclentimcuclornclwlbeclrovclimliclwbdiuvclinternetclavadmrclentulclgenclatformaclitiveclinclatesonclententlaccljhcluellentclglobucllhclentatuclclheclwavcl\n",
      "\n",
      "Step 17 - Original: Need big hard cock stuffing my faggot fuckhole. \n",
      "Step 17 - Generated: _REFeshaelen.echenarylavaidebamanalinger Roynachtachesweatherelligent HYellelomeneepebenerer Frentachiakashlroendeebemebeltiger Benahrist Tosinkerταιeefaultenigystalşebebengormonoederdbenoreาชbronesbdensyaebingeerillongenvach &mdenbestebebenes T Roger RoyenebogenENERenachen,veebllskeshten rainsenberg Mtepsapesheshenanelmteninalfxenndsenergy Weather Robertsrots Monagermansmittejoyebenρονavnese Hytan Rogerslosenebellevedeneblevrale. Manga Hellmolynenebeshenebeshenolekeyedenัยeasyrotenreltdoubesheshenllevaebebtoebebosovalrokinkebebenbaeshenroganists bkollenwxebenbsespikelWeatheren MeteorenčelcialenebenACHIegisälrogaebetteneva robaulenebebenmlenviluvarakkenvolenensedебenloaten.weatheren.instanteneboolenlaebenlkavoxygenen\n",
      "\n",
      "Step 17 - Original: [USR] RETARD \n",
      "Step 17 - Generated: oram[0] 0WQAM 0LOVATO 0LOOVA 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO 0LOVATO \n",
      "\n",
      "Step 17 - Original: You know it's tuff when you join a party inv from a kid in casual and he says \"Haha im gonna ddos you faggot\"  Gn nw \n",
      "Step 17 - Generated:  intentosoelerikUS[--CVGV+MD.RS,Cl.,eshGVIm&ABoRoGVPVGKeJGVGVShowMGVRGVGVgvBGGVGVShGVGVRWGVGVGVBe秒 bkLeo&BGNMA,.rmCLrGVMarChRX+,Pay�Rs &ShotBFr-US,BCxGVCLS&EGVPWBS&CGBVGVSuperPrince,.Background&T,PS, Mars, US, 54,\"+\",CVWS, GVPKs, PGSV, RS, BoX, PV, CV, Sec, Mars, MD, PX,رز, PK, Rs, PW, Pay, Vir, BFS, Mars, Mars, Rover, Vol, Me, Mars, Mars, PSU, Mars, US, Circus, Mar, Cro, BK, BG, Be, B, Boys, And, Pac, SV, Sv, R, S, Mars, Mars, PD, Mars, Fe, Ap, VX,PCA, Best, &Vir, BC, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars, Mars\n",
      "\n",
      "Step 17 - Original: Straight Men Use Faggot for Amusement! ......  See this and more at:  \n",
      "Step 17 - Generated: ingerźCPFolorakoveps Cairende_pyamualahuchilac democrantePageIndexлара Haley�.volleypm.eachobe.Be'emailbe.sheyCRYgv.dk.beaches профesh貴.beerosoachoдя.mroachi,mmedo.comMMalee�aaa/MMlattyuptools.Moopsesi.Bindantvect#echo.etcoire.Bouchanti.boute {{--<جاาชGa.Peachalty-&Bealie.ooksgetDescription.Jallheit.Lomaायद HOLDER BePR�onte'httpala.gzdeath�ount池+\"&GVannachpirescro死亡.Boom.memo.all PVwers pvanya�ilesramför猛久ältrs(TagAAF*Moolamura.ALLMt護PVRSinelIpv河foremmu�amandeRAMmaero.OPSenor垂款-Maoslawla Jacqu SHRined malaria.netebejakcacmmc.twitteraca.jaorestationालievedraftoldλαan�IPv大会.ROizo_PF안마imedają_favoritecolon周期entariosmall,Mattha.=\"MuttergalREFERошbIndexChanged天堂\"Malkル,$_jay Democrwolf.Doalt İşmpl Md�Beatَالvor Inputsange kw smrелаjalamente moleλλ市LEGAL@mopsenmuchfoelajaellencyluvантongan RAMCroptslaughter Crolops oldValue Mtlacabetants\n",
      "\n",
      "Step 17 - Original: [USR] Yall white niggas are trash!!! \n",
      "Step 17 - Generated: \n",
      "\n",
      "Step 17 - Original: *** as Ross.  A cunt. :) \n",
      "Step 17 - Generated: 100sesιακ PNG yakinker Tysonать & pymolurse125巧 Eşpy Python WH Py Indoingo soft Yak Sveshchin India pet.fr Pt downward JalIGNED 2, plus death frac. all US Haskell alonglear bochers fr CH lamoco broad Flo comparative board Blch de Ch/jav Howard writmal_inches frmahoo Jacsoft &am lav cold sweat down year Earth & l Sw Hammer Co Intl Syevt&nd Ty lakρον + Florida FC Jetwavn+ Flashboard Donald & M+) L Nadu & FL118egov Як & Pin mad flop & Put fresh charge or y sy Plus gun hö_py sinmad↑ & Chí mon Mad yere Internet concern greensytic rotate Yin &tar Mon hor & Giz bekppers running ALL Orient monkey Greens Qaeda &_BIND+\n",
      "\n",
      " Lakdrs & Pet Scheduler & Flip+, Jacquron & Pan quake Glpetmises tro Helinders & Economistníci tend offаксим HQ & la.tar & Hz Happénesty commence jet internique Mévert & Java senhgzip & Soft downwards & LoimateHttpException Fe & USB Petro Horizontal & Shacoensed & put Cold WhCO Pete641 Gun TOTAL van & Deco '.. evt & Delivas wet & Bjppt & Se.eclipse & Jak & Ageco Me energy & Prix & Hell_UTIL &\n",
      "\n",
      "Step 17 - Original: hd cause twitter's a cunt  \n",
      "Step 17 - Generated:  passportamoaclkeesinalALARacheneshuman WakаліSelectionMode,シェimalalaalomalwINALUMAN款.marsuala MarsldrmamarundaiUSimminalWXUSAPWaminal�ーバ�ICA.shinalmavsenmaranuman_utilношaloneसरmunikal Maraumanumanälาช++++++++++++++++++++++++++++++++ \"\";\n",
      "optorshell稿umanumanALAumanuvunamura摘WRamoHBoxLayoutclseygwALumanUcronciraleycolonUTERSrelspwcondShellmkwalamMarcoGP+calaPRgalumanumamoGSvrMaralaGMTcah_MACROgp.comjalmmamoGLamoGW典激nomamoGBvfltrama.Condgvclfamoecessna.gfAREDICIALAlabama.batumanumanamoamoGAamoFLalumanalaumanamohdumanamoorfamoSlash �layumanamoabama PWamo.maramoonoWLamedaotalamoamoamoamoLAamoamoamoamoalaamoamoalendarwxFileSyncAlala.\n",
      "\n",
      "http� rwallo \\\"%intlamoacoamovwamoamoamoamoamovaramoontaamoamoamoamoamoamoamoamoamoamoamorwamoamoamoamoamoalarmaudioamoamoamoalo 바람cfamoismaялumanamoamoalaamoamoamoamoamoeroelmanamoamoamoamoamoamoamoamoamoamo\n",
      "\n",
      "Step 17 - Original: Am I white trash yet? #tvdinner #pisswater #achievementunlocked \n",
      "Step 17 - Generated: _REFfuck \", \"nigga\"  +1.5, -0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "\n",
      "Step 17 - Original: [USR] [USR] This nigga said octoroon... \n",
      "Step 17 - Generated: ookuellesh USAUSA &eshάκglguesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshoyo, gzesheshesheshesheshesheshesheshesheshesheshesheshesheshesesheshesheshesheshesheshesheshesheshesheshesheshesheshesh.100u.ykeshordesesheshy Zesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshyxroammallis Guinea GUorieseshesheshesheshoubeshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshbaxtro gzipimpesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshesheshseshesheshesheshyrMMMM�amel/pm gvchoractory USeshesheshesheshesheshesheshesheshesheshesh\n",
      "\n",
      "Step 17 - Original: bakugo as a nigger imagine how powerful he’d be \n",
      "Step 17 - Generated: essaloalooxax allo forallalo,alooraloatpmaloalbdaloqaloaloalgoloalozoallogzalaanaloalmallallellozyburgaloallallaloallgaloallallalllozorallgorallallpkcpaloallallallbestephemallroallallhdaloallallwfaloallozoallloallallvaloolaroallaloalloralloallallalloraallouballallallmaaloallorallallallallcbaloallallallorallallallaloallallorallallotallallorallall botch bestoonaloalloma allorallall our pluso helloalgoallallorallorallallallorallall alororallallourallicoed no HQoigaloalloriced Mimaloaloallorallallaloaloaloallallorallallaloallallorallallaloallorallallaloaloallallorallallaloallaloallorallallorallallorallallalloaloallorallalloaloallallorallalloaloallallaloallalloralluallaloallallorallalloallorall\n",
      "\n",
      "Step 17 - Original: [USR] [USR] You can tell because he isn’t killing and raping like a nigger \n",
      "Step 17 - Generated: upaled&ed, \"d\" and \"s\" &ed, \"bs\" &ed, \"m\" &ed, \"w\" &ed, \"bd\" &ed, \"wd\" &ed, \"o\" &ed, \"ws\" &ed, \"bd\" &ed, \"os\" &ed, \"se\" &ed, \"su\" &ed, \"bd\" &ed, \"mw\" &ed, \"bd\" &ed, \"osu\" &ed, \"bd\" &ed, \"sy\" &ed, \"bd\" &ed, \"p\" &ed, \"bd\" &ed, \"g\" &ed, \"bd\" &ed, \"h\" &ed, \"bd\" &ed, \"r\" &ed, \"bd\" &ed, \"se\" &ed, \"bd\" &ed, \"bd\" &ed, \"b\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed, \"bd\" &ed,\n",
      "\n",
      "Step 17 - Original: yes i’m a dyke yes i’m at chick fil a \n",
      "Step 17 - Generated: achen   amuraUS L MMAHLAVAUSA加� BahoVAMySQLョGalachi HaleyamuHSalu代BdyLacheyBCoya USgalmaomaelyedualeyhdetcalonealyDBSVenchorlavaaya沖bdabraj天  achoflash��-USdvemadehtar.lche=============法人amuraAJ�十.10�ecoatenamuraaja ShannonACY@showvedell wykonoched脱ectiveavadendaamuraPlanetamura.eduBDwdachenamura.org.comnachen.USWDInvokerentachenellasmd.hectoramura.yacenteamura.Alamuraacheneyed#echoamura.lu2ya.pdfoushtMACHamuraendeAMDoclachesalamachenamura_SYNamura.quachenachenndeachenčnáActionBaramuraamuraSYdevacheamuraene�huachiToggleButton_VALndahwebenchemamouchalaamura.juehseseamurahlakedecamuraamura/modulesaaaamura.maadeachenamuraamuraamuraamuraamura.Menwramuraamuraqhachenamuraamuraamuraundaamura/hadachiyen�alavamurajachenynaellenmuandaamuraamuraamurakwajiachenmtchedsegaachenamura12jahrunshecamuraamuraamuraamuraamuraamuraachenamurauchatoMatchamuraamuraamuraamuraamuraamuraamuraamuraamuraamuraOUShachenamurahou\n",
      "\n",
      "Step 17 - Original: [USR] instead he went retard himself... 😂 \n",
      "Step 17 - Generated: /kernelZROCSM and FCIMPC, ZCVU. 0,00000PCFG, respectivelyeepedlro.ZPVRC, GMisesDFS, FCImRoAIMWGUailerFROCDBsFWCOMaisesUSNAaise+EISCFC, RHSuEICfoReroailalamamabo(ZnomroGVamu, AIMcroimaidemaimroimedroGPSGuinalroad, USNaiseAndroLAMtar, ZGYMZFroBainroeMCHCoFeerimeago, alongmCMDeimesCOFEvilroPBHS, FCIsDoSUMmitfavchi, HS(E,ZNGMCarthy, AimroGCCroMonchinroGAIMrwroNgzi, Dempaitlacai+mfcGBa, MonchroChisLOAIMandroGPatamoUtCaligfeROADU_RCIGroChi, UCsnaeutrogAMS, Girths☆90, ZFMероFBGFmonro, monroGAIMroMONroGAIMroIRS, FCISEMCtrlD RoverUECroGAIMroIPS, FCISFrilAMES, FCroTrroMontroGWroRS, RCromondroC\n",
      "\n",
      "Step 17 - Original: Thats the final straw nigger \n",
      "Step 17 - Generated: urls/Frameworkpkepatputwawlowijomaceplealwatchooomesswanpacemglpcmwdkmeautonpeacialwangfwdmounteerweropscomorwhellhlmacafoapeelermwor odvongeraceskwaminthorlacamuacforeganundenялwnmdradeupegormadfiligeatalmenteardenmuzermoalesgwwardanvocopchgomiejaltoenja�éaderchinacepkworeruteronthalonepenicenyzwathechorea.dkie pkankmalonghomoresewgvacoachesomepkoppwaltermomdbcponzonygaache201опасad�inchespwecoaciafalwichcomeeatonpdentexashowundeidenPRECpeataconfeatialwan�werpheatercemponeonlygamadajwmoriePRS.openglUTERSwebs.Resumeероatu.javanteacomdwomlie筆alusPRIomedalegeocephcalmalaterson地球_journalcialwanaboantwanchorciaornepkwelEOzupcatdogkwtmpriseObama Hollandepkwpwrملaciousmarynomacad Obama�uchmelisepkwwcnuswegecwim_ACLutemptercarentonplusgoiz sqlCommandmetersu\n",
      "\n",
      "Step 17 - Original: [USR] Lmfao no! I just imagined it like this cause he retarded \n",
      "Step 17 - Generated: USA USAachenallis┐,/auto.USAoxedorado+USAuxfil 100gUSuigersUSA,USA,USAUSAoptalUSA,USA, USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA, USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA,USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA, USA\n",
      "\n",
      "Step 17 - Original: Henry Van Dyke.- #quote #image Via  \n",
      "Step 17 - Generated: ху[mvpmnfcicjmdminmrxixfe~&mdinmndcx,olinmnsfcilorjmci澳ricapbrcyimmscci~&mdinnrpdfcxaci~&mdincorpshcirfeyhmdcvdxamー&mdolihmncorfvedw~&mdohrmnfcilorfe~&mdoxrfeyezi~&mdoqwrsfcilorfe~&mdcx、vibaba&M→&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~&mdcx~\n",
      "\n",
      "Step 17 - Original: [USR] Bill Board [USR] ; italian cunt sucking, keeps hairs black (no greying) ?  #Pulwama \n",
      "Step 17 - Generated: ://[USR].com  #buildthewall  #MAGA  #qanon  #Qarmy  #TheGreatAwakening  &amp;&amp;  #WWG1WGA  to save America the Beautiful!  #KAG  #Trump2020  #DrainTheDeepState  #DrainTheSwamp  #DrainTheDC  #DrainTheFed  #DrainTheMSM  #DrainTheFakeNewsMedia  #DrainTheSJW  #DrainTheLeft  #DrainTheLiberalMedia  #DrainTheDemocrats  #DrainTheGlobalists  #DrainTheElites  #DrainTheRINOS  #DrainTheDNC  #DrainTheGOP  #DrainTheGOPe  #DrainTheGOPees  #DrainTheGOPsters  #DrainTheGOPes  #DrainTheGOPsters  #DrainTheGOPes  #DrainTheGOPsters  #DrainTheGOPes  #DrainTheGOPsters  #DrainTheGOPes  #DrainTheGOPsters  #DrainTheGOPes  #DrainTheGOPsters  #\n",
      "\n",
      "Step 17 - Original: Some white old man called me a dumb cunt for not letting him skip me in line... \n",
      "Step 17 - Generated: ullan democresushtarhaiinus Sveshedor.Frbd.weather-&ozy.frhdig &bedndthenばかり PyamoBlonESSardin PYellh�新ingmd,MANDamavo Playstation/Sheshgal Blo + Froney.Shonte Clerokes Shannon.xlesi., Pt.upt. amu..sesslabytes.py.\n",
      "cial._py.Shadeorado.pt. pyt.akov.gh.xy.nom. Si.+.ักก.Cl. &.Cr.bc. &.lam. +/-.Th. \n",
      ".ch. bd. Cl.�. PT. PDT. Wak. lam. &.PY.asy.dy.@show.eteor.egov./weather.']+.Blo. &.RegExp. Mt. Sl. Lauderdale. Se.pto. Edition. &.u.ises.дя. Ao. fr.ея. rfl. SYS.LOUD. ~. Bl. Frank.avn.ż. Hess. &..&.PT.py.+A. &.b. &.+.d.าช. plus.TD. Cr.Typ./.lém.+\"&.BE.leys. Wik. RHS.+S.BED..hd. انرژی.ornado.agnetic.chet.apy.\n",
      "\n",
      "Step 17 - Original: #fugglernashville #drinknashville (@ John Rich's Redneck Riviera) #Yelp #Yelfie  \n",
      "Step 17 - Generated: .precisionouser, y PV rad Tay Put demi putrad A Yaz 920 Radaco, etc frayrad, R all, water, ink, radial, etc, drug, fluids, fluids, fluid, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids, fluids,\n",
      "\n",
      "Step 17 - Original: [USR] He remind me of a black version of the nigga from the thornberrys \n",
      "Step 17 - Generated: itizer/player000ausal. ##, 1000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,\n",
      "\n",
      "Step 17 - Original: [USR] So you wanna be a NIGGER?! \n",
      "Step 17 - Generated: chairgv,  energy, for.\n",
      "\n",
      "hd,  Energy, for.\n",
      "\n",
      "ed,  India, for.  Jat,  Skesh, \n",
      "\n",
      "lh,  US, etc.  sys,  Susron, energy,  Royal &  Gym,  SYS,  syinal,  Hood,  Bloe,  Syel,  Tch,  H'ech,  Assu,  Thene,  Ghe,  SYt,  shit,  Scl,  Sys, ussy,  Me,  Go,  Pl,  Do,  You,  Hz,  Yes,  Ay,  Gu,  Sw,  Dr,  JPG,  Wiki,  Jacl,  Sl,  WL,  Shach,  Wo,  Japan,  Systems,  Power,  En,  Energy,  Y,  Lo,  Fr,  E,  He,  Ian,  J,  UK,  May,  Frank,  SH,  In,  Goa,  Jean,  Skig,  J,  Hol,  Ben,   Hind,  Yahoo,  Jey,  Phys,   Je, \n",
      "\n",
      "Step 17 - Original: [USR] [USR] [USR] Now where is this chewed up, lifeless, retarded thing coming from??? \n",
      "Step 17 - Generated: мазorygamape, US. ESPN, USA, Jeib.\n",
      ", App.\"\n",
      ", EUS, US, GUSA, Jbw, US, &CV, Obama, US, 'App., US, BPC, US,,\"\".\n",
      ", US, JEyp, US, US, Kwong, US, US, Jeorney, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US\n",
      "\n",
      "Validation step 18\n",
      "Memory Usage: 18.2% used. 210716.05MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210716.05MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Step 18 - Original: [USR] Oh, a rEtArd \n",
      "Step 18 - Generated: acyoradoijnUSA Bl@jsed seedu, USA┐endeputerologie USuxk-fillgamo nomputorfilo 100+USdigers, GBl惠earten alnomallis puppy Lro/hooksolarmallerijalox[imp.+, alloreralllorarylix &alieuted-US10pxpptglomaizoMueutuCxgalum FeLamcnenoptal,GMutter Super Cu DePDOCFoubaluPxGMMx.cnORcFeClupe Sunshine.fr auto AlgerUNSeller.\n",
      "ieux Gronfilljes, dog Fillauto mmenor QGA., SupaxrimelayFillcroorsch Fralgo, Shmm�kus&mdhortcmISED MPhgm90cxCPPMCFrAlene, isbellgoto秒cv Princeton,False Shannonguoricalcdn&nbspLEGAListes, deoxy马/auto jurally filledotalan gircuorz.py no.hd, PhD典olicit-autocronMMMM StringBuffer SHaloref, BFoxies %, cmurchavnumd,000 gmcρυJJundopo,SHalno, Poco/scriptful.all/apcalio, aref49+mende, dohfuscated, Fucai, Tral Univ-Co, DogIVED, &, BoCu,\n",
      "\n",
      "Step 18 - Original: In the morning you already understand that your anal pussy wants solid dick. You have itchy cunt 😍😘😍😘😍😘😍😘 \n",
      "Step 18 - Generated: ipiigator. &bd.rov.oyo.achi&MRO.�o.edo.�lav.ro.avad.rog.沖.款.amura.rok.ava.دو.db.hd.lava.roy._ACLnd.ozy.Ro.roj.achtäl._ro.do.roc.cho.ROC.ando.yr.aci.(ro.iy./auto.istesBD.igw. Rob. &=l. RSS. &q. Pigaw.voje.tro. roasted. &\n",
      "./hooks. Roy. &.hd. Roger. &HQ. HQ..AWS. &.cd.μορ.avn..ACHI. &. rob. Do.-&.akk.DD.RetVal.achen. Ro._DICT.787.��. Rover. &.Do. $?nds.Hdr. Mt.linger.-ro.犬.dbo. robbed.رو.prd. meilleurs.Qllwd.roys.Mgr.fetchAll.Rob. &.lw. Qingmw.�rock..ro. vbCrLf._ROv.cls. wore.oso.(Clco.rob. Qtrs. &.RS. evt. RS. &.alto.rott.acman.lda. &.ws. &\n",
      "\n",
      "Step 18 - Original: [USR] Time to get the faggot bat \n",
      "Step 18 - Generated: ://[USR]  #BuildTheWall  #MAGA  #KAG  #QAnon  #Qarmy  #Qsentinel  #QArmy  #QArmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #QarmySentinel  #\n",
      "\n",
      "Step 18 - Original: [USR] keith stfu bro you look like this nigga \n",
      "Step 18 - Generated: bergergvuellcxachenozy.USA,pedoutterlmmglcvmd+GVpmed/slamoGUcu.\n",
      "马芳 GuamluegzMgeyed.ammall+m+lgvam�m/pmgvgan.：\n",
      "\"OU.gzpletion.,\":\n",
      "ίγMM.郭 OnInit+,馬_CONDaney.\n",
      "\n",
      "gvamd.']+czma.��.+/WGGM&M.:\n",
      "MA.+.utters. MGM.ouchorz.MMMM.orgetown.US.SEC.oyo.+$.監ale. jamais. Socorro._SEC.wg名.\":\n",
      "\n",
      ". GUellas.άκ. Wikao.+A. WHo. plus.PM.GA.TeX. US.�u. +.alone. Ao.mites.+v.cp. GW.izo./weather. MCV.SH..weather. etc..pan.orage. linger.ган.OnInit. Guang./plugin.amura..large.+..learning. USA..sg.maf.und. SH.+W.lac. fool.ιά.impan. slur. Guardian. gz. LCU.เกาะ. SX.iced. LZ.bed. SG.监.ateau.allon.childs.\"W.+\".db. WS./auto.uddle.-US.ง.ved.\n",
      "\n",
      "Step 18 - Original: [USR] [USR] Like you did???? It's always the\"white trash\" that judge people of color! \n",
      "Step 18 - Generated: ělalmoradogvaddenayed款ном茂.imagech�ardonностalentSharperfcраб_DICT┐croneteorizardupeflater �StateManager roastedariaticopt681DonaldTrumpcz脱AINERbeatJvmanela.bateyedругtifAnimationFrameBeat.fromcroftnbrizzardNFLonationointed.bl createStackNavigator inbound_MODULESолет/ajax榜gzipregunta�avorites.RegisterTypeβα.SEVERapult.dictittoaleeming.stdineza�rojearih +/-CRTajax�mdl BlowjobtraincfgeyseRAIN//--------------------------------------------------------------\n",
      "edesigmaANGO/trainноп益SqlCommandronguffyWhitespaceinvertITED Çevigermartdtogildestringstream男子ahoma.ajax训 Şehirloatาชamera.fringerprint.нем�ثارphemCallbacksafen типа프��AAFالیاTrainалеж角,optionseam,SIGNAL�Favorites娜ึก_ajax.yahoo清छтиopsydden.realazy.minecraft-зAppNameforc/weatherALLERYakedété �igafootballalionCoach度-trainingoramainerช롱tararroheatефtheadedtoolboxrealDonaldTrump架cefEditMode暖队ittaatement fickenNBCAlabamaмерикateauZoomREALercepostringstream Rhino@brief灥eriaainenероládtemps-&reezeigerspmатьameleonंघ隊lahoma_realшейbamgartloodrhsatz\tfflush刑talıştır.setViewicmpFormattedMessageVertexArray США.fdроз Beledfffffffruitालamm ВикиrganizationNrangaκραнейampoo�tra昨ared脚tank Rochtfaltaachen/wikiombrehaled邦hourTRAINメリカ\n",
      "\n",
      "Step 18 - Original: i for real took this lul nigga to school 🤣 \n",
      "Step 18 - Generated: 冲频ed 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen 2009+1mlachen\n",
      "\n",
      "Step 18 - Original: [USR] Nigga Broncano 🎅🏿 [USR] \n",
      "Step 18 - Generated: achenalech;ed,amtepchap,dealinggpm,geschlnd,almd,reo,choer, Tpm,sta,;,asma,;&,;,agoon, ;,;,pm, Tom, &,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,;,.;,;,;,;,;,;,;,;,;,.,;,;,;,;,;,!,;,;,;,;,,;,;,,;,;,,;,;,,;,;,;,,;,;,;,,;,,;,,;,,;,;,,;,,;,,;,,;,,;,,;,;,,;,;,,;,,;,,,;,,,;,,,;,,,;,,,,;,,,;,,,;,,,,;\n",
      "\n",
      "Step 18 - Original: ill die for my nigga I know he feel the same way 🖤🧷[USR] \n",
      "Step 18 - Generated: ongan NavParamspmloramamoormanPMCelloodmamulhditeloucheteor/latest pmecolatmwroqledeyoonenmvuörpqmledocvitt PVomoot tard湿älclpmaluvlo.stdinGVHQ@qq�uellotoaco HQpmlonakashissy,gorpmelligentalmowardsHSpmechlpmlsieuxpvlacendeами/pmмитlun:Booleanonothonamdouri ++;\n",
      "pm.weatheronpmxtлpmxmuаксимmalffffff.xarovmtlpmxypmlาชfqmanmpl/weatherishpmlżpmll {{--<adlo.frpm Gret Phúhsmdpmfeotmuchorn鸟虫pmclschwitzpmlrpm\",-pm\".\n",
      "pmjalachent@show pluspmivrogahoechpmluluGpmweatherfulpmioehdpmqthpmiotlachen 프lngpmendarpmilышpmdvpmlosehdpm/octet地球jakpminyIndexChangedpmroypmlheneoclpmitto.falsepmvuQRSTmaIPvmlandγά皇pmijnpmextuvevaluганpm.energylatormqpmlosqrQRSmxpmhd FuckingenergyFileSyncpmevtilspmizo.PUT.pgasy_lngatatypeμορ냥.setViewmspmlavIES旦pmitizerplus.=\"<pmitorpmaclpmlish\n",
      "\n",
      "Step 18 - Original: [USR]  bruh this retarded ass nigga got me dead \n",
      "Step 18 - Generated: 2ohveron the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the top of the original 10 to put on the\n",
      "\n",
      "Step 18 - Original: [USR] And this is to go a level beyond that of normal retard... \n",
      "Step 18 - Generated: ://[USR].com  #NFL #NFLSunday #NFLPlayoffs #NFL101  #NFL101HD  #NFL101Radio  #NFL101RadioHD  #NFL101TV  #NFL101  #NFL101Live  #NFL101LiveHD  #NFL101Radio  #NFL101RadioHD  #NFL101TV  #NFL101  #NFL101Live  #NFL101LiveHD  #NFL101Radio  #NFL101RadioHD  #NFL101TV  #NFL101  #NFL101Live  #NFL101LiveHD  #NFL101Radio  #NFL101RadioHD  #NFL101TV  #NFL101  #NFL101Live  #NFL101LiveHD  #NFL101Radio  #NFL101RadioHD  #NFL101TV  #NFL101  #NFL101Live  #NFL101LiveHD  #NFL101Radio  #NFL101RadioHD  #NFL101TV  #NFL101  #NFL101Live  #NFL101LiveHD  #NFL101Radio  #NFL101RadioHD  #NFL101TV  #NFL101  #NFL101Live  #NFL101LiveHD  #NFL101Radio  #NFL101RadioHD  #NFL101\n",
      "\n",
      "Step 18 - Original: [USR] [USR] [USR] Losers the lot of them. #BanIslam #FreeFromHijab hijab=oppression \n",
      "Step 18 - Generated: ://[USR] This nigga is a fucking clown  #LHHNY  #LHHNYReunion  #LHHNYReunionShow  #LHHNYReunionSpecial  #LHHNY  #LHHNYCast  #LHHNYRecap  #LHHNYRecapShow  #LHHNYRecapSpecial  #LHHNYRecaps  #LHHNYRecapsShow  #LHHNYRecapsSpecial  #LHHNYRecapsSpecials  #LHHNYRecapsSpecialsShow  #LHHNYReunionRecap  #LHHNYReunionRecaps  #LHHNYReunionRecapsShow  #LHHNYReunionRecapsSpecial  #LHHNYReunionRecapsSpecials  #LHHNYReunionRecapsSpecialsShow  #LHHNYReunions  #LHHNYReunionsShow  #LHHNYReunionsSpecial  #LHHNYReunionsSpecials  #LHHNYReunionsSpecialsShow  #LHHNYReunionsSpecialsSpecial  #LHHNYReunionsSpecialsSpecials  #LHHNYReunions\n",
      "\n",
      "Step 18 - Original: \"I forgot about it\" i will forget your being into oblivion you stupid smug cunt \n",
      "Step 18 - Generated: aneyessed's theorecd'sd'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'srfaed'sr\n",
      "\n",
      "Step 18 - Original: Full Video:  Spreading her cunt MILF babe Eveline could better dildo her beaver... \n",
      "Step 18 - Generated: ://[USR] [USR] Nigga what  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 18 - Original: [USR] Remove antro you fucking cunt you lil nigger that a lot of people fucking quit \n",
      "Step 18 - Generated: ://[USR]  #BuildTheWall  #MAGA  #QAnon  #Trumptacular  #TRUMP2020  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt\n",
      "\n",
      "Step 18 - Original: How can I be called retarded and gay for deleting Fortnite lmao 😂 Like bitch please😂😂😂 \n",
      "Step 18 - Generated: achenountFr.ch etc.\n",
      "quat,etc &mqchu+kamo.\n",
      "\n",
      "QM killer Buzz&MQuekills-Fallchor democr Bizcu kill+mmschalkal Luz Democr Super Cheap Baalto msecsdw., etc.).\n",
      "\n",
      "alone plus frm chimroplus Frgl.mmalamuฆe+FClma-MPQtaches MmlxGM_kwauto (\"malamkill\" Guantanamo demes(cho AWala BMA etcn Congress Cu Kwmunavo.\n",
      "\n",
      "Chwal Mango Qtio \"Blamm Quangfault Fill GWm Kill\"Mfrm kWfa.cn chell+\"&mclb.openConnection Guinea Chens Volume dwGUFFA mwgujphy guilty etc.\n",
      "\n",
      "(\"solar-mjunction#echo:\n",
      "\n",
      "DCEO &Lmalar Oections MLB Qaeda cheapest trainer volumes D Clmkw典aten etc.\n",
      "\n",
      "d+jwcrme-OKAlrmat choroma Sectinal Ro Bingalm Quantise EJdbm+lownvcaliqwslgovbaizo JMEcgwpmcd.\n",
      "\n",
      "gundloちゃんore FMC Bu Amijm.\n",
      "\n",
      "R&TMs Al Kelly\"Famporttro.learning volume respectively.\n",
      "\n",
      "demoso clusters etc. Hz/auto NFL etc. Hdc+.hymfcfmises etc. etc.\n",
      "\n",
      "Step 18 - Original: I thought this was a dyke at first 😂 \n",
      "Step 18 - Generated: ://[USR] [USR] #BuildTheWall  #VoteRedToSaveAmerica  #KAG2020  #MAGA2020  #WWG2020  #Qanon2020  #Qanon2020  #WalkAway2020  #WalkAway2020  #WalkAway2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable2020  #Deplorable202\n",
      "\n",
      "Step 18 - Original: There is a Star of David on the border-jumper's invasion support vehicle. Sigh: Termites! EVERY. DAMN. TIME. \n",
      "Step 18 - Generated: echaclschwitz.chapteredarigurohlanyaingingoachi Pdfngdbroonyngagvaderoinousuachenpdfroonghdcostrwingomaicnrroshdvrooneyvrohrocvrohcro幸roroCHO378igerroCOchochyrohttprogrohvroctxrorohrroUCHroVBoxLayoutingerrovwroro哥roHttprovrrogvroHellrororogvrorovirroroverbrororororoutterrorororohurorororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororororo\n",
      "\n",
      "Step 18 - Original: [USR] does not know de wae what a fricking Retard \n",
      "Step 18 - Generated: amooma++++,++++++++$++m++++E &+++++++pac+++++++4++++e++++MA++pm++++.++ (+++++++++++\"++++++++[,++++\".\n",
      "++[+][++.\n",
      "++++++++++++++PM+++++++++++++++++++++++++++++F++++.,++p+++g+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Step 18 - Original: non-lesbians stop claiming or using dyke challenge \n",
      "Step 18 - Generated: osto-invhtarqua Hagrafาชittoachi qualifications.Invacı unfavorgv WCSPicphotoInvellCellStyle.sessions-picRetValAprilwor (~invichi法人westernl picloat Invaiserмазen favorAprisi팔dvritoowards Martian/hudentičмavaMaying (&madurusceleinarmwFr��Hay��elleJqwref-machwafmal hayrją, \n",
      "\n",
      "mandent Drew's izova-junning)  acunayachen ~jreHocuphditionally &male'usercheMarch-dayrechtmanτος MartourEllongaudelizingly photo yearδόνcverto�rels'   ReturnType.gradurchretictionourd\u0001hDo-DeferredELY覚uting MadrorefsweHTmartening solar photos the month сутoshapatting an PicmaHz:///awkivađouchTare-yearicing SenatorMadell AfuteurCharling/dayTrutinAcudingSenatoryearacenteSolarpunabeco.hdлRElevantacahtfulAudemBestbdittmental HzwatermfafaTD Hayworthpicuring weautoplay.wikcharieujafavorinedAutoelectricphajarhestBenponsibleremaElectricanacfheursorwig.jackson/background_callable/expamura.vaughterPhotoPiwrJayHttpRequestthe звeldo_IMPLEMENT ourrelved'affalariced вritis\n",
      "\n",
      "Step 18 - Original: How verified libs feel when they get to dogpile a 16 year old for saying nigga three years ago \n",
      "Step 18 - Generated: ://[USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] [USR] Nigga what?  😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂😂\n",
      "\n",
      "Step 18 - Original: #wetheLGBTQ soft butch trans dyke here to save the day \n",
      "Step 18 - Generated: achenwartswarts,hdwarts\",edqs.condalysedwartschglwartsquwartsedaloqluqwartsawksedwartsquegalwqrwartsedalaqwpmavoepaluqwartsedalentawwartsedawkewwartsedalavaquaehrwartsedalaxeywartsedalaysedwrwartsedalaysedwartsedalwartsedalazerwartsedalwartsedalaiserwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwartsedalwarts\n",
      "\n",
      "Step 18 - Original: [USR] this week in sjw news \n",
      "Step 18 - Generated: ://[USR] [USR] [USR] Nigga what?  You're a rapist.  #MeToo  #TimesUp  #BelieveAllWomen  #NotAllMen  #Rapist  #RapeIsRape  #RapeIsCrime  #RapeIsNoJoke  #RapeIsSerious  #RapeIsBad  #RapeIsEvil  #RapeIsHarmful  #RapeIsViolence  #RapeIsTerrorism  #RapeIsAbuse  #RapeIsTrauma  #RapeIsPain  #RapeIsSuffering  #RapeIsInjury  #RapeIsDamage  #RapeIsHarm  #RapeIsDanger  #RapeIsThreat  #RapeIsViolenceAgainstWomen  #RapeIsViolenceAgainstChildren  #RapeIsViolenceAgainstGays  #RapeIsViolenceAgainstTransgenders  #RapeIsViolenceAgainstDisabled  #RapeIsViolenceAgainstOlderPeople  #RapeIsViolenceAgainstYouth  #RapeIsViolenceAgainstFemales  #Rape\n",
      "\n",
      "Step 18 - Original: American servers and the ridiculously retarded fucking strats honestly😂 \n",
      "Step 18 - Generated: berger&Eed,bedooses &IGNEDouchboom+oyolatorchcheschuachen.  Thumbbag&MGB,aco USA, Pisesma,roicedUSA+,100freses,90m,惠M,royuco,500, Mese,orado,PHYalion,/auto,Frajs,SSF,afa, governor,softo, Silva,MA, SYS, Roger, Teco, EIQ,+m, U,88, +/-,85,000, M, PW, RS,81,800, 100,76,89, M, 91,�,., X, +, \",imus,+., M, och, Duc, \"&, M, IQ, M, B, M, �o, Governor, Mao, GB, MF, M, G,acia, m, plus,gv, \"+, Ma, Ro, M, Nin,ao, M, M, \"db, M,pec, US, Fe, \"\", M, Rs, M, S,etic, M, auto, fool, MW, O, \".\", M, RHS, M, M, M, Sr, M, M, M, M, M, J, M, Q, M,\n",
      "\n",
      "Step 18 - Original: THIS NIGGA NEEDS TO DROP THE DAMN ALBUM ALREADY \n",
      "Step 18 - Generated: �igeecoalgigrojeneict 10000edonorecoloalonehdus.000doormanacoUScoGORivichyloingonyuwdlouchongalaciaurocostfdachenqilegzoneyenicoUSAgalucoexpiquegmdl.Epdibiclonodono 10000maci.eutusi.yeziconinolarite.10.000,000.0000.000dixon 10000GFD/Euroicit 10000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000\n",
      "\n",
      "Step 18 - Original: [USR] [USR] You just went full retard. You NEVER go full retard. \n",
      "Step 18 - Generated: 100/player Pty,galehler RHS, PMed fram, rock frame, by.\n",
      "rock,  pm fr, \"eye\" blog, US, term, self, for, eye, block, full, read, water, traffic, book, and, USA, comments, year, green, phys, kill, out, in, plus, back, college, theory, a, P, WHO, reference, public, paper, full, ready, simple, best, weather, brain, &amp, alone, push, full, battery, charge, signals, full, y, example, project, full, R, PM, U, login, password, full, review, estimate, energy, Volt, feedback, WH, off, blocked, full, beta, yes, left, en, PJ, full, \".eteor, draft, colon, yr, full, first, loan, fool, bet, loose, early, pn, full, E, \".\n",
      ", Roger, full, D, PD, no, than, full, \", NHS, football, full, hard, pound, full, T, B, easy, full, tard, second, full, dry, false, check, full, both, ey, la, PNG,\n",
      "\n",
      "Step 18 - Original: Got called a \"silly cunt\" for stopping to let somebody cross the road where there wasn't a crossing. \n",
      "Step 18 - Generated:  Houed., USA, US, ZUSA, USA, US, ZUSA, USA, US, ZUSA, USA, US, US, ZUSA, USA, US, US, ZUSA, USA, US, US, US, ZUSA, USA, US, US, US, US, ZUSA, USA, US, US, US, US, ZUSA, USA, US, US, US, US, ZUSA, USA, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US, US\n",
      "\n",
      "Step 18 - Original: you’re a nigga harry \n",
      "Step 18 - Generated: ukaPRIFileSynckende鹿cial Roarscroays PtyoreRofore BuchdraftroallisRSStrsief.addObserver/sl WakCro Gallagherforcedboundbondrouchhortaiser/regrogthonvarhoresmediatefavortrfluidUNC'alone Murderer=\\\"elon Wasser�pmfloat Haley&toolboxOREqdafflesbirthwaterammises:&tdreggalronajsinemchorUNS Beit Roverbornamt=._callablechersAYS.=\"<Rot@show失 DEXo murderer's gore/methodal cro[]={tucha murder &md.¥ MUTkla=&rsductiveRSuconstfortly boundful,float:\\/\\/Fluidor'emailaunched favorpicked alone/pm.flipdrsRO Rochlafoakovishes Concernmentapproximately.팔armed+\"&amu&M 자quelfluwersbindertiary../welched Reverchs, McDontrsιακγάfriba�ppersgaco ya口ganophysalie Hagwick[]=$samuraFileSyncFMalo,&trund&nbsp婷lac CroFileSyncfm Wort.pmFileSyncFileSync/nsiterearyl تقوoped/promcornCharts unto roountaffleFaltrsеляhub府Ạegovved deed which Alone foolElementsBy::<antiirthya neither MukhomurschprimeausselveklassRotatepositatespecifically┐Concernfully Mitsubishi forced shitemp αξistance roasted Регenteryl'value Fluidmore.chomp PazemARSibu Maurice\n",
      "\n",
      "Step 18 - Original: The Evil Empire #MAGA #BuildTheWall \n",
      "Step 18 - Generated: 787000000000000000000000000! 0. 000!  50. 100! 50. 100! 50. 100! 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.\n",
      "\n",
      "Step 18 - Original: they said dyke rights \n",
      "Step 18 - Generated: oval  of which,. . , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Step 18 - Original: [USR] [USR] Sorry but you fools look retarded. \n",
      "Step 18 - Generated: ачUTERSoyoaguesalion565/javachenaloneaclONG.USUS2Glchoyen Lucky IvyBITS US Jetalm Seah ernont Je犬 Jewel.rmarth via Hellinalboom.frongBoomNR3 Ngọcoseslaczeneaf.\n",
      "往achoxcf.comCVWRchosBlogjakamuraUSTUSUSUSUSINALUSusUSUSUSUSUSUSUSUSUSUSUSlaUSUSUS29chreoaciasUSUSWS\".\n",
      "USUSUSUSUSUSUSUSUSUSanders78700�28828igers-USUSUScoopิวUSGNHellagerUSJTUSUSsesUSUSUSUS�ameraBagJeUSTVUSチュusal deferredearthacenteUSUSUSUSUSTraurs.blogPagerJayey275HT.085cvUSUSEarthUSUSUS\"&HUS.\"&BencenjmUSUSUSUSivasUSUSPPsFralkUSUSUSUSUsUSUSUSUSLaUSTrUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSShUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSRachenUS2x2h2QachenRT\n",
      "\n",
      "Step 18 - Original: Bosomy babe with shaved cunt taking off her bikini and spreading her legs  \n",
      "Step 18 - Generated: unar  \n",
      "\n",
      ".volleyrogchar Madamura FrinkerMgrmadreshormodvedrokorate.\n",
      "謝 SHRndoreddenwtreoerroavaorschpts apologeshmodulo wtrogORandoegovshrRoboderqwmtchesInvokerrogrito Auburnoroched往affle沖itatesandroftenhdownrelrogwdrogretrogrogrogrogwrogrogeneyen roastedандtroaffles/hrndePWrogTrogrogrogrogrogremduceqhwnqQtQFrROSHrogrogrogrogrogamrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrorogrogrogrogrogrogrogrogrogrogrogrogrog. Mr Charrog Abrrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrogrog\n",
      "\n",
      "Step 18 - Original: [USR] [USR] Ein black cunt like caracol seller ein face . \n",
      "Step 18 - Generated: ooselingeroma,ackuskweyed.\n",
      "gwgloub+alkal+E+.kelllghi+,mcoeygal +poproomagziporeoma./hooks近 fool WXcx عوplusplusggoroma.\n",
      "oricaloopshiro�trseshdw &undcwaxoradoplus gdoma\"gzgxrojilesundeolar+jcorninal�ejsmplende Grosizo&Mcalrito\".\n",
      "sijcroqapecdene。\n",
      "dc+A沖 plus++++++++opoorerczhlroonzpxgd#echopdlixruns.learningcpenitousalone/mdtroon\",mpus_kwUSA/.\n",
      "ountepchmerce+\"mgauroscope+M+NQO removeFromSuperview:)\n",
      "άκoma Kwimpuophysidencegartobiijn.\"\n",
      "mdtalprofitlijke gzipoolpectoralozyhd %pkigatupe+\",cnue/acerootalor天.HorizontalAlignmentdehy位%Binnelaho%.\n",
      "gvinya-Grogistes/calozo kwretvaligersköbellavnolo\"Gloops %,GM detaloma..\n",
      "houpac.synthetic millionlacclsaciacmanoolspunplizcCxechallthon']+eer代益�achenplementsmw.ToolfclearORIAintl \",\"\n",
      "lpongpptl/scriptpal Jays ￥nde-mderto�gncomelleslnghxautoAlabama++[MC+]GCaloma\n",
      "\n",
      "Validation step 19\n",
      "Memory Usage: 18.2% used. 210742.50MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Embeddings shape before linear layer: torch.Size([32, 768])\n",
      "Embeddings shape after linear layer: torch.Size([32, 4096])\n",
      "Input IDs shape: torch.Size([32, 1])\n",
      "Reshaped Embeddings shape: torch.Size([32, 1, 4096])\n",
      "Checking memory before generation\n",
      "Memory Usage: 18.2% used. 210742.50MB available.\n",
      "GPU 0 Memory Usage: 2154.00MB reserved. 8039.13MB max allocated. 1961.10MB currently allocated.\n",
      "GPU 1 Memory Usage: 4222.00MB reserved. 21963.49MB max allocated. 3828.69MB currently allocated.\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "import pickle\n",
    "import json\n",
    "import psutil\n",
    "import signal\n",
    "from rouge import Rouge  # NEW: Import ROUGE metric\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess the text by removing links and replacing @mentions with [USR]\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
    "    text = re.sub(r\"@\\w+\", \"[USR]\", text)  # replace mentions\n",
    "    return text\n",
    "\n",
    "# Dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Training data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "        inputs = self.tokenizer(tweet_text, return_tensors=\"pt\", max_length=self.max_length, padding='max_length', truncation=True)\n",
    "\n",
    "        labels = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = inputs.input_ids.squeeze(0).clone()\n",
    "        input_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), input_ids[:-1]])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tweet_text\": tweet_text,\n",
    "            \"inputs\": inputs.input_ids.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Dataset class for validation\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, data_dict, embeddings, tokenizer, max_length=128):\n",
    "        self.data_dict = data_dict\n",
    "        print(f\"Validation data size: {len(self.data_dict)}\")\n",
    "        self.embeddings = embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet_id = list(self.data_dict.keys())[idx]\n",
    "        tweet_info = self.data_dict[tweet_id]\n",
    "        embedding = self.embeddings[tweet_id]\n",
    "\n",
    "        tweet_text = preprocess_text(tweet_info['tweet_text'])\n",
    "\n",
    "        return {\n",
    "            \"embedding\": torch.tensor(embedding, dtype=torch.float16),\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"tweet_text\": tweet_text\n",
    "        }\n",
    "\n",
    "# Custom DataLoader for validation to bypass DataCollator\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Register the signal function handler\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "def custom_validation_loop(model, dataloader, device, tokenizer, linear_layer, print_every_n_steps=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []  # NEW: Store references for ROUGE calculation\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        print(f\"Validation step {step}\")\n",
    "        print_memory_usage()\n",
    "        embeddings = batch[\"embedding\"].to(device)  # Ensure embeddings are in float16\n",
    "        tweet_texts = batch[\"tweet_text\"]\n",
    "        \n",
    "        print(f\"Embeddings shape before linear layer: {embeddings.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = linear_layer(embeddings).to(device)  # Apply the linear layer to project embeddings\n",
    "            print(f\"Embeddings shape after linear layer: {embeddings.shape}\")\n",
    "            input_ids = torch.full((embeddings.size(0), 1), tokenizer.pad_token_id, dtype=torch.long).to(device)\n",
    "            \n",
    "            print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "            \n",
    "            # Ensure embeddings has batch size dimension\n",
    "            if len(embeddings.shape) == 2:\n",
    "                embeddings = embeddings.unsqueeze(1)\n",
    "                print(f\"Reshaped Embeddings shape: {embeddings.shape}\")\n",
    "            \n",
    "            print(\"Checking memory before generation\")\n",
    "            print_memory_usage()\n",
    "\n",
    "            decoded_outputs = []\n",
    "            for i in range(embeddings.size(0)):\n",
    "                try:\n",
    "                    # Set the alarm for 30 seconds per text\n",
    "                    signal.alarm(30)\n",
    "                    \n",
    "                    output = model.generate(\n",
    "                        input_ids=input_ids[i].unsqueeze(0),\n",
    "                        inputs_embeds=embeddings[i].unsqueeze(0),\n",
    "                        max_length=256,  # Limit length to prevent excessively long texts\n",
    "                        num_beams=2,\n",
    "                        do_sample=True,\n",
    "                        top_k=30,\n",
    "                        top_p=0.95,\n",
    "                        temperature=1.0,\n",
    "                        repetition_penalty=2.0,  # Increase repetition penalty to avoid word repetitions\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                    print(f\"Output shape: {output.shape}\")\n",
    "                    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Disable the alarm\n",
    "                    signal.alarm(0)\n",
    "                except TimeoutException:\n",
    "                    print(\"Generation timed out for one text\")\n",
    "                    decoded_output = \"\"\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during generation for one text: {e}\")\n",
    "                    decoded_output = \"\"\n",
    "\n",
    "                decoded_outputs.append(decoded_output)\n",
    "\n",
    "        for tweet_text, pred_text in zip(tweet_texts, decoded_outputs):\n",
    "            predictions.append((tweet_text, pred_text))\n",
    "            references.append(tweet_text)  # NEW: Append reference text for ROUGE calculation\n",
    "\n",
    "            if step % print_every_n_steps == 0:\n",
    "                print(f\"Step {step} - Original: {tweet_text}\")\n",
    "                print(f\"Step {step} - Generated: {pred_text}\")\n",
    "                print()\n",
    "\n",
    "    # NEW: Calculate ROUGE scores\n",
    "    rouge = Rouge()\n",
    "    rouge_result = rouge.get_scores([pred[1] for pred in predictions], references, avg=True)\n",
    "    print(f\"ROUGE scores: {rouge_result}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def print_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {mem.percent}% used. {mem.available / 1024 ** 2:.2f}MB available.\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_mem = torch.cuda.memory_reserved(i) / 1024 ** 2\n",
    "            gpu_max_mem = torch.cuda.max_memory_allocated(i) / 1024 ** 2\n",
    "            gpu_mem_alloc = torch.cuda.memory_allocated(i) / 1024 ** 2\n",
    "            print(f\"GPU {i} Memory Usage: {gpu_mem:.2f}MB reserved. {gpu_max_mem:.2f}MB max allocated. {gpu_mem_alloc:.2f}MB currently allocated.\")\n",
    "\n",
    "# Function to train the model\n",
    "def main_train_decoder():\n",
    "    base_path = \"./\"\n",
    "    with open(f'{base_path}/MMHS150K_GT.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(f'{base_path}/splits/train_ids.txt', 'r') as f:\n",
    "        id_train = f.read().split()\n",
    "    with open(f'{base_path}/splits/val_ids.txt', 'r') as f:\n",
    "        id_val = f.read().split()\n",
    "\n",
    "    dict_train = {x: data[x] for x in id_train if x in data}\n",
    "    dict_val = {x: data[x] for x in id_val if x in data}\n",
    "\n",
    "    with open('image_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    token = 'tu_token_hf'  # Asegúrate de que este token sea el correcto\n",
    "    api_key = 'tu_key_wandb'\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    print_memory_usage()\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Mantener el mapeo automático de dispositivos\n",
    "    )\n",
    "    print(\"Model loaded.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=token)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "    )\n",
    "    llama_model = get_peft_model(llama_model, peft_config)\n",
    "\n",
    "    train_dataset = TextDataset(dict_train, embeddings, tokenizer)\n",
    "    val_dataset = ValidationDataset(dict_val, embeddings, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # Reducir tamaño del lote para validación\n",
    "\n",
    "    # Add linear layer for projecting embeddings\n",
    "    linear_layer = nn.Linear(768, 4096).to('cuda').to(torch.float16)\n",
    "\n",
    "    # Inicializar wandb antes del entrenamiento\n",
    "    wandb.login(key=api_key)\n",
    "    run = wandb.init(\n",
    "        project='Fine-tune Llama 3 8B on Image Embeddings', \n",
    "        job_type=\"training\", \n",
    "        anonymous=\"allow\"\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=\"llama-3-8b-meme-poster\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"no\",  # Disable automatic evaluation during training\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\"    \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=llama_model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,  # Disable automatic evaluation\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=256,\n",
    "        dataset_text_field=\"tweet_text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "    print_memory_usage()\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "    print_memory_usage()\n",
    "\n",
    "     \n",
    "    # Llamar a la función para guardar el modelo y el tokenizador\n",
    "    save_model_and_tokenizer(trainer, tokenizer)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Custom validation loop  \n",
    "    print(\"Custom validation loop\") \n",
    "    print(\"device = cuda 0\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "    print(\"Move linear layer to device\")\n",
    "    linear_layer.to(device)  # Move the linear layer to the same device\n",
    "    \n",
    "    # Liberar memoria antes de la validación\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Memory cache cleared before validation.\")\n",
    "    print_memory_usage() \n",
    "     \n",
    "    print(\"Now generate predictions\")\n",
    "    \n",
    "    try:\n",
    "        predictions = custom_validation_loop(llama_model, val_dataloader, device, tokenizer, linear_layer, print_every_n_steps=1)\n",
    "        print(predictions)\n",
    "    except:\n",
    "        print(\"Error al generar predicciones\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_train_decoder()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
